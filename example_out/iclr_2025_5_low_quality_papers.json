{
    "zuuhtmK1Ub": {
        "venue": "ICLR 2025",
        "title": "Differentiable Implicit Solver on Graph Neural Networks for Forward and Inverse Problems",
        "link": "https://openreview.net/forum?id=zuuhtmK1Ub",
        "abstract": "Partial differential equations (PDEs) on unstructured grids can be solved using message passing on a graph neural network (GNN). Implicit time-stepping schemes are often favored, especially for parabolic PDEs, due to their stability properties. In this work, we develop a fully differentiable implicit solver for unstructured grids. We evaluate its performance across four key tasks: a) forward modeling of stiff evolutionary and static problems; b) the inverse problem of estimating equation coefficients; c) the inverse problem of estimating the right-hand side; and d) graph coarsening to accelerate forward modeling. The increased stability and differentiability of our solver enable excellent results in reducing the complexity of forward modeling and efficiently solving related inverse problems. This makes it a promising tool for geoscience and other physics-based applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": "- By employing an implicit scheme with optimized gradient computation, the proposed method reduces the required number of time steps. \nThey present a differentiable framework for both forward and inverse methods, enabling a learnable numerical approach based on discrete time steps. \nAdditionally, the paper explores applications in inverse problems, often employing irregular unstructured grids as used in practical scenarios.\n\n- * The work tries to build a framework that works with mesh coarsening, forward, and inverse problems.\n\n- Figure 1 effectively illustrates the overall pipeline, demonstrating experimental results that apply the combination of GNN and FVM to both forward and inverse problems.\n\n- The question is interesting and combining GNN with finite element method seems natural.",
        "weaknesses": "- While the underlying idea is promising, the paper would benefit from stronger experimental or theoretical justification for the proposed methodology. Additional clarity and motivation for the approach would enhance the paper\u2019s impact.\n\n- * The novelty of the work is limited. Incorporating FVM into GNN is not new and considered in, e.g., [Jessica et al. ICML 2024 https://arxiv.org/abs/2311.14464 ] and [Horie et al. ICML 2024 https://arxiv.org/abs/2405.16183v1 ]. The construction of gradients presented in Section 2.3 seems strongly related to the adjoint method, which is a standard way to deal with inverse problems. The implicit method for GNN is considered in the area of implicit GNNs, e.g., [Gu et al. NeurIPS 2020 https://arxiv.org/abs/2009.06211 ]. The authors state that these are their novelty, but there is existing work for each. The authors should cite these works and clarify the added novelty from the authors.\n* The evaluation is weak. There is only one baseline for the experiment in Section 3.2 and nothing for the ones in Section 3.3 and 3.4. With the current form, the reviewer cannot asses the effectiveness and superiority of the model.\n* The presentation is not clear. The figure may miss the labels (a), (b), and so on for Figures 2, 3, and 4. It is not clear what is \"data 1\", \"fitting 1\", \"data 2\", and \"fitting 2\" in Figures 2 and 3.\n\n- First and foremost, the paper feels incomplete. The biggest concern is the lack of discussion about other approaches that use GNNs or integrate FVM with deep learning to solve PDEs. A \u201cRelated Work\u201d section should be added to explain how the proposed model differs from recent studies and highlight its novelty. Although Section 2 on theory explains the problem setup to some extent, more detailed steps and methods for training the proposed approach should be included. Section 3, the experimental part, merely lists the results for forward and inverse problems without discussing how this method compares to existing GNN- and FVM-based approaches. For instance, the study \"Learning to Solve PDE-constrained Inverse Problems with Graph Networks\" solves inverse problems using GNNs\u2014how does the proposed method differ from this approach, and what advantages does it offer? Experimentally, does it outperform in solving inverse problems?\n\n- 1. The writing is subpar. There are many typos and grammatical errors. For example, \"Compute $\\nabla_bL$  with (12), whats is equivalent so the solution of a single linear system.\" should be \"Compute $\\nabla_bL$ with (12), which is equivalent to solving a single linear system.\"\n2. One main focus of this paper is the incorporation of implicit solver. However, using an iterative solver and in a deep learning setting is well-studied in the Deep Equilibrium Models (DEQ) literature. The authors should compare their method with DEQ.\n3. The experiments are not very convincing. The results in Section 3.4 is very poor and in no experiments the authors compare their method with other methods."
    },
    "zuKrRYM3Tg": {
        "venue": "ICLR 2025",
        "title": "Quantized Approximately Orthogonal Recurrent Neural Networks",
        "link": "https://openreview.net/forum?id=zuKrRYM3Tg",
        "abstract": "In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained popularity due to their ability to manage tasks involving long-term dependencies, such as the copy task, and their linear complexity. However, existing ORNNs utilize full precision weights and activations, which prevents their deployment on compact devices.\n\nIn this paper, we explore the quantization of the weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). The construction of such networks remained an open problem, acknowledged for its inherent instability. We propose and investigate two strategies to learn QORNN by combining quantization-aware training (QAT) and orthogonal projections. We also study post-training quantization of the activations for pure integer computation of the recurrent loop. The most efficient models achieve results similar to state-of-the-art full-precision ORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 3-bits quantization.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            5,
            1
        ],
        "strengths": "- * The investigation on weight quantization effects on ORNNs is interesting and insightful (sec 3.4).\n* Based on the experiments the proposed QRNNs (especially with STE-Bjorck) seem to perform well, even till 5-6 bits.\n* The paper is clearly written and easy to follow (except a few minor points).\n\n- The paper is easy to read, and the proposed QORNN is effective even at long-term dependency tasks.\n\n- * Seems fairly easy to implement.\n* Authors thoroughly motivate and explain challenges of constructing and training ORNNs & instabilities caused by quantization. \n* A comparison of model sizes in resulting QORNN models vs. other methods.\n\n- This a very well-written paper with an extensive literature review, a great introduction to ORRNs and their challenges, many experiments and a thorough appendix with many implementation details for reproducibility",
        "weaknesses": "- * The biggest shortcoming of this paper is the limited novelty. Several points regarding this:\n    * The authors combine two off the shelf ORNN training algorithms with the most simple (and arguably outdates, see later) flavor of QAT. In other words, they add $q_k(W_i)$ to these algorithms (cf line 3, 4 in algorithms 1, 2, respectively) and assume STE.\n    * While I found the investigation in sec 3.4 interesting (see above), I would have expected that these insights come back in their algorithm design (or at least that the authors evaluate such metrics for their proposed approach, look at question whether these metrics in practice correlate with QORNN performance etc).\n* On the quantization side, they seem to miss/ignore a lot of innovation that happened in the last 5ish years which could help to potentially have much better performance at low bit-widths. Most importantly:\n    * They argue keeping the scale fixed is common practice (line 241/242). Since the introduction of LSQ [1] this is long not common practice anymore.\n    * It is unclear to me why the authors not consider per-channel weights. As per-channel weights still work with full fixed-point/integer arithmetic [2], this is supported by almost any HW and commonly used by most quantization literature in the past years. This adds an additional degree of freedom that might be very helpful for ORNNs as by changing the scale ($\\alpha$), as one could ensure that rows (or columns, depending on notation) still have a norm of 1 which seems important (cf sec 3.2).\n* The proposed QORNNs are actually not orthogonal as the name or text suggest. Only the latent weights ($W_i$) are orthogonal, but the actual quantized weights used for inference ($q_k(W_i)$) are only approximately orthogonal. As the authors themselves show (cf figure 1, sec 3.4), this doesn\u2019t give any guarantees and could be detrimental.\n* As the paper positions itself more as \u2018explorative\u2019 (and QAT contribution is very limited), I would expect that they also more closely explore PTQ approaches. There are several degrees of freedom that are unexplored, e.g. setting the correct scale (alpha) or applying/adapting common PTQ algorithms such as AdaRound [3] or GPTQ [4].\n* Minor:\n    * The experimental evaluation is limited to only \u2018toy-sized\u2019 datasets/tasks. \n    * While it is nice they obtain bounds for q_min/max, the established bounds are so loose that from a practical perspective such bounds are not useful (nor similar to the earlier study they are used to design or evaluate the algorithm).\n    * I do miss a comparison to challenges in quantizing transformers or SSMs. While it is arguable whether comparing to transformers it out of the scope of such a paper (as the authors claim), at least discussing/comparing whether the challenges in ORNNs are similar or different to transformers/SSMs would be helpful (e.g. do they suffer from similar outliers as observed in transformers, cf. [5,6]). \n    * Regarding SSMs, there is some recent work that would be interesting to compare to [7, 8]. \n\n**References:**\n* [1] Esser et al., ICLR 2020, Learned Step Size Quantization.\n* [2] Nagel et al., 2021, A White Paper on Neural Network Quantization.\n* [3] Nagel et al. ICML 2020, Up or Down? Adaptive Rounding for Post-Training Quantization.\n* [4] Frantar et al., ICLR 2023, Gptq: Accurate post-training quantization for generative pre-trained transformers.\n* [5] Bondarenko et al., EMNLP 2021, Understanding and Overcoming the Challenges of Efficient Transformer Quantization.\n* [6] Dettmers et al., NeurIPS 2022, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\n* [7] Pierre et al., ES-FoMo 2024, Mamba-PTQ: Outlier Channels in Recurrent Large Language Models\n* [8] Abreu et al, NGSM 2024, Q-S5: Towards Quantized State Space Models\n\n- - There are awkward and missing citations throughout the paper. For example, the citation for fastGRNN should have been first on line 114 instead of line 135. Besides, I think the representation, such as \"activations for LSTM Hou et al. (2017)\" in line 105, \"Penn TreeBank dataset Marcus et al. (1993) (PTB)\" is awkward.\n- There are some issues on writing (see minor issues and questions below)\n\nMinor Issues\n1) There are some minor points on writing:\n- Line 90: \u201cThe reasons .. is\u201d -> \u201cThe reasons .. are\u201d\n- Lines 122 and 321: footnotes are written incorrectly\n- Footnote 4: \u201cSections 3.4\u201d should be revised correctly\n- Equation in line 203, footnote 5, caption of Table 3, and line 466: please add a comma to the end of the sentence\n- Line 286: add a colon to the next of the bolded sentence\n- Subtitle 3.4: \u201cQORNN are\u201d -> \u201cQORNN is\u201d or \u201cQORNNs are\u201d\n- Tabels 1 and 2, \u201cfromKiani et al. (2022)\u201d -> \u201cfrom Kiani et al. (2022)\u201d\n- Table 2, \u201csizes for Copy, MNIST\u201d -> \u201csizes for Copy-task, sMNIST\u201d\n2) Are the words \"steps\", \"power\", \"timesteps\" and \"length\" the same meaning? Mixed terms can confuse the reader. I recommend revising them for clarity. Other examples include copy/copy-task, SSMs/SSM/SSSM, etc.\n\n- * The proposed methods seem very straightforward: combine the strategy of constructing ORNNs with STE, which is a standard and well-known technique in the quantization literature.\n* L286-L300: authors derive bounds for approximate orthogonality of q(W) which they themselves state are too loose to be useful in practice. It would be quite insightful to track and report $||W_q W_q^T \u2013 I ||$ during training, to see if the reason of proposed method working well in practice is due to $q(W)$ being fairly close to being orthogonal or not.\n* L079: authors claimed SotA results on pMNIST, however they did not compare against other 4-bit sequence-to-sequence models, for instance SSM models such as Mamba [1], transformer models such as LLaMA-3 [2] etc. \n* Considered benchmarks are very small by today standards. While most of them seem standard in ORNN literature, it would make a story more convincing if authors included some of the more recent real-world datasets and benchmarks. For instance, it would be insightful to evaluate the proposed methods on some of common reasoning language tasks (MMLU, HellaSwag, Winogrande).\n\n[1] Gu et al., \u201cMamba: Linear-Time Sequence Modeling with Selective State Spaces\u201d. ArXiV: 2312.00752\n\n[2] Dubey et al., \u201cThe Llama 3 Herd of Models\u201d. ArXiV: 2407.21783\n\n- There is unfortunately not any real innovation in the paper. The only contribution of the paper is applying QAT with a straight-through estimator (a very old idea in quantization literature) to existing optimization methods for learning ORNNs. In fact, the QAT technique is not even state-of-the-art as they could learn the quantization ranges $\\alpha$ through gradient by adopting LSQ$^{[1]}$ or PACT^{[2]}. \n\nThe theoretical analysis of the impact of quantization in orthogonal matrices is not optimal. It is well known that using MinMax quantization for low-bit quantization (< 6 bits) leads to significant degradation. This is why search-based methods are normally adopted that find the scale or range that minimizes the Forbenious ( or other norms) between quantized and unquantized vectors (commonly known as MSE-based ranges). It would be more interesting to see the plots of Figures 1 & 2 using optimal $\\alpha$ values rather than ones based on the maximum range. \n\n[1] LSQ: Learned Step Size Quantization, Esser et al. \n[2] PACT: Parameterized Clipping Activation for Quantized Neural Networks"
    },
    "zkNCWtw2fd": {
        "venue": "ICLR 2025",
        "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
        "link": "https://openreview.net/forum?id=zkNCWtw2fd",
        "abstract": "Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingual pre-trained language models have achieved remarkable success, yet they often optimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias. The approach fine-tunes multilingual language models using a mix of monolingual and cross-lingual question-answer pair batches sampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets show that the proposed method consistently achieves comparable or superior results in zero-shot retrieval across various languages and retrieval tasks compared to monolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingual training. These results demonstrate the effectiveness of the proposed approach for learning language-agnostic representations that enable strong zero-shot retrieval performance across diverse languages.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": "- 1. Addresses a relevant challenge in multilingual information retrieval.\n2. Provides comprehensive experimental validation across multiple benchmark datasets (XQuAD-R, MLQA-R, MIRACL).\n\n- The paper shows that two standard batching strategies are complementary for information retrieval tasks, as the combination of them shows improvements.\n\n- 1.  This paper proposes a hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias.\n2.  The hybrid batch training strategy simply modifies the training data batches without necessitating the introduction of loss functions or new architectural components.",
        "weaknesses": "- 1. The primary contribution merely combines two existing training approaches with probability weights, presenting a straightforward and obvious solution.\n2. The paper employs translated QA pairs as data augmentation, creating an unfair comparison with baseline methods that do not utilize this advantage.\n\n- 1. Limited evaluations are only QA datasets (e.g., the main text only shows XLM-R and LaBSE). Also, the main text consists of many large tables where each does not present as much information as the space it takes, e.g., the authors could summarize how many languages/scenarios the proposed method shows improvements instead of providing large tables like Table 3, Table 4, Table 5, etc.\n\n2. It is not clear if the proposed method is actually effective. In many cases, the improvements appear rather small. For example, in Table 1, on XQuAD-R for XLM-R (0.792 vs 0.798; 0.705 vs 0.700; 0.593 vs 0.593). Are they even statistically significant?\n\n3. As this paper mainly provides empirical observations, it would be stronger if the paper provides insights on which scenario (e.g., what kind of base model or dataset) where hybrid batching is expected to show significant improvements and when it does not. The current paper pretty much reports experimental findings which could limit its usefulness. Several questions remain, for example, what is the size and mixed of training data does one need to see the impact of this hybrid batching? I expect that if there is limited training data, the impact would be marginal.\n\n- 1. The proposed hybrid batch training strategy only modifies the input training data, which lacks novelty.\n2. This paper lacks sufficient analysis to the field of multilingual information retrieval. It does not adequately demonstrate the shortcomings of existing work nor the importance and necessity of this study.\n3. The experiments only compare the performance of different input strategies but not various multilingual information retrieval methods."
    },
    "zbIS2r0t0F": {
        "venue": "ICLR 2025",
        "title": "Allostatic Control of Persistent States in Spiking Neural Networks for Perception and Computation",
        "link": "https://openreview.net/forum?id=zbIS2r0t0F",
        "abstract": "We introduce a novel model for updating perceptual beliefs about the environment\nby extending the concept of Allostasis to the control of internal representations.\nAllostasis is a fundamental regulatory mechanism observed in animal physiology\nthat orchestrates responses to maintain a dynamic equilibrium in bodily needs and\ninternal states. In this paper, we focus on an application in numerical cognition,\nwhere a bump of activity in an attractor network is used as a spatial-numerical\nrepresentation. While existing neural networks can maintain persistent states, to\ndate, there is no unified framework for dynamically controlling spatial changes in\nneuronal activity in response to enviromental changes. To address this, we couple\na well-known allostatic microcircuit, the Hammel model, with a ring attractor, re-\nsulting in a Spiking Neural Network architecture that can modulate the location of\nthe bump as a function of some reference input. This localised activity in turn is\nused as a perceptual belief in a simulated subitization task \u2013 a quick enumeration\nprocess without counting. We provide a general procedure to fine-tune the model\nand demonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare\nit with biological data. Finally, we analyze the dynamics of the network to un-\nderstand the selectivity and specificity of different neurons to different categories\npresent in the input. The results of this paper, particularly the mechanism for mov-\ning persistent states, are not limited to numerical cognition but can be applied to a\nwide range of tasks involving similar representations.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3,
            3
        ],
        "strengths": "- 1. Novelty of the Approach\nIntegrating an allostatic control mechanism into a spiking neural network architecture is a novel approach with potential implications for understanding self-regulation in neural systems.  \u00a0 \n\n2. Dynamic Control of Persistent States\nThe model successfully demonstrates the dynamic control of persistent states in response to environmental changes, a crucial aspect of cognitive processing.  \u00a0 \n\n3. Qualitative Reproduction of Behavioral Aspects\nAlloNet qualitatively reproduces certain behavioral aspects of subitization, such as the relationship between reaction time and numerosity.\n\n- The authors provide a novel approach to applying allostatic principles to control internal representations within spiking neural networks. This approach may inspire further cross-disciplinary exploration of allostasis and neural network control. In particular, the use of allostasis as a tool of synchronizing the alignment between internal representations and external stimuli could make the model useful in applications requiring real-time adaptability, such as robotics or artificial agents interacting with unpredictable environments.\n\n- * The proposed model is original to my knowledge, and is fairly simple to understand.\n* The authors have compared observations from their models such as reaction times to those of humans performing the same task.\n* A testable prediction, i.e., that small numerosities are encoded using a magnitude-based system (ring attractor in this case, which is central to the model), could potentially be validated using experiments with animals/humans.\n\n- 1. The authors propose a novel network model based on the ring attractor, enhanced with high/low gain modulation neurons to manage bump shifts, with the Hammel model controlling these shifts. This enables flexible positioning of the bump to match external signals.\n2. Some aspects of the model\u2019s performance on counting tasks align with human data, particularly when varying the synaptic time constant.\n\n- 1.\tThe introduction section provides a comprehensive overview of both numerosity and attractor networks.\n2.\tThe integration of physiological concepts into neural circuit modeling is innovative and inspiring.",
        "weaknesses": "- Limited Biological Plausibility: While the model draws inspiration from biological systems like the Hammel model for temperature regulation, the direct application of such a model to numerical cognition might oversimplify the underlying biological mechanisms.\n\nSpecificity of the Model: The paper focuses heavily on subitization as an application. It would be beneficial to explore additional cognitive tasks to demonstrate the generalizability of AlloNet.\n\n- * The paper presents allostasis as a beneficial mechanism but does not sufficiently compare it to traditional homeostatic mechanisms or other neural adaptation frameworks. This limits understanding of when allostatic control is most useful or essential, making it difficult to gauge the model's full significance\n\n* The experimental setup is limited to idealized, controlled tasks. Real-world applications, however, typically involve noisy, unpredictable inputs that can disrupt internal representations, which this model might struggle with. Current experiments do not address such robustness.\n\n* The motivation for using a ring attractor with a \"bump of activity\" as the representation for the task of numerical cognition (e.g., subitizing) is not clear. What is the (systems) neuroscience evidence for this? \n\n* The authors note that error rates increase with numerosity and time, but more in-depth insights into the reasons for these errors, such as bump instability, could clarify model limitations\n\n- * While the authors are able to recapitulate some observations in humans using their model, i.e., that reaction times increase with numerosity (for a specific value of the time constant), the actual reactions times from the model are far greater than human reaction times (almost 10000 ms vs less than 500 ms) ([Dehaene, 2011](https://psycnet.apa.org/record/2011-10610-000); [Kutter et al., 2023](https://www.nature.com/articles/s41562-023-01709-3)). Do the authors have an explanation for why this is the case, and could it be resolved with better choices for hyperparameters?\n* I'm not entirely convinced with the authors' argument that they are able to properly reproduce reaction time patterns. The sharp jump in reaction time for a numerosity of 4 is only observed with a different, faster time constant value of 900ms, while the gradual increase in reaction times with numerosity is only observed with a slower time constant of 1000ms. Furthermore, in the 900ms case, the reaction times for lower numerosities do not match the human data at all, as the authors admit. Could the authors clarify this? I would expect the model to match human observations for a single value of the time constant in order to claim that it reproduces experimental results.\n* The model is evaluated only on a single task, i.e., subitising. Especially given the use of a ring attractor and the authors' claims of the model's generalisability, it would be important to evaluate the model on other tasks involving numerosity estimation or tracking a magnitude, such as head direction integration ([Valerio & Taube, 2012](https://www.nature.com/articles/nn.3215)) or average estimation ([Lee & Ma, 2020](https://cognitivesciencesociety.org/cogsci20/papers/0304/0304.pdf)).\n* The task is also limited in that it restricts numerosity from 1 to 4. To better compare the observations from the model to previous experiments ([Kutter et al., 2023](https://www.nature.com/articles/s41562-023-01709-3)), the authors should incorporate both small and large numbers (from 0 to 9, for example). If the authors want to align their work better with [Kutter et al. (2023)](https://www.nature.com/articles/s41562-023-01709-3), two different representations could be used for 0-4 vs 5-9 \u2013 and in this case it would be important to test whether discrete attractors (fixed points for each of 0-4) match the data better than when using a ring attractor.\n* There are other issues with the results and their interpretation. For example, in Fig. 3B, it is not clear how much more variable the reaction times are for higher numerosities with longer time constants (as claimed on line 312). Furthermore, from Fig. 3D, it seems that the \"quality score\" is very low for numerosity 3 and longer time constants (while for shorter time constants, quality is 0 for numerosity 2 but quality is higher for 3), why is this the case, and is there any experimental evidence of this in animals/humans?\n* The authors claim that the neural dynamics of their model are similar to those of several brain regions in human recordings, but this is not substantiated with a metric or even a plot showing qualitative similarity. Furthermore, some of the analysis of neuronal responses has only been done for a single, arbitrary neuron, but it would be important to look at population responses when making comparisons to human data.\n* Another weakness is the specificity of these results to hyperparameter and architectural choices. The lack of learning also makes it harder to adapt this model to more complex tasks.\n* Finally, the writing lacks clarity and contains grammatical, formatting and some typographical issues. Examples:\n\n  * Line 50 (\"can represent to track changes...\", unclear what this means), Line 53 (\"resemble to the\" -> \"resemble the\"), Line 158-159 (\"functions to take input for..\"), Line 249 (\"in an standard...\"), Line 463-465 (sentences are vague, \"we allowed to work on\"), etc.\n  * Several in-text citations are not properly enclosed within brackets.\n  * \"Excitatiory\" -> \"Excitatory\" (Table 1), inconsistent use of \"ise\" vs \"ize\" (British vs American English), \"... as a computation model ...\" -> \"computational\", lack of proper spacing before and after parentheses, etc.\n\n  I would encourage the authors to carefully revise the paper to improve its clarity and fix any other grammatical/typographical issues.\n\n- 1. The counting task demonstration is very simple; the numerical information is directly encoded in the external signal\u2019s firing rate.\n2. Although certain model properties align with human data, they do so under varying synaptic time constants, which are not well explained. Additionally, the model\u2019s reaction times are significantly slower than human responses, by about an order of magnitude.\n3. The necessity of using a spiking neural network is not adequately justified.\n4. The model\u2019s structure, specifically the gain modulation neurons and the Hammel model component, lacks biological explanation.\n5. The network dynamics and mechanisms are underexplained. For instance, in the connection weight formula $w_{ij}$, the term $d_{ij}$ is undefined, though it appears to represent the distance between neurons in the ring.\n\n\n### Mino \n1. In Equation 1, $d_{ij}$ is undefined.\n2. Figure 1, with its four subplots, could be clearer if organized differently.\n3. In Figure 2, specify units for the x-axis (probably ms).\n4. The synaptic decay constants \\tau used are unusually long compared to standard neuron models. More explanation of these values and their role would improve readability.\n\n- 1.\tImproper Citations:  For instance, in lines 173-174, citing Zhang, K. (1996) is essential for discussing the asymmetry in continuous attractor connections. Additionally, it appears that Fig. 1c is likely influenced by Fig. 1d in Wilson, R.I. (2023), but lacks appropriate citation.\n2.\tLack of Clarity on Network Dynamics and Analysis: The model does not provide a clear definition of network dynamics or a detailed theoretical analysis of the results.\n3.\tBiologically Implausible Model Setup: The model\u2019s structure and parameters are set in bio-unplausible ways. Specifically:\n\t- The synaptic time constants far exceed the plausible range, making the comparison across time constants in Figs. 4 and 5 less meaningful.\n\t- LGM and HGM neurons were designated as inhibitory. I agree that it is a feasible way to drive the ring attractor, but there has been both experimental and theoretical evidence showing that P-EN neurons in Drosophila brains fulfill this role as excitatory neurons (see Zhang, W., Wu, Y. N., & Wu, S., 2022; Mussells Pires, P., Zhang, L., Parache, V., Abbott, L. F., & Maimon, G., 2024).\n4.\tFailure to Model Human Behavior Accurately: Although section 3.1 attempts a loose comparison between the model and human behavior, the model does not successfully replicate human behavioral patterns.\n5.\tUnpolished text and figures: The text contains several typos, unified terminologies and missing punctuation."
    },
    "zyGrziIVdE": {
        "venue": "ICLR 2025",
        "title": "Exploration by Running Away from the Past",
        "link": "https://openreview.net/forum?id=zyGrziIVdE",
        "abstract": "The ability to explore efficiently and effectively is a central challenge of reinforcement learning.\nIn this work, we consider exploration through the lens of information theory.\nSpecifically, we cast exploration as a problem of maximizing the Shannon entropy of the state occupation measure.\nThis is done by maximizing a sequence of divergences between distributions representing an agent's past behavior and its current behavior.\nIntuitively, this encourages the agent to explore new behaviors that are distinct from past behaviors.\nHence, we call our method RAMP, for ``$\\textbf{R}$unning $\\textbf{A}$way fro$\\textbf{m}$ the $\\textbf{P}$ast.''\nA fundamental question of this method is the quantification of the distribution change over time.\nWe consider both the Kullback-Leibler divergence and the Wasserstein distance to quantify divergence between successive state occupation measures, and explain why the former might lead to undesirable exploratory behaviors in some tasks. \nWe demonstrate that by encouraging the agent to explore by actively distancing itself from past experiences, it can effectively explore mazes and a wide range of behaviors on robotic manipulation and locomotion tasks.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            5,
            3
        ],
        "strengths": "- Though the problem of state space exploration is very extensively covered in the RL literature, the proposed RAMP method provides what appears to be a novel approach to accelerating state space coverage. Due to its strategy of choosing policies maximizing divergence of state space coverage from that achieved by previous policies, it makes sense that RAMP will be more effective at rapidly exploring the state space than existing unsupervised RL methods (e.g., APT, SMM, Proto-RL) that simply maximize state occupancy measure entropy, and the experiments provide some support to this. Moreover, though the actual learning procedure used in RAMP is essentially a combination of existing techniques ([Eysenbach et al., 2020] for $r_{KL}$, [Durugkar et al., 2021] for $r_{W}$, and SAC [Haarnoja et al., 2018]), the combined approach detailed in Sec. 3.4 and Algorithm 1 appears to be novel and is interesting, and the fact that both KL-divergence and Wasserstein distance versions of RAMP are provided adds to its flexibility and significance. For these reasons, RAMP is likely of interest to the community and definitely merits further investigation.\n\n- 1. The problem addressed is important to the community.\n2. The new objective function is theoretically motivated and provides new insights to compute good exploration policies.\n\n- The strength of the paper lies in a fairly clear presentation of the motivation and methodology. The idea of \"running away from the past\" is not strictly novel but the paper proposes an algorithmically viable way to instantiate such an idea. The paper presents a fairly clear math formulation and has carried out ablations on choices of the algorithmic designs. The experimental ablation also seems fairly comprehensive.\n\n- The paper is written well, and the proposed intrinsic exploration objective is novel. The use of Wasserstein distance instead of the typical KL divergence is an interesting/novel choice.",
        "weaknesses": "- Despite the strengths discussed above, I have concerns about the experimental evaluation and theoretical results:\n1. Most importantly, the \"state coverage\" performance metric upon which the comparisons of Sections 5.2 and A.1 rely is insufficiently justified as a good proxy for measuring exploration and for making fair comparisons between the algorithms considered. As described in the third paragraph of Sec. 5.2, this metric is obtained by discretizing the space of Euclidean (x-y or x-y-z) coordinates of the agent's state space, recording whether each grid cell has been visited or not during training, then returning the percentage of the grid cells that have been visited. There are two main issues with using this notion of state coverage as a proxy for exploration. First, the state space dimensions in most of the environments are far larger than 2 or 3 (e.g., 18 for HalfCheetah, 113 for Ant), and, for many of these environments, pose information other than location in Euclidean space (e.g., joint angles, velocities) is far more important for learning to operate within the environment and for specific downstream tasks. Second, recording only whether a grid cell has been visited or not ignores more complex visitation behavior, such as the empirical state visitation frequency defined at the beginning of Sec. 2. To render the state coverage metric used more meaningful, it would be helpful to include ablations over the other dimensions of $S$ or comparison with other coverage notions, such as Shannon entropy of the empirical state visitation frequency.\n2. Implementation details for the RAMP algorithm, the algorithms compared with, the discretization used in the state coverage metric, and other aspects of the experiments are not provided. The experimental results are therefore not reproducible in their current form. In addition, across all experiments, the lack of implementation details makes it difficult to assess the fairness of comparison with existing methods and even the comparisons between $RAMP_{KL}$ and $RAMP_{W}$. This makes it difficult to evaluate the significance of the experimental results, weakening the overall contribution. To remedy these issues, a thorough description of the implementation details is needed.\n3. The qualitative results in Sec. 5.1 are difficult to understand, leaving the practical differences between $r_{KL}$ and $r_W$ unclear. See the questions below for specific concerns.\n4. The connection between Theorems 2 and 3 and the rest of the paper is unclear, and the assumptions made are so strong as to immediately imply the results. For the former concern, a description of what $\\pi$ and $\\pi'$ of Theorems 2 and 3 correspond to in the RAMP method is missing, making it unclear how the results are meant to be applied. Regarding the second concern, it is assumed variously that $|| \\rho^{\\pi} - \\rho^{\\pi'} || \\leq \\varepsilon_0$, $|| \\hat{r} - r^{\\pi} || \\leq \\varepsilon_1$, and that the average reward $J_{\\hat{r}}(\\pi') = \\langle \\rho^{\\pi'}, \\hat{r} \\rangle$ is sufficiently larger than $J_{\\hat{r}}(\\pi) = \\langle \\rho^{\\pi}, \\hat{r} \\rangle$ to ensure that the desired inequalities hold. Under these assumptions, the proofs follow with some straightforward manipulation of inequalities. To make the results more consequential, it would be helpful to clarify how they are meant to be applied in the context of the paper, then weaken the assumptions accordingly.\n\n- 1. In Section 2, different justifications for introducing the learning objective pursued by the agent are wrong or weak in several aspects:\n\na. The justification line 108 for going from equation (1) to equation (2) is in my opinion wrong. Using the entropy of the policy as proxy to the entropy of the state distribution is a huge approximation. Maximizing the entropy of the policy does not provide a good state coverage in general nor in most practical cases. Note that if it was sufficient to maximize the entropy of the policy to get a uniform distribution of states, it would not be necessary to introduce a complex algorithm as the authors do.\n\nb. Line 128 authors justify to use the Wasserstein distance instead of the KL-divergence as the KL does not account for a potential geometry in the state space. This fact result from the original choice to define as exploration objective the entropy over the state space, which does not account for a potential geometry of the state space. So by choosing to maximize the Wasserstein distance instead of the KL, the authors change the original hypothesis that that the objective is to have high state entropy. While it can be discussed that it is a potential better framework to account for some geometry, it makes most of the previous mathematical justifications irrelevant.\n\n2. The authors claim in Section 3.4 that it is sufficient to optimize with any RL algorithm the reward model from Section 3.2 or Section 3.3 to maximize the objective equation (2) or equation (4). It is equivalent to neglecting the entropy of the policy. Authors, nevertheless, eventually use SAC, which is an algorithm that regularizes the MDP rewards with the log-likelihood of actions. This should be clarified.\n\n3. Only the final values are reported in the experimental section. From my personal experience, complex exploration methods may be unstable, and the learning curves provide important insights. Adding them in the paper would make the results more trustworthy.\n\n4. In the experiments, there is no statistical evidence that the method at hand outperforms the concurrent methods. Most confidence intervals overlap.\n\n5. I think that the related work should include [1, 2], and probably other, more recent, works.\n\n- The idea of \"running away from the past\" is not strictly novel. From a theoretical standpoint, running away from old trajectories might not always be optimal and it is not clear theoretically what is gained by adopting such an approach. From an empirical standpoint, the ablations are carried out on the continuous control tasks, most of which do not seem to require extensive exploration to solve. It is not very clear if the claimed gains are really due to the exploration bonus, or some other unknown side effect.\n\n- 1) Prior Work/Baselines: The paper misses several crucial works on intrinsic exploration (c.f., 1, 2, 3 for a survey). Particularly, there are works that use the model epistemic uncertainty/disagreement as an intrinsic reward which works well in practice and also scales favorably (4., 5., 6.). \n2) Theory: In particular, the model epistemic uncertainty is theoretically a well-studied objective (7., 8.). In 8, the authors derive a connection between maximizing the model epistemic uncertainty and maximizing information gain/conditional entropy of the trajectories, while also showing convergence for sufficiently smooth dynamics. \n3) Unclear motivation: Given the theoretical and experimental strengths of the method discussed above, its unclear to me what particular gap the authors are trying to address with their intrinsic reward. I'd appreciate the authors elaborating further on this. Furthermore, I think all the aforementioned works should be discussed in the paper and in particular one of the baselines should use the model epistemic uncertainty as the intrinsic reward. Perhaps one weakness the authors might raise is that the aforementioned works are computationally more expensive as they have to learn an ensemble of networks to quantify disagreement. However, this should also be empirically shown in the experiments (as the proposed method also learns a model to estimate the intrinsic reward). \n4) Hyperparameters are not provided in the paper, which makes it difficult for me to assess how sensitive the results are to the choice of hyperparams. In particular, I am curious about how $\\beta$ affects the performance of the algorithm. How can we appropriately select $\\beta$? Furthermore, doesn't the method suffer from sample inefficiency for large values for $\\beta$, i.e., when lots of data from the buffer is discarded?\n5) Scalability: Its unclear to me whether the proposed method would scale reasonably well to more high-dimensional settings such as POMDPs/visual-control tasks (note that 5, 6 also work for POMDPs). Could the authors elaborate further on this?\n\nI am happy to raise my score if my concerns above are addressed. \n\n1. https://arxiv.org/abs/2109.00157\n2. https://www.sciencedirect.com/science/article/pii/S1566253522000288?casa_token=ScYOIGv6D2wAAAAA:buNFoXMZLqPiWzo0CLpe3K-ac_nxundN5855FT0QwSnE6jhpm6VwPFS0UHyt1E9WXJePruqZsg\n3. https://www.mdpi.com/1099-4300/25/2/327\n4. https://arxiv.org/pdf/1906.04161\n5. https://arxiv.org/abs/2005.05960\n6. https://arxiv.org/abs/2110.09514\n7. https://arxiv.org/pdf/2006.10277\n8. https://arxiv.org/pdf/2306.12371"
    }
}