{
    "zuuhtmK1Ub": {
        "venue": "ICLR 2025",
        "title": "Differentiable Implicit Solver on Graph Neural Networks for Forward and Inverse Problems",
        "link": "https://openreview.net/forum?id=zuuhtmK1Ub",
        "abstract": "Partial differential equations (PDEs) on unstructured grids can be solved using message passing on a graph neural network (GNN). Implicit time-stepping schemes are often favored, especially for parabolic PDEs, due to their stability properties. In this work, we develop a fully differentiable implicit solver for unstructured grids. We evaluate its performance across four key tasks: a) forward modeling of stiff evolutionary and static problems; b) the inverse problem of estimating equation coefficients; c) the inverse problem of estimating the right-hand side; and d) graph coarsening to accelerate forward modeling. The increased stability and differentiability of our solver enable excellent results in reducing the complexity of forward modeling and efficiently solving related inverse problems. This makes it a promising tool for geoscience and other physics-based applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- By employing an implicit scheme with optimized gradient computation, the proposed method reduces the required number of time steps. \nThey present a differentiable framework for both forward and inverse methods, enabling a learnable numerical approach based on discrete time steps. \nAdditionally, the paper explores applications in inverse problems, often employing irregular unstructured grids as used in practical scenarios.",
            "- * The work tries to build a framework that works with mesh coarsening, forward, and inverse problems.",
            "- Figure 1 effectively illustrates the overall pipeline, demonstrating experimental results that apply the combination of GNN and FVM to both forward and inverse problems.",
            "- The question is interesting and combining GNN with finite element method seems natural."
        ],
        "weaknesses": [
            "- While the underlying idea is promising, the paper would benefit from stronger experimental or theoretical justification for the proposed methodology. Additional clarity and motivation for the approach would enhance the paper\u2019s impact.",
            "- * The novelty of the work is limited. Incorporating FVM into GNN is not new and considered in, e.g., [Jessica et al. ICML 2024 https://arxiv.org/abs/2311.14464 ] and [Horie et al. ICML 2024 https://arxiv.org/abs/2405.16183v1 ]. The construction of gradients presented in Section 2.3 seems strongly related to the adjoint method, which is a standard way to deal with inverse problems. The implicit method for GNN is considered in the area of implicit GNNs, e.g., [Gu et al. NeurIPS 2020 https://arxiv.org/abs/2009.06211 ]. The authors state that these are their novelty, but there is existing work for each. The authors should cite these works and clarify the added novelty from the authors.\n* The evaluation is weak. There is only one baseline for the experiment in Section 3.2 and nothing for the ones in Section 3.3 and 3.4. With the current form, the reviewer cannot asses the effectiveness and superiority of the model.\n* The presentation is not clear. The figure may miss the labels (a), (b), and so on for Figures 2, 3, and 4. It is not clear what is \"data 1\", \"fitting 1\", \"data 2\", and \"fitting 2\" in Figures 2 and 3.",
            "- First and foremost, the paper feels incomplete. The biggest concern is the lack of discussion about other approaches that use GNNs or integrate FVM with deep learning to solve PDEs. A \u201cRelated Work\u201d section should be added to explain how the proposed model differs from recent studies and highlight its novelty. Although Section 2 on theory explains the problem setup to some extent, more detailed steps and methods for training the proposed approach should be included. Section 3, the experimental part, merely lists the results for forward and inverse problems without discussing how this method compares to existing GNN- and FVM-based approaches. For instance, the study \"Learning to Solve PDE-constrained Inverse Problems with Graph Networks\" solves inverse problems using GNNs\u2014how does the proposed method differ from this approach, and what advantages does it offer? Experimentally, does it outperform in solving inverse problems?",
            "- 1. The writing is subpar. There are many typos and grammatical errors. For example, \"Compute $\\nabla_bL$  with (12), whats is equivalent so the solution of a single linear system.\" should be \"Compute $\\nabla_bL$ with (12), which is equivalent to solving a single linear system.\"\n2. One main focus of this paper is the incorporation of implicit solver. However, using an iterative solver and in a deep learning setting is well-studied in the Deep Equilibrium Models (DEQ) literature. The authors should compare their method with DEQ.\n3. The experiments are not very convincing. The results in Section 3.4 is very poor and in no experiments the authors compare their method with other methods."
        ]
    },
    "zPxlHOLxmh": {
        "venue": "ICLR 2025",
        "title": "From Counseling Transcript to Mind Map: Leveraging LLMs for Effective Summarization in Mental Health Counseling",
        "link": "https://openreview.net/forum?id=zPxlHOLxmh",
        "abstract": "The increasing number of patients with mental health illness has heightened the cognitive load on therapists, making it challenging for them to provide personalized care that each patient requires. Summarizing counseling sessions can aid mental health practitioners in recalling key details. However, most existing research on summarization focuses primarily on text-based summaries which often require significant cognitive effort to read and interpret. Visual-based summary such as mind maps is proven to help enhance cognitive understanding by giving a quick overview of topics and content. Nevertheless, due to the complex nature of counseling which involves substantial qualitative data, generating visual-based summaries using traditional AI models can be challenging. With the recent advancements in Large Language Models (LLMs), these models have demonstrated the capability to perform tasks based on instructions and generate outputs in various formats. In this study, we develop a web-based summarization tool that serves as a pipeline in performing summarization of counseling transcripts into visual-based mind map summaries using LLMs. We conducted a human evaluation to validate the effectiveness of the generated visual-based summary based on criteria of accuracy, completeness, conciseness and coherence. Our findings show that our web-based summarization tool can effectively extract key points from counseling transcripts and present them in visual-based mind maps, demonstrating its potential in enhancing insights for therapists, ultimately simplifying the process of documenting counseling sessions.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            1,
            3
        ],
        "strengths": [
            "- The integration of LLMs, specifically GPT-4o Mini, with PlantUML for generating mind maps from qualitative counseling data is a creative and original contribution. This approach moves beyond traditional text-based summaries by providing a visual representation of complex counseling sessions, which is underexplored in the mental health domain.",
            "- - Interesting problem of visual summaries is explored. Mind maps are an effective and quicker way of information communication, so using such things to help therapist is a good idea.\n- It is good that the authors have developed a web based tool which can help people.\n- The paper is easy to follow.",
            "- - The authors provide a web based tool that can be used by mental health professionals for building mind-map visual diagrams of past notes\n- They show that their tool can build reasonably good mind maps evaluated using human experts.",
            "- Originality:\nThe paper introduces the use of mind maps for summarizing counseling transcripts, which is an interesting shift from typical text summaries. Applying language models to generate these visuals in a mental health context is a practical adaptation, though it builds on existing summarization methods.\nQuality:\nThe method is clearly explained but lacks depth in certain areas. While the pipeline from prompt design to mind map visualization is understandable, the approach could benefit from a stronger methodological foundation or more comprehensive testing. Human evaluations are included, but the evaluation setup could be more rigorous to fully support the tool\u2019s effectiveness.\nClarity:\nThe paper explains the problem and solution in a straightforward way, with enough detail to understand the main approach. Visual examples help clarify how the mind maps work, although some sections could be streamlined for readability.\nSignificance:\nAddressing the cognitive load for therapists is a relevant issue, and this tool could be helpful in simplifying their workflow. While the impact is practical, the approach is incremental and may inspire further exploration of visual summarization but is unlikely to be transformative on its own."
        ],
        "weaknesses": [
            "- 1. Limited Sample Size and Representativeness\n\nThe study utilizes only 20 randomly selected samples from the MEMO dataset for the evaluation. This small sample size is not sufficiently representative of the diverse range of counseling conversations that occur in real-world settings. As a result, the findings may not generalize well to broader applications. To strengthen the validity of the results, it would be beneficial to include a larger and more varied dataset. This expansion would help in capturing a wider array of counseling scenarios and linguistic nuances, thereby enhancing the robustness of the tool's performance and its applicability to different contexts.\n\n2. Reliance Solely on Human Evaluation Lacking Reproducibility\n\nThe evaluation of the generated visual summaries is based entirely on human assessments from a small group of participants who are researchers in the field of Information Technology, not mental health professionals. This approach raises concerns about the reproducibility and objectivity of the results. Human evaluations can be subjective and may vary significantly between different evaluators. Incorporating quantitative evaluation metrics, such as adapted versions of ROUGE or BLEU scores for summarization tasks, could provide more objective measures of the tool's performance. Additionally, involving mental health professionals in the evaluation process would offer insights that are more aligned with practical therapeutic needs, thereby increasing the reliability and validity of the findings.",
            "- - No substantial contribution is there in this paper for it to be a part of main research track, maybe a better fit for demo track for some conference.\n- Only 20 mind maps evaluated by only 3 people with using just 1 LLM as the summariser might not be enough to be considered as a thorough investigation. I would suggest the authors to conduct a more detailed analyses by using more LLMs in the place of GPT-4o Mini to summarise the transcripts and then perform a comparison of different LLMs here.\n- In a nutshell, the paper does not propose a new problem, does not have a new dataset, does not propose any novel method, and does not contain any interesting observations or analysis. It is a very engineering pov paper, which is not a good fit for this venue.",
            "- - The paper lacks novelty - as it is merely an application of using LLMs for a very specific use case. The prompting itself is also not smart or novel in any way as it just has a structured output that is parsed into a mind-map. I dont believe this would be of interest to many people.\n- There are no comparisons to text based summaries for this use case and it is not clear how these visual summaries could be more useful to a mental health professional. Several ablations are missing.\n- The contributions is really just the prompt and the web based tool.\n- There is also no comparison between the use of different kind of models / LLMs use",
            "- 1. Limited Sample Size in Evaluation\nAlthough the authors mention that they selected 20 transcripts as a \u201cpreliminary\u201d sample, the decision to limit the entire study to these 20 samples is problematic given that the original MEMO dataset contains 212 transcripts. Relying solely on such a small subset, when a much larger set is available, raises concerns about the representativeness of the findings. A larger, more diverse sample could provide a stronger basis for evaluating the tool\u2019s reliability across different counseling scenarios and patient-therapist interactions. Expanding the sample size is essential to enhancing the credibility of the results and giving a fuller picture of the tool's performance.\n2. Insufficient Evaluation Criteria and Sample Size of Evaluators\nThe evaluation approach has two issues: (1) the limited expertise of the evaluators and (2) the small number of evaluators. Referring to the evaluators as \u201cparticipants\u201d implies they were not specially trained or highly qualified to evaluate counseling summaries, which could compromise the quality of feedback. A robust evaluation for this type of tool typically requires input from domain experts \u2014 such as mental health professionals \u2014 who can reliably assess criteria like accuracy, relevance, and therapeutic usefulness based on their experience.\nAdditionally, the study relies on only three evaluators, which is insufficient for a reliable, statistically meaningful assessment. This small evaluator pool, combined with limited experience in mental health, limits the confidence one can have in the evaluation results.\n3. Lack of Comparisons with Other Summarization Methods and Models\nThe paper does not compare mind maps with alternative summarization formats, such as text-based summaries or visual formats like concept maps. This absence weakens the justification for mind maps as the preferred format, as their claimed cognitive benefits remain untested. Additionally, the study only uses GPT-4o Mini, without comparing it to other language models. This limits the scope since other models might perform better or capture nuances differently.\n\nBy addressing these issues \u2014 expanding the sample size, involving a more qualified evaluator pool, and incorporating comparisons with other methods \u2014 the study would achieve a higher standard of rigor and offer stronger evidence for the proposed tool\u2019s effectiveness."
        ]
    },
    "y2ch7iQSJu": {
        "venue": "ICLR 2025",
        "title": "Budget-constrained Active Learning to De-censor Survival Data",
        "link": "https://openreview.net/forum?id=y2ch7iQSJu",
        "abstract": "Standard supervised learners attempt to learn a model from a labeled dataset.  Given a small set of labeled instances, and a pool of unlabeled instances, a budgeted learner can use its given budget to pay to acquire the labels of some unlabeled instances, which it can then use to produce a model. Here, we explore budgeted learning in the context of  survival datasets, which include (right) censored instances, where we know only a lower bound c_i on that instance\u2019s time-to-event t_i.  Here, that learner can pay to (partially) label a censored instance \u2013 eg, to acquire the actual time t_i for an instance [eg, go from (3yr, censor) to (7.2yr, uncensored)], or other variants [eg, learn about 1 more year, so go from (3yr, censor) to either (3.2yr, uncensored) or (4yr, censor)].  This serves as a model of real world data collection, where followup with censored patients does not always lead to complete uncensoring, and how much information is given to the learner model during data collection is a function of the budget and the nature of the data itself. Many fields, such as medicine, finance, and engineering contain survival datasets with a large number of censored instance, and also operate under budget constraints with respect to the learning process, thus making it important to be able to apply this budgeted learning approach. Despite this importance; to our knowledge no other work has looked into doing this. We provide both experimental and theoretical results for how to apply state-of-the-art budgeted learning algorithms to survival data and the respective limitations that exist in doing so. Our approach provides bounds and time complexity  theoretically equivalent to standard active learning methods. Moreover, empirical analysis on several survival tasks show that our model performs better than other potential approaches that might be considered on several benchmarks.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- - The selected problem is very relevant and interesting for real life data applications. The paper well states the problem and aspire to provide a solution.\n- Authors propose three alternative methods to demonstrate the strength of their approach.",
            "- The use of active learning in survival analysis is an interesting idea.",
            "- - This paper is clear, domains are well introduced (whereas the paper is at the intersection of survival analysis and active learning)\n- The results are convincing, and the authors have made the effort to compare themselves with \u201cnaive\u201d methods (sanity checks part), even though there is no work dealing with this case.",
            "- 1) The **problem addressed in this paper is relevant** and the existing research on the topic of AL for survival data is indeed limited. \n2) The proposed approach is **an original combination of existing ideas**, namely budgeted learning and BatchBALD for AL, with the latter being \"tweaked\" to work with survival data."
        ],
        "weaknesses": [
            "- 1) Paper contains multiple typos, see e.g line 210. \n2) Terms are not clearly defined, e.g. Algorithm 1, line 275, what is a_BB_surv ? \n4) The paper is not very clearly written, there is missing description of  the learner, there is missing debate on the distributional properties of the BALD model.  \n5) The results in the main section seems cherry-picked, e.g. it would be good to include more points in the budget constraints rather than 0, 1, 5, 20.\n6) It would be good to provide study on the synthetic dataset to demonstrate how the proposed methods perform in the controlled setting.",
            "- The use of de-censoring as the label in survival analysis does not have much practical usage. It could be used in \"reliability\" in industry, though. The written English is not clear. The use of words and punctuation like  \"and\" and \",\" make the sentences hard to understand. For example, see  lines 269 and 282.",
            "- - I have some minor comments, it's more a question of presentation than content.",
            "- 1) The **contributions are not very clear**. The authors should clearly state which contributions they are claiming. Is the extension of BatchBALD to a budget-constrained setting the central contribution? If so, why is the theoretical analysis included in the appendix rather than in the main text? Is the adaptation of BatchBALD to survival data the central contribution? Both? The narration is ambiguous with respect to this, thus unnecessarily increasing the difficulty of assessing novelty.\n2) The **novelty is unfortunately low**. It is not clear how the proposed approach for accounting for the budget constraint is different from the existing approaches for budgeted learning from the literature. The adaptation of BatchBALD proposed for survival data is also very incremental and not adequately grounded. The authors also mention that \"so far no method has been developed looking at\nbudgeted learning with survival data\" - however, there are other works in the literature looking at applying BALD for right-censored data (see e.g., https://arxiv.org/pdf/2402.11973). How does the proposed approach relate to these works?  \n3) There are **several concerns regarding the soundness** of the proposed approach. The extension of BatchBALD to a budget-constrained setting is not clearly described and is not adequately grounded. The proposed approach is an adaptation of the algorithm proposed in (Khuller et al., 1999) \"by omitting the first two lines\". However, it is not clear what lines the authors refer to and, most importantly, what are the theoretical grounds for doing so. Similarly, the adaptions of BatchBALD for survival data are essentially presented as \"algorithmic tweaks\" (e.g., setting probabilities to zero and renormalizing) without a proper theoretical justification. Lastly, it is not clear how the authors handle the fact that the queried instances can still be censored - are the censored times assumed to be known a priori before querying? If so, how limiting is that assumption in terms of the applicability of the proposed approach? \n4) The **limited application scope** of the proposed approach is also a topic of concern. Although I agree with the authors that AL for survival data is a topic worth investigating, and the literature is indeed lacking on this, the application scope is, in my opinion, broader than the medical domain application presented in the paper. This limits the target audience of the paper quite a lot. I would encourage the authors to consider revising their presentation to broaden the scope of application to other domains as well. For example, the authors mention applications in finance and engineering in the abstract - it would be interesting to discuss those as well. Lastly, the authors consider only approaches where time is discretized into bins (they use Multi-Task Logistic Regression as the underlying survival model), and the proposed adaptation of BatchBALD to handle survival data in Section 5 relies on this assumption, which can be quite limiting. This should be discussed in the paper. \n5) The presented **claims are not adequately supported by empirical evidence**. Looking at the results in Table 1, the improvements over standard BatchBALD are very marginal. However, the text claims that \"BBsurv outperforms other algorithms when budget is equal to 20 across all 3 real world datasets\", which does not seem to always be the case. In fact, even \"Entropy\" seems to perform quite comparably - did the authors perform a statistical significance test? Confusingly, the Table 1 caption states that \"Budget = 10\", which is inconsistent with the text. Moreover, the results in Table 1 seem to be inconsistent with Figures 1 and 2. If one considered a vertical line at Budget = 10 (or 20, whichever it is), shouldn't the values and relative order of the approaches be consistent with Table 1?\n6) The **presentation can be significantly improved**. There are several typos, poorly constructed sentences, incorrect grammar usage, sentences that are too informal for scientific paper, etc. I strongly encourage the authors to carefully revise the writing and overall presentation of the paper. Other aspects of the presentation, such as mathematical derivations, can be significantly improved. For example, the authors should include numbers for the equations. Similarly, it is unclear how the authors transition from the eq. of the mutual information at the beginning of Section 4.1 to the computation of its right-most term 2 equations below. The right term is an expected entropy, while 2 equations below only the entropy is considered. Can you clarify?"
        ]
    },
    "xTrAA3UKPa": {
        "venue": "ICLR 2025",
        "title": "SWGA: A Distributed Hyperparameter Search Method for Time Series Prediction Models",
        "link": "https://openreview.net/forum?id=xTrAA3UKPa",
        "abstract": "We propose a distributed hyperparameter search method for time series prediction models named SWGA (Sliding Window Genetic Algorithm). Compared to current genetic algorithms for hyperparameter search, our method has three major advantages: (i) It adopts a configurable sliding window mechanism to effectively combat overfitting from distribution shifts inherent in time series data. (ii) It introduces a warm-up stage using Bayesian optimization-based methods to generate a good initial population. (iii) It supports distributed hyperparameter search across multi-node computing clusters, enhancing both scalability and efficiency. To demonstrate SWGA's efficacy, we conduct hyperparameter search experiments on time series datasets from various domains. The experiment results show that our method consistently finds a hyperparameter configuration that achieves better performance on out-of-sample time series data compared to the traditional genetic algorithm. On average, it reduces the out-of-sample loss by about 56.1%.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- 1. SWGA introduces a combination of genetic algorithms with a sliding window approach tailored specifically for time series forecasting, addressing the distribution shift and non-stationarity challenges unique to this domain.\n\n2. The authors have provided detailed computational procedures by providing multiple algorithm boxes and demonstrated their impact on real-world applications.",
            "- * The algorithm is parallelizable, significantly decreasing the computation time required to find the optimal set of hyperparameters.\n* Evaluation is conducted on a sufficient number of widely known real-life datasets.",
            "- * It\u2019s a relevant problem for time series forecasting.\n* It shows some potential for the sliding window strategy.",
            "- - the paper is clear and concise, though it presents some nits reported below\n - the method is applied on multiple datasets and on a variety of different models."
        ],
        "weaknesses": [
            "- 1. The paper writing has too much redundancy that can be simplified or moved to the appendix. It is also not clear why the proposed method is distinguished from other hyper-parameter optimization approaches.\n\n2. In the experiment section, there is also a lack of comparison with other hyperparameter optimization methods specifically designed for time series data.\n\n3. The SWGA algorithm\u2019s performance might be sensitive to parameters like window size, population size, and mutation rates. However, the paper lacks an exploration of how these parameters impact outcomes.",
            "- * In the experiments, the proposed algorithm is only compared with the traditional genetic algorithm. It would be beneficial to evaluate other hyperparameter search techniques, such as the classic sliding window search and k-fold validation technique.\n* The models evaluated in the experiments lack diversity. There are 3 tree-based models (CatBoost, LightGBM, XGBoost), 1 recurrent model (LSTM), and 1 attention-based model (Transformer).\n* The authors mention high computational costs and inefficient exploration of large hyperparameter search spaces as disadvantages of commonly used techniques. However, the experiments only involve accuracy comparisons. Wouldn\u2019t it be beneficial to demonstrate that the proposed hyperparameter search technique converges more quickly?\n* In Appendix A.1, search spaces are provided for the DLinear and PatchTST models, which are not included in the experiments.\n* The proposed algorithm combines variations of three widely used search techniques from the literature with minimal modifications. The novelty does not seem satisfying.",
            "- - The contribution is minor. I can\u2019t agree that using TPE instead of Random Search is an actual contribution. To fill the initial population, doing a small HPO with TPE should have better results than random configurations. It\u2019s too obvious. Also, to show the sliding window is a better HPO strategy, it needs to first deliver better quality and second is cost effective. While the authors\u2019 experiments (Table 1 & 2) show SWGA has better quality than GA, I am not sure the baseline of GA is properly implemented due to lacking of detail. Also, there are tons of HPO methods that can be combined with the sliding window and they can be used to show sliding window is indeed helpful.\n\n- The technical correctness is hard to access given the current state. Many important details are missing. For example: \n  - How do the authors ensure that the only baseline GA is comparable with the SWGA? For me, to see if the sliding window helps, taking Figure 1 as an example, it should be a single GA on the average performance of 12 splits. Assume population size is M, then SWGA takes trains M*12 times and the single GA baseline also trains M*12 times.\n   - Line 333: \u201cwe use seven historical timesteps to predict one timestep ahead\u201d Does this mean the authors sample segments of size 8 from training and validation set? How many are sampled?\n\n- The experiments, especially Table 3, does not support the contribution.Table 3 only shows HPO helps, not why sliding window or warm start is a good strategy. The part of scalability does not fit into the current story. The contribution of the paper, as claimed by the authors, are the sliding window and warm up. The focus is not distributed training or scheduling etc.\n\n- The terminology and notations are not precise and conventional. For example:\n  - Line 35 \u201dthe model can achieve better performance on out-of-sample data with a matching distribution\u201d What does out-of-sample data with a matching distribution mean? It\u2019s also strange that the authors call the prediction window as out-of-sample data. It may or may not be out-of-sample.\n  - Line 141, please check notation, many misusages.",
            "- Important weaknesses:\n - the contributions are strictly practical, thus it is of fundamental importance that the method is accessible to researchers and reproducible. Particularly, ICRL strongly encourage to add a \u201cReproducibility Statement\u201d (https://iclr.cc/Conferences/2025/AuthorGuide), and the paper is missing it. Furthermore, no code is provided as supplementary material, making the results hard to reproduce and inaccessible\n - table 1 shows that it\u2019s the TPE addition that outperforms the other method, but no experiment shows that the proposed method is better than just TPE.\n - results are not reported with confidence intervals. Thus, there is no way to check if the results are just due to variance.\n - No information is given on the training procedure of the GA counterpart. Furthermore, multiple GA algorithms are present in the literature, but it\u2019s never reported which is the one that is applied for the results.\nMinor weaknesses\n - line 178, TPE is criticized for its ability to scale badly with dimensionality, though the paper uses it to initialize the population, inheriting its downside\n - the 4th contribution point is pointless, given that no code is provided as supplementary material\n - the 2nd contribution point states that the proposed approach should address the distributions shift induced by the nature of time-series. However, nowhere in the paper this is investigated, and specifically, nowhere is addressed what are the other options apart from SWGA and why they should fall short in time series predictions\n - in the \u201cconclusion\u201d section, it is stated \u201cAdditionally, we also demonstrate the good scalability of SWGA\u201d. However, no reference to other algorithms/approaches is given. Thus, with a lack of baselines, it\u2019s completely irrelevant\n\nMy recommendation is to reject the paper.\nThe main reasons behind this opinion are:\n - the contributions are minors, and they can be summarized in a smart initialization of the GA using TPE, and a sliding window training approach.\n - the lack of reproducibility of the results\n - the narrow comparison with other approaches.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nWriting concerns:\n - citations should be between parenthesis if not part of the main text (e.g. line 39-40 and then the rest of the paper, respectively use \\citep for parenthesis and cite for normal citation)\n - line 68, TPE was never defined (will be defined later, though it should be defined on the first usage)\n - \u201cBut, their full\u201d line 107\n - \u201cK-fold cross-validation effectively reduces the risk of overfitting\u201d: how? It\u2019s a validation method, not a regularization\n - The equation on line 140 is not numbered, and it contains 2 \u201ci\u201d indexes. Please use different letters to avoid confusion\n - line 161 \u201cwhile domain adaptation is to\u201d\n - line 258 263, the pseudocode contains a line break that is irrelevant\nPersonal opinions:\n - contribution 2 and 3 are not contributions, but positive aspects of the method, maybe better to write them outside of the bullet points\n - the first two paragraphs in 4.1 are repetitive, and the explanation on how the GA works should be part of the background section"
        ]
    },
    "wJVZkUOUjh": {
        "venue": "ICLR 2025",
        "title": "EXAGREE: Towards Explanation Agreement in Explainable Machine Learning",
        "link": "https://openreview.net/forum?id=wJVZkUOUjh",
        "abstract": "Explanations in machine learning are critical for trust, transparency, and fairness. Yet, complex disagreements among these explanations limit the reliability and applicability of machine learning models, especially in high-stakes environments. We formalize four fundamental ranking-based explanation disagreement problems and introduce a novel framework, EXplanation AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine learning, particularly from stakeholder-centric perspectives. Our approach leverages a Rashomon set for attribution predictions and then optimizes within this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that minimize disagreement with diverse stakeholder needs while maintaining predictive performance.  Rigorous empirical analysis on synthetic and real-world datasets demonstrates that EXAGREE reduces explanation disagreement and improves fairness across subgroups in various domains. EXAGREE not only provides researchers with a new direction for studying explanation disagreement problems but also offers data scientists a tool for making better-informed decisions in practical applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- - The paper addresses a critical challenge faced by researchers and practitioners: how to proceed when even explainable AI tools disagree on feature importance. Additionally, it incorporates model and stakeholder rankings, making the approach quite comprehensive.\n- The paper tackles its proposed problem by integrating methodologies from several different areas, including the XAI literature as well as general AI methods for optimization challenges.\n- The insight to make the process end-to-end differentiable is both creative and practically useful.\nIn the appendix, the authors demonstrate the impact of the choice of $\\epsilon$ on the Rashomon set, which serves as a valuable methodological sensitivity analysis.\n- The empirical results test their methods across a variety of settings: 6 OpenXAI datasets, both synthetic and empirical; and 2 pre-trained models (logistic regression and artificial neural networks).",
            "- The technical framework of end-to-end optimization problem which involves constructing the Rashomon set, DMAN, sorting networks and multi-heads architecture is interesting.",
            "- The scientific contribution presented in the paper is valid and interesting. The proposed method has the means to create machine learning models that can produce explanations that match an arbitrary ideal. The paper presents a new, model-agnostic method to accomplish this, which would be a meaningful and important contribution to the field.",
            "- The problem that the paper focuses on is interesting and the paper does a good job at disentangling the four scenarios of explanation disagreement. \n\nOriginality and Significance (of the problem): High\nQuality and Clarity: Poor"
        ],
        "weaknesses": [
            "- The paper integrates several methods and techniques to address the explainability disagreement issue; however, it can feel somewhat dry and lacks the depth and technical details that would enable readers to fully appreciate the contributions and identify strengths and weaknesses. Technical jargon used to describe the methodology needs precise definitions, mathematical arguments should be clearly defined and explained, and additional background information would offer useful entry points for readers. Most of the following points align with this suggestion. This lack of precise definitions also contributes to my limited confidence in the recommendations, as it made it challenging to fully assess the work's potential impact.\n- The loss functions for $L_{sparsity}$ and $L_{diversity}$ are not defined clearly in the paper or the appendix.\n- Precise mathematical definitions of what a mask is and how it is derived are essential for readers to understand the methodology in depth, as this concept is central to the approach.\n- Consider adding a sentence or footnote to define core terms in your algorithm, as these abstract concepts can vary in meaning:\n    - \"attribution set\", \"model representations\", \"model characterizations\", \"end-to-end optimization\".\n    - For instance, in the sentence \"Training a Differentiable Mask-based Model to Attribution Network (DMAN) that maps feature attributions from model characterizations for use in the next stage,\" it would be helpful to clarify precisely what \u201cmodel characterizations\u201d entail.\n    - Also, in Equation 4, where $f_{DMAN}^*$ is defined as the optimal surrogate model in the Rashomon set that describes feature attributions, it appears to be the loss between $f_{DMAN}$ and a set comprising ${\\text{masks}, \\text{attributions}}$. Minimizing the output to such abstractly defined elements would benefit from more clarity.\n- It would be helpful to include insights or references for the result in row 196. Why is faithfulness proportional to agreement? Is this a theoretical result, an empirical finding from the paper, or something else?\n- Figure 1 provides few entry points for readers and doesn\u2019t seem to aid in understanding at its current placement. Consider either removing it or adding more descriptive captions to clarify each step (similar to Figure 2, which includes more context). Suggestions include captions for the rankings, lightbulb, etc. Additionally, why are stakeholders grouped together in the first half but not in the second half?\n- Figure 2 is clear, but it seems to appear too early in the paper. Moving it to the end of Section 3 might make it more helpful, as readers would have more context to interpret it.\n- Is the fairness improvement an explicit objective of the EXAGREE model? If so, please explain the rationale and mechanism. If it\u2019s an outcome of the empirical analysis, please clarify this in the paper, as empirical results may not generalize across all applications.\n- In row 276, in the explanation of the \"Ranking Supervision and Correlation Metric,\" it would be beneficial to provide more context and motivation for this metric and how it fits in the big picture of your methodology before defining it.",
            "- However, this paper should be rejected because:\n(1) it is built on weak understanding of explainability\n(2) there is a weak connection between \"explanation disagreement\" and the solution\n(3) has questionable experiment design and metrics without sufficient justification\n(4) it is unrefined\n\nMy biggest concern is that the paper uses local explanatory models (i.e. LIME, Integrated Gradients) to generate global (model-level) explanations. In Section 2.3, the authors mention that they have adapted feature attribution methods for local explanations \"by averaging feature attributions across all instances to obtain global attributions.\" These methods were not designed to be used this way. Although SHAP does have functionality to provide model-level feature attribution, it takes an average of the **absolute** attribution across instances. \n\nMy impression of the paper's proposed solution, EXAGREE, is that it attempts to address \"explanation disagreement\" by finding a model that aligns with stakeholder expectations (i.e., based on domain knowledge) through examining its post-hoc explanation. There is one critical assumption here: the post-hoc explanation is faithful to model behavior --- something we cannot take for granted (see e.g. [Adebayo et al. (2019)](https://arxiv.org/abs/1810.03292)). Besides, I don't think the solution addresses the problem of \"explanation disagreement\", but is rather a model-selection tool using post-hoc explanations. I see that there are two cases of \"explanation disagreement\" (both of which is mentioned in the paper):\n1. Models with similar performance give different explanations (explanation method fixed)\n2. Explanation methods provide different explanations for one model (model fixed)\nEXAGREE addresses 1 to an extent but not 2 --- the paper does not make this clear. The authors seem to suggest that the \"stakeholder centric\" approach can address complex disagreements (Section 2.2). But I don't see how it addresses case 2.\n\nMoreover, I am not convinced that \"higher agreement between rankings implies greater faithfulness\". Bad actors might want explanations that hide the discriminatory nature of their models, hence want features to be ranked a certain way. In fact, several works have highlighted that explainability methods are prone to manipulation:  [Slack et al. (2020)](https://arxiv.org/abs/1911.02508), [Aivodji et al. (2019)](https://proceedings.mlr.press/v97/aivodji19a.html), [Goethals et al. (2023)](https://arxiv.org/abs/2306.13885). Explainability methods are tools to gain insight into a model (to potentially build trust) not project our desired belief upon the model.\n\nAs a result, the metrics, which are based on an unsubstantiated assumption that agreement $\\implies$ faithfulness, and the empirical results fall short of achieving the goals outlined in the introduction: to identify models \"that provide fair faithful and trustworthy explanations.\"\n\nThe experimental design uses the \"ground truth\" explanation, the coefficients of the LR model, as \"stakeholder needs.\" It is inappropriate to compare this to the explanations of the ANN model. I do not understand why we would want ANN model explanations to agree with LR explanations. Note that this is quite different from what [Agarwal et al. (2022)](https://arxiv.org/abs/2206.11104) did in their experiments. The experiment setup in general is quite confusing.\n\nIn line 463, the discussion regards explanation methods as \"stakeholders\". I question whether it is appropriate to frame it this way as it is difficult to imagine a stakeholder wanting rankings \"like LIME\".\n\nFurthermore, the paper in its current state does not seem refined. The authors introduce the problem of explanatory multiplicity but do not make an effort to elaborate on how and why it hinders trust in the model (what about the explanation method?). Also, figures 1 and 2 are not helpful in improving the readers' understanding of the EXAGREE process. Figure 1 is especially confusing regarding what it is meant to portray.",
            "- The paper\u2019s formalization of four types of disagreements lacks sufficient motivation and clarity, especially in comparison to the foundational work it draws from. For example, the classification of \"model disagreement\" as a type of \"explanation disagreement\" is unclear - different models that produce the same predictions can indeed have different internal mechanisms of doing so - in this case explanations should disagree and illuminate this fact rather than obscure it. This notion aligns with the concept of Rashomon sets, where multiple models with similar predictive performance can have significantly different decision boundaries. I encourage the authors to clarify their rationale for categorizing model disagreement within the explanation disagreement framework and to elaborate on how their approach handles cases where differing explanations for similar predictions might reveal essential model behaviors rather than obscure them. There is no \"model disagreement\" problem.\n\nThe paper's contributions then are better studied and understood in the context of a related line of inquiry about adversarial attacks and explanation fairwashing, such as Slack et al (https://doi.org/10.1145/3375627.3375830). The primary problem addressed appears to involve modifying the explanations produced by a Rashomon set of models to align with a predefined set of explanations from an external oracle. This connection is effectively illustrated, though not explicitly addressed, in Table 1, where metrics such as FA, RA, SRA, and others are reported for the ANN model. Notably, the original OpenXAI benchmark (Agarwal et al.: https://dl.acm.org/doi/10.5555/3600270.3601418) does not provide ground truth for ANNs. What this paper does (I think) is use the LR coefficients as ground-truths to measure metrics such as FA, RA, SRA, etc against a **different** model - an ANN! I recommend the authors clarify their novel approach to calculating these metrics for the ANN model and discuss the ethical implications of aligning ANN explanations with LR model coefficients.\n\nFinally, the paper\u2019s entire framing around \"explanation agreement\" (motivated in Section 2) could be made clearer. Rather than resolving \"model disagreement,\" the proposed SAEM approach seems to modify model explanations without altering predictive accuracy, which could be viewed as a form of adversarial attack on explanations. I encourage the authors to address how this approach contrasts with adversarial manipulations of explanations (if at all), discuss potential connections to explanation fairwashing, and consider any ethical implications that arise from intentionally adjusting explanations while maintaining predictive outputs. What is presented in this paper as an SAEM to resolve the apparent \"model disagreement\" class of explanation disagreement problems is essentially a means to make the FIS score for the ANN model to match the coefficients from the LR model trained on the same data - this is an adversarial attack.",
            "- The paper has a lot of weakness in my opinion and I delineate them as follows. You do not need to address all of them in the rebuttal, I have marked points that are major weakness, you can focus on them. \n\n1. Starting with Figure 1 on Page 1 -- totally unclear. How does the right side of the figure indicate agreement while the left does not? (this figure is not crucial to understand the paper, but I am just making a point of why the paper is so unclearly written and you do not need to address this point in the rebuttal, there are many more important problems. )\n\n2. [Major] In Section 2 (Preliminaries), there are several mistakes in the formulation:\n\n**a)** M* is not defined in Eq. 1. I assume what you mean is the optimal model by M* (the model with the best performance on the data). \n\n**b)** a_1, a_2,...,a_p are not defined as features.  \n\n**c)** In Equation 1, I think the sign should be >= instead of <= (the loss of the other models will be higher than M* (assuming M* refers to optimal model)\n\n**d)** Why is $\\epsilon$ multiplied by L(M*), usually similar performing models are supposed to be an absolute threshold, not relative to the loss of the original model. \n\n3. [Most Major] In Section 2.1, you mention 4 axes of explanation disagreement. I am with you on the model and method disagreement, but what does stakeholder and ground truth disagreement mean? \n\n**a)** In Stakeholder disagreement you mention \"Different stakeholders in S might prefer different rankings\". Why would that ever happen? Do these stakeholders want something that is not real and want fake explanations that aligns with their mental expectations of what a model should care about? What is an example of such stakeholders wanting something different from reality? Your current example of a data scientist wanting statistical significance and domain expert valuing different features **does not** answer this. Does wanting statistical significance and valuing different features amount to wanting different ranking of features (this also assumes that these rankings won't be same in the first place)? And even if they want different rankings, should a technique be optimized to suit their demands and provide fake explanations? \n\n**b)** In ground truth disagreement you mention \"ground truth interpretations (i think you mean explanations) from interpretable models can conflict with post-hoc explanations\" -- yeah this is obvious, but does it matter? If I have an interpretable model, why do I care what a post-hoc technique says, I will never ever use that for such a model? Why would one do that? \n\n**c)** In the primary objective of the paper you write: \"identify a well-performing model that minimizes disagreement (or maximizes agreement) between model explanations and stakeholder expectations. \" -- where stakeholder expectation is either something we should not give them because it is fake or something I do not understand. Your rebuttal will help me understand if it is the latter (a solid convincing example is required). \n\n5. Section 2.3, I think the title of that subsection should be \"Evaluation Metrics\" and not \"Evaluation Matrices\". Anyway, in that section you mention two metrics: faithfulness assessment and fairness assessment. In faithful assessment you give a further breakdown in several metrics -- however, most of these metrics seems highly correlated, for e.g., the attribution values directly affect the ranking, so FA, RA, RC, SA, SRA, PRA should have extreme correlation (when you consider the attribution values with its sign). Can you measure this please? If they are highly correlated, then that is effectively one metric and not six. If they are not correlated, can you explain why that is not the case (except the case where you consider the absolute values, its obvious then). \n\n6. You have used the word \"mask\" in line 256, without defining, what does it mean? Also in line 259 what does the phrase \"bridging the gap between models in Rashomon set and their feature attributions mean\" mean? each model in the Rashomon set (or any model for that matter) has a feature attribution (produced by any technique), what is the gap between them -- they don't even lie in the same space, one is in the weight space and other is in the explanation space (they have different dimensionality). \n\n7. [Major] You have this complicated (and not at all well explained) pipeline of computing SAEM which is basically the models that have the highest agreement with stakeholder requirements. Please tell me why do you need this pipeline? You have the models that are trained on that dataset, you can use that to compute the set of the models that are best performing (according to some threshold $\\epsilon$), and then compute the explanations of these models using whatever method you like, and then just give those models to the stakeholders with whom they have the highest agreement. What is the job of the models like DMAN, you are just using that to predict the model that will have the highest agreement (is that a correct understanding of the model's job?), but you don't need that you have the ground truth and you even say this in lines 268-269. \n\n8. [Major] The above were the problems with the problem setup, now coming to the experiment section. \n**a)** In table 1 you mention k= 0.25, what does k stand for? You have used k and l in Section 2.1 without ever defining them. For the same reason, I do not comprehend what does the k in caption of Figure 3 stand for. \n\n**b)** Why is SAEM technique bolded in Tables 1 and 2. The standard practice in ML papers to the bold the best performing method, but your technique, and this is misleading. \n\n**c)** If SAEM selects the models with the highest agreement with the stakeholder, why is it not the best for LR in Table 1 and 2? (especially in Table 2, it is pretty behind techniques that are not optimized for agreement, which alludes to the case that explanation disagreement might not even show up in actual experiments?)  \n\n**d**) how is FIS_LR computed? Is this not just the feature importance and since you are using ground-truth explanations from pre-trained LR, this should have 100% agreement and should be the best 8 number of times (currently in Table 1 it is best 0 times)?"
        ]
    },
    "v3XabZsB7j": {
        "venue": "ICLR 2025",
        "title": "CNN Variational autoencoders' reconstruction ability of long ECG signals",
        "link": "https://openreview.net/forum?id=v3XabZsB7j",
        "abstract": "Can variational auto-encoders (VAEs) generate flexible continuous latent space for long electrocardiogram (ECG) segments and reconstruct the input? A folded VAE architecture is introduced in this study which is able to encode long ECG segments by splitting an input segment into folds and process them in sequence using a narrow field-of-view in the encoder and concatenate them at the end, instead of processing the long segment at a time. The VAE decoder follows similar folding and concatenation strategy for reconstruction of the original ECG segments. The proposed folded VAE architecture is able to generate better reconstruction of long 30-second ECG segments compared to unfolded classical VAE approach which often produce trivial reconstruction of long ECG segments. Experimental results show that the latent representation generated by our folded VAE architecture not only retains rich compressed information but also aids designing interpretable models by providing decision-making insights.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- The paper's application area is an important problem of long-sequence reconstruction, which could enable the capture of essential clinical information in the latent space.",
            "- - Comprehensive Experimentation: The paper includes experiments with multiple datasets (MESA and MIT-BIH), demonstrating the generalizability of the proposed architecture across different signal sources and classification contexts.\n- Practical Applications: The application of this model to sleep stage classification is valuable and opens doors for future research on ECG-based sleep monitoring systems, which has relevance in healthcare.",
            "- 1. The problem of reconstructing long ECG sequence is interesting.\n\n2. The experiments were conducted on 2 datasets and consider not only reconstruction but also classification tasks for performance evaluation.",
            "- The question of whether variational auto-encoders can generate a flexible continuous latent space for long electrocardiogram (ECG) segments and reconstruct the input is interesting."
        ],
        "weaknesses": [
            "- 1. The paper's presentation is poor and does not meet the standards of ICLR or other relevant AI/ML venues. The authors are advised to proofread the paper carefully, as there are numerous grammatical errors, including missing commas, throughout the text, including in the abstract. Many of these errors could be easily fixed by using available grammar checkers, suggesting that the manuscript may not yet be ready for submission.\n2. The proposed method lacks novelty, as it primarily focuses on splitting signals in the input space.\n3. The results are not well-presented, making it unclear what the main contributions of this work are.",
            "- - Lack of Quantitative Metrics for Reconstruction Quality: The reconstruction results rely on visual analysis without quantitative metrics, which may lead to subjective conclusions.\n- Suboptimal Sleep Stage Classification Results: The model\u2019s classification accuracy (mean of 65% across subjects) is lower than other baseline methods.\n- Over-Reliance on Folding Technique Without Comparison to Alternative Approaches: While folding offers improved reconstruction, the authors do not compare this method against alternative architectures (e.g., hierarchical or multi-scale VAEs, or LSTM-based methods) that could also encode long signals effectively. \n- Clarity and Language: Some sections, particularly in the methodology, could benefit from improved clarity. For instance, equations used to describe the encoding and decoding of ECG segments could be elaborated on to better illustrate the folding approach.",
            "- 1. The premise of the study can be made more clear. Currently It is not clear what is the benefit of the presented method against simply using a sliding window on a long ECG sequence by reconstructing short sequences across the windows\n\n2. Some of the technical components can be better described. For instance, it is not clear what the summation means in equations (1-2) \u2014 does it really mean summation by averaging over the representation obtained from different folds? If yes, why? Similarly, it was not well justified why we can merge 30 8x4 features into 8x120 future map.\n\n3. The relation between the VAE described in 2.4 and the specific model described in 2.8 is not clear.\n\n4. There were baselines or comparative studies conducted evaluating the presented method with existing approaches to handle this, especially by simply learning to reconstruct for short sequences and apply to long sequences with a sliding window.",
            "- -\tRelated work section is missing completely. Can the authors comment on what work is out there that investigated similar problems as they do? How do for example the following works relate to the work proposed here (just to name a few):\n1. Comparison of Autoencoder Encodings for ECG Representation in Downstream Prediction Tasks. Christopher J. Harvey, Sumaiya Shomaji, Zijun Yao, Member, IEEE, Amit Noheria , 2024\n2. Multi-Domain Variational Autoencoders for Combined Modeling of MRI-Based Biventricular Anatomy and ECG-Based Cardiac Electrophysiology , Marcel Beetz, Abhirup Banerjee and Vicente Grau, Frontiers in Physiology, 2022\n3. Joint optimization of a \u03b2-VAE for ECG task-specific feature extraction. Viktor van der Valk, Douwe Atsma, Roderick Scherptong, and Marius Staring, arXiv 2023\n4. Feasibility of ECG Reconstruction From Minimal Lead Sets Using Convolutional Neural Networks, Maksymilian Matyschik; Henry Mauranen; Pietro Bonizzi; Jo\u00ebl Karel, IEEE 2020 Computing in Cardiology\n\n\nAs no related work is mentioned, also no baselines or comparisons are performed. I think the simplest comparison would be to take the vanilla VAE and take short sequences, and then concatenate. How does this compare to the proposed folded VAE?\n\nFurther the presentation of results is very poor. There are long vectors of numbers put into the text. Please create e.g. tables that show what method you use, what the result is for different architectures or modifications, and display this in a structured way."
        ]
    },
    "w5h443GIGo": {
        "venue": "ICLR 2025",
        "title": "On the Convergence of Symbolic Pattern Forests and Silhouette Coefficients for Robust Time Series Clustering",
        "link": "https://openreview.net/forum?id=w5h443GIGo",
        "abstract": "Clustering algorithms are fundamental to data mining, serving dual roles as exploratory tools and preprocessing steps for advanced analytics. A persistent challenge in this domain is determining the optimal number of clusters, particularly for time series data where prevalent algorithms like k-means and k-shape require a priori knowledge of cluster quantity. This paper presents the first approach to time series clustering that does not require prior specification of cluster numbers. We introduce a novel extension of the Symbolic Pattern Forest (SPF) algorithm that automatically optimizes the number of clusters for time series datasets. Our method integrates SPF for cluster generation with the Silhouette Coefficient, computed on a two-stage vector representation: first transforming time series into Symbolic Aggregate approXimation (SAX) representations, then deriving both bag-of-words and TF-IDF vectors. Rigorous evaluation on diverse datasets from the UCR archive demonstrates that our approach significantly outperforms traditional baseline methods. This work contributes to the field of time series analysis by providing a truly unsupervised, data-driven approach to clustering, with potential impacts across various temporal data mining applications where the underlying number of clusters is unknown or variable.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- S1. Timely and important problem especially due to the rise of IoT applications and the need for unsupervised data exploration\nS2. Simply and intuitive ideas\nS3. Results support the overall claims in the paper",
            "- S1. The paper addresses the relevant problem of automatically determining the number of clusters.\nS2. The empirical evaluation makes use of a large number of benchmarking datasets.",
            "- (S1) Incorporating BoW and TF-IDF with the concepts of the SPF algorithm sounds like a very sensible approach. Both are a good choice for term-based similarity evaluation and are still commonly used in other settings.\n\n(S2) Aside from minor issues, the submission is well-written and easily understandable while providing an extensive overview of the formulas related to the problem.\n\n(S3) The problem setting is significant as k-estimation is a significant part of clustering in general, which also applies to the setting of time series clustering. The usage of SPF is well-founded due to its low complexity. Introducing k-estimation to the approach helps mitigate one of its weaknesses."
        ],
        "weaknesses": [
            "- W1. Lack of technical depth\nW2. Unclear how different methods/distances can be compared\nW3. Missing potential baselines\nW4. Duplicate references or wrong references",
            "- W1. The method assumes that silhouette coefficient is a suitable metric for finding the best number of clusters, without justifying this choice. This is a major concern as the silhouette coefficient considers (Euclidean) distance to cluster centres, which is not aligned with the clustering objective of the SPF method. The paper should provide justification for using the silhouette coefficient, or discuss potential limitations of this choice given the SPF method's clustering approach. Moreover, the silhouette coefficient is a well-known metric, so it is unclear what the novelty should be.\nW2. The empirical evaluation does not consider the SPF method, but only weak baselines constructed from the proposed method, meaning that the empirical evaluation does not allow assessment of the performance of the proposed method with respect to state of the art. It is important to compare directly to SPF in the experiments, in order to demonstrate improvement over state of the art.\nW3. The empirical evaluation only considers performance metrics accuracy and near-miss-rate, different from other work in the field, and in the SPF paper (e.g. NMI), making it impossible to compare with those works directly.\nW4. The discussion of related work is overly brief, and fails to present clear assessment of the suitability of existing methods and metrics. E.g. Davies-Bouldin Index and its perceived suitability for the task. Also, there is a large body of work on similarity assessment of time series or clustering of time series, e.g. Keogh et al 2005, Rakthanmanon  et al 2012, Paparrizos et al 2015. The paper should discuss these, and explain differences and similarities with the proposed method.\nW5. On the other hand, references UTSAD and STGAT seem out of context, as they do not address clustering of time series. The paper should clarify the relevance of UTSAD and STGAT to the proposed work, or remove these references if they are indeed not directly related.\nW6. The paper contains several redundant sections, such as the description of SAX.\nW7. There are some minor issues, such that Li et al 2019 appears twice in the references, there is a typesetting error in the definition of pi_i(T_i).",
            "- (W1) Novelty: The abstract of the submission makes the claim that there are no time series clustering methods capable of working without the specification of cluster number k. However, such methods exist already:\n\na) \u201cSpectral Clustering for Time Series\u201d by Fei Wang and Changshui Zhang (2005) is able to discover the optimal number of clusters based on the eigenstructure using a threshold on the value of the eigenvalues.\nb) \u201cClustering Time Series with Hidden Markov Models and Dynamic Time Warping\u201d by Tim Oates et. al. (1999) also provides a way to estimate the number of clusters based on Dynamic Time Warping. However, even if the submission is not the only method that does k-estimation on time series, it is still a valid and useful direction. It also appears to be the only method that does so for the Symbolic Pattern Forest algorithm.\nc) The paper \u201cTrendlets: A novel probabilistic representational structures for clustering the time series data\u201d by Johnpaul C I et al. (2020) uses the Silhouette Score for cluster number analysis for time series as well, though it does so based on hierarchical clustering methods. This paper should be explicitly covered in related work or even a competitor.\n\n(W2) Despite TF-IDF being considered the better of the two proposed strategies, there is no actual description of the performance metrics outside of the graph and the overall relative performance value. Similarly, near misses should be added to the text for BoW. The results of both BoW and TF-IDF are the same in the Tables in the supplementary files, though Figure 1 claims that TF-IDF performed slightly better.\n\n(W3) As the method works by optimizing the silhouette score, both the values for the score and the actual clustering performance with the given parameters should be indicated. While the cluster numbers match, the detected clusters may not necessarily correspond to the actual ground truth clusters, which could further mean that different cluster numbers may lead to a better performance. Furthermore, an analysis of the stability of the parameters should have been performed, especially as the method has multiple parameters, which themselves include an upper and lower bound. Additionally, an intuition behind choosing the parameters should be given if they strongly affect the performance.\n\n(W4) Regarding the actual experiment, a better analysis of the behavior should be done, considering under what conditions the k-estimation of each of the three approaches failed and whether or not a reason behind it could be established. The section on Relative Improvement is redundant as it only recontextualizes prior results, and the space could be used to do a more in-depth result analysis instead. Similarly, the remaining 2 pages could have been used for this.\n\n(W6) Neither the parameter w nor the alpha ranges seem to be specified anywhere. The code is unavailable, though it should be possible to reimplement given the information provided. Still, this hampers the reproducibility of the results.\n\n(W7) There should be citations for TF-IDF and BoW. Other papers also do not consistently do it, so it is not a major issue. Nonetheless, it would have been better if it had been done. Furthermore, UCI should be cited upon first mention outside of the abstract, not just at a later point.\n\nMinor Issues:\n* Linear time complexity time series clustering with symbolic pattern forest by Li et. al., is cited twice as 2019a and 2019b despite referring to the same paper \n* The formatting appears to be broken for lists, as they are just written in a line without comma separation (see line 291 and lines 314-315) \n* A similar issue happened with the variables for the optimization problem, as they are also not properly separated in line 305\n* The subscript on several equations appears to be broken (see (22)/319 and (23)/321)\n* The near miss metric should probably be more dynamic based on the ground truth cluster number, as claiming 2 clusters for a 3-cluster setting seems more problematic than claiming 70 for 71 true clusters. The chosen datasets generally only have a few clusters, so the current definition isn\u2019t problematic for the submission. It may be relevant for the extension to the full UCI database, however. \n* The formulation for Near Misses, as currently given, would also include all correctly determined cluster counts but does not do so in the evaluation."
        ]
    },
    "w2C7gJqaai": {
        "venue": "ICLR 2025",
        "title": "Integrated Multi-system Prediction via Equilibrium State Evaluation",
        "link": "https://openreview.net/forum?id=w2C7gJqaai",
        "abstract": "This study presents a new paradigm of prediction, Equilibrium State Evaluation (ESE), which excels in multi-system prediction where systems interact with each other and every system needs its own prediction. Unlike mainstream prediction approaches, ESE views each system as an integral part under one structure and predicts all systems simultaneously in one go.  It evaluates these systems' equilibrium state by analyzing the dynamics of their attributes in a holistic manner, instead of treating each system as an individual time series. The effectiveness of ESE is verified in synthetic and real world scenarios, in particular COVID-19 transmission, where each geographic region can be viewed as a system.  So cases spreading across regions against the medical competency and demographic traits of these regions can be considered as an equilibrium problem rather than a time series problem.  Extensive analysis and experiments show that ESE is linear in complexity and can be 10+ times faster than SOTA methods, yet achieving comparable or better prediction accuracy.  More importantly, ESE can be integrated with these prediction methods to achieve both high accuracy and high speed, making it a powerful prediction mechanism, especially for scenarios that involve multiple systems. When the dimensionality of the multi-system increases, e.g. more systems joining, the advantages of ESE would become even more apparent.",
        "decision": "Reject",
        "review scores": [
            1,
            5,
            1
        ],
        "strengths": [
            "- -",
            "- The primary strength of this paper lies in its effort to apply a novel conceptual framework for modeling interconnected systems. It draws on ideas from Nash equilibrium, zero-sum constraints, and cointegration. This approach offers an interesting perspective by reinterpreting multi-system predictions through game-theoretic concepts, particularly utilizing Nash equilibrium principles and payoff functions to illustrate the mutual influences among subsystems. While these concepts are mainly interpretative, they provide a fresh way to think about multi-system interactions, which could have significant implications for interdisciplinary applications across fields such as economics, epidemiology, and regional forecasting. The paper demonstrates the practical feasibility of the proposed model, ESE, by applying it to a real-world COVID-19 dataset. This highlights the model's potential to capture complex interdependencies and offers insights into multi-region transmission dynamics. The presentation is generally clear, with structured explanations of equilibrium concepts and the role of each component, such as the equilibrium index. Furthermore, if the model\u2019s empirical performance in terms of accuracy and computational efficiency is further substantiated, it suggests that ESE could serve as a competitive alternative for high-dimensional applications where subsystems are highly interconnected.",
            "- There aren't any significant strengths in the paper"
        ],
        "weaknesses": [
            "- **I stopped reviewing this paper, as there were multiple issues. The objective and the problem setting are not clear, assumptions are not clearly stated, related work are missing, and writing is very poor.**\n\nThe definition of a \"system\" in the first place is not clear. The authors casually drop \"properties\" of this said system/multi-system, which vaguely correspond to some concepts, such as Nash equilibria, but not clear at all if those are assumptions/setting the authors consider. \n\nThe paper is almost impossible to read, with abuse of notation used absolutely for no reason. It is not really clear what problem the paper is dealing with and it is very poorly formulated throughout.\n\nIt is not clear how the COVID example fits into the Constraints 1 & 2. Why Eq. (4) needs to hold for the COVID case for instance?\n\nRelated work section mentions mostly datasets and equilibrium examples from different domains, but I could not spot any ML related work that consider a similar problem. \n\nWhy do you define ${\\cal M} {\\cal S}$ to be the sum of the target variables in systems, whereas it was first defined as the set of systems? The notation should be improved across the board, but this is just one example.\n\nWhy does Eq. (1) hold *in general*? Is that a setting you consider?\n\nYou cannot refer to Constraint 1 when defining Constraint 1 itself?\n\nLine 158 - Please refer to where did you describe your ESE method as claimed.\n\nWhat is the takeaway from Figure 1 exactly?\n\nThe connection to the Nash equilibrium is never formally introduced or motivated. It is not clear where  Lemma 1 comes from. There are no proper citations as well.",
            "- A significant weakness of the paper lies in its insufficient support for the claim that the system will reach a true equilibrium state. While the model employs Nash equilibrium and zero-sum constraints to define static equilibrium conditions, these concepts alone do not provide a mechanism to ensure that the system will naturally progress toward equilibrium over time. Without a dynamic framework or time-dependent interactions\u2014such as differential equations or explicit stability conditions\u2014there is no mathematical basis for assuming that the system will move from an arbitrary initial state toward equilibrium. Furthermore, although the paper incorporates cointegration in its training process, this is a statistical technique that assumes the existence of a long-term equilibrium relationship rather than actively guiding the system toward it. This distinction weakens the model\u2019s theoretical foundation, as it does not establish how equilibrium is achieved dynamically. Another critical limitation is that the equilibrium definition based on Nash equilibrium and payoff functions does not exclude the possibility of stable oscillatory behavior. This means the system could theoretically settle into a stable oscillation rather than converging to a true steady state. Additionally, the ESE model shows strong mathematical similarities to traditional multi-compartment models, which use conservation principles to maintain balance across subsystems. While the introduction of game-theoretic concepts like Nash equilibrium and payoff functions offers a fresh interpretive layer, it does not constitute a substantial mathematical advancement over established multi-compartment models. To improve the model\u2019s robustness, the authors would need to provide a formal analysis of convergence conditions, explicitly address the exclusion of oscillatory states, and better differentiate their approach from conventional multi-compartment modeling frameworks.",
            "- - The paper is confusing to follow. The introduction is poorly written. The mathematical notation is very dense without sufficient explanation. \n- Complex method workflow is not fully explained\n- It is not apparent to me why proportions must follow zero-sum rule. Doesn't explain how to handles growing-shrinking total system values.\n- It is unclear why is specific normalization is chosen \u03bb\u1d62,\u2c7c = (\u03b1\u1d62,\u2c7c - mean(at\u2c7c)) / (max(at\u2c7c) - min(at\u2c7c)), why is this normalization beneficial as compared to other ways to normalize? \n- Algorithm 1 is vaguely described, and testing and training process aren't well defined. It doesn't map succinctly with the figure 2 of the paper."
        ]
    },
    "v3DwQlyGbv": {
        "venue": "ICLR 2025",
        "title": "Paramanu-Ganita: An Efficient Pre-trained Generative Mathematics Language Model with Chain-of-Thought Instruction Fine-Tuning",
        "link": "https://openreview.net/forum?id=v3DwQlyGbv",
        "abstract": "In this paper, we pose the following question: whether domain specific pretraining of tiny generative language models from scratch with domain specialized tokenizer and Chain-of-Thought (CoT) instruction fine-tuning results in very competitive performance on mathematical reasoning than LLMs which are trained on trillion of tokens and humongous parameters? Secondly, we pose our second RQ: whether domain specific pretraining from scratch is environmentally sustainable, highly cost efficient? To address these research questions, we present Paramanu-Ganita, a 208 million-parameter novel Auto Regressive (AR) decoder based language model on mathematics. We performed pretraining from scratch on 31.5 billion tokens using a context size of 4096 on a mixed mathematical corpus consisting of mathematical web pages, mathematics related source code such as AlgebraStack, mathematical textbooks, Chain-of-Thought (CoT) templatised mathematical StackOverflow question answers pairs, and mathematical lecture notes in LaTeX curated by us. We also trained a math and code specialised BPE tokenizer. We proposed and performed Chain-of-Thought instruction fine-tuning of Paramanu-Ganita on the MetaMathQA dataset. We evaluate our model on GSM8K and MATH mathematical benchmarks, and on logical deductive reasoning (LogiQA) and multiple choice high school and college level math questions from SAT (AGIEVAL-SAT-Math), GRE/GMAT questions (AGIEVAL-AQuA-RAT), college and high school level math questions from MMLU.\nOur model Paramanu-Ganita, despite being 34 times smaller than the 7B LLMs, outperforms general LLMs by approximately 30% points, and even math-specialised LLMs by 3-23% points in GSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the various models by 6-8% points. On other benchmarks such as LogiQA logical deductive reasoning benchmark, mathematical high school level multi-choice questions (MMLU-math-high-school), GRE-GMAT level quantitative questions (AGIEVAL-AQuA-RAT), SAT level math questions, Paramanu-Ganita was better than the others by about 1-4% points. The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language models are just not restricted to those with humongous number of parameters. Paramanu-Ganita took only 170 hours of A100 training whereas large LLMs such as the math-specialised LLM, LLEMMA 7B, was trained for 23,000 A100 equivalent hours. Thus, our approach of pretraining powerful domain-specialised language models from scratch for domain adaptation is much more cost-effective and environmental friendly than performing continual training of LLMs.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- The paper is well-written. I appreciate the background and the description of how the model is trained. The idea of targeting mathematics is important and building LLMs specializing in math (at least some part of it) is important. \n\nThe dataset is an important contribution, however I am not sure whether the authors plan to make it public.",
            "- - Good empirical results despite smaller model size, demonstrating the effectiveness of their approach\n- Demonstrates that smaller, more efficient models can achieve good mathematical reasoning performance",
            "- 1. A novel decoder model, that is 34 times smaller than existing LLMs and can outperform them by a huge margin\n2. A detailed explanation of the training process required\n3. Detailed benchmarking on GSM8K, MATH and other datasets.\n4. Emphasis on the training time required and compared it to other existing LLMs, showing computation and environmental prowess in training an exclusive tiny model from scratch."
        ],
        "weaknesses": [
            "- I feel the paper explores an interesting direction, but there are some concerns:\n\n1. Firstly, GSM8k tests basic math word problem skills and given the model's GSM8k performance is pretty poor, I do not feel the model is ready yet. I think more experimentation is required. Also, how are Table 2 values computed? It seems the MetaMath paper reports GSM8K performance to be 82.3. Why is it 66.5 here? [1]\n\n2. What is mostly missing from the paper are proper motivations and justification as to what \"contributes\" or what is expected to contribute to the \"improved\" performance? \n\n - Looking at this from a different point of view, why did the authors not start with MetaMath, then say change the tokenizers or change the dataset? Then, slowly demonstrate how all the innovations are truly necessary. At the least such ablations would have showed the necessity of new models. \n\n - Secondly, given the model's performance is not so great, what are we gaining by spending so much training time and cost?\n\n3. One more important aspect is, what are the domains that the model targets? What are the grade levels? Is it the expectation that we will also do IMO problems starting from GSM8k? Or, are we targeting sub-disciplines algebra, pre-algebra, calculus etc.? I think this depth is also missing, so is related papers that investigate the need of such models [2].\n\n[1] https://openreview.net/forum?id=N8N0hgNDRt\n[2] MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning",
            "- - The overall presentation of the paper still needs much improvement. The paper is not in ready-to-review or ready-to-submit status. The figures are pretty rough and unclear for what the authors want to express. For example, Figure 2 shows GPU Power Usage during pretraining of Paramanu-Ganita. But what conclusions do the authors want to make here? How does it illustrate the environment friendly nature of the model? For the figure 1, what does the blue line mean here?\n- Limited Ablation Studies. The paper doesn't analyze the relative importance of different components of their training data (web text vs. code vs. lecture notes). It is unclear why the authors want to utilize these data sources and why the data mixture should be adopted as it is in the paper.\n- Contamination issues. The model achieves good performance on GSM8K and MATH with 200M parameters. It is unclear whether there is data contamination issue.",
            "- 1. The paper uses Qwen-72B to label the corpus and use a score >= 0.6 for training the model, ensuring only a high-quality dataset is used. However, apart from this, the training process used is not novel. Specifically, there is no novelty in the model architecture or training paradigm used that can justify the complete novelty of the paper and also puts into question the improved performance of a 208 million parameter model over LLMs\n2. The paper does not touch upon, newer and difficult mathematical datasets such as MATHBENCH or JEEBENCH. These are some datasets that were released after training cutoff time for some models, ensuring they are not part of their training data. These datasets are also much more difficult as compared to gms8k. This will ensure that the proposed model is robust in solving difficult problems that it hasn't seen before.\n3. Will the checkpoint-filtered corpus used for training be publicly available?\n4. How does the model perform on out-of-distribution data points, this can be checked by first doing a sanity check of data memorization/contamination [1]. Performing simple algorithms 1 and 2 from the paper will ensure that the model has not seen the evaluation dataset, making the results more robust.\n5. The empirical analysis is missing from the paper. A thorough qualitative comparison of reasoning chains produced by Paramanu-Ganita versus other models on a few representative problems from the benchmark datasets. For example, what errors are made by existing LLMs vs. Paramanu-ganita and in which area does it improve?\n\nReference\n[1] Golchin, Shahriar, and Mihai Surdeanu. \"Time travel in llms: Tracing data contamination in large language models.\" arXiv preprint arXiv:2308.08493 (2023)."
        ]
    },
    "xcHIiZr3DT": {
        "venue": "ICLR 2025",
        "title": "Vision-Based Pseudo-Tactile Information Extraction and Localization for Dexterous Grasping",
        "link": "https://openreview.net/forum?id=xcHIiZr3DT",
        "abstract": "This study addresses the challenges of tactile perception in robotic dexterous hand grasping by focusing on two main tasks: 1) Acquiring tactile information from everyday objects using vision, termed \"pseudo-tactile\" information, and 2) Building a Dexterous Hand (RH8D) model in Isaac Sim for real-time fingertip contact localization. Utilizing Isaac Sim enables safe, cost-effective experimentation and high-precision simulations that facilitate data collection for model validation. The research establishes a scientific connection between simulated 3D coordinates, actual 3D coordinates, and pseudo-tactile information derived from point clouds, quantified through normal vectors and grayscale variance analysis. Results demonstrate the ability to extract clear object surface textures, accurately locate fingertip contact points in real-time (with precision up to $0.001 m$), and provide tactile information at contact points. This framework enhances robotic grasping capabilities and offers low-cost sensory data. The source code and dataset are publicly available now.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            3
        ],
        "strengths": [
            "- This work introduces an approach to extract pseudo-tactile information from vision and contact locations for robotic grasping tasks. The topic is interesting, the pipeline is presented with details, and the experiment results show the effectiveness of the approach with high accuracy.",
            "- Open-sourced dataset of point cloud images is made available.",
            "- The work has clear writing and good structure which makes it easy to follow and understand. Figures are well-made and informative. It is well-motivated to address the hard-to-acquire tactile perception by using vision for dexterous robotic hands.",
            "- This paper addresses the problem of obtaining tactile information during grasping based on only vision perception, and provides a clear method for object surface texture extraction with 3D point cloud input. The strengths are listed below.\n1. The authors provide abundunt and clear explanation for method presentation, and present a simple yet effective approach for point cloud preprocessing and feature extraction.\n2. The authors give a thorough representation on the expermental setup, and conduct real-world experiments on a dexterous hand for validation.\n3. The result of the experiments seems very ideal, indicating the effectiveness of the proposed method."
        ],
        "weaknesses": [
            "- The overall contribution is marginal. The vision-based \"pseudo tactile\" features are derived using some common tools for point cloud data processing. The simulation for finger tip localization is from replicating and transforming the real motion data into the simulated environment. It is also claimed that the combination of this vision-based information and the fingertip contact points can enhance tactile feedback reliability in robotic grasping. However, this is not clear. The experiment does not show how the robotic grasping is improved.",
            "- Details are so unclear it is difficult to fully understand the paper. For instance, \n- Paper repeatedly talks about the \"Y component of the normal vector\" without clearly defining a coordinate frame. Authors mention employing \"policy fine-tuning techniques\" on page 7 without ever mentioning a policy up until this point.\n- The role of simulation in the paper is not clear. The paper mentions \"This real-time linkage of each joint\u2019s degrees of freedom with actual dexterous hand movements and the simulation platform allowed us to record the spatial coordinates of each grasping contact point in the simulation accurately.\" This sounds like a simple forward kinematics problem that would only require a mathematical model of the robot \u2013 not a full-fledged simulation.\n- Paper claims that intel realsense has \u201csub-mm accuracy\u201d. This is not supported by documentation from the manufacturer: https://dev.intelrealsense.com/docs/tuning-depth-cameras-for-best-performance?_ga=2.110331777.520332705.1730517789-101245430.1730517789#section-verify-performance-regularly-on-a-flat-wall-or-target\n- Section 4.2 claims to assess localization precision. It is unclear what the ground truth is, how this is being measured and what measurement is being compared against this ground truth. Referenced Table 4 is difficult to understand, has no mention of errors or comparative ground truth. \n- An RMSE error is reported with no clarity on what quantities are being compared.\n- It is also unclear how related work Section 2.3 is connected to this work. Pseudo-haptics is associated with giving humans touch sensory feedback, whereas \u201cpseudo-tactile\u201d in this paper is related to analyzing the surface tactile properties of objects.\n\nMinor comment:\nPaper is very poorly formatted. Main result tables are placed in the appendix and are difficult to understand. Results in Tables 4,5 and 6 have poor choice of units (meters when dealing with textures that are likely sub-cm scale), and numbers are represented with arbitrary precision with no regard for the precision and error rates of the depth measurement device, ie. Intel Realsense cameras. Bullet points in the results section seem to have headers that have the exact font formatting as the rest of the text.",
            "- I do not think this work has solid contribution or concrete experimental results. The proposed method simply combining existing techniques such as extracting point cloud from RGBD camera, and using simulator to simulate grasping. Instead of quasi-static grasping, I would encourage authors to extend it to more dynamic manipulation tasks and explore whether the extracted object features can be leveraged for these complicated tasks.",
            "- While I recognize some of the article's contributions, I still have the following concerns.\n1. From my point of view, this paper is a bit lack of novelty, because some parts of the method chapter are biased towards engineering practice rather than innovation, such as point cloud preprocessing, camera coordinate system transformations, and other types of work are actually common in robotics work, and cannot be listed as points of innovation. Meanwhile, the texture extraction method in the section overlaps most with [1].\n2. While generating pseudo-haptic sensing is one of the important contributions of the article, I didn't see that the authors had measured how good the quality of the generated haptic signals were, both quantitatively and qualitatively.\n3. The article's experiments still seem inadequate to me and lack comparison with previous work. For the contact position localization part, are there any previous baselines that can realiza this? For example, 3D point cloud keypoint prediction baselines, etc. Meanwhile, the authors didn't conduct the ablation studies on the proposed method, such as the different effectiveness on selections of KDTree radius, normal threshold, etc.\n\n[1] Budiyanta, N. E., Yuniarno, E. M., & Purnomo, M. H. (2021, December). Human point cloud data segmentation based on normal vector estimation using pca-svd approaches for elderly activity daily living detection. In TENCON 2021-2021 IEEE Region 10 Conference (TENCON) (pp. 632-636). IEEE."
        ]
    },
    "uy9oR0nYCW": {
        "venue": "ICLR 2025",
        "title": "Toward Robust Real-World Audio Deepfake Detection: Closing the Explainability Gap",
        "link": "https://openreview.net/forum?id=uy9oR0nYCW",
        "abstract": "The rapid proliferation of AI-manipulated or generated audio deepfakes poses serious challenges to media integrity and election security. Current AI-driven detection solutions lack explainability and underperform in real-world settings. In this paper, we introduce novel explainability methods for state-of-the-art transformer-based audio deepfake detectors and open-source a novel benchmark for real-world generalizability. By narrowing the explainability gap between transformer-based audio deepfake detectors and traditional methods, our results not only build trust with human experts, but also pave the way for unlocking the potential of citizen intelligence to overcome the scalability issue in audio deepfake detection.",
        "decision": "Reject",
        "review scores": [
            1,
            5,
            3,
            1
        ],
        "strengths": [
            "- - Authors provide an analysis of explainable and interpretable methods for audio deepfake detection methods. The authors compare these methods on two different benchmark datasets.",
            "- 1. The paper introduces an interpretability framework for the audio domain, incorporating interpretability techniques from visual and natural language processing into audio Deepfake detection, providing a clearer interpretative path for the model's black-box decision-making process.\n\n2. Two interpretability methods (attention visualization and occlusion techniques) are systematically explored to assess the interpretability of Transformer-based audio detection models, with a comparison of each method's strengths and weaknesses.\n\n3. Cross-validation using the ASVspoof and FakeAVCeleb datasets demonstrates the model's generalization capability across varying data distributions, simulating data shift scenarios common in real-world applications.",
            "- The authors have analyzed the information available from Attention Rollouts and Image Occlusion methods. They also noted that some very short frames from audio signal representation are influential to transformers in classifying and these frames typically appear in groups, which, if further explored, may potentially lead to better interpretability of audio deepfake classifications.",
            "- 1. The paper is very well written, the concepts are easily understood and the limitations of the work are discussed as well.\n2. The subject of explainability in audio deepfake detection is a significant and timely problem, especially as deepfake generation technology is readily accessible to the public and has a high capacity of being misused. Research into this subject is very important due to the impact on society this technology can have.\n3. The paper makes a difference between interpretability and explainability and proposes that explainable methods should provide interpretable explanations that are sample-specific, time-specific, and feature-specific. This robust definition of explainability would ensure greater applicability of these methods to interpret black-box models."
        ],
        "weaknesses": [
            "- - The paper is merely an analysis paper of existing explainable methods. There are no significant novel contributions to this paper. The authors mention in line 052 that the contributions are \"Empirical evaluations of novel explainability methods for audio transformers\". However, I cannot judge what novelty is in attention visualization for transformers. It has happened a lot in literature. \n- When authors compare explainable methods for audio deepfake detection, they must show the results on existing approaches such as Shapley[1]. \n- The authors should include more explainable mechanisms as baselines, such as [2], [3].\n- The authors should show results on multilingual deepfake audio datasets such as MLAAD, DECRO, and WaveFake since AST and Wav2vec are pre-trained on the AudioSet dataset, primarily English. Evaluating a multilingual dataset would help strengthen the analysis.\n- Authors show results on the ASVspoof5 dataset. The authors mention that the dataset was released in June 2024 (line 750). However, The dataset is still not available for public use and review. Authors should show the results on the publicly available datasets.\n- FakeAVCeleb dataset is primarily an audio-video deepfake dataset, not only for audio deepfake detection. The FakeAVceleb dataset contains 500 real videos, which means there should be 500 real audio. However, authors in line 7716 mention 9712 real audio samples. Why is there a difference in the numbers?\n- Even the baseline models such as GBDT, Wav2Vec and AST are general audio architectures, not specifically designed for audio deepfake detection. Authors must show results on models such as ASSIST, RawGAT-ST and some state space models such as RawBMamaba.\n- More then 50% of the paper is just explaining trivial and non paper contributions only.\n\n\n\n[1] Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations\n[2] Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF\n[3] Focal Modulation Networks for Interpretable Sound Classification",
            "- 1. The author mentions three limitations in the Limitation section, of which the latter two could serve as directions for future work. However, the first limitation needs to be addressed in the current phase of the task: relying solely on the ASVspoof and FakeAVCeleb datasets may not cover the full range of audio deepfake techniques encountered in real-world scenarios, presenting a dataset limitation. A wider variety of datasets and scenarios is needed to demonstrate the robustness and completeness of the current approach.\n\n2. The introduced attention visualization and occlusion techniques are computationally intensive, potentially affecting the efficiency of practical deployment. Further analysis and comparison of computational costs would be beneficial.\n\n3. The paper\u2019s contribution is somewhat limited, as occlusion and attention visualization are commonly used techniques in computer vision and natural language processing. While adapting these methods for audio Deepfake detection is interesting, the lack of specific modifications tailored to the characteristics of audio forgery detection tasks reduces the overall impact of the proposed approach. Simple method transfer limits the originality of the contribution.",
            "- As the authors themselves have pointed out, attention roll-out and image occlusion-based analysis have been in existence for quite some time, but the novelty of the proposed work lies in applying them in spectrograms to aid in the explainability of audio deepfake analysis. However, how these attention roll-out and image occlusion-based analyses are aiding explainability specific to the audio deepfake analysis is not adequately explained, and how their contribution differs from already existing contributions of attention roll-out and image occlusion-based analysis methods in image feature explainability remains unclear. \n\nThey have utilized the occlusion method in an attempt to explain how the model is reaching these decisions, but as they themselves pointed out, it was not helpful in explaining the model\u2019s decision-making. They have also used an attention visualization method and stated that they can attribute specific frames that were instrumental to classification. However, using attention visualization to attribute where a transformer model is putting importance is not novel, and their analysis does not show enough contribution specific to explaining the decision process in audio deepfake classification in transformers. \n\nThey have proposed a new benchmark, which consisted of training on one existing dataset and testing on another. They have not provided adequate explanations as to how their novel benchmark would be more helpful in audio deepfake classification, and the idea of testing on a new dataset itself is not particularly novel.",
            "- 1. Both the proposed methods for audio explainability, occlusion and attention visualization, are already existing methods that are very common in literature, especially for vision and language tasks. This is also recognized by the paper, which mentioned that their contribution is porting the methods to the audio task. But to do that, the paper didn't make any modality-specific changes to the methods or edit the methods in any way. For the occlusion method, it's well-known that mel-spectrograms can be treated as images and the paper directly used them on the traditional method. For the attention-visualization method, this is modality agnostic and is based on transformer architecture which takes tokens as input. The roll-out attention method was introduced for natural language tokens and the paper claims to adapt the method for audio tokens, but it's unclear what kind of modifications they did except just replacing the tokens. So the novelty introduced by the paper in these methods is questionable.\n2. The paper provided the results of their methods in Figure 4 and Figure 5. First of all, the paper understands that the result in Figure 4 doesn't help explain the model's decision-making (line 420) and instead is suggestive of transformer model behavior. Second of all, the result in Figure 5 is also not very helpful in human interpretation. When these methods are applied to a visual or textual domain, a human can more easily interpret the segment and decision-making rationale. This suggests that more work needs to be done in the audio domain to make the results more human-interpretable. These observations are also recognized by the paper in their limitation section. So the usability of these methods is questionable.\n3. The paper claims to introduce a novel benchmark to evaluate the generalization capabilities of deepfake audio classifiers, where they train the model in the ASVspoof5 dataset and evaluate it on the FakeAVCeleb dataset. Here the only contribution of the benchmark is training on one dataset and evaluating on another dataset. This mechanism is already well-known and well-practiced to show the generalization capability of a model to out-of-domain datasets. Here, there is no contribution to the dataset, no new evaluation metric is introduced or any other changes are proposed. Thus, it's questionable to consider this benchmark as a contribution. The abstract also claims to \"open-source a novel benchmark for real-world generalizability\". The question is what is there to \"open-source\" here, as the datasets are already available."
        ]
    },
    "zuKrRYM3Tg": {
        "venue": "ICLR 2025",
        "title": "Quantized Approximately Orthogonal Recurrent Neural Networks",
        "link": "https://openreview.net/forum?id=zuKrRYM3Tg",
        "abstract": "In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained popularity due to their ability to manage tasks involving long-term dependencies, such as the copy task, and their linear complexity. However, existing ORNNs utilize full precision weights and activations, which prevents their deployment on compact devices.\n\nIn this paper, we explore the quantization of the weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). The construction of such networks remained an open problem, acknowledged for its inherent instability. We propose and investigate two strategies to learn QORNN by combining quantization-aware training (QAT) and orthogonal projections. We also study post-training quantization of the activations for pure integer computation of the recurrent loop. The most efficient models achieve results similar to state-of-the-art full-precision ORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 3-bits quantization.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            5,
            1
        ],
        "strengths": [
            "- * The investigation on weight quantization effects on ORNNs is interesting and insightful (sec 3.4).\n* Based on the experiments the proposed QRNNs (especially with STE-Bjorck) seem to perform well, even till 5-6 bits.\n* The paper is clearly written and easy to follow (except a few minor points).",
            "- The paper is easy to read, and the proposed QORNN is effective even at long-term dependency tasks.",
            "- * Seems fairly easy to implement.\n* Authors thoroughly motivate and explain challenges of constructing and training ORNNs & instabilities caused by quantization. \n* A comparison of model sizes in resulting QORNN models vs. other methods.",
            "- This a very well-written paper with an extensive literature review, a great introduction to ORRNs and their challenges, many experiments and a thorough appendix with many implementation details for reproducibility"
        ],
        "weaknesses": [
            "- * The biggest shortcoming of this paper is the limited novelty. Several points regarding this:\n    * The authors combine two off the shelf ORNN training algorithms with the most simple (and arguably outdates, see later) flavor of QAT. In other words, they add $q_k(W_i)$ to these algorithms (cf line 3, 4 in algorithms 1, 2, respectively) and assume STE.\n    * While I found the investigation in sec 3.4 interesting (see above), I would have expected that these insights come back in their algorithm design (or at least that the authors evaluate such metrics for their proposed approach, look at question whether these metrics in practice correlate with QORNN performance etc).\n* On the quantization side, they seem to miss/ignore a lot of innovation that happened in the last 5ish years which could help to potentially have much better performance at low bit-widths. Most importantly:\n    * They argue keeping the scale fixed is common practice (line 241/242). Since the introduction of LSQ [1] this is long not common practice anymore.\n    * It is unclear to me why the authors not consider per-channel weights. As per-channel weights still work with full fixed-point/integer arithmetic [2], this is supported by almost any HW and commonly used by most quantization literature in the past years. This adds an additional degree of freedom that might be very helpful for ORNNs as by changing the scale ($\\alpha$), as one could ensure that rows (or columns, depending on notation) still have a norm of 1 which seems important (cf sec 3.2).\n* The proposed QORNNs are actually not orthogonal as the name or text suggest. Only the latent weights ($W_i$) are orthogonal, but the actual quantized weights used for inference ($q_k(W_i)$) are only approximately orthogonal. As the authors themselves show (cf figure 1, sec 3.4), this doesn\u2019t give any guarantees and could be detrimental.\n* As the paper positions itself more as \u2018explorative\u2019 (and QAT contribution is very limited), I would expect that they also more closely explore PTQ approaches. There are several degrees of freedom that are unexplored, e.g. setting the correct scale (alpha) or applying/adapting common PTQ algorithms such as AdaRound [3] or GPTQ [4].\n* Minor:\n    * The experimental evaluation is limited to only \u2018toy-sized\u2019 datasets/tasks. \n    * While it is nice they obtain bounds for q_min/max, the established bounds are so loose that from a practical perspective such bounds are not useful (nor similar to the earlier study they are used to design or evaluate the algorithm).\n    * I do miss a comparison to challenges in quantizing transformers or SSMs. While it is arguable whether comparing to transformers it out of the scope of such a paper (as the authors claim), at least discussing/comparing whether the challenges in ORNNs are similar or different to transformers/SSMs would be helpful (e.g. do they suffer from similar outliers as observed in transformers, cf. [5,6]). \n    * Regarding SSMs, there is some recent work that would be interesting to compare to [7, 8]. \n\n**References:**\n* [1] Esser et al., ICLR 2020, Learned Step Size Quantization.\n* [2] Nagel et al., 2021, A White Paper on Neural Network Quantization.\n* [3] Nagel et al. ICML 2020, Up or Down? Adaptive Rounding for Post-Training Quantization.\n* [4] Frantar et al., ICLR 2023, Gptq: Accurate post-training quantization for generative pre-trained transformers.\n* [5] Bondarenko et al., EMNLP 2021, Understanding and Overcoming the Challenges of Efficient Transformer Quantization.\n* [6] Dettmers et al., NeurIPS 2022, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\n* [7] Pierre et al., ES-FoMo 2024, Mamba-PTQ: Outlier Channels in Recurrent Large Language Models\n* [8] Abreu et al, NGSM 2024, Q-S5: Towards Quantized State Space Models",
            "- - There are awkward and missing citations throughout the paper. For example, the citation for fastGRNN should have been first on line 114 instead of line 135. Besides, I think the representation, such as \"activations for LSTM Hou et al. (2017)\" in line 105, \"Penn TreeBank dataset Marcus et al. (1993) (PTB)\" is awkward.\n- There are some issues on writing (see minor issues and questions below)\n\nMinor Issues\n1) There are some minor points on writing:\n- Line 90: \u201cThe reasons .. is\u201d -> \u201cThe reasons .. are\u201d\n- Lines 122 and 321: footnotes are written incorrectly\n- Footnote 4: \u201cSections 3.4\u201d should be revised correctly\n- Equation in line 203, footnote 5, caption of Table 3, and line 466: please add a comma to the end of the sentence\n- Line 286: add a colon to the next of the bolded sentence\n- Subtitle 3.4: \u201cQORNN are\u201d -> \u201cQORNN is\u201d or \u201cQORNNs are\u201d\n- Tabels 1 and 2, \u201cfromKiani et al. (2022)\u201d -> \u201cfrom Kiani et al. (2022)\u201d\n- Table 2, \u201csizes for Copy, MNIST\u201d -> \u201csizes for Copy-task, sMNIST\u201d\n2) Are the words \"steps\", \"power\", \"timesteps\" and \"length\" the same meaning? Mixed terms can confuse the reader. I recommend revising them for clarity. Other examples include copy/copy-task, SSMs/SSM/SSSM, etc.",
            "- * The proposed methods seem very straightforward: combine the strategy of constructing ORNNs with STE, which is a standard and well-known technique in the quantization literature.\n* L286-L300: authors derive bounds for approximate orthogonality of q(W) which they themselves state are too loose to be useful in practice. It would be quite insightful to track and report $||W_q W_q^T \u2013 I ||$ during training, to see if the reason of proposed method working well in practice is due to $q(W)$ being fairly close to being orthogonal or not.\n* L079: authors claimed SotA results on pMNIST, however they did not compare against other 4-bit sequence-to-sequence models, for instance SSM models such as Mamba [1], transformer models such as LLaMA-3 [2] etc. \n* Considered benchmarks are very small by today standards. While most of them seem standard in ORNN literature, it would make a story more convincing if authors included some of the more recent real-world datasets and benchmarks. For instance, it would be insightful to evaluate the proposed methods on some of common reasoning language tasks (MMLU, HellaSwag, Winogrande).\n\n[1] Gu et al., \u201cMamba: Linear-Time Sequence Modeling with Selective State Spaces\u201d. ArXiV: 2312.00752\n\n[2] Dubey et al., \u201cThe Llama 3 Herd of Models\u201d. ArXiV: 2407.21783",
            "- There is unfortunately not any real innovation in the paper. The only contribution of the paper is applying QAT with a straight-through estimator (a very old idea in quantization literature) to existing optimization methods for learning ORNNs. In fact, the QAT technique is not even state-of-the-art as they could learn the quantization ranges $\\alpha$ through gradient by adopting LSQ$^{[1]}$ or PACT^{[2]}. \n\nThe theoretical analysis of the impact of quantization in orthogonal matrices is not optimal. It is well known that using MinMax quantization for low-bit quantization (< 6 bits) leads to significant degradation. This is why search-based methods are normally adopted that find the scale or range that minimizes the Forbenious ( or other norms) between quantized and unquantized vectors (commonly known as MSE-based ranges). It would be more interesting to see the plots of Figures 1 & 2 using optimal $\\alpha$ values rather than ones based on the maximum range. \n\n[1] LSQ: Learned Step Size Quantization, Esser et al. \n[2] PACT: Parameterized Clipping Activation for Quantized Neural Networks"
        ]
    },
    "zkNCWtw2fd": {
        "venue": "ICLR 2025",
        "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
        "link": "https://openreview.net/forum?id=zkNCWtw2fd",
        "abstract": "Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingual pre-trained language models have achieved remarkable success, yet they often optimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias. The approach fine-tunes multilingual language models using a mix of monolingual and cross-lingual question-answer pair batches sampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets show that the proposed method consistently achieves comparable or superior results in zero-shot retrieval across various languages and retrieval tasks compared to monolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingual training. These results demonstrate the effectiveness of the proposed approach for learning language-agnostic representations that enable strong zero-shot retrieval performance across diverse languages.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. Addresses a relevant challenge in multilingual information retrieval.\n2. Provides comprehensive experimental validation across multiple benchmark datasets (XQuAD-R, MLQA-R, MIRACL).",
            "- The paper shows that two standard batching strategies are complementary for information retrieval tasks, as the combination of them shows improvements.",
            "- 1.  This paper proposes a hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias.\n2.  The hybrid batch training strategy simply modifies the training data batches without necessitating the introduction of loss functions or new architectural components."
        ],
        "weaknesses": [
            "- 1. The primary contribution merely combines two existing training approaches with probability weights, presenting a straightforward and obvious solution.\n2. The paper employs translated QA pairs as data augmentation, creating an unfair comparison with baseline methods that do not utilize this advantage.",
            "- 1. Limited evaluations are only QA datasets (e.g., the main text only shows XLM-R and LaBSE). Also, the main text consists of many large tables where each does not present as much information as the space it takes, e.g., the authors could summarize how many languages/scenarios the proposed method shows improvements instead of providing large tables like Table 3, Table 4, Table 5, etc.\n\n2. It is not clear if the proposed method is actually effective. In many cases, the improvements appear rather small. For example, in Table 1, on XQuAD-R for XLM-R (0.792 vs 0.798; 0.705 vs 0.700; 0.593 vs 0.593). Are they even statistically significant?\n\n3. As this paper mainly provides empirical observations, it would be stronger if the paper provides insights on which scenario (e.g., what kind of base model or dataset) where hybrid batching is expected to show significant improvements and when it does not. The current paper pretty much reports experimental findings which could limit its usefulness. Several questions remain, for example, what is the size and mixed of training data does one need to see the impact of this hybrid batching? I expect that if there is limited training data, the impact would be marginal.",
            "- 1. The proposed hybrid batch training strategy only modifies the input training data, which lacks novelty.\n2. This paper lacks sufficient analysis to the field of multilingual information retrieval. It does not adequately demonstrate the shortcomings of existing work nor the importance and necessity of this study.\n3. The experiments only compare the performance of different input strategies but not various multilingual information retrieval methods."
        ]
    },
    "z2QdVmhtAP": {
        "venue": "ICLR 2025",
        "title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations",
        "link": "https://openreview.net/forum?id=z2QdVmhtAP",
        "abstract": "Reconstructing visual images from fMRI data presents a challenging task, particularly when dealing with limited data and compute availability. This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that subjects' brain signals naturally align in this common space during training, without the need for explicit alignment. This is leveraged to demonstrate that aligning subject-specific adapters to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios, where training the adapter with limited data achieves faster and better performance. We also introduce a novel method to select the most representative subset of images for a new subject, allowing for fine-tuning with 40\\% less data while maintaining performance. These advancements make fMRI data collection more efficient and practical, reducing the burden on subjects and improving the generalization of fMRI reconstruction models.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- + In this paper, the rationalization of the shared visual representation space proposed by MindEye2 is slightly explained from a neuroscience perspective.\n+ This paper explores the interpretability of the proposed method.",
            "- The experimental results are superior compared to other methods. However, see weaknesses.",
            "- First, the authors introduce a novel approach for aligning subject-specific fMRI signals to a common visual representation space through Adapter Alignment (AA). This method efficiently manages multi-subject fMRI reconstruction by pre-training on a reference subject and using lightweight adapters to align new subjects, eliminating the need for end-to-end training for each individual.\n\nSecond, the authors provide compelling evidence for the existence of a shared visual representation space. They show that brain signals naturally align within this common space during training, even without explicit alignment mechanisms. This discovery is significant as it sheds light on how visual information is consistently represented across different human brains.\n\nMoreover, the authors present a novel data selection strategy using a greedy algorithm to identify representative images. If effective, this strategy could substantially reduce data collection demands, which is particularly valuable in neuroscience research where fMRI acquisition is costly and resource-intensive. Impressively, this approach achieves a 40% reduction in required training data while maintaining performance."
        ],
        "weaknesses": [
            "- + This proposed alignment strategy relies on visual stimuli shared by multiple subjects, however this assumption is often difficult to realize in real scenarios, i.e., the images in the fMRI-image pairs used to train the model are hardly shared across subjects. This severely limits the usability of the method. In almost all papers that use NSD for visual decoding [1-4], the visual stimuli of different subjects do not overlap in the training set, which is more accepted setting.\n+ The innovations in this paper are limited. Compared to MindEye2 [2], the only difference is simply the addition of a training phase for MindEye2's ridge regression supervised with MSE loss, and the results achieved are less than impressive.\n\n**Reference**\n\n[1] Paul S. Scotti et al. Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors. NeurIPS 2023.\n\n[2] Paul S. Scotti et al. MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data. ICML 2024.\n\n[3] Weihao Xia et al. Dream: Visual decoding from reversing human visual system. WACV 2024.\n\n[4] Shizun Wang et al. MindBridge: A Cross-Subject Brain Decoding Framework. CVPR 2024.",
            "- Details of the method are completely missing from the paper. Thus it was difficult to determine what was the contribution of the paper.\n\nSeveral concepts are mentioned and introduced, but no technical details are provided. \n\n\nIt seems the adapter network is an encoder-decoder architecture. However, details are missing. \n\nThe greedy image selection algorithm is not described anywhere. \n\nThe authors mention, \"Recent works have achieved impressive results by mapping fMRI data to latent diffusion model (LDM) spaces (Takagi & Nishimoto, 2023; Scotti et al., 2023; Lu et al., 2023; Xia et al., 2024), while simultaneously integrating multiple modalities. Despite this progress, these methods have not been thoroughly tested for their generalization performance across a larger population of subjects.\" However, in this paper, it doesn't seem that they have overcome this challenge.  \n\n\nThe method is tested on a limited set of subjects. In such runs, the authors show a superior performance. However, it is not clear if it will generalize to new data.",
            "- The reviewers have several concerns regarding this work:\n\n1). Limited Subjects and Datasets: The authors aim to reconstruct visual images from fMRI using the proposed method; however, they only utilized data from a few subjects (e.g., a total of 4) within the NSD dataset. Additionally, only a single dataset is involved in this work. This limitation in subjects and datasets can impair the generalizability of the proposed framework and restrict its broader applicability. The reviewers suggest incorporating additional task-based fMRI data, such as from the HCP dataset, to reconstruct diverse cognitive activities like language and emotional responses.\n\n2). Potential Overfitting: The Adapter Alignment (AA) method may be prone to overfitting due to the limited training subjects and images. The limited shared images may not provide a comprehensive representation across diverse datasets, and the authors do not discuss strategies to mitigate overfitting in training. \n\n3). Details on AA. In this work, several critical aspects of AA, such as the selection of a reference subject, bin size for image selection, and potential configurations for non-linear adapters, are not clearly addressed. A more in-depth discussion on these points could help enhance and demonstrate the advancement of AA.\n\n4). Details on Greedy Heuristic Search: The authors employ a greedy heuristic algorithm for selecting image subsets; however, the methodology lacks sufficient detail. For instance, the authors state, \u201ca greedy heuristic such as given below achieves an (1 \u2212 1/e) approximation ratio.\u201d Reviewers would like clarification on whether this approximation ratio was proven by the authors or referenced from existing literature."
        ]
    },
    "yIRtu2FJvY": {
        "venue": "ICLR 2025",
        "title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes",
        "link": "https://openreview.net/forum?id=yIRtu2FJvY",
        "abstract": "Variant effect predictors (VEPs) are designed to predict the impact of protein variants on cellular function, traditionally using data from multiple sequence alignments (MSAs). This assumes that natural variants are fit, a premise challenged by pharmacogenomics, where some pharmacogenes have low evolutionary pressure. In this context, deep mutational scanning (DMS) datasets are of particular interest since they provide quantitative fitness scores for variants. In this work, we propose a transformer-based matrix variational auto-encoder architecture and evaluate its performances on $33$ DMS datasets corresponding to $26$ drug target and absorption-distribution-metabolism-excretion (ADME) proteins available in the ProteinGym benchmark. Our model trained on MSAs (matVAE-MSA) outperforms a model similar to the widely used VEPs in pharmacogenomics, and sets a new zero-shot prediction benchmark for $2$ proteins related to the Noonan syndrome. We compare matVAE-MSA with matENC-DMS, a model with similar capacity, but trained on DMS data in a 5-fold supervised cross-validation framework. matENC-DMS outperforms matVAE-MSA for $15$ out of $33$ DMS datasets, including all ADME, and certain drug target proteins. Although our models do not outperform the best baseline models, our results help shed new light on the role of evolutionary pressure for the validity of the premise of VEP design. In turn motivating the development of DMS datasets to improve VEPs on pharmacogene-related proteins.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- - The authors explore novel architectural innovations to the DeepSequence/EVE family of models. In particular, they use self-attention layers where the attention map is determined from predicted contacts in AF2 structures. They also place more expressive priors on the latent space in matVAE-MSA. These are innovative ideas that have not yet been considered in the field. \n\n- The authors clearly benchmark their method to other state-of-the-art methods and clearly show the impact of their architectural modifications on model performance. This is one of the clearest papers I have read.",
            "- * **Novelty**: To the best of my knowledge, the authors' proposed framework is novel.\n* **Impact**: The authors' motivation (i.e., assessing how well evolutionary pressure corresponds to fitness and the corresponding impact on variant effect prediction) is solid, and such studies would likely be of interest to the machine learning for proteins community.",
            "- - A clear presentation on the new transformer-based module for the encoder and decoder.\n- A comprehensive investigation and analysis on the impact of different designed modules to the prediction task.",
            "- The experimental results are cleanly laid out and the authors do a nice job of not over-selling their results."
        ],
        "weaknesses": [
            "- - The primary weakness of this paper is that their VAE model does not outperform existing unsupervised variant effect predictors (like ESM or DeepSequence). They find that using a more complicated prior does not improve performance and that self-attention layers with attention maps defined using AF2 contacts does not help. \n- The authors do go on to say that their encoder-only model trained on DMS data does outperform unsupervised variant effect predictors, but they do not compare to unsupervised variant effect predictors fine-tuned on DMS data. Their are many ways this fine-tuning has been proposed in the past and the authors should benchmark matENC-DMS to those methods: https://pubmed.ncbi.nlm.nih.gov/35039677/, https://www.nature.com/articles/s41467-024-51844-2, and https://arxiv.org/abs/2405.06729. \n- The authors should try pre-training a VAE on MSA data and then fine-tuning the encoder of the MSA on DMS data. \n- A central goal of the paper seems to be to identify the settings in which DMS data is useful for improving variant effect predictions. In Fig. A8, the authors are unable to find any correlations between metadata of the protein and performance difference between the DMS-trained and MSA-trained models. However, they don't consider structural features of the protein itself. Analysis along those lines would be interesting.",
            "- Despite the potential impact of the authors' work, I believe that the authors' submission has significant issues that prevent me from recommending acceptance at this time. I provide details on the major issues below:\n\n* **Unclear motivation for model design choices**: The authors spend a significant amount of time experimenting certain modeling/architecture choices (e.g. using a mixture of gaussians prior rather than a unimodal prior), which in the end don't have an impact on model performance. Indeed, this is listed as one of the authors' main contributions in the introduction. Could the authors comment on their rationale for exploring these modeling choices? Did previous results for this task find that more expressive priors led to improved performance? Or was there a more principled reason to assume that these specific priors would lead to better performance? Without more context it's hard for the reader to understand why these results are being presented. On a related note, it would be great to see an ablation study assessing the impact of training a predictor on DMS datasets using representations from previous methods (e.g. DeepSequence) compared to the same task with the authors' proposed architecture. Without this information, it's difficult for the reader to understand whether any boosts in performance for the models trained on DMS data can be attributed to the authors' proposed encoder network or if the results are solely due to training on DMS data.\n* **Unclear significance of experimental results**: Perhaps most importantly, it's not clear to me that any meaningful conclusions can be drawn from the experimental results (e.g. those presented in Table 3). In particular, given the large error bars it's difficult for the reader to assess if the provided results are statistically significant. Could the authors provide results from e.g. a t test? Moreover, it's not clear to me how the authors selected their final model hyperparameters (e.g. learning rates). The authors mentioned that their choices \"preserv[ed] stability and convergence\", but without more details it's hard to tell if these values were cherry-picked. Were these parameters e.g. chosen via cross-validation/performance on a held-out validation set? Given these issues, it's thus unclear whether the authors' claims are supported by their experimental results.\n* **Not self-contained/writing issues**: Given that ICLR is a general machine learning conference (as opposed to a more biology-focused venue), it would greatly improve the manuscript for the authors to spend more time in the introduction describing the problem setup and significance. Indeed, the introduction section to the manuscript feels extremely rushed, with little time spent on introducing the problem setting tackled by the authors. For example, providing a gentler introduction to domain-specific terms like deep mutational scanning, On the other hand, a significant amount of space is spent describing hyperparameters (e.g. sections 3.1/3.2) or details of individual datasets (e.g. section 2.3 + Table 1), which could be relegated to the Appendix. I would thus recommend that the authors restructure the manuscript so that the main text is self-contained for a general ICLR reader, with ancillary experimental details moved to the appendix to make space as needed. On a related note that could save some space, it's unclear to me why the authors spend a significant amount of time introducing certain mathematical/machine learning concepts which are subsequently not used in the method (e.g. introducing matrix decomposition before stating that transformer layers are used).",
            "- - The presentation of the motivation is unclear (Q1, 3).\n- The justification for the experimental design and significance of the results is not clearly articulated (Q2, 3, 4, 6).\n- The design of the prediction tasks appears to be questionable (Q3, 5, 8).\n- The comparison with baseline methods is incomplete (Q7).",
            "- Unfortunately I see a number of weaknesses with the current paper. First, the use of a transformer as a VAE for modeling protein families is not new and exists in ProT-VAE. Nonetheless, given that this new architecture does not outperform existing simple VAE architectures it is not clear what contribution this paper is making with this? While the matENC-DMS results are interesting, there is no benchmarking with other supervised approaches. In particular, the paper \"Learning protein fitness models from evolutionary and assay-labeled data\" addresses combining evolutionary data with DMS data. At a minimum the authors should compare to this approach. Overall, I wasn't able to see what contributions the authors are making. The new VAE model doesn't seem to add any new insights into how to design better unsupervised generative models and it does not outperform existing approaches. The supervised model is not benchmarked against any similar approaches and does not provide any new insights."
        ]
    },
    "xvsNb5y9CN": {
        "venue": "ICLR 2025",
        "title": "Sample-Imagined Generator: Efficient Virtual Sample Generation Method for Off-policy Reinforcement Learning with Sparse Rewards",
        "link": "https://openreview.net/forum?id=xvsNb5y9CN",
        "abstract": "Off-policy reinforcement learning (RL) requires extensive real interaction with environment to gain experience for policy learning, presenting a challenge of low sample efficiency, especially in the condition of sparse rewards. To address this, we propose a Sample-Imagined Generator (SIG) which automatically trains a sample generator during environment interaction and could adaptively generate valuable imagined samples for policy learning. Through SIG, the policy greatly reduced the interaction with the environment during training and achieved comparable or even higher performance with those trained only through real interactions. SIG could be combined with any off-policy RL algorithm. Experiment in 5 continuous control tasks demonstrate that by substituting imagined samples for real ones to supplement the experience pool, SIG accomplishes tasks with significantly less interaction with the environment, notably improving sample efficiency across 10 off-policy reinforcement learning algorithms.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. The writing and presentation is mostly clear.\n2. The experiments study both off-policy RL settings and offline-to-online RL settings on 5 off-policy or offline RL algorithms.",
            "- 1. Model-based data generation techniques are of great interest to the RL community, making this paper quite relevant.\n1. SIG is paired with a wide range of RL algorithms in the empirical evaluation.",
            "- - Compatibility: The method is designed to work with various off-policy RL algorithms, increasing its potential impact and applicability.\n- Self-validating mechanism: The closed-loop structure of the SSG, including the Action Validation Module, aims to ensure high-quality imagined samples.\n- Adaptive sampling: The SII module's ability to adjust imagined trajectory length and sampling ratio could potentially optimize the use of imagined samples during training.\n- Reduced environmental interactions: SIG aims to achieve comparable or better performance with fewer real environment interactions, which could be valuable in scenarios where interactions are costly or limited.",
            "- - The experimental set-up covers many complex continuous control tasks with sparse reward. \n\n- The results show that plugging SIG into specific off-policy RL algorithms can improve sample efficiency."
        ],
        "weaknesses": [
            "- 1. Experiments need improvements\n\n    1.1. Important baselines are missing. The author needs to compare SIG against SYNTHER [1], a method focuses on using imagined replay to improve RL's sample efficiency.\n\n\n    1.2. The author needs to compare SIG against REDQ [2], and see if REDQ can be further improved by SIG\n\n\n    1.3. Some claims are not rigorous or have factual errors. The baseline selection is very confusing. \"we combine the state-of-the-art MCAC with SAC, TD3, GAE, OEFD, CQL, ... a total of 10 off-policy algorithms\". First, \"GQE enhances training stability based on SAC by incorporating a sophisticated reward estimation\", GQE is more commonly called GAE instead, and it was published before SAC. It is not an off-policy algorithm. \n\n    CQL is an *offline RL* algorithm. OEFD is based on DDPG (DDPG is an off-policy algorithm), but OEFD is more of an algorithm to leverage demonstrations (with state reset, hindsight, q filter, etc). Also combining them with MCAC will not make 10 off-policy algorithms. Furthermore, RLPD [3] is considered the current state-of-the-art approach rather than MCAC.\n\n\n    1.4. The plots in Figure 3 are hard to read. For example, if you make SAC / SAC+SIG in the same color, but in solid / dash lines (same change for other algorithms), it may be better for the readers to tell (1) if SIG is improving (2) how different the performance across algorithms.\n\n2. The selection about $H_{sig}$, $R_{sig}$, and $T_{sig}$ and other hyperparameters are all heuristic-based or a little bit arbitrary. More ablation studies are needed to test if they are sensitive to hyperparameter choices, e.g. $K$, $f_{less}$, $r_{max}$\n\n[1] Lu et al., Synthetic experience replay\n\n[2] Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model\n\n[3] Ball et al., Efficient Online Reinforcement Learning with Offline Data",
            "- I lean to reject primarily because (1) SIG is evaluated across a wide range of RL algorithms but is not compared to any model-based RL baselines such as MBPO [1] \n and PETS [2], and (2) Length-adaptive Trajectories Generation, Ratio-adaptive\nSampling, and Interaction-adaptive Switch Time seem unmotivated and their effect on performance is not ablated. I would like to see ablations on each of these components to understand how important they are for improving sample efficiency. \n\n* Since SIG is not compared to other model-based imagination algorithm, it's difficult to assess whether SIG is improving over existing methods in at least one aspect;\n\n* Eq. 11 essentially says that model rollouts are very short when the SSG loss is large ($L_{ssg}/l_0 \\approx 1$ implies that $H_{sig} \\approx 0$) and that model rollouts correspond to full trajectories when the SSG loss is 0. However, this does not seem well-motivated; the MBPO paper [1] provides theoretical justification for using shorter model rollouts.  Is there empirical evidence motivating the use of longer model rollouts? I'm wondering if performance would improve if exclusively shorter rollouts were used. In other words, does length-adaptation actually improve performance? MBPO also implements a similar linear increase in the trajectory length over the course of training (e.g see appendix C in [1]). How does SIG's length adaptation relate to MBPO's?\n\n* Eq. 12 SIG integrates more model data into learning as the SSG loss decreases. MBPO keeps this quantity fixed throughout training (e.g. 400 model samples generated per environment step). Is there a benefit to linearly increasing the number of model samples generated vs. keeping it fixed?\n\n* SIG improves sample efficiency for the following setups:\n  * SAC: Lift, Door, Extraction (3/5 tasks)\n  * TD3: Lift (though TD3 + SIG doesn't seem to solve the task), Door, Extraction  (3/5 tasks)\n  * OEFD: Lift, Door, Push  (3/5 tasks)\n  * CQL: Lift  (1/5 tasks)\n  * SM: Lift, Door, Extraction, Push, Navigation  (5/5 tasks)\n  * TM: Lift, Extraction, Push  (3/5 tasks)\n  * OM: Lift, Door  (2/5 tasks)\n  * CM: None  (0/5 tasks)\nWhile I'd agree that SIG can improve sample efficiency in some tasks with some algorithms, the paper should discuss why it offers no improvement -- or worse performance -- in other task/algorithm combinations (e.g. CM + SIG and CQL + SIG). \n\n* Figure 3 would be easier to read if curves for <algo> and <algo> + SIG had the same color but differen line styles (e.g. solid vs dashed). Also, figure labels are very tiny and difficult to read. Please use larger font sizes! \n\n\n1. \"This issue can be mitigated by improving the algorithm\u2019s target updating or value estimation method\nto encourage exploration, thereby improving sample efficiency.\" This statement is seemingly disconnected from the previous paragraph. Why should we immediately jump to target updates and value estimation to resolve issues with sample efficiency? What's the motivation?\n\n2. \"This module could verify the rationality of the imagined states\" It's unclear what rationality means here.\n\n[1] When to Trust Your Model: Model-Based Policy Optimization. https://arxiv.org/abs/1906.08253\n[2] Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. https://arxiv.org/abs/1805.12114",
            "- - Lack of comparison with model-based RL: SIG bears similarities to model-based RL approaches, but the paper fails to compare it with existing model-based methods, leaving its novelty and effectiveness in context unclear.\n- Limited exploration of high-dimensional state spaces: The paper does not address how SIG performs with high-dimensional state spaces, which could be a significant limitation for real-world applications. Generating new states with high-dimensional state spaces (like images) is much more difficult.\n- Insufficient experimental results: While the authors claim to have tested SIG with 10 off-policy RL algorithms across 5 continuous control tasks, this range of experiments is still relatively narrow and may not fully demonstrate the method's robustness and generalizability.\n- Poor presentation: The paper suffers from unclear writing and organization, making it difficult for readers to follow the proposed method and understand its contributions.\n- Lack of computational analysis: The paper does not discuss the computational overhead of implementing SIG, which could be significant due to the additional neural networks and sample generation processes.",
            "- - The paper is not well-written. Section 4 is very hard to follow. The authors introduce a lot of modules and components with similar, sometimes confusing names, such as 'Length-adaptive Trajectories Generation.' \n\n- The introduction talks about some existing approaches requiring 'meticulously designed hyperparameters,' yet SIG trains three modules with many hyperparameters: learning rate, network size/depth, covariance on noise, the intervals defined for the reward imagination module, etc. I believe this argumentation is neither fair nor provides a clear motivation for the proposed approach.\n\n- The circular definition in Equation (8) is confusing.\n\n- There are typos: In a lot of places, the hat on a s or r is not placed the letter but the whole character, such as \\hat{s_{t+1}} instead of \\hat{s}_{t+1}.\n\n- Font sizes of labels/titles on figures are tiny, hence hard to read.\n\n- Quantitative results do not indicate a clear sample-efficiency benefit of using SIG with an off-policy RL algorithm. SIG usually achieves similar performance, but sometimes it seems to be slower. \n\n- The paper does not include qualitative results relating to how imagination works or improves during training."
        ]
    },
    "xi3sDtf8A0": {
        "venue": "ICLR 2025",
        "title": "L-MSA: Layer-wise Fine-tuning using the Method of Successive Approximations",
        "link": "https://openreview.net/forum?id=xi3sDtf8A0",
        "abstract": "With the emergence of large-scale models, the machine learning community has witnessed remarkable advancements. However, the substantial memory consumption associated with these models has emerged as a significant obstacle to large-scale training. To mitigate this challenge, an increasing emphasis has been placed on parameter-efficient fine-tuning methodologies, which adapt pre-trained models by fine-tuning only a subset of parameters.  We observe that in various scenarios, fine-tuning different layers could lead to varying performance outcomes, and selectively fine-tuning certain layers has the potential to yield favorable performance results. Drawing upon this insight, we propose L-MSA, a novel layer-wise fine-tuning approach that integrates two key components: a metric for layer selection and an algorithm for optimizing the fine-tuning of the selected layers. By leveraging the principles of the Method of Successive Approximations, our method enhances model performance by targeting specific layers based on their unique characteristics and fine-tuning them efficiently. We also provide a theoretical analysis within deep linear networks, establishing a strong foundation for our layer selection criterion. Empirical evaluations across various datasets demonstrate that L-MSA identifies layers that yield superior training outcomes and fine-tunes them efficiently, consistently outperforming existing layer-wise fine-tuning methods.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. **Novel Layer Selection Metric:** The paper introduces a new metric for layer selection based on the Method of Successive Approximations (MSA), which is theoretically grounded and offers a fresh perspective on fine-tuning strategies.\n\n2. **Theoretical Foundations:** The authors provide a comprehensive theoretical analysis within the context of deep linear networks, which strengthens the credibility of the proposed approach.\n\n3. **Empirical Validation:** The paper demonstrates the effectiveness of L-MSA across multiple datasets and tasks, showing consistent improvement over several existing layer-wise fine-tuning methods.\n\n4. **Parameter-Efficiency Focus:** By targeting specific layers for fine-tuning, the method aims to reduce computational costs, addressing a key challenge in training large-scale models.\n\n5. **Clear Contributions to Layer-Wise Fine-Tuning:** The research highlights the potential of selectively fine-tuning layers to achieve better performance, contributing to the growing field of parameter-efficient fine-tuning methods.",
            "- 1. The task of parameter-efficient fine-tuning is crucial for practical applications of pretrained models.\n\n2. The authors performed an in-depth analysis on the effectiveness of fine-tuning at the layer level.",
            "- - A theoretical analysis for the proposed method has been provided (Sec. 3 & Sec. A.1).\n    \n- The empirical evaluation of the proposed method includes a good variety of datasets and competing methods. This allows to assess the applicability of the proposed method in different contexts.\n    \n- The validation of the proposed method includes an ablation analysis this is effective towards obtaining insights on how the different components of the proposed method contribute to its performance and how it compares w.r.t. full fine-tuning.\n    \n- The paper adequately highlights the limitations of the proposed method (Sec. 5)",
            "- - The paper is well-written and easy to follow.\n- The experimental results seem to be promising, surpassing Full Fine-tuning on part of the dataset"
        ],
        "weaknesses": [
            "- 1. **Lack of Comparison with State-of-the-Art Methods:** The paper does not compare L-MSA with prominent methods like **LoRA**[Hu et, al], which limits the evaluation of its relative effectiveness and practical impact.\n\n2. **Disjointed Presentation:** The flow of the paper is disrupted by referencing equations and concepts out of order, requiring readers to frequently navigate back and forth, which hampers comprehension and readability.\n\n3. **Inadequate Contextualization of Contributions:** The novelty of the proposed method is not sufficiently contextualized against a broader range of parameter-efficient fine-tuning techniques, making it harder to assess its uniqueness and value.\n\n4. **Generalization Concerns:** The paper acknowledges that the approximated updated loss may not always guarantee strong generalization to test data, which raises questions about the robustness of the approach in diverse real-world scenarios.\n\n5. **Computational Demands:** Despite aiming for parameter efficiency, the method still involves substantial computational overhead due to both forward and backward propagation, which could limit its practicality for very large-scale models.\n\n6. **Limited Scope of Empirical Comparisons:** While the paper evaluates L-MSA across several tasks, the range of comparative baselines is not exhaustive, potentially missing out on broader insights.\n\n[Hu et. al, ICLR 2022, LoRA: Low-Rank Adaptation of Large Language Models]",
            "- 1. The paper's novelty is somewhat limited, as the idea that fine-tuning different layers can lead to varying performance outcomes has already been widely explored in previous research. It is not clear what is the main contribution of the paper. \n\n2. The proposed method, which relies on the Method of Successive Approximation, lacks clear motivation. It\u2019s unclear why this approach would lead to improved layer selection or how it compares favorably to other existing methods. In other words, what are the benefits of the proposed approach?\n\n3. For the theoretical analysis, it is not clear what is the main contribution. The authors should also connect the theoretical analysis to the proposed method and discuss why the proposed method can lead to better layer selection and fine-tuning. \n\n4. The explanation based on PMP is interesting, but the main technical contribution is not clear. It would be helpful if the authors could clarify where the primary novelty lies.\n\nOverall, while the paper presents a reasonable idea, it suffers from poor writing, insufficient motivation and unclear contribution.",
            "- - The originality of the paper is somewhat reduced. While the method put forward by the paper outperforms (in some cases) the considered baselines, as admitted by the paper (Sec. 4.1) \u00a0there are already methods in the literature that aim at a more targeted fine-tuning process.\n    \n- In its current form the paper does not feel self-contained, there are several aspects of the paper that are delegated to the appendix. For instance, the models considered in experiments of Sec. 4.2 are not explicitly indicated. In other cases, the protocols and motivations behind the conducted experiments are not clear. Here a proper balance must be achieved to ensure the paper provides sufficient details as to allow the reader to critically analyze the proposed method and its conducted validation.\n    \n- While pairing a method with its corresponding theoretical analysis is very desirable, the extend to which such analysis is currently provided (Sec. 3) is just too shallow as to be informative. \u00a0Perhaps it could be completely moved to the appendix in order to allocate space to further elaborate on other parts of the paper that are currently not detailed enough.\n    \n- Some statements seem to lack supporting evidence. For instance, in several parts of the paper (l.535) statements are made regarding to the reduced computational costs of the proposed method. The evaluation section is missing however a proper experiment addressing that aspect.\n    \n- The improvement put forward by the proposed method is not that outspoken. For instance, in Table 1 it is observed that the proposed method is better in only 2/4 considered settings.\n    \n- In Fig. 6, averaged results over different datasets are reported. This not only hinders the variations in performance across datasets, but also behavior observed for the different methods across the considered datasets.\n    \n\n- Weak positioning; Good part of the related work is centered around other not directly related aspects. For instance, Sec. 6.1 discusses large architectures, which has close no link w.r.t. the proposed method. \u00a0Similarly l.497-502 from Sec. 6.2 is related to prompt-based methods. Thus having a relatively weak link with the proposed method. This weak positioning w.r.t. related efforts becomes more evident when we consider that almost none of the related methods considered in the experimental section (Sec. 4) are covered in the related work section. In addition, I would suggest looking at the two references below which seem to be very related to the proposed method.\n    \n    - Youngmin Ro, Jin Young Choi, \"AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks\", AAAI 2021\n    - Basel Barakat; Qiang Huang, \"Enhancing Transfer Learning Reliability via Block-Wise Fine-Tuning\",  ICMLA 2023",
            "- - This paper emphasizes the advantages on large-scale models, but the datasets used seem to focus on extremely small datasets such as CIFAR, which show the worst performance on experimental fine-tuning on Imagenet-C. The only LIFT that seems to outperform is a paper that has not yet been published through peer review. This makes me worry about its practical prospects.\n- Not enough experiments, this paper is motivated by the fact that large-scale models consume memory, but there is no comparison of memory overhead in the paper."
        ]
    },
    "xVw8YNEtH3": {
        "venue": "ICLR 2025",
        "title": "Reset Method based on the Theory of Manifold Optimization on Real Manifolds",
        "link": "https://openreview.net/forum?id=xVw8YNEtH3",
        "abstract": "Manifold optimization is prominent in the fields of applied mathematics, statistics, machine learning, and in particular, deep learning. By leveraging the intrinsic geometric properties of manifolds, constrained optimization problems can be transformed into unconstrained optimization problems on certain manifolds.  An innovative method, Reset Method, is introduced that combines manifold optimization and standard methods (SGD, Adam and AdamW), aiming to enhance the improvement of precision. The efficacy of our proposed method is corroborated by extensive deep learning experiments, providing visible higher precision.",
        "decision": "Reject",
        "review scores": [
            1,
            5,
            3
        ],
        "strengths": [
            "- N/A",
            "- The paper proposed a method for optimizing on real manifolds. \nSome theories are provided with support from empirical results.",
            "- The restart method was first introduced by O'donoghue & Candes [1] in the Euclidean settings. This paper suggests an approach to extend this method to the Riemannian setting. If revised properly, this could be an interesting perspective for improving generic optimizers."
        ],
        "weaknesses": [
            "- See summary",
            "- It is not clear about the insight/motivation of each mentioned/proposed method presented through out the paper. The first one are derivations in inequalities (6) and (7). There is not picture or figure to illustrate clearly the advantage of method.  I could only imagine that the first order of Taylor approximation is not good enough, thus we could obtain a better one via Armijo search, when some certain conditions satisfied. That goes through an interpolation between some bounds and expect that the interpolation will help to achieve a better bound. The work also mentioned the Barzilai-Borwein method but there is no explanation about reason for  computing the correlation between two vectors. \n\nExperiment results: For Image generation task, in Table 2, when adding your method with different type of manifold (in fact it is not clear for me why we have different type of manifold here), the proposed method is at best with spv, but does not work better with  \"e\", \"fr\", \"o\", etc.  In Table 3, there are some mixed performances between those method when comparing with each other except the \"spv\". Is there any explanation for the performance of each method?\n\nFor image generation task, there is no picture shown, rather than the number. Since average precision is already high, is the improvement noticeable in the pictures?",
            "- **Presentation**: Firstly, several serious representation issues make the paper incoherent and difficult to follow, obscuring the key messages and contributions. Key issues include:\n\n+ *Related Works and Preliminary Sections*: The related works and preliminary sections are placed in the appendix. To improve clarity, many sections could be condensed or moved to the appendix, creating space for these key areas\u2014for example, explanations of SGD, Adam, AdamW, and most experimental results in the tables.\n\n+ *Main Algorithm Presentation*: The algorithm presentation has gaps in clarity and completeness. For instance, variables such as $ \\zeta_{i_1} $ and $ \\zeta_{i_2} $ appear in the pseudocode but are unused in subsequent steps. In Eq. (5), the process of obtaining $ \\alpha_{i+1} $ is unclear. Additionally, the line \"Set $ R_{x_{i+1}}(-\\alpha_{i+1} \\nabla f(x_{i+1})) \\leftarrow B_{x_i}(x_i) $\" is confusing, as we cannot \"set the retraction\" to a specific value.\n\n+ *Function $ B_{x_i}(x_i) $*: The definition and role of $ B_{x_i}(x_i) $ are unclear and potentially redundant. What are its inputs and outputs? Although it\u2019s described as a \"step size correction function,\" suggesting it outputs the step size, in Eq. (5), it produces $ x_{i+1} $, implying it represents the next model. Furthermore, it is stated that $ B_{x_i} $ is selected from SGD, Adam, or AdamW\u2014all Euclidean optimizers\u2014suggesting $ x_{i+1} $ might not lie on the manifold, making it impossible to compute $ x_{i+2} $ directly from $ x_{i+1} $. Additionally, the use of $ B_{x_i}(x_i) $ as notation is confusing since it appears to take only a single input $ x_i $. The reasoning for adding a correction step with operator $B $, which is supposed to be a primary contribution, is also not discussed.\n\n+ *Experimental Results*: The experimental results were provided on many real manifolds. However, it is unclear if the improvements are attributed to the Reset method or the manifolds. In particular, it would be more fair to compare the proposed method with other Riemannian optimizers, such as RSGD. \n\n+ *Minor Formatting Issues*: Multiple minor issues are present, such as inconsistent font sizes in table captions, unusual word choices (e.g., \u201ccontenting\u201d in the pseudocode), inconsistent font styles (e.g., $x_i$ in the text but not italicized in Eq. (5)), table formatting inconsistencies, and citation format errors (e.g., \\cite{} vs. \\citep{}).\n\n**Soundness:** In both theory and experiments, it is critical to compare the Reset method with other Riemannian optimizers, such as RSGD [2] or Riemannian-SAM [3], in addition to Euclidean baselines like SGD, Adam, and AdamW.\n\nThe theoretical contributions are relatively limited. Theorem 4.2 only demonstrates the gradient deviation of the Reset method when compared with SGD, Adam, and AdamW, and it includes an unavoidable term $\\epsilon_0^2 $, indicating a potentially unfavorable trait of the Reset method. Additionally, comparing the Reset method, a Riemannian optimizer, to Euclidean methods may be unfair from a theoretical perspective.\n\nThe experimental results are also not particularly insightful. Specifically, all results on the CIFAR-10, CIFAR-100, STL-10, and SVHN datasets achieve accuracies of at least 98%, making it challenging to discern performance differences. On the Market-1501 and DukeMTMC-reID datasets, variations of the Reset method applied to the same base optimizer yield considerable performance differences, suggesting that the method may be sensitive to variant choices, making tuning more challenging.\n\n**Contribution:** To my understanding, the restart method was proposed by O'donoghue & Candes [1]. This work seems to be a trivial extension of this work to the Riemannian setting, which makes its contributions limited. Moreover, the motivation and intuition of the proposed algorithm are unclear, mostly due to the incohesive presentation."
        ]
    },
    "xFvHcgj1fO": {
        "venue": "ICLR 2025",
        "title": "OML-AD: Online Machine Learning for Anomaly Detection in Time Series Data",
        "link": "https://openreview.net/forum?id=xFvHcgj1fO",
        "abstract": "Time series are ubiquitous and occur naturally in a variety of applications -- from data recorded by sensors in manufacturing processes, over financial data streams to climate data. Different tasks arise, such as regression, classification or segmentation of the time series. However, to reliably solve these challenges, it is important to filter out abnormal observations that deviate from the usual behavior of the time series. While many anomaly detection methods exist for independent data and stationary time series, these methods are not applicable to non-stationary time series. To allow for non-stationarity in the data, while simultaneously detecting anomalies, we propose OML-AD, a novel approach for anomaly detection (AD) based on online machine learning (OML). We provide an implementation of OML-AD within the Python library River and show that it outperforms state-of-the-art baseline methods in terms of accuracy and computational efficiency.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- S1. Online anomaly detection is critical for the massive amounts of data, streams, and edge devices.\nS2. Good coverage of methods in online learning broadly\nS3. Results support the overall claim",
            "- S1. This paper introduces a lightweight plugin that effectively transforms a basic online forecasting model into an online anomaly detection model. This innovative approach enhances the adaptability of existing methods that are trained on historical data.\n\nS2. The writing in the paper is generally clear and well-structured, effectively communicating the design of the OML-AD framework.\n\nS3. This paper addresses a critical issue in anomaly detection: the inability of models trained on historical data to adapt to changes in future data. By tackling this problem, the research holds practical significance for real-world applications.",
            "- The proposed method is capable of handling time series data with concept drift.",
            "- 1. The method is integrated into the widely-used library River.\n2. The proposed method is simple."
        ],
        "weaknesses": [
            "- W1. Lack of technical depth\nW2. Missing baselines\nW3. Missing progress in the area for benchmarks\nW4. Missing progress in the area for evaluating anomaly detectors",
            "- W1. I think the overall approach has too little workload and lacks innovation, as it simply uses the prediction error between the predicted values of an existing online forecasting model and the actual values, with a threshold to determine if it\u2019s an anomaly. Even the method for updating \u03bct and \u03c3t, which are key to calculating the threshold, is based on previous studies.\n\nW2. The experiments conducted in the paper primarily utilize a narrow range of datasets, including one synthetic dataset and one real-time dataset with added synthetic anomalies. The authors should consider adding more real-world datasets, particularly those with varying degrees of concept drift and different types of anomalies. \n\nW3. While the paper compares the proposed approach to a few existing methods, the selection of baseline models appears limited. For a more robust evaluation, the authors should include a wider variety of state-of-the-art anomaly detection techniques.",
            "- The issues highlighted in the paper are not novel, as time series prediction is a well-researched problem. Building upon this foundation, only minor modifications to models are required to perform prediction-based time series anomaly detection, contrary to the paper's claim that \"there currently is little effort exploring online learning for prediction-based anomaly detection.\"\n\nThe experimentation is insufficient:\n1. The comparative algorithms used are from 2018 and only two such algorithms are considered.\n2. There is a lack of crucial experiments, such as ablation studies.",
            "- 1. The writing is horrible. The whole paper needs to be rewritten completely. The introduction, related work, and preliminary seem to be redundant, and the key points cannot be reflected clearly. \n\n2. The contribution is limited. It is hard to find the contribution. It is hard to understand how the proposed method is relevant to online learning in the main content. \n\n3. The experiment is weak. Why not use KDD cup datasets? There are 250 time series in the KDD cup. Also, a robust method should be flexible to handle different data drift, including no drift, slow drift, fast drift, and random drift, as the non-stationarity in the real world is diverse in different applications."
        ]
    },
    "xFezgECSLa": {
        "venue": "ICLR 2025",
        "title": "On the Design and Analysis of LLM-Based Algorithms",
        "link": "https://openreview.net/forum?id=xFezgECSLa",
        "abstract": "We initiate a formal investigation into the design and analysis of LLM-based algorithms, i.e. algorithms that contain one or multiple calls of large language models (LLMs) as sub-routines and critically rely on the capabilities of LLMs. While LLM-based algorithms, ranging from combinations of basic LLM calls to complicated LLM-powered agent systems and compound AI systems, have achieved remarkable empirical success, the design and optimization of them have oftentimes relied on heuristics and trial-and-errors, which is largely due to a lack of formal and analytical study for these algorithms. To fill this gap, we start by identifying the computational-graph representation, \ntask decomposition as the design principle, and some key abstractions, which then facilitate our formal analysis for the accuracy and efficiency of LLM-based algorithms, despite the black-box nature of LLMs. Through extensive analytical and empirical investigation in a series of case studies, we demonstrate that the proposed framework is broadly applicable to a wide range of scenarios and diverse patterns of LLM-based algorithms, such as parallel, hierarchical and recursive task decomposition. Our proposed framework holds promise for advancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding the choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new algorithm design. To promote further study, we include our source code in the supplementary materials.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- The stated objectives of the paper are excellent if they can be achieved.",
            "- The paper advocated analysis of LLM-based algorithms.",
            "- The paper tackles an interesting and important problem.",
            "- I like the proposed idea of analyzing the LLM-based algorithms as computational graphs. It's definitely the most natural way, similarly to analyzing standard algorithms and data flow."
        ],
        "weaknesses": [
            "- The paper does not seem to deliver very much of the stated contributions. Proposition 1 is the key result, and it is quite weak.\n\nThe real issue is that a computation graph is a precise mathematical object that uniquely defines a computation, whereas this LLM-based computation graph is neither precise nor does it uniquely define a computation. The lack of uniqueness stems from the non-determinism of any LLM-based call.\nAs a consequence, how can one compare a \"normal\" computation graph with this LLM-based computation graph?\nFurther, the analysis should be stochastic (with expected complexity), rather than the proposed deterministic complexity. An LLM is inherently stochastic, so one cannot specify the outcome of an LLM call as a deterministic object.\n\nThe paper states: use \u201caccuracy\u201d to refer to the broader concept of \u201cquality\u201d, and an \u201cerror metric\u201d can be any metric that measures how much the output of an algorithm deviates from certain criteria.\"\n\nWhere do the costs come from in  C(prefilling), C(decoding)?\n\nIt seems like all of these costs are just qualitative.\n\nhypothetically categorize LLMs into two types: Type-1 LLMs are only prone to the first failure mode, while Type-2 LLMs are prone to both\n\nToo much speculation in this article\n\nIt seems like the article builds up to Proposition 1,and then we ask \"so what!\". This is a pretty weak conclusion, and it does not even look like to has much strength as a formal mathematical expression.",
            "- The theoretical part proposes a framework for analysis which looks like a simplistic variant of analysis of parallel algorithms, something one learns during undergrad CS studies. The 'empirical' evaluation in the body of the paper is just a qualitative description of application of the proposed (rather standard) methodology to a few problems. There is 'numerical evaluation' in the appendix, which is not convincing and lacks detail.\n\nWhile I welcome the idea of systematic analysis of algorithms, including LLM-based ones, the paper lacks both theoretical novelty and empirical justification. Significant effort has to be spent to bring this paper to the level of a publication at a major conference such as ICLR.",
            "- First, the paper is far too long for a conference format (the appendix is twice\nas long as the main paper and includes sections that should be part of the main\npaper, like related work). This paper would be more suitable as a journal paper.\n\nWith regards to the proposed evaluation framework, very little of it seems to be\nspecific to LLMs, or rather the LLM-specific parts are provided by the\ninvestigator. It seems that this would be difficult in practice -- how would I\ncharacterize the capabilities of any given LLM in a way that allows to determine\nwhat the output would be for a given prompt?\n\nThe insights the analyses in the paper provide are very generic: \"the optimal\nvalue of m that minimizes costs might depend on the choices of cost metrics and\nassumptions of LLM inference service, among other factors\", \"the minimum error\nof the overall algorithm might be obtained by some intermediate value of m that\nachieves a balance between these two failure modes\", \"each option of retrieval\nhas its own pros and cons\". None of these are actionable, and it is unclear that\nthe proposed framework is necessary to obtain them. It is unclear whether other\ninsights are generally true, in particular \"since a smaller value of m makes\neach sub-task easier, it is reasonable to expect that the overall error E(y)\nwith this metric will also become smaller as m decreases\" -- while the\nindividual errors might decrease, combining multiple steps potentially compounds\nindividual errors, resulting in an overall increase in error.\n\nOther parts of the proposed framework are unclear. Section 3.3 describes the\nanswer being generated by majority voting, but the correct answer appears only\nonce. Dividing the input text into chunks, it seems that there would be a lot of\nincorrect and \"don't know\" answers and, hopefully, a single correct one (for the\nchunk that did contain the answer). How can majority voting possibly return the\ncorrect result in this case?",
            "- The main problem of that paper is that, in my opinion, there is no takeaway from it. The proposed analysis mostly ends on framing the selected patterns as computational graphs, showing a basic bound, sometimes followed by a very generic statements like \"so there is a tradeoff, period\". I do think that we should have a principled way of analyzing LLM-based algorithms and the proposed framework looks promising, but little was done to use it. To be more precise:\n\n- In Section 3 you analyze the map-reduce pattern, and in my opinion the most interesting things happen here.\n    - You derive (or rather state since it is straightforward) the bound on the cost (Equation 4). Then, you use it to show (actually state) that it is minimized at $m=min(n, \\bar m)$. Yes, I agree that if the cost is linear or sub-linear with respect to the input, the best idea is to use as large chunks as possible (again, rather a straightforward statement).\n    - Then, you derive a bound on the cost with quadratic complexity and find a minimizer for that, which is approximately $L_{sys}$. This is actually interesting and surely non-trivial, but sadly no further discussion is provided.\n    - Then, you analyze the parallel setting. Although the analysis is sound, I was a bit disappointed when I understood the key takeaway: \"when m is very big, the cost increases if we make it even bigger; when m is very small, the costs increases if make it even smaller\". Unfortunately, I find it straightforward -- there is always a tradeoff in the size of distributed computation parts and using too small or too big is never a good idea.\n    - Then, you state that the cost is minimized by $m\\asymp n/p$. That would be interesting, but I cannot find any justification for that statement.\n    - Also, the asymptotic notation you use is a bit hard to parse. The only variable that has unbounded support is n, which should make other variables either constants or implicit functions of n, which is not specified directly. It looks like you take asymptotic of m, although it's bounded by n.\n    - The \"implication\" in line 323 literally states that \"The optimal value of m might depend on various factors, period\".\n    - Section 3.2 applies the derived formulas to a counting example. However, (1) the cost analysis is little beyond simply rewriting the formulas, (2) the takeaway is again straightforward (overall counting error is smaller if chunks are smaller)\n    - Section 3.3 describes clearly the needle-in-a-haystack example. But then, equally clearly states the conclusion \"the optimal value of m is a tradeoff, it can't be too big or too small period\". Please, explain me why any kind of analysis is needed to make such a claim?\n\n- In Section 4 you decribe the hierarchical decomposition pattern. Now, there are two listed conclusions: (1) reasoning has much lower cost than retrieval, (2) making the algorithm sequential or parallel has pros and cons. Although I agree with both, at the same time I don't see how did your framework help you derive those. They were not conclusions, you stated both during analysis and that's fine, since both need no explanation. But then, again, you haven't shown benefits of using your framework.\n\n- In Section 5 you analyze recursive decomposition and state a bound on the error (and that's literally all). The question is: why do we need that bound? Is it helpful in any way? Why there is no discussion?\n\n- As detailed, although I like the high-level idea of the framework, the paper shows no significant usage of it. Having said that, I'd be happy to be proven wrong, so I'm open to the discussion.\n\nOther comments:\n\n- I suggest adding a brief example of any LLM-based algorithm in the beginning. Initially I thought more about algorithms like Dijkstra, so specifying that with a simple one-sentence example would be helpful.\n- It takes 4 pages until you start any analysis. I suggest making the initial descriptions (in fact, everything) much more concise, since the framework you propose is rather simple (which is a benefit), but then reading 4 pages of \"how to decompose an algorithm into a computation graph\" sounds much too lengthy.\n- It's a bit confusing that you name the paragraphs the same way in sections 2.3 and 3.1. If you repeat the same names in Section 2 (which is introducing the framework), then in 3.1 it sounds like an unintended repetition at first glance.\n- Figure 13 is actually very helpful in understanding the description. Please move it to the main text, even one of them, even smaller.\n- The paper is missing a discussion of limitations and related works."
        ]
    },
    "x8mr9zGkpr": {
        "venue": "ICLR 2025",
        "title": "Attributing Model Behavior: The Predominant Influence of Dataset Complexity Over Hyperparameters in Classification",
        "link": "https://openreview.net/forum?id=x8mr9zGkpr",
        "abstract": "Understanding the drivers of machine learning performance is essential for optimizing model accuracy and robustness. While significant attention has been given to hyperparameter tuning and data preprocessing, the impact of intrinsic data complexity (e.g., class overlap, feature overlap, dimensionality, etc) remains less explored. This study investigates the comparative influence of data complexity and hyperparameter configurations on the performance of classification algorithms, specifically Random Forests (RF), Support Vector Machines (SVM), Decision Tree (DT), Adaptive Boosting (AB) and Multi-layer Perceptron (MLP). Using 270 diverse OpenML datasets and 304 hyperparameter configurations, we employ functional analysis of variance (fANOVA) and Ordinary Least Squares (OLS) regression to quantify the relative importance and effect sizes of hyperparameters and complexity meta-features. Our results reveal that data complexity exerts a more substantial influence on both bias and variance components than hyperparameter tuning, underscoring the importance of addressing intrinsic dataset challenges. These findings suggest that efforts to mitigate data complexity factors, such as class overlap or imbalance, may yield greater performance improvements than extensive hyperparameter optimization. This study provides actionable insights for machine learning practitioners and highlights the need for further research into the interplay between dataset properties and algorithmic performance.",
        "decision": "Reject",
        "review scores": [
            1,
            5,
            3,
            3
        ],
        "strengths": [
            "- 1) the paper is easy to read\n2) The paper confirms a fact that is well know by most data science / ML prectioners, the complexity of the data set matters for classification performance.",
            "- The paper addresses a gap by directly comparing the impacts of dataset complexity and hyperparameters on model performance. Previous studies have often examined these factors separately, but this research provides a unified framework to assess their relative influence on bias and variance. \n\nThe paper comprehensively considered nearly 300 datasets and over 300 configurations, enabling a more convincing conclusion.\n\nApart from the numerical results, the paper also includes very detailed arguments of why this happens and what this indicates.\n\nSome of the estimated coefficients shown in the OLS summary table do align with our common understanding of how random forests deal with bias-variance tradeoffs, e.g. coefficients associated with min samples leaf, bootstrap, and max features.",
            "- - extensive experiments\n\n- clearly written, easy to read\n\n- the topic of bias/variance tradeoff is very important",
            "- - Well structured and clearly written paper.\n- A many ways a very comprehensive experiment.\n- Tackles an important issue for ML practitioners."
        ],
        "weaknesses": [
            "- 1) The content, experiments and conclusions of the papers are very outdated. It reads like a paper that was written 15-20 years ago. Most citations are from many years ago. Hence the contribution are practically irrelevant to the current state of ML / data science in 2024. Hence there is no significant contribution or relevance to the ICLR community.\n\n2) Furthermore, the main conclusion of the paper is that dataset complexity (class overlap, dimensionality, etc) matters when training a classifier (RF or SVM). These are well known fact that are thought in introductory ML class and hence there is no new information provided here.",
            "- 1. lack of details on the experiment design. \n\n (1) as the paper claims, nearly 300 datasets of varying sample sizes, response categories, and feature dimensions are used. Why are they comparable? I believe 0-1 loss is not a typical loss function people use for multiclassification problems.  And high-dimensional datasets wouldn't react necessarily the same as n>p datasets in terms of hyperparameters. \n \n(2) hyperparameters like C and gamma have huge variations in scale. How is it being included in OLS? Is it logarithmized?\n\n (3) For readers not familiar with meta-features of datasets, it would be very helpful to at least sketch some general ideas of how these meta-features are defined. Are those features immune to data transformation?  The same for how fANOVA works. \n\n\n2. lack of details on why the experiments are conducted in such a way. \n\n (1) In my perspective, the number of trees is RF's one of the most important hyperparameters. Why is this not considered?\n \n(2) I believe the neural net is the framework that people are most curious about. The authors also mention it in the introduction. Why is that not considered? \n \n(3) Based on the pymfe package, there are plenty of meta-features that characterize data complexity from different perspectives. Why specifically these 3, N1, T2, C1, are chosen?\n\n3. Some of the results that are confusing to me. \n \n(1) if those meta-features are immune to data transformation, how can we benefit from your research even though we know that data complexity itself is much more important than tuning hyperparameters? if not, shouldn't you include some examples of how bias and variance are reduced after some preprocessing of the data that reduces data complexity? For example, class imbalance issues can be alleviated by reweighing samples or bootstrapping.\n\nIn general, I do agree that the data's quality is much more important than tuning parameters. If the data is always linearly separable, I believe logistic regression would suffice. It's just the data quality is not something we can work on but the model and the model's parameter choice. Please correct me if I am wrong. \n\n (2) Based on your OLS example, the features included are all significant neq to 0. If the trend is determined, does it mean that choosing a smaller c or some certain kernel can always help with the prediction?",
            "- I think the paper sends, in essence, a wrong message to readers.\nAuthors are basically suggesting that hyperparameter optimization\nisn't useful, which I disagree with.  As I write this, I am tuning\nhyperparameters of a neural network classifier, and the AUROC has gone\nfrom 0.55 to 0.75, exclusively due to the (gradually improving) choice\nof hyperparameters. The conclusion is at best narrowly limited to SVM\nand RF, but that makes the manuscript 1) not that useful, given\nlimited scope 2) misleading, since many readers may walk away with a\nwrong impression that the conclusions apply generally \n\nTo be clear, not disagreeing with the idea that hyperparameter tuning\nhas a natural limit, and going beyond that may require additional or\ndifferent data. But the paper leaves an impression to the reader that\nhyperparameter tuning doesn't help in general, which I disagree with. At a minimum, the title should say that the results are limited to SVM and RF binary classifiers. \n\nAlso keep in mind that \"optimizing dataset complexity\" is a vague and\nhardly actionable advice. I personally don't quite know how to\noptimize dataset complexity, whereas hyperparameter tuning is well\nunderstood. This should be clearly stated/discussed.",
            "- I have two main concerns. The first is that the experiments, while in some ways very comprehensive, are in other ways very limited:\n\n- Only two classification algorithms.\n- No missing values in the dataset is a very strong criterion.\n- A limited number of complexity measures.\n\nThe second is that when carefully interpreted, the results are not that general or actionable:\n- N1 is in essence a classifier (probably along the lines of LDA). So the results can basically be summarized that most of the bias and variability of RF and SVM can be explained by running another reasonable classifier and seeing how it performs. That of course makes perfect sense, but it can also be derived from what we already know, that classifiers tend to perform similarly (the differences between classifiers are less than the differences between datasets).\n- We should be more careful when interpreting the result that model performance can be attributed more to dataset characteristics than to hyperparameters. First, it is the nature of commonly used classifiers that they are relatively robust in terms of hyperparameter selection - being easy to tune is what makes them popular. Second, . And third, the range of several parameters is limited. For example, would results change if max_features was allowed to go below 0.1 or above 0.9? Or if 20 different kernels were considered? Similarly, the experiments are limited to 1500 features, which diminishes the importance of regularization.\n- In practice, I can in most cases freely tune the parameters and select models. I can't really change my problem (or dataset) though.\n- The paper does not consider model selection, which I would in this context consider as part of hyperparameter tuning. I would not be surprised that a lot more can be attributed to model selection than to tuning the parameters in this paper. Choosing a different model is also actionable.\n\nThere are also other methodological concerns (see Questions)."
        ]
    },
    "wwO8qS9tQl": {
        "venue": "ICLR 2025",
        "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
        "link": "https://openreview.net/forum?id=wwO8qS9tQl",
        "abstract": "How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. While not a replacement for human evaluations, we aim for ALMANACS to be a complementary, automated tool that allows for fast, scalable evaluation. Using ALMANACS, we evaluate counterfactual, rationalization, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- * The authors do a great job citing prior research on desiderata for post-hoc explanations in Section 3 and 6. In particular, I appreciate that the authors acknowledge the limitations of considering only simulatability as a desirable criteria, as illustrated nicely in L185 with your example that a naive explanation that would just provide the model weights would perfectly enable simulatability. It is also clear from the authors' discussions of related empirical studies/scholarship on simulatability, and description of each of the explanation methods, that they are well-read on contemporary scholarship on XAI.\n* Overall, the authors' methodology and benchmarking approach is described clearly. I also appreciate the efforts the authors took to include details about their methodology \u2013 e.g., the prompt templates used and the instructions presented to users in their study protocol \u2013 to facilitate reproducibility.",
            "- This paper addresses a highly valuable problem: the design of a general, automated evaluation method for explanation tools.",
            "- The main strenght of the paper is its generalizability and simplicity, it does indeed provide a nice scalable method to automate the evaluation of a lot of XAI techniques, which is (as the authors say) not a replacement for human studies, but a nice addition to them. As LLMs do get better the next few years, one can imagine human testing gradually being less necessay in a lot of circumstances.",
            "- 1. This paper is written quite clearly and easy to follow. \n\n2. The sourcing of subjective, opinion-based test questions is interesting and could be used in other LLM benchmarks. \n\n3. The authors take special care of the label leakage issue by using a separate test set."
        ],
        "weaknesses": [
            "- I will consider adjusting my score if my below concerns are addressed.\n\n**Weakness 1: Validity of using an LLM**. My primary critique of the paper is that from the authors' experimental validation, I remain unconvinced by one of the authors' key claims: that an LLM predictor \"can replace humans as [an] automated evaluator of explanations\" in this context. To make this point, the authors argue that \"the automated GPT-4 predictor is consistent with human evaluations\". However, I need more detail to understand how the results in Section 5.2, and Figures 4b-c, support this argument.\n* The main piece of evidence that the authors use to justify that the LLM is \"consistent with humans\" is the wide error bars for all of the treatments in the \"all\" condition in Figure 4. But, if I understand correctly that the authors defined the \"All\" category by creating a big pool across all of the different tasks, then of course the error bars are going to be wide here because there's variance across tasks. A better measure of statistical significance wouldn't create a big pool, but instead conduct separate statistical tests for each of the 5 tasks (each \"distribution\"). I'm skeptical of this measure being used as justification, and am open to hearing other justifications instead.\n* As the authors already noted in their paper, I think it is interesting that there _are_ some significant differences across conditions in the human experiments \u2013 for example, that humans did much better when given explanations in the hiring decisions context \u2013 and these trends are not found by the LLM.\n* I would be more amenable to this work if the authors _did not_ claim that the LLM predictor's results are necessarily \"consistent with humans\", and instead note that there are discrepancies, describe in detail what these discrepancies are (e.g., expand further on some of the findings you already have, like how LLMs tend to be better at the simulation task). I think you can still try to make the argument that there _is_ value in using an LLM to approximate the information different XAI methods give that might be useful for simulation, without necessarily needing to argue that it _will be predictive_ of how a human will perform. But this argument needs to be more fleshed out. (Maybe the LLM is an \"upper bound\" of the predictive information available in an explanation to aid simulation; but humans may struggle/fail to infer how to use these information to actually complete the task accurately \u2013 an argument made by [1]).\n\n**Weakness 2: Biases introduced by using models to simulate other models.** I am wondering if the reason why using another LLM (e.g., GPT-4) to predict the outputs of another LLM (e.g., flan-alpaca-gpt4-xl), performs well, has something to do with how the two models were developed \u2013 for example, flan-alpaca-gpt4-xl was trained to behave as similarly as possible to GPT-4. \n* In other words, these models already do a great job at simulating the outputs of other models _because of the way that they were trained_, in a way that humans cannot. \n* This makes me question the appropriateness of using large pretrained models as the \"predictor\". Past work that has used secondary \"predictor\" models in simulatability tasks doesn't share this same challenge \u2013 to my understanding, the predictor in these past studies was initialized from scratch.\n* If the authors believe it is necessary to use a large pretrained model as the predictor, perhaps they can acknowledge how these connections between the predictor vs. the model being explained as one factor that contributes to the predictor's good performance on the task. They might also be able to design additional experiments to explore this \u2013 e.g., is it true that a predictor that is the same (or a similar) model as the model being explained, will always perform better than other predictors that are unrelated?\n\n\n**Weakness 3: More discussion of limitations of existing scenarios; or extendibility of the benchmark itself.** I would appreciate more discussion in the main text about what exactly the scenario categories are, the limitations of using a template-based approach, and how someone reading your paper could potentially contribute a new category of scenario, or a new template, to your benchmark. More broadly, it is unclear if you intend to support extendibility of your benchmark (i.e., the ability of users to add additional explanation methods, predictor models, or scenario prompts). \n\nNit: The text in your figures is inaccessible via a screen reader. To fix this, you can include figures as PDFs instead of PNGs.\n\n[1] https://proceedings.neurips.cc/paper_files/paper/2022/file/0b9536e186a77feff516893a5f393f7a-Paper-Conference.pdf",
            "- 1. The goal of developing an efficient tool for automated testing of model simulatability is not adequately achieved. User studies must minimize the influence of prior knowledge to ensure objective and effective testing of explanation quality. However, the proposed method relies heavily on the training data of the large language model (LLM), which may skew results and introduce biases. While the authors acknowledge this concern and caution against over-reliance on ALMANACS, it remains critical. If the reasoning generated by ALMANACS cannot reliably measure the simulatability of explanations, then automation in this context becomes meaningless. How do the authors demonstrate that the prior knowledge of GPT-4 as a predictor does not significantly affect the evaluation of explanations?\n\n2. The focus on binary Yes/No questions, while suitable for preliminary implementation, severely limits the applicability of the tool, especially in scenarios that require outputs of richer forms.\n\n3. The conclusions drawn in the paper are not convincing. The authors designed 15 templates for each topic, with each template containing 15 placeholders, generating the dataset by replacing these placeholders with different texts. Two issues arise here:\n   - Could the non-placeholder components of the templates significantly influence the model's predictions? If so, the explanations generated from data produced by the same template are likely to be similar, rendering the replacement of placeholders ineffective. In this case, the hundreds of data points generated are effectively indistinguishable from a single data point.\n   - For the average of experimental results to be meaningful, the distribution of the experimental data should be uniform. However, the authors did not verify the uniformity of the datasets across different topics. Instead, they concluded that all explanation methods are insufficient based on average results from various topic datasets, which is unconvincing.\n\n4. Some key details in the paper are unclear. Part of them can be found in the appendix but they should really appear in the main text. Concretely:\n   - In Section 2.1, the authors should clarify why choosing GPT-4 as a predictor is crucial, as it is the primary tool for evaluating explanations. How might different kinds of LLMs impact their work?\n   - In Section 2.2, it is challenging to understand how to calculate the probability of a Yes answer without reference to the appendix.\n   - In Section 2.2, how did the authors compare different embedding methods? What metrics were employed, and why was the Sentence-BERT model all-mpnet-base-v2 chosen?\n   - In Section 2.2, the authors conducted a suite of evaluations to assess the models\u2019 capabilities but did not provide any details or results from these evaluations. How can they demonstrate that the model explained has sufficient capability to address the questions posed in their benchmark?",
            "- The overarching weakness of the paper is that the author's failed to discover where their method shows discriminatory results in simulatability. I think that until this happens the story of the paper doesn't feel finished to me. I think that some experiments which would really help are those which show that the method can actually convey some actionable information for the LLM, just to show that some insightful results can be gleened from ALMANACS and how exactly to setup tests to do that, so that future researchers using this understand how to use it in their experiments.\n\nThe issue is, if I were to literally give random noise to the LLM instead of the explanations it would also likely do nothing, so you have to show some observable effect from your framework to validify it.",
            "- 1. The results are mostly negative. While this is somewhat expected for a challenging benchmark, it is unclear what the takeaways are from the negative results. For example, when the GPT-4 interpreter fails to predict a model prediction based on the explanations (of other related inputs), is it due to the irrelevance of the training inputs (in which case the embedding model is too weak), or the inability of the GPT-4 model in using the explanation information (in which case the model or the prompting may need to be changed), or the low quality of the explanations? It seems to me that only the last case truly highlights the issue of the explanations, and even so, there is no further insight on identifying the specific aspect of the explanation causing the low quality. \n\n2. I notice that both models studied are quite small, flan-alpaca-gpt4-xl with 3B parameter and vicuna-7b-v1.3 with 7B parameters. They are also released over 1 year ago, which is quite old in the context of recent LLM development. I would recommend trying the newer and larger models, such as llama 3(.1), or the closed source models from OpenAI or Anthropic (without the attention or integrated gradient access). \n\n3. Accuracy or F1 score is not included as the metrics. I tend find them more intuitive, compared to metrics such as KLDiv. \n\n4. There are some missing recent related works on studying the self-explanations (i.e., rationales) of LLMs, such as [1] and [2]. Both work show mixed results, echoing the general conclusions by this paper. \n\n[1]. Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. Huang et al. \n\n[2]. Are self-explanations from Large Language Models faithful? Madsen et al."
        ]
    },
    "wYVP4g8Low": {
        "venue": "ICLR 2025",
        "title": "Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture",
        "link": "https://openreview.net/forum?id=wYVP4g8Low",
        "abstract": "The widespread use of multilayer perceptrons (MLPs) often relies on a fixed activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden layers. While effective in many scenarios, this uniformity may limit the network\u2019s ability to capture complex data patterns. We argue that employing the same activation function at every node is suboptimal and propose leveraging different activation functions at each node to increase flexibility and adaptability. To achieve this, we introduce Local Control Networks (LCNs), which leverage B-spline functions to enable distinct activation curves at each node. Our mathematical analysis demonstrates the properties and benefits of LCNs over conventional MLPs. In addition, we demonstrate that more complex architectures, such as Kolmogorov\u2013Arnold Networks (KANs), are unnecessary in certain scenarios, and LCNs can be a more efficient alternative. Empirical experiments on various benchmarks and datasets validate our theoretical findings. In computer vision tasks, LCNs achieve marginal improvements over MLPs and outperform KANs by approximately 5%, while also being more computationally efficient than KANs. In basic machine learning tasks, LCNs show a 1% improvement over MLPs and a 0.6% improvement over KANs. For symbolic formula representation tasks, LCNs perform on par with KANs, with both architectures outperforming MLPs. Our findings suggest that diverse activations at the node level can lead to improved performance and efficiency.",
        "decision": "Reject",
        "review scores": [
            5,
            1,
            3,
            3
        ],
        "strengths": [
            "- - The method, though loosely inspired by Kolmogorov\u2013Arnold Networks (KANs), is novel and presents an interesting approach to adaptive activation functions.\n- The mathematical formulation is thorough, and the description is clearly articulated, making the technical details easy to follow",
            "- 1. using variable activation functions has been recently popularized by KANs but KANs have a high computational burden. The proposed methods seems to be more computationally efficient\n2. local support property for localized updates and robustness to input perturbations is important in certain areas.",
            "- The paper is very well written and, mostly, easy to follow. The question whether distinct activation functions confer a benefit to neural networks is an interesting, albeit theoretical question (research into architectures and activation functions has become of less interest, since the ML community realized that compute/scale is the single most important performance factor).",
            "- 1)  The use of B-spline functions for neuron-specific activation provides a novel way of allowing each neuron to adapt its behavior, which is promising for capturing complex and localized patterns in data.\n\n2) The non-linearity is preserved. The issue with ReLU, where the neurons get stuck with zero gradient, is resolved here. The vanishing gradient issue faced with tanh and sigmoid functions is also resolved here.\n\n3) The dropout mechanism is also automatically implemented with B-spline, reducing the unexpressive nodes to zero.\n\n4)  The paper provides empirical results comparing LCNs with KANs and MLPs across multiple benchmarks, including basic ML tasks, computer vision datasets (MNIST, FMNIST), and symbolic regression tasks.\n\n5) The local support property of B-splines reduces the impact of irrelevant neurons on the gradient, leading to a form of regularization that helps improve generalization and potentially reduces overfitting.\n\nOverall, the idea is certainly interesting and seems to have some compute advantages over KANs and accuracy advantages over MLPs."
        ],
        "weaknesses": [
            "- - Comparisons with MLPs and KANs show only marginal improvements. The limited performance gains cast doubt on the practical utility of LCNs, given their added complexity.\n- Throughout the paper, the authors claim that LCNs improve explainability, yet they do not provide any concrete example to illustrate how  LCNs would be more interpretable than other methods. A real example would significantly strengthen this claim.\n- The authors present theoretical arguments for efficiency, such as sparse gradient updates, but there\u2019s no indication of how this translates to actual hardware efficiency. Theoretical sparsity may not correspond to measurable hardware speedups, which is a critical consideration for practical use.\n\n\n**Minor**\n- While the authors suggest LCNs are \"simple,\" the architecture is still complex compared to conventional activation function setups in MLPs.\n- There are a few typos (e.g. \"putting it\" on line 51 -> \"putting them.\" (?))\n\n**Recommendations**\n- To give a more comprehensive view of the method\u2019s efficacy, maybe you could experiments with MLPs that use other activation functions, such as Swish or Mish .\n- It would help if Figure 1 also included a representation of the KAN architecture for comparison, which would contextualize how LCNs differ visually and structurally from KANs.",
            "- 1. While the paper has been written in a beginner-friendly manner, almost 2 pages are dedicated to writing out the simple chain rules derivative expressions which most readers familiar with ML would be aware of. It would have been better to use that space to provide more experimental details.\n2. Sevaral claims like \" LCN exhibited faster learning\" aren't backed up by numbers.\n3. KANs have been shown to not work well for vision tasks, once the problem complexity increases (https://arxiv.org/abs/2407.16674), why won't LCNs suffer from the same issue? Especially given the current experiments which are extremely basic and problems where MLPs achieve near-perfect accurary.\n4. Fig 3 arbitrarily stops the number of parameters for MLPs at a low value while scaling the same for LCNs\n5. \"This flexibility improves the network\u2019s capacity to capture both global and localized data patterns, resulting in enhanced accuracy\nand efficiency across a range of tasks.\" -- The paper doesn't really show any performance (accuracy or otherwise) improvement over MLPs, so these claims need to be seriously reconsidered.",
            "- The empirical results do not match the confident presentation. The performance of LCNs is mixed, sometimes better / sometimes worse than MLPs and KANs. The analysis suggests explanations and theoretical insights without going into any real detail. Many sections read like they were written by an **LLM** trying to convince, rather than actually understand and explain - see questions below. See, e.g., line 195: 'The authors define ... to support the use in the LCN model.' (seriously?)\n\nMLPs with ReLUs have universal approximation capabilities, so it does not makes sense to argue for a need for more flexible nonlinearities.\n\n49: The motivation of KANs was not to provide more flexible activation functions.\n\n53: multiple activation functions also coexist in KANs.\n\nFig 1: top left, typo",
            "- 1) The paper could be written better. There are lots of repeated lines and not enough explanation. Many of the notations are not explained in terms of what they represent, specially in the equations.\n\n2) Figure 1, with comparisons between MLP and LCN, was well formed. I would have liked to see the comparison between LCN and KAN in a similar way, as it is the SOTA work being referred to in every section.\n\n3) Explanation and visualization of B-spline could have been given (optional).\n\n4) While it is mentioned that LCNs are more computationally efficient than KANs, the empirical evidence supporting this is minimal, and the figures comparing LCNs and KANs lack sufficient metrics. There is no detailed figure-wise analysis of how B-spline activations compare with KAN's univariate function combinations.\n\n5) There is no ablation study that explores the impact of different B-spline configurations (e.g., degree of the spline, number of basis functions) on the performance of LCNs. Such a study would be critical to understand the role of the various components in the model's success.\n\n6) The numbers for experiments in symbolic representation tasks are not given. It is just mentioned in text that the LCN performs superior. It would be useful to say the margin by which it will perform better.\n\n7) The experiments are wrt to the LCN, and MLP mostly. It would be interesting to see the difference in expressive power between CNNs and LCNs in the image classification tasks with MNIST and FMNIST."
        ]
    },
    "w5pErXbwQl": {
        "venue": "ICLR 2025",
        "title": "Noise-Robust Preference Losses for Deep Regression Models",
        "link": "https://openreview.net/forum?id=w5pErXbwQl",
        "abstract": "Deep regression models are widely employed for tasks such as pricing and forecasting. In industrial applications, it is common for analysts to adjust model outputs before they are deployed in commercial products. These adjustments, which we name \"analyst influences\", not only ensure the quality of the final products but also provide training data to improve model performance over time. However, due to the huge volumes of data, analyst influences can be applied broadly and can lack precision, hindering training effectiveness. To resolve the issue, we propose a novel framework Preference Learning from Analyst Influence which creates a weighted loss function that explicitly accounts for the relative quality levels of the training samples in comparison to model outputs. This approach effectively mitigates the impact of coarse training instances. Our extensive experiments on real-world data drawn from airline revenue management demonstrate that the proposed framework not only enhances pricing stability but also improves alignment with analyst influences compared to baselines.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- 1.\tThe paper is well-structured and clearly written. The authors have successfully communicated complex concepts in a manner that is accessible to readers.\n2.\tThe paper raises an interesting question. How does real-world analyst intervention (or \u2018analyst influence\u2019) affect the effectiveness of model training, and how does adjusting to the level of quality of the training samples relative to the model output mitigate the impact of rough training examples.\n3.\tThe structure and iconography of the paper is clear.",
            "- 1. The paper investigates a real-world problem and models the problem well. The problem setting is interesting.\n\n2. The proposed method utilizing the analyst influence data is well-motivated in this setting. The PLAI fits the practical problem.\n\n3. Experiments on industry data are well-designed.",
            "- The paper is well-organized, and the proposed loss function includes some theoretical analysis"
        ],
        "weaknesses": [
            "- 1.\tThe paper is not sufficiently experimental. For example, the richness of the data is insufficient. The paper compares PLAI with several baseline methods, but it could benefit from a comparison with other state-of-the-art methods or recent advances in robust regression techniques. This would provide a more comprehensive. understanding of PLAI's performance relative to the current research landscape.\n2.\tThe background research for the article was insufficient and it is suggested that the literature review section should be expanded to cover more existing work relevant to the research topic. This includes recent research developments, classic papers, and high-quality work that is widely recognized in the field.\n3.\tThe link to the experiment is not given, are the results is not verifiable.",
            "- 1. The problem is orginated from practical problem, as a result, the background should be stated clearly. The background is not so easy to understand in section 3.3. It is not so clear to interpret Figure 2.\n\n2. The contribution of this proposed method is not so obvious. The proposed method seems to be a continual learning for deep regression tasks. The distinction and novelty of this paper should be emphasized.\n\n3. Baselines only cover MAE, MSE, et al., and some basic losses; more recent works in this field should be compared to make the work more sound.\n\n4. Experiments on open datasets should be conducted to ensure the reproducibility of this work. Table 1 should show statistical significance.",
            "- W1: The proposed method lacks novelty. Weighted loss functions have been extensively studied, and the potential application and contribution of the proposed PLAI loss function are limited.\n\nW2: The authors spend half a page explaining \"airline revenue management\" and \"Bid price prediction,\" which is a digress from the main subject and does not interest most readers.\n\nW3: There is no comparison with state-of-the-art models; only different loss functions were compared. Additionally, the proposed PLAI loss does not show significant improvement in influence accuracy. Compared to MAE loss, the proposed PLAI improves influence accuracy by only 2%."
        ]
    },
    "vw0NurJ7UX": {
        "venue": "ICLR 2025",
        "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
        "link": "https://openreview.net/forum?id=vw0NurJ7UX",
        "abstract": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60\u00d7 to 2.81\u00d7 faster than FP16 models and exceeds QuaRot models by 1.2\u00d7 to 1.3\u00d7.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- - The authors showed the possibility that per-tensor static quantization can outperform per-token dynamic quantization.\n\n- They measured the real time-to-first-token (pre-filling) speed-up.",
            "- - I like the fact that the paper reports wall-clock inference speed on various devices (rtx 3090 and a100). This is missing from many quantization works, due to the difficulty of implementing kernels, but is nevertheless much needed.\n\n- The presentation is clear and the visualizations are well-prepared.\n\n- The generative quality of the method has been carefully measured, with many ablation studies.",
            "- The paper describes a novel method that deals with the known outlier problem for quantization on the token dimension. The proposed method carries simplicity and novelty in its current description, and also has a low-cost when executing it in practise."
        ],
        "weaknesses": [
            "- (1) The authors merely showed the effectiveness of PrefixQuant with per-tensor static quantization only $\\textbf{when the context length is 2048}$ (Table 2, 3, 4, 5, and 6). Since 2048 context length is relatively short, per-tensor static quantization might work. However, when the context length is 8192, for example, the activation size would be 8192 (context length) $\\times$ 4096 (model hidden size) = 33554432. Then, even if using 8-bit per-tensor static activation quantization, 33554432 / 256 (8-bit) = 131072 numbers have to be represented in only a single integer on average, which would naturally incur more severe quantization error than when the context length is 2048. In other words, in the case of per-tensor static activation quantization, as the context length goes longer, the larger numbers have to be represented in only a single integer on average, thus causing per-tensor static quantization to perform worse.\n\nHowever, in the case of per-token dynamic quantization, no matter how long the context length is, just 4096 (model hidden size) / 256 (8-bit) = 16 numbers have to be represented in only a single integer on average. Considering that many long-context LLMs are sought-after these days, it is necessary to compare PrefixQuant with per-tensor static quantization with previous per-token dynamic quantization methods like QuaRot when the context length is 8192 or longer. Without the comparison in a long-context setting, it is not convincing that PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization (mentioned in Abstract).\n\n(2) The paper focuses on perplexity and common sense reasoning tasks as the performance measure. More experiments are required to assess the effectiveness of the proposed method on broader challenging subjects like MMLU.",
            "- - The biggest concern is the conceptual and technical novelty of the proposed method. As the authors mention in section 2, the idea of adding prefix tokens to mitigate the outliers has been already explored by two prior works: QFeP (Yang et al., arXived May 2024), and CushionCache (Son et al., EMNLP 2024). In particular, the central claim of this paper, i.e., such prefix makes the static quantization useful, has already been argued by CushionCache. If I understood correctly, it seems like the authors are claiming that there are two differences to these works. (1) PrefixQuant requires less computation than predecessors for optimizing the prefix, and (2) PrefixQuant outperforms these methods. The advantage (1) does not seem to be very critical practically (as these are one-time cost), and does not originate from a particularly technically novel component. The advantage (2) seems to come mainly from additionally considering Hadamard rotation, grid search, and block-wise fine-tuning, which are not original contributions of this paper. In fact, CushionCache already demonstrates that their method can be combined with Hadamard rotation to further boost the performance.\n\n- It seems like the paper is claiming that the prefix plays a complementary role to Hadamard rotation, by arguing that Hadamard rotations are for addressing \"channel-wise outliers\" and the prefix are for addressing \"token-wise outliers.\" However, I find this point very unclear and misleading, because previous empirical observations suggest that for many LLaMA-like models the outliers are localized in terms of both channels and tokens (e.g., Sun et al., COLM 2024). Thus, removing channel-wise outliers should also resolve token-wise outliers, logically. I request for a more concrete justification.\n\n- The authors could have included evaluations on more realistic tasks, such as GSM-8k or MMLU.\n\n- Looking at table 18, the claim that static per-tensor quantization by PrefixQuant outperforms existing dynamic quantization methods does not seem to be 100% true. At W8A8-like quantization on overparameterized models, i.e., with only very small degradation in performance, I still observe that QuaRot consistently outperforms PrefixQuant w/o FT. It seems likely that QuaRot+FT may also outperform PrefixQuant+FT.",
            "- 1. It is not super clear how the proposed method work, specially, I do not really understand why prefix certain outlier tokens in the KV cache can prevent the generation of new outlier tokens. I actually do not really understand whether there is a skipping mechanism in the autoregressive of generation or the authors suggest this would make the KV cache more quantization friendly. I would doubt the effectiveness of the method if they mean the later.\n2. The performance without fine-tuning is actually not super strong, especially on the 70B models, it is actually maybe better if the author can change Table3 to add a column to indicate whether these other methods are fine-tuned or not so that the readers can understand the results better.\n3. I doubt the run-time numbers in Table 5 continues to show advantages when the models are scaled to 70B. When models are memory-bound, whether it is a dynmaic/static quantization does not matter too much since most of the time are spent on loading the weights from HBM so that the arithmetic units on GPUs are under-utilized anyway. Do authros have results with 70B models, and do they still observe a speedup? If not, it is better to make sure these limitaitons are clearly addressed in the paper."
        ]
    },
    "vgvnfUho7X": {
        "venue": "ICLR 2025",
        "title": "Beyond accuracy: understanding the performance of LLMs on exams designed for humans",
        "link": "https://openreview.net/forum?id=vgvnfUho7X",
        "abstract": "Many recent studies of LLM performance have focused on the ability of LLMs to achieve outcomes comparable to humans on academic and professional exams. However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results. We seek to look more deeply into the question of how and whether the performance of LLMs on exams designed for humans reflects true aptitude inherent in LLMs. We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking. We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil. With respect to the evaluation of LLM abilities, we show that the tools of Item Response Theory (IRT) provide a more informative evaluation of model performance than the usual accuracy metrics employed in previous studies. Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in \u201chuman-like\u201d patterns versus LLMs that do not. We also show how to quantitatively identify cases in which exam results are not reliable measurements of an LLM's ability. Using the tools of IRT we can also identify specific questions that appear to be either much easier, or much harder, for machines than for humans, and we give some reasons for those differences. Overall, our study shows that the conventional focus on accuracy as the primary performance metric for LLM studies does not allow us to deeply understand the true capabilities of LLMs and compare them to that of humans. Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. The paper is well-organized and clearly articulates both the limitations of using accuracy as the sole metric and the benefits of IRT for evaluation. Diagrams and data tables effectively support the paper\u2019s arguments, making complex psychometric methods accessible to a broader audience. \n2. The paper employs rigorous experimental design and utilizes a comprehensive dataset, enhancing the robustness of its findings. By analyzing various models, including GPT-3.5 and LLaMA variants, the authors demonstrate the generalizability of IRT\u2019s applicability. The study further uses well-defined psychometric to validate its claims, supporting the soundness of the technical approach.\n3. This work holds significance as it points out weaknesses in LLM evaluations. By moving beyond accuracy, the paper demonstrates that psychometric techniques can better represent model abilities quantitatively.",
            "- The paper is well written.",
            "- 1. The study emphasizes the importance of construct validity, highlighting potential limitations of existing exams in measuring LLM abilities, thereby promoting a deeper understanding of LLM performance.\n2. By employing IRT, the research provides a more nuanced analysis of LLM performance, distinguishing between human-like and non-human-like response patterns, which leads to more reliable ability assessments.\n3. The study leverages a dataset of over 5 million student performances, providing a strong empirical foundation for analyzing LLM behavior, which enhances the credibility of the findings."
        ],
        "weaknesses": [
            "- The paper has several notable shortcomings. \n1. Firstly, the idea of using psychometrics and IRT to replace traditional metrics like accuracy in AI benchmarking was proposed well before 2021, diminishing the novelty of the approach. \n2. The use of IRT to compare the response patterns of LLMs with those of humans has already been widely explored in existing research.\n3. The technical methods employed in the paper, such as IRT and Fisher information maximization, are already extensively applied in AI evaluation, further reducing the originality of the study's methodology.\n\nPresentation needs to be polished , and it remains some typos. \nResults section, line 271 , \u201cLMM\u201d may be a typo\nMethods section, line 237, \u201crun\u201d -> \u201dran\u201d",
            "- To me, Figure 1 suggests that accuracy and ability estimates and highly correlated.\n\nThis approach seems to have been already studied by: Liu, Yunting, Shreya Bhandari, and Zachary A. Pardos. \"Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis.\" arXiv preprint arXiv:2407.10899 (2024). I agree that this paper is recent, though.\n\nThe approach of using IRT for making more efficient benchmarks seems to be taken by Polo et al. (2024) and Zhuang et al. (2023), papers cited by the authors. However I do not feel that considering a IRT model trained on human responses, as stated by the authors, can be considered enough novel. Plus, the way the estimation of IRT parameters (LLM ability estimates) is done (if it depends on a prior, then it is biased) can hinder the reproducibility of results and the validity of findings.",
            "- 1. The paper relies on a single dataset for evaluating LLMs, which introduces bias. Given the complexity of large models, a more comprehensive evaluation is necessary, making it essential to use a variety of datasets for assessment.\n2. The paper exclusively employs the IRT model for assessment. However, there are many other cognitive diagnostic models available that can evaluate learners' abilities, such as MF[1], MIRT[2], and NCD[3]. The authors should explore these alternative models in greater depth to provide a more robust evaluation framework.\n3. The paper's technical innovation appears to be limited, primarily focusing on using IRT to evaluate LLMs. The methods employed mainly rely on prompting techniques, which do not demonstrate significant advancements in the evaluation approach.\n\n[1] Andreas Toscher and Michael Jahrer. Collaborative fltering applied to educational data mining. KDD cup, 2010.\n\n[2] Mark D Reckase. Multidimensional item response theory models. In Multidimensional item response theory, pages 79\u2013112. Springer, 2009.\n\n[3] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying Chen, Yu Yin, Zai Huang, and Shijin Wang. Neural cognitive diagnosis for intelligent education systems. In Proceedings of the AAAI Conference on Artifcial Intelligence, pages 6153\u20136161, 2020."
        ]
    },
    "vVlNBaiLdN": {
        "venue": "ICLR 2025",
        "title": "ESMGain: Effective and Efficient Prediction of Mutation\u2019s functional Effect via ESM2 Transfer Learning and robust Benchmarks",
        "link": "https://openreview.net/forum?id=vVlNBaiLdN",
        "abstract": "Functional effect prediction of mutations, especially for properties like catalytic activity, holds greater significance for clinicians and protein engineers than traditional pathogenicity predictions. Recent approaches leveraging static ESM1 embeddings or multimodal features (e.g. embeddings, structures, and evolutionary data) either (1) fall short in accuracy or (2) involve complex preprocessing pipelines. Moreover, functional effect prediction suffers from (3) a lack of standardized datasets and metrics for robust benchmarking. We address these challenges by systematically optimizing ESM2-based functional effect prediction: Through extensive ablation studies, we demonstrate that fine-tuning significantly outperforms static embeddings, scaling laws for model size are non-transferable and LoRA matches full fine-tuning performance, deviating from trends observed in natural language processing. Our framework, ESM-Effect, fine-tunes 35M ESM2 layers with an inductive bias regression head achieving state-of-the-art performance. It slightly surpasses multimodal competitor PreMode indicating redundancy in structural and evolutionary features. We further propose a benchmarking framework featuring robst test datasets and strategies, and the relative Bin-Mean Error (rBME), as a metric designed to emphasize prediction accuracy in challenging, non-clustered, and rare gain-of-function regions. rBME better reflects model performance compared to commonly used Spearman\u2019s rho, as evidenced by improved plot-based analyses. As ESM-Effect exhibits mixed transferability to different unseen mutational regions, we identify multiple areas for improvement such as finer-grained pretraining strategies.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. The authors introduce important ideas for better evaluating fine-tuned models: (a) evaluating models on completely held out proteins and (b) developing a metric that prioritizes performance on LoF and GoF variants over neutral variants.\n2. Their fine-tuning approach demonstrates superior performance compared to existing methods, such as PreMode and augmented versions of unsupervised models.\n3. Through ablation studies, the authors establish that using larger versions of ESM2 does not significantly improve performance and that employing separate models for reference and mutant sequences provides some benefits.",
            "- 1. The proposed method performs the best for functional effect prediction in the dataset.\n2. The methodology of ESMGain can predict functional effects without the limitation of feature redundancy and task specificity.",
            "- 1) The paper is well-structured and clearly written.\n\n2) The proposed method achieves state-of-the-art performance on selected datasets in functional effect prediction.\n\n3) By employing two independent ESM2 models to embed wildtype and mutant sequences separately, the paper addresses potential information loss in mutation representation, enhancing the model\u2019s ability to capture subtle differences. Ablation studies demonstrate that only using ESM2 embeddings effectively captures most of the relevant information on DMS datasets,  effectively reducing the reliance on additional data modalities.\n\n4) The paper proposes a novel benchmarking framework for functional effect prediction incorporates a cross-protein generalization test within the same protein family.",
            "- 1. The generalization test is of interest. In Figure 3, authors show the different distribution of labels for two different proteins from the same family and convincingly show why generalization between proteins (even in the same family) is not easy."
        ],
        "weaknesses": [
            "- 1. Limited dataset evaluation: The authors do not evaluate their method on the large compendium of DMS datasets that are available in ProteinGym (217 datasets covering 2.5 million mutations), instead focusing on only 5 datasets (Figure 2). To convincingly prove that their fine-tuning approach outperforms existing methods, they should expand their analysis to more datasets.\n\n2. Insufficient comparison to existing fine-tuning approaches: PreMode and augmented unsupervised models are not the only approaches that have been proposed to fine-tune protein language models on DMS datasets. See https://www.nature.com/articles/s41467-024-51844-2 and https://arxiv.org/pdf/2405.06729. These papers explore strategies such as parameter-efficient fine-tuning and fine-tuning jointly on multiple DMS assays that this paper does not consider. In particular, the approach proposed in the second paper listed above shows improved performance on entirely held out proteins, which is in stark contrast to the poor generalization to new proteins exhibited by ESMGain in Fig. 4. \n\n3. While the idea to compute separate correlation metrics for LoF, neutral, and GoF variants is clever, the method of dividing variants into these categories by splitting the ground-truth scores into thirds is arbitrary. A more robust method, such as a Gaussian mixture model with three components, could provide a more principled assignment of variants to these classes.",
            "- 1. The organization needs improvement. Some terms like \"PTEN\" didn't have full names. The size of font in those figures is too small to read and is not consistent. The section 7 should be in the section of the experiential setup.\n2. In Fig4, \"LoF, Neutral and GoF\" in captions should be the same as the text in x axis of figure. How about the performance in all other baselines like competitor PreMode in Fig4?\n3. Have you conducted multiple train-test split seeds in ablation study of ESMGain? Why the result of the original ESMGain in the ablation study is different from the one in Fig2? Do they use different datasets or strategies to train and test?",
            "- 1) The novelty of this paper is limited. The use of dual ESM2 embeddings to separately represent wildtype and mutant sequences, along with the introduction of the Harmonic Spearman metric to address label imbalance, appears more incremental than groundbreaking.\n\n2) It seems that the motivation of the proposed benchmarking framework is underdeveloped. While focusing on cross-protein generalization within the same family is technically interesting, it lacks a clear connection to real-world situations where this type of evaluation would be essential.\n\n3) While ESMGain performs well on the tested DMS data, its generalization to other samples within the same protein family is weak (cross-family tests). The model may be overfitting in the specific training proteins.",
            "- 1. Paper is poorly structured, making it very hard to read:\n\n\t- Introduction contains contents which would better fit to related work or background (\u201cNotably, PreMode was pre-trained to predict the binary measurement of \u201cpathogenicity\u201d for 4.7 million mutations and uses AlphaFold2\u00a0predicted protein structure, Multiple Sequence Alignments (MSAs) and pre-trained ESM2 650M embeddings as features (John Jumper, 2021).\u201d) And it also presents some results and their discussion (\u201cThat leads us to hypothesize that the signal provided by protein structure, MSAs, and embeddings is largely redundant for the task of effect prediction. PreMode\u2019s ablation studies show minimal performance drop when any of these modalities is ex- cluded, suggesting that they capture overlapping information for functional effect prediction. This explains ESMGain\u2019s superior performance in turn: its fine-tuned embeddings are task-specific and the single modality avoids the redundancy.\u201d). I suggest honoring the usual structure of the paper and using introduction just for motivation and a very brief (not so detailed) teaser for the contributions of the paper.\n\n    - Chapter 4, which should be describing the technical novelty and the method does not provide that many details, for example Figure 1 illustrating the method is never referenced in the text. I suggest to use a figure and equations to better describe the regression head, instead of the textual description at the end of section 4.2.\n\n    - No table summarizing results. The reported numbers are scattered across text and some figures, making it very hard to get a glimpse of the results. I suggest a more transparent summarization of the results, such as by using a table.\n\n2. Poor formatting of the paper.\n\n    - Authors are not economical with the space by being sometimes too verbose, repetitive in repeating their contributions or for example by wasting the whole first page just on abstract. Being more economical would enable the authors to make bigger figures which have too small fonts and are hard to read. I suggest making figure large enough so the fonts can be legible. \n\n    - References are poorly formated. Some references starting with \u201c\u2026\u201d. AlphaFold referenced as \u201c(John Jumper, 2021)\u201d - note that AlphaFold was a collective effort. I suggest proper citing and formatting of references.\n\n3. Insufficient literature survey. Authors only have 13 references. I suspect authors were trying to fit into the page limit of 10 pages including references - this is not necessary references dont count in the page limit. I suggest making proper literature survey and crediting relevant work. For example, I miss the reference to ProteinGym, arguably one of the most influential benchmarks in this area.\n\n4. Insufficient benchmarking. Authors only focus on the comparison to PreMode (which was still not peer reviewed) and only compare on 5 proteins. I suggest to compare for example to AlphaMissense as well.\n\n5. The key contribution of having separate ESM2 heads for wildtype and for the mutated sequence is questionable. Authors claim this to give them the key improvement by the underlying inductive bias. To me it is not clear how to decide what is wildtype and what is mutation. What if the mutation is adopted by evolution and becomes the \u201cnew wildtype\u201d and then gets mutated again? There is no fundamental reason to distinguish between the sequences. So I believe that using the distinction between the sequences based on the dataset definition and then adapting the two heads to this definition only leads to overfitting to the dataset, potentially explaining any benefit gained from these separate heads. I dont have a concrete suggestion how to prove authors point, because I think the point is wrong. If authors stand by their point they should present convincing evidence supporting that their \u201cinductive bias\u201d is not just overfitting to the dataset definition of what is wildtype and what is mutation.\n\n6. The model seems to improve over PreMod on just 2-3 out of 5 proteins (Figure 2), this does not seem very convincing. My suggestion would be to get other datasets (maybe something relevant could be found in ProteinGym) and show improvement on other dataset as well.\n\n7. The Harmonic Spearman is just introduced at the end of the paper and not motivated well enough. Could authors explain the choice of using harmonic average? Could authors clearly compare harmonic spearman to normal spearman? How does it change the evaluation of all the benchmarked models? A table summarizing the results (as suggested in Weak point 1) would help.\n\n\nI suggest to reject this paper for the following reasons. (i) The paper is not is well placed in literature, comparison to AlphaMissense is missing and the survey of the related work is not sufficient. (ii) The key contribution of using two separate ESM2 models for the wildtype and the mutated sequence is questionable and the claim of bringing a useful inductive bias is not supported by strong evidence, the improvement coming from this choice might be due to overfitting to the dataset definition of what is mutant and what is original sequence. (iii) The results dont seem as strong, only showing improvement for 2-3 out of 5 proteins. More convincing evaluation using other dataset would be necessary. (iv) The technical novelty of separate fine-tuning of two ESM models with a custom regression head is limited. (v) The writing is poor, making it hard for the reader to asses the contributions, the results of the method and its placement in the literature."
        ]
    },
    "uuCcK4cmlH": {
        "venue": "ICLR 2025",
        "title": "IDS-Agent: An LLM Agent for Explainable Intrusion Detection in IoT Networks",
        "link": "https://openreview.net/forum?id=uuCcK4cmlH",
        "abstract": "Emerging threats to IoT networks have accelerated the development of intrusion\ndetection systems (IDSs), characterized by a shift from traditional approaches\nbased on attack signatures or anomaly detection to approaches based on machine\nlearning (ML). However, current ML-based IDSs often lack result explanations\nand struggle to address zero-day attacks due to their fixed output label space. In\nthis paper, we propose IDS-Agent, the first IDS based on an AI agent powered\nby large language models (LLMs). For each input network traffic and a detection\nrequest from the user, IDS-Agent predicts whether the traffic is benign or being\nattacked, with an explanation of the prediction results. The workflow of IDS-Agent\ninvolves iterative reasoning by a core LLM over the observation and action gen-\neration informed by the reasoning and retrieved knowledge. The action space of\nIDS-Agent includes data extraction and preprocessing, classification, knowledge\nretrieval, and results aggregation \u2013 these actions will be executed using abundant\ntools, mostly specialized for IDS. Furthermore, the IDS-Agent is equipped with\na memory and knowledge base that retains information from current and pre-\nvious sessions, along with IDS-related documents, enhancing its reasoning and\naction generation capabilities. The system prompts of IDS-Agent can be easily\ncustomized to adjust detection sensitivity or identify previously unknown types\nof attacks. In our experiments, we demonstrate the strong detection capabilities\nof IDS-Agent compared with ML-based IDSs and an IDS based on LLM with\nprompt engineering. IDS-Agent outperforms these SOTA baselines on the ACI-IoT\nand CIC-IoT benchmarks, with 0.97 and 0.75 detection F1 scores, respectively.\nIDS-Agent also achieves a recall of 0.61 in detecting zero-day attacks.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3
        ],
        "strengths": [
            "- - The IDS-Agent is the first LLM agent designed specifically for intrusion detection.\n- The author presents a clear pipeline regarding the design of IDS-Agent.",
            "- * IDS-Agent is the first LLM-powered agent for intrusion detection, with capabilities for explanation, customization, and zero-day attack detection\n* It uses a reasoning-followed-by-action pipeline, where the core LLM decides the optimal tools for data processing and results aggregation, especially utilizing RAG for retrieving external knowledge\n* IDS-Agent outperforms existing ML-based IDSs in detection accuracy and provides better interpretability; it can also effectively follow sensitivity instructions and detect zero-day attacks",
            "- The paper is well-written and presents some good ideas on the hot topic of using LLM. The main contributions are zero-day threat detection and the ability to provide explainability over the decisions."
        ],
        "weaknesses": [
            "- I appreciate that the authors clearly present the proposed method design and consider the use of the LLM agent for intrusion detection. However, this work has the following major weaknesses.\n- The contribution of LLM in the proposed method is very limited. It seems that only in the aggregation-related stage, LLM shows a weak contribution compared to the traditional voting mechanism.\n- In the experimental results (Table 1), Majority Vote achieves very close performance on the CIC-IoT 23 dataset and even higher performance on the ACI-IoT 23 dataset. Therefore, I am worried whether LLM for aggregation can work as the authors expect.",
            "- * The core detection capability of IDS-Agent still derives from the ML models in the toolbox; in other words, I think the main contribution of IDS-Agent is to propose a stronger ensemble method by utilizing LLMs and external knowledge (with RAG). However, due to some issues regarding the framework design (see Q1-4), it feels that the LLMs do not play a significant role in the process except for outputting human-understandable explanations by texts. As for the advantage of interpretability the authors claim, the final output does not give sufficient explanation as the readers might assume (see Q5). Besides, most of the components in IDS-Agent (pipeline, long-term memory, RAG) use existing approaches with no big changes specialized for this scenario. In general, the contribution of the paper might be reduced by these concerns.\n\n* The paper does not include a comprehensive survey about related work in the field of network intrusion detection. A non-exhaustive list is below. Specifically, I recommend the authors survey not only AI-related conferences but also the big-4 security conferences. Accordingly, the evaluation will also be enhanced if IDS-Agent is compared to these SOTA methods specific to IDS rather than just general machine learning models. \n  - ML/DL methods: [a-c]\n  - Pretrained model methods: [d-f]\n  - LLM methods: [g,h]\n\n[a] Realtime Robust Malicious Traffic Detection via Frequency Domain Analysis, CCS 2022\n\n[b] Detecting Unknown Encrypted Malicious Traffic in Real Time via Flow Interaction Graph Analysis, NDSS 2023\n\n[c] HorusEye: Realtime IoT Malicious Tra\ufb00ic Detection Framework with Programmable Switches, USENIX Security 2023\n\n[d] ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification, WWW 2022\n\n[e] NetGPT: Generative Pretrained Transformer for Network Traffic, arxiv 2023\n\n[f] Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer with Multi-Level Flow Representation, AAAI 2023\n\n[g] HuntGPT: Integrating Machine Learning-Based Anomaly Detection and Explainable AI with Large Language Models (LLMs), arxiv 2023\n\n[h] ChatIDS: Explainable Cybersecurity Using Generative AI, arxiv 2023\n\n* In addition to the comparative experiment, there are some other issues regarding the evaluation (see Q6-10).\n\n* Minors:\n  - page 2, line 66: (Shinn et al., 2023)) -> (Shinn et al., 2023)\n  - page 4, Figure 1: Traffics -> Traffic",
            "- * The paper's main contribution is the application of LLMs for IoT Networks, which seems contradictory given IoT\u2019s focus on resource-constrained edge devices, where lightweight models, such as TinyML, are typically applied. Given these constraints, the authors should clarify the specific advantages of LLMs in this domain and provide practical examples to show how this approach can be beneficial.\n\n* On Line 56, the authors state, _\"particularly in safety-critical IoT scenarios where understanding the rationale behind alerts is crucial.\"_ The term \u201crationale\u201d needs clarification: does it refer to interpretability, explainability, or something unique to IoT networks? Also, it\u2019s unclear why understanding alert rationale is more critical in IoT scenarios than other network types. The authors should provide specific examples or cases illustrating why this distinction matters, particularly for IoT. Likewise, in Lines 297-299, the authors state that _\"this paper focuses on intrusion detection in the IoT environment, which presents more complexities and challenges than traditional networks.\" _ The authors should enumerate and explain these challenges relative to traditional networks.\n\n* In Lines 072-075, the authors describe a knowledge-based feature with long\u2014and short-term memory components that resembles a traditional LSTM approach. While interesting, comparing this method with conventional LSTMs would help clarify its unique contributions. How does this approach differ from an LSTM regarding performance, and what benefits does it bring? The authors could consider discussing metrics such as speed, memory use, and interoperability to clarify why their proposal is preferable to a standard LSTM.\n\n* \u201cFlowID\u201d is mentioned multiple times but lacks a formal definition. Since the definition of FlowID varies across sources, the authors should define it clearly to avoid ambiguity.\n\n* The paper\u2019s proposed method does not appear to operate in real-time, raising concerns about practical applicability. The authors should justify their choice of an offline approach that takes up to 8.65 seconds per instance for attack detection. For example, in a DDoS scenario, such a delay could result in extensive network downtime. The authors should discuss the potential impacts and outline the architectural and hardware challenges of implementing a real-time solution.\n\n* The authors should specify which features were selected for each dataset and conduct a detailed analysis regarding preprocessing. In particular, they should indicate which attacks were included in training and testing sets. The apparent randomness of selection could mean that similar attack types (e.g., flood attacks) appear in both training and testing sets, potentially skewing results.\n\n* In Lines 343-345, the authors mention constructing a knowledge base for IoT attacks using 50 online blogs and 50 research papers. It would be helpful if the authors could explain their criteria for selecting these sources to clarify the knowledge base\u2019s reliability and relevance.\n\n* The quantitative analysis section is underdeveloped. For example, further information on false classifications in the multi-class classification would help readers better understand where the model\u2019s challenges lie.\n\n* Additionally, in the quantitative analysis, the authors should benchmark their system against traditional IDS tools like Zeek, Suricata, and Snort. These rule-based systems are often effective against the standard attacks found in the datasets and could provide valuable points of comparison and complementary data sources for the proposed system. Additionally, discussing how LLM-enhanced IDS might complement these tools for a more comprehensive defense strategy would add depth.\n\n* The zero-day attack analysis is somewhat confusing. The authors use ACGAN to generate CAN protocol samples, but the paper focuses on IoT networks. The relevance of comparing CAN protocol attacks with IoT-related attacks, like HTTP slow loris, is unclear. The authors should clarify how these two attack types are analogous or justify why this comparison is valid.\n\n* In Lines 455-457, the authors state, _\"We set the threshold as 0.7 in our experiments.\"_ A more thorough analysis of this threshold would be beneficial. A performance curve across varied thresholds could offer insights into sensitivity and precision adjustments.\n\n* Additionally, the authors should consider comparing their proposal with traditional deep learning methods for zero-day attack detection. Since many deep learning (DL) models can perform attack detection in an unsupervised manner [1], such a comparison would contextualize the benefits of the proposed approach.\n\n* The performance evaluation should be expanded. The appendix mentions an 8.65-second processing time per sample, but more detail is needed. A breakdown of processing times for each stage\u2014preprocessing, analysis, information retrieval, and inference\u2014would be beneficial. Furthermore, discussing how the system would perform in a real-world setting would provide important context.\n\nReference:\n[1] Guo, Yang. \"A review of Machine Learning-based zero-day attack detection: Challenges and future directions.\" Computer Communications 198 (2023): 175-185."
        ]
    },
    "vK8C37eHXM": {
        "venue": "ICLR 2025",
        "title": "Sample what you can't compress",
        "link": "https://openreview.net/forum?id=vK8C37eHXM",
        "abstract": "For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate  jointly learning\na continuous encoder and decoder under a diffusion-based loss\nand showing that it can lead to higher compression and better generation.. \nWe demonstrate that this approach yields better reconstruction quality as compared to GAN-based\nautoencoders while being easier to tune. \nWe also show that the resulting representation is easier to model\nwith a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss.\nSince our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach ``Sample what you can't compress'', or SWYCC for short.",
        "decision": "Reject",
        "review scores": [
            6,
            3,
            1,
            3,
            3
        ],
        "strengths": [
            "- **Update after rebuttal:**\n\n```\nThe authors have addressed my questions. I agree with other reviewers that more visual samples and discussions about related works can be helpful. I will keep my rating as this work shows promising results for the strength of diffusion autoencoders for general images.\n```\n\n---\n\n\n1. Diffusion loss for autoencoder training is an important direction to explore. This work is one of the first works that show promising results.\n2. The proposed method outperforms prior GAN-based autoencoder on ImageNet with common metrics CMMD and FID. The trend of compression ratio also shows the advantage of the diffusion loss.\n3. The variance visualization is interesting and provides more insights about the what information are sampled.",
            "- 1. The authors provide full explanation of technical detail, including all architecture details and training process. \n\n2. Wide and well-explained ablation studies was conducted to explore the importance of various components of the model. The experiments devoted to exploration of number of denoising steps and cfg sclaes helps to understand the importance of correct choice of these parameters for quality improvements.",
            "- Adding an additional U-Net and using diffusion model to improve the performance is promising, it should be able to get better results.",
            "- - The paper provides extensive detail on the architectural design of SWYCC, including the detail structure of the encoder and decoder. However, while this information is valuable for understanding the framework, it is not the primary contribution of the paper and does not introduce novel architectural concepts.\n    \n- The inclusion of a classifier-free guidance experiment is a notable strength, as this approach introduces a new aspect to the diffusion-based decoder. However, it is important to note that the classifier-free technique employed significantly increases computation time during the sampling stage, effectively doubling it, which also poses challenges in practical applications.",
            "- 1. The proposed approach for image compression is simple and effective. It makes sense that it should work better than GAN based methods, as diffusion models beat GANs in many different applications. GANs are indeed incredibly difficult to train.\n2. The paper is overall clear and written well.\n3. There are several ablation studies."
        ],
        "weaknesses": [
            "- 1. The 2-stage pipeline makes the propose method a bit less compelling. The autoencoder is still supervised by MSE + LPIPS loss while the diffusion loss is more like for refinement (although it is joint training).\n2. The authors claim that the coarse reconstruction is just for speeding-up the training as the diffusion decoder \"should converge to true distribution\" (L209), I did not find in the paper for neither: (i) experiments that show without LPIPS loss, the model converges to similar performance; (ii) a theory that guarantees autoencoder with only diffusion loss learns similar representation as LPIPS autoencoder. The theory of diffusion models guarantees p(x|z) can be correctly modeled, but LPIPS should have impact on the latent representation z and thus may change the distribution p(x|z). Thus the claim is not well-justified to me.\n3. It would be better to have more qualitative visual comparisons. For example, for different compression rates, and with more diverse samples, to justify the improvement of the proposed method. FID may be overly-related to the LPIPS weight.",
            "- 1.There is lack of sufficient metrics for evaluation. It would be better to provide some editional metrics calculation such as LPIPS (https://richzhang.github.io/PerceptualSimilarity/) or Inception Score (https://arxiv.org/abs/1606.03498).\n2. Furthermore the authors provided limited evaluation datasets and comparison models. Additional comparisons with some state-of-the-art methods, such as DC-VAE (https://arxiv.org/pdf/2011.10063) or VAEBM (https://arxiv.org/abs/2010.00654) on some other datasets, such as LSUN (https://arxiv.org/abs/1506.03365) or CelebA-HQ-256  , would provide a better understanding of the quality of the model.\n3. Only low-resolution data. Conducting further experiments with higher resolution images would be beneficial to understand the capabilities of the model.",
            "- - What's the difference between the proposed method and those utilizing diffusion models for super resolution? I cannot see a big difference. \n- Yet the training method is not well explained, do you need to train $E$ and $D_{Initial}$? Or just $D_{refine}$ is trained?\n- In Sec. 3.1, it is said that the impact of $D_{Initial}$ is investigated in Table 1, which is not.\n- When displaying the reconstruction results, the raw inputs are necessary to see the difference.\n- Besides the figures, more quantitative metrics are necessary to justify the method.",
            "- - The paper includes too few image comparisons. It does not show many comparisons between different model compressions, making it difficult to evaluate how SWYCC stands relative to other approaches.\n    \n- There is no comparison with Stable Diffusion XL's VAE, which is one of the widely used VAE models.\n    \n- The method relies solely on matrices for image generation, which is not common in image reconstruction compression. While it is acceptable for the authors to report measurements they find appropriate, it is also important to include other widely used metrics in the field, such as PSNR, LPIPS, and rFID.\n    \n- The paper lacks mention and discussion of previous diffusion-based autoencoders, such as DiffusionAE ([link](https://diff-ae.github.io/)) and DiVAE ([link](https://arxiv.org/abs/2206.00386)). Care should be taken not to claim to be the first when there are multiple works prior to this.\n    \n- Training the decoder under diffusion loss is a concept introduced in 2022. Although the idea of using diffusion as a decoder has been recognized for its benefits, it has not gained popularity compared to GAN-based training primarily due to its higher computational cost. This concern is not adequately addressed in the paper.",
            "- 1. As far as I understand, the proposed approach is not novel. See [1] for example. The only differences that I see are some optimization/loss tweaks. If the authors could clarify the differences and why their approach is novel, that would be great. Currently, [1] is not discussed in the manuscript.\n\n2. A comparison with several previous methods is missing, specifically a comparison on the rate-perception-distortion plane [2]. When designing compression methods that only aim for minimal distortion (e.g., MSE), we usually compare them on the rate-distortion plane, as the authors did in figure 8. However, when designing high-perceptual-quality compression methods, we usually compare them on the rate-perception-distortion plane, as these three desired forces are at odds with each other [2].\n\n3. The authors only demonstrate their method on ImageNet. But what about simpler data sets, such as CelebA,CIFAR, etc.? Is the proposed approach still more effective than GANs?\n\n4. The limitations section is very limited. The authors discuss only the limitation of almost all diffusion methods: requiring a large number of inference steps to produce high-quality images. What about model size, training time, required data set size, etc., as compared to GAN based methods?\n\n[1] Konpat Preechakul et al., \"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation\", CVPR 2022.\n\n[2] Yochai Blau and Tomer Michaeli, \"Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff\", ICML 2021"
        ]
    },
    "vjbIer5R2H": {
        "venue": "ICLR 2025",
        "title": "Improved Risk Bounds with Unbounded Losses for Transductive Learning",
        "link": "https://openreview.net/forum?id=vjbIer5R2H",
        "abstract": "In the transductive learning setting, we are provided with a labeled training set and an unlabeled test set, with the objective of predicting the labels of the test points. This framework differs from the standard problem of fitting an unknown distribution with a training set drawn independently from this distribution. In this paper, we primarily improve the generalization bounds in transductive learning. Specifically, we develop two novel concentration inequalities for the suprema of empirical processes sampled without replacement for unbounded functions, marking the first discussion of the generalization performance of unbounded functions in the context of sampling without replacement. We further provide two valuable applications of our new inequalities: on one hand, we firstly derive fast excess risk bounds for empirical risk minimization in transductive learning under unbounded losses. On the other hand, we establish high-probability bounds on the generalization error for graph neural networks when using stochastic gradient descent which improve the current state-of-the-art results.",
        "decision": "Reject",
        "review scores": [
            1,
            8,
            3,
            1
        ],
        "strengths": [
            "- - Novel analysis of unbounded loss functions in the transductive learning setting\n- Mathematical rigour in deriving the theoretical bounds\n- Practical applications to GNN scenarios",
            "- This paper is the first to derive concentration inequalities for the supremum of empirical processes sampled without replacement for unbounded functions, presenting a novel result. Furthermore, these concentration inequalities are utilized to refine the risk bounds for transductive learning and graph neural networks found in the literature.",
            "- The transductive setting has gained renewed attention recently, as many practical problems are better suited to transductive learning than the traditional iid statistical framework. In this context, the work is particularly relevant.\n\nThe concentration bounds presented are non-trivial, and their proof involves sophisticated mathematical tools.",
            "- The paper studies potentially improved risk bound for transductive learning following the conventional localized method. The main difference the author claims is that the risk bounds are for unbounded functions. However, such claim, together with the technical results for  unbounded functions, are very questionable."
        ],
        "weaknesses": [
            "- **Weaknesses:**\n\n- Limited scope of unbounded loss functions:\n\n  - Analysis is restricted to sub-Gaussian and sub-exponential functions (and sub-Weibull in appendix)\n  - Other important classes of unbounded loss functions are not addressed\n\n\n- Insufficient comparison with prior work:\n\n  - The paper overlooks crucial related work, particularly [1] (Maurer & Pontil, 2021). While [1] focuses on inductive settings, their theoretical foundations appear relevant. A comparative analysis between Theorems 1 and 2 and the results in [1] is needed.\n\n- Limited Contribution:\n  - The current contribution of theoretical analysis in the GNN framework is limited. The current results are general and independent of the Graph properties. \n\n**Minor Comments:**\n\n-  In Assumption 3, \"\u03b1-H\u00f6lder\" is misspelled\n- Add the explanation of Hoeffding's reduction method to the appendix\n- Use \"Boundedness\" instead of \"Boundness\"\n- Use \"techniques\" instead of \"technologies\"\n- Line 221 \"We mainly follows the traditional technique...\" --> \"We mainly follow the traditional technique\"\n\n---\n\n**References:**\n\n- [1] Maurer, A., & Pontil, M. (2021). Concentration inequalities under sub-gaussian and sub-exponential conditions. Advances in Neural Information Processing Systems, 34, 7588-7597.",
            "- The paper lacks numerical results to support the derived risk bounds. Additionally, as it does not provide lower bounds for the risk, it remains unclear whether the resulting bounds could be further improved.\n\nThere are some typos in the paper:\n\nLine 221: we mainly \"follows\"\nLine 222: we \"introduced\"\nLine 405: w_1^{T+1}   ->   w^{T+1}",
            "- 1. The authors do not offer any motivation for addressing unbounded loss. A discussion on why this is an important and relevant problem to study would be beneficial.\n\n2. It is unclear what the significance of Theorems 3 and 4 is. Let\u2019s consider Theorem 3 as an example. Given other terms, the upper bound can at best be\n$$ \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{m^2 u}.$$\nThe authors claim this bound is state-of-the-art for $m = o(N^{2/5})$. If we set $m = N^{1/5} = o(N^{2/5})$, then $u = N - N^{1/5} \\leq N $, and the upper bound is at least\n$$ \\geq \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{N^{2/5} \\cdot N} \\geq N^{3/5} \\log\\left( \\frac{1}{\\delta}\\right).$$\n\nGiven that this is the highest the proven upperbound can be, it is difficult to see why such a bound would be of interest as $N$ can be quite large, making the bound potentially vacuous. I may likely be missing something here, and I would be happy to engage with the authors during the discussion session to gain further clarity and adjust my score.",
            "- There are several major technical drawbacks.\n\n\n\n1. While this paper claims that the risk bounds for transductive learning are for unbounded loss functions, the assumptions required for the results are essentially designed for bounded loss functions. For example, the main results Theorem 3 and Theorem 4 need the assumption that $E[f^2] \\le B E[f]$. It is well known that such assumption, $E[f^2] \\le B E[f]$, holds mainly for bounded loss functions, such as that in the classical local Rademacher complexity work (Bartlett Local Rademacher Complexities, AOS 2005). It turns out that while the paper claims risk bounds for \"unbounded loss\", but the results rely on the assumption which mainly hold for bounded loss functions.\n\n2. It is well known that Rademacher complexity or local Rademacher complexity based methods derive distribution-free risk bounds that do not need distributional assumptions. In contrast, the risk bounds in the main results Theorem 3 and Theorem 4 require sub-Gaussian and sub-exponential loss functions. It is not clear which loss functions are  sub-Gaussian or sub-exponential, and such restriction on the loss functions can significantly limit the application scope of the derived bounds.\n\n3. There are no detailed comparison to the current state-of-the-art risk bounds for the main results in Theorem 3 and Theorem 4, such as the existing transductive bounds in (Tolstikhin et al. 2014, Localized Complexities for Transductive Learning. COLT 2014). Without comparison to prior art, the significance of these results is not clear and questionable.\n\n4. The risk bounds in the main results, Theorem 3 and Theorem 4, do not convergence to 0 under the case that $m = N^{\\alpha}$ or \n$m = N^{\\alpha}$ with $\\alpha \\in (0,1/2]$, and they even diverge to $\\infty$ if $\\alpha \\in (0,1/2)$.  This is in a strong contrast to existing risk bounds for excess risk bounds where such bounds should always at least converge to $0$, and it is really misleading to claim such risk bounds are improved ones."
        ]
    },
    "xriJVaTh4C": {
        "venue": "ICLR 2025",
        "title": "Gaussian Loss Smoothing Enables Certified Training with Tight Convex Relaxations",
        "link": "https://openreview.net/forum?id=xriJVaTh4C",
        "abstract": "Training neural networks with high certified accuracy against adversarial examples remains an open challenge despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods, perhaps surprisingly, can perform worse than looser relaxations. Prior work hypothesized that this phenomenon is caused by the discontinuity, non-smoothness and perturbation sensitivity of the loss surface induced by tighter relaxations. In this work, we theoretically show that Gaussian Loss Smoothing (GLS) can alleviate these issues. We confirm this empirically by instantiating GLS with two variants: a zeroth-order optimization algorithm called PGPE which allows training with non-differentiable relaxations, and a first-order optimization algorithm, called RGS, which requires gradients of the relaxation, but is much more efficient than PGPE. Extensive experiments show that when combined with tight relaxations, these methods surpass state-of-the-art methods when training on the same network architecture for many settings. Our results clearly demonstrate the promise of Gaussian Loss Smoothing for training certifiably robust neural networks and pave a path towards leveraging tighter relaxations for certified training.",
        "decision": "Reject",
        "review scores": [
            6,
            1,
            3
        ],
        "strengths": [
            "- I think the following strength itself is enough for accept.\n- The paradox of certified training is an important subject to study.\n- The proposed method (GLS) is well-motivated and backed up by theoretical results.\n- The empirical results on small perturbation settings in Table 3 are quite impressive.",
            "- GLS introduces novelty in using Gaussian smoothing across the loss landscapes, strongly problematic in certified training.",
            "- 1. The writing is fine. There is little difficulty understanding the paper."
        ],
        "weaknesses": [
            "- This paper is well-motivated, but has a minor weakness in the performance (on large perturbation) detailed as follows:\n- Table 3 (Table 5 in Appendix) shows the results for small (large) perturbation settings. The results for large perturbation is not significant. IBP performs well for large perturbations and other tighter methods does well for small perturbations. This is because of the continuity, smoothness and sensitivity of the loss landscape. Thus, to check the effectiveness of GLS, I think it is crucial to check the result of tighter methods on large perturbation settings. However, setups (i) MNIST $\\epsilon=0.3$ and (ii) CIFAR-10 $\\epsilon=8/255$ show that \nGLS (or RGS) (i) does not show a significant performance gain (DP-RGS (IBP) vs MTL-IBP; 88.69 vs 88.68) or (ii) shows a worse performance (29.25 vs 29.62). This should be discussed in the main text (not in Appendix). I don't think the performance itself is a reason for a rejection, but it needs more discussion.\n- In Table 1, \"the more precise DeepPoly bounds now yield the best certified accuracy across all settings, even outperforming accuracy at low perturbation radii'', but not for the large perturbation (IBP vs DeepPoly-PGPE; 77.23 vs 74.28, 25.72 vs 22.19).\n- In GRAD Training, \"IBP dominates the other methods, confirming the paradox of certified training\", but in the original paper of CROWN-IBP, CROWN-IBP outperforms IBP for a larger network (see their Table 2 https://arxiv.org/pdf/1906.06316). \n- (For a larger model,) IBP outperforms the other methods (e.g, CROWN-IBP, CAP) for large perturbation, not for small perturbation (e.g., see Table 1 in Lee et al. (2021)). This implies that the paradox plays more important role for a larger perturbation.\n\n\n\nPlease check the Questions part together. I think unclear presentation is also a weakness.",
            "- ## Writing Style\n\n- **Introduction**: In some respects, the introduction could be improved about adversarial certified robustness. It is abrupt and feels more work-related.\n  \n- **Transitions and Structure**: Transitions from one section to another are missing; hence, making the paper hard to follow. Statements such as, \"While CROWN-IBP is not strictly more or less precise than either IBP or DEEPPOLY,\" necessarily need references or explanations.\n\n- **Undefined Terms**: Terms such as \"soundness\" and \"sensitivity\" are undefined.\n\n- **Results of experiments**: These are presented in a very untidy fashion.\n\n\n## Related Work\n\n- **Definitions**: Some more clear definitions can be provided, for instance, adversarial attack and adversarial robustness because the latter is different from certified robustness.\n  \n- **Redundant Subsections**: Sections 2.1 would seem to represent redundant subsections: \"Training for Robustness\" and \"Adversarial Training.\"\n\n\n## Theoretical Findings\n\n- **Lack of Rigor**: Theoretical statements, like Theorem 3.1, are informal and not mathematically precise. References to results, including Stein's Lemma [2], could be good in the proof.\n  \n- **Flaw in Proofs in Lemmas**: \nIn the proof of Lemma B.1 terminology and symbols that are important are not defined (for example, \\(\\delta \\theta\\), \\(P_{\\epsilon_1}\\), \\(P_{\\epsilon_2}\\), \\(P_{\\mathcal{N}(0, \\sigma^2)}\\)). The authors seem to take the limit $\\delta \\theta$ to $0$ but it is never stated anywhere in the proof and the integral of the derivative of the loss  $L^\\prime$  should be a multi dimensional integral as the input space is multi dimensional.\nIn Lemma B.2's proof  the simplifications are excessive,  the structure of the proof is highly defective in many places and it needs a thorough revision. \n\n\n\n## Experimental Results\n\n- **Lack of Detail**: Neither the dataset nor the architecture used in Figure 1 are specified.\n  \n- **Performance Gains**: Although the experimental results are somewhat improved, they are not very significant compared to previous methods; therefore, this added complexity is questionable in value. Presentation of the standard deviations over multiple runs would have given more robust conclusions since apparent gains are marginal.\n\n\n# Major Concerns\n\n1. **Theoretical Rigor**: The proofs are not rigorous; better structure and formalization are required. The use of Stein's Lemma might improve the underlying framework, giving credibility to the theoretical results.\n  \n2. **Marginal Gains and Complexity**: GLS brings in computational complexity especially with PGPE, while performance benefits are marginal.\n\n3. **Readability and Clarity**: The writing style and the structure take away from clarity, making the paper difficult to follow.\n\n\n[2] Stein, \u201cEstimation of the Mean of a Multivariate Normal Distribution,\u201d The Annals of Statistics, 1981.",
            "- 1. Contribution: This paper proposes to apply GLS to existing certified training methods such as IBP and DeepPoly, and one can use PGPE or RGS to compute the GLS. Note that, however, all of IBP, DeepPoly, GLS, PGPE and RGS are existing methods. Theorem 3.1 is a direct application of an existing result. So if I understand correctly, the only novelty of this submission is combining these things together, and thus the contribution is incremental.\n2. Performance: How does the proposed method perform? From Table 1, it seems that the proposed method (PGPE) is not significantly better than the standard method (GRAD), if not worse. It is true that PGPE makes DeepPoly better than IBP, but the way it does that is to make IBP worse, not making DeepPoly better. On MNIST (0.3) and CIFAR-10 (8/255), the best PGPE method is worse than the best GRAD method. On the other two settings, it is only slightly better. For all settings, IBP-PGPE is worse than IBP-GRAD (standard). Thus, this result suggests that one probably should not use PGPE. There is no evidence that the proposed method works in practice.\n3. Experimental results: Tables 1 and 3 seem contracting with each other. In Table 1, standard IBP on CIFAR-10 (2/255) has natural accuracy 48.05% and certified accuracy 37.69%; in Table 3, the two reported numbers are 54.92% and 45.36%. Probably this is because CNN5 is better than CNN3, but then what is the point of Table 1? Why not always use a bigger model (CNN7 seems even better). And why not compare with RGS in Table 1? I don't think there is anything preventing you from comparing with RGS in Table 1. The authors report DP-RGS to be the best method in Table 3, but since there are so many problems with the tables I do not trust this result.\n\nOverall, this submission proposes to combine a bunch of existing methods, but the experiments show that this is even worse than the original methods. Thus, I recommend rejecting this submission."
        ]
    },
    "zbIS2r0t0F": {
        "venue": "ICLR 2025",
        "title": "Allostatic Control of Persistent States in Spiking Neural Networks for Perception and Computation",
        "link": "https://openreview.net/forum?id=zbIS2r0t0F",
        "abstract": "We introduce a novel model for updating perceptual beliefs about the environment\nby extending the concept of Allostasis to the control of internal representations.\nAllostasis is a fundamental regulatory mechanism observed in animal physiology\nthat orchestrates responses to maintain a dynamic equilibrium in bodily needs and\ninternal states. In this paper, we focus on an application in numerical cognition,\nwhere a bump of activity in an attractor network is used as a spatial-numerical\nrepresentation. While existing neural networks can maintain persistent states, to\ndate, there is no unified framework for dynamically controlling spatial changes in\nneuronal activity in response to enviromental changes. To address this, we couple\na well-known allostatic microcircuit, the Hammel model, with a ring attractor, re-\nsulting in a Spiking Neural Network architecture that can modulate the location of\nthe bump as a function of some reference input. This localised activity in turn is\nused as a perceptual belief in a simulated subitization task \u2013 a quick enumeration\nprocess without counting. We provide a general procedure to fine-tune the model\nand demonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare\nit with biological data. Finally, we analyze the dynamics of the network to un-\nderstand the selectivity and specificity of different neurons to different categories\npresent in the input. The results of this paper, particularly the mechanism for mov-\ning persistent states, are not limited to numerical cognition but can be applied to a\nwide range of tasks involving similar representations.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. Novelty of the Approach\nIntegrating an allostatic control mechanism into a spiking neural network architecture is a novel approach with potential implications for understanding self-regulation in neural systems.  \u00a0 \n\n2. Dynamic Control of Persistent States\nThe model successfully demonstrates the dynamic control of persistent states in response to environmental changes, a crucial aspect of cognitive processing.  \u00a0 \n\n3. Qualitative Reproduction of Behavioral Aspects\nAlloNet qualitatively reproduces certain behavioral aspects of subitization, such as the relationship between reaction time and numerosity.",
            "- The authors provide a novel approach to applying allostatic principles to control internal representations within spiking neural networks. This approach may inspire further cross-disciplinary exploration of allostasis and neural network control. In particular, the use of allostasis as a tool of synchronizing the alignment between internal representations and external stimuli could make the model useful in applications requiring real-time adaptability, such as robotics or artificial agents interacting with unpredictable environments.",
            "- * The proposed model is original to my knowledge, and is fairly simple to understand.\n* The authors have compared observations from their models such as reaction times to those of humans performing the same task.\n* A testable prediction, i.e., that small numerosities are encoded using a magnitude-based system (ring attractor in this case, which is central to the model), could potentially be validated using experiments with animals/humans.",
            "- 1. The authors propose a novel network model based on the ring attractor, enhanced with high/low gain modulation neurons to manage bump shifts, with the Hammel model controlling these shifts. This enables flexible positioning of the bump to match external signals.\n2. Some aspects of the model\u2019s performance on counting tasks align with human data, particularly when varying the synaptic time constant.",
            "- 1.\tThe introduction section provides a comprehensive overview of both numerosity and attractor networks.\n2.\tThe integration of physiological concepts into neural circuit modeling is innovative and inspiring."
        ],
        "weaknesses": [
            "- Limited Biological Plausibility: While the model draws inspiration from biological systems like the Hammel model for temperature regulation, the direct application of such a model to numerical cognition might oversimplify the underlying biological mechanisms.\n\nSpecificity of the Model: The paper focuses heavily on subitization as an application. It would be beneficial to explore additional cognitive tasks to demonstrate the generalizability of AlloNet.",
            "- * The paper presents allostasis as a beneficial mechanism but does not sufficiently compare it to traditional homeostatic mechanisms or other neural adaptation frameworks. This limits understanding of when allostatic control is most useful or essential, making it difficult to gauge the model's full significance\n\n* The experimental setup is limited to idealized, controlled tasks. Real-world applications, however, typically involve noisy, unpredictable inputs that can disrupt internal representations, which this model might struggle with. Current experiments do not address such robustness.\n\n* The motivation for using a ring attractor with a \"bump of activity\" as the representation for the task of numerical cognition (e.g., subitizing) is not clear. What is the (systems) neuroscience evidence for this? \n\n* The authors note that error rates increase with numerosity and time, but more in-depth insights into the reasons for these errors, such as bump instability, could clarify model limitations",
            "- * While the authors are able to recapitulate some observations in humans using their model, i.e., that reaction times increase with numerosity (for a specific value of the time constant), the actual reactions times from the model are far greater than human reaction times (almost 10000 ms vs less than 500 ms) ([Dehaene, 2011](https://psycnet.apa.org/record/2011-10610-000); [Kutter et al., 2023](https://www.nature.com/articles/s41562-023-01709-3)). Do the authors have an explanation for why this is the case, and could it be resolved with better choices for hyperparameters?\n* I'm not entirely convinced with the authors' argument that they are able to properly reproduce reaction time patterns. The sharp jump in reaction time for a numerosity of 4 is only observed with a different, faster time constant value of 900ms, while the gradual increase in reaction times with numerosity is only observed with a slower time constant of 1000ms. Furthermore, in the 900ms case, the reaction times for lower numerosities do not match the human data at all, as the authors admit. Could the authors clarify this? I would expect the model to match human observations for a single value of the time constant in order to claim that it reproduces experimental results.\n* The model is evaluated only on a single task, i.e., subitising. Especially given the use of a ring attractor and the authors' claims of the model's generalisability, it would be important to evaluate the model on other tasks involving numerosity estimation or tracking a magnitude, such as head direction integration ([Valerio & Taube, 2012](https://www.nature.com/articles/nn.3215)) or average estimation ([Lee & Ma, 2020](https://cognitivesciencesociety.org/cogsci20/papers/0304/0304.pdf)).\n* The task is also limited in that it restricts numerosity from 1 to 4. To better compare the observations from the model to previous experiments ([Kutter et al., 2023](https://www.nature.com/articles/s41562-023-01709-3)), the authors should incorporate both small and large numbers (from 0 to 9, for example). If the authors want to align their work better with [Kutter et al. (2023)](https://www.nature.com/articles/s41562-023-01709-3), two different representations could be used for 0-4 vs 5-9 \u2013 and in this case it would be important to test whether discrete attractors (fixed points for each of 0-4) match the data better than when using a ring attractor.\n* There are other issues with the results and their interpretation. For example, in Fig. 3B, it is not clear how much more variable the reaction times are for higher numerosities with longer time constants (as claimed on line 312). Furthermore, from Fig. 3D, it seems that the \"quality score\" is very low for numerosity 3 and longer time constants (while for shorter time constants, quality is 0 for numerosity 2 but quality is higher for 3), why is this the case, and is there any experimental evidence of this in animals/humans?\n* The authors claim that the neural dynamics of their model are similar to those of several brain regions in human recordings, but this is not substantiated with a metric or even a plot showing qualitative similarity. Furthermore, some of the analysis of neuronal responses has only been done for a single, arbitrary neuron, but it would be important to look at population responses when making comparisons to human data.\n* Another weakness is the specificity of these results to hyperparameter and architectural choices. The lack of learning also makes it harder to adapt this model to more complex tasks.\n* Finally, the writing lacks clarity and contains grammatical, formatting and some typographical issues. Examples:\n\n  * Line 50 (\"can represent to track changes...\", unclear what this means), Line 53 (\"resemble to the\" -> \"resemble the\"), Line 158-159 (\"functions to take input for..\"), Line 249 (\"in an standard...\"), Line 463-465 (sentences are vague, \"we allowed to work on\"), etc.\n  * Several in-text citations are not properly enclosed within brackets.\n  * \"Excitatiory\" -> \"Excitatory\" (Table 1), inconsistent use of \"ise\" vs \"ize\" (British vs American English), \"... as a computation model ...\" -> \"computational\", lack of proper spacing before and after parentheses, etc.\n\n  I would encourage the authors to carefully revise the paper to improve its clarity and fix any other grammatical/typographical issues.",
            "- 1. The counting task demonstration is very simple; the numerical information is directly encoded in the external signal\u2019s firing rate.\n2. Although certain model properties align with human data, they do so under varying synaptic time constants, which are not well explained. Additionally, the model\u2019s reaction times are significantly slower than human responses, by about an order of magnitude.\n3. The necessity of using a spiking neural network is not adequately justified.\n4. The model\u2019s structure, specifically the gain modulation neurons and the Hammel model component, lacks biological explanation.\n5. The network dynamics and mechanisms are underexplained. For instance, in the connection weight formula $w_{ij}$, the term $d_{ij}$ is undefined, though it appears to represent the distance between neurons in the ring.\n\n\n### Mino \n1. In Equation 1, $d_{ij}$ is undefined.\n2. Figure 1, with its four subplots, could be clearer if organized differently.\n3. In Figure 2, specify units for the x-axis (probably ms).\n4. The synaptic decay constants \\tau used are unusually long compared to standard neuron models. More explanation of these values and their role would improve readability.",
            "- 1.\tImproper Citations:  For instance, in lines 173-174, citing Zhang, K. (1996) is essential for discussing the asymmetry in continuous attractor connections. Additionally, it appears that Fig. 1c is likely influenced by Fig. 1d in Wilson, R.I. (2023), but lacks appropriate citation.\n2.\tLack of Clarity on Network Dynamics and Analysis: The model does not provide a clear definition of network dynamics or a detailed theoretical analysis of the results.\n3.\tBiologically Implausible Model Setup: The model\u2019s structure and parameters are set in bio-unplausible ways. Specifically:\n\t- The synaptic time constants far exceed the plausible range, making the comparison across time constants in Figs. 4 and 5 less meaningful.\n\t- LGM and HGM neurons were designated as inhibitory. I agree that it is a feasible way to drive the ring attractor, but there has been both experimental and theoretical evidence showing that P-EN neurons in Drosophila brains fulfill this role as excitatory neurons (see Zhang, W., Wu, Y. N., & Wu, S., 2022; Mussells Pires, P., Zhang, L., Parache, V., Abbott, L. F., & Maimon, G., 2024).\n4.\tFailure to Model Human Behavior Accurately: Although section 3.1 attempts a loose comparison between the model and human behavior, the model does not successfully replicate human behavioral patterns.\n5.\tUnpolished text and figures: The text contains several typos, unified terminologies and missing punctuation."
        ]
    },
    "zSUXo1nkqR": {
        "venue": "ICLR 2025",
        "title": "TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction",
        "link": "https://openreview.net/forum?id=zSUXo1nkqR",
        "abstract": "The growing demand for transparency and interpretability in critical domains has driven increased interests in comprehending the explainability of Message-Passing (MP) Graph Neural Networks (GNNs). Although substantial research efforts have been made to generate explanations for individual graph instances, identifying global explaining concepts for a GNN still poses great challenges, especially when concepts are desired in a graphical form on the dataset level. While most prior works treat GNNs as black boxes, in this paper, we propose to unbox GNNs by analyzing and extracting critical subtrees incurred by the inner workings of message passing, which correspond to critical subgraphs in the datasets. By aggregating subtrees in an embedding space with an efficient algorithm, which does not require complex subgraph matching or search, we can make intuitive graphical explanations for Message-Passing GNNs on local, class and global levels. We empirically show that our proposed approach not only generates clean subgraph concepts on a dataset level in contrast to existing global explaining methods which generate non-graphical rules (e.g., language or embeddings) as explanations, but it is also capable of providing explanations for individual instances with a comparable or even superior performance as compared to leading local-level GNN explainers.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3,
            3
        ],
        "strengths": [
            "- - Providing global explanations is a crucial aspect in the development of trustworthy GNN models.\n - The paper provides adequate background information through preliminaries and related sections.\n - The effectiveness of the proposed method is evaluated by multiple datasets.",
            "- - The proposed method is intuitive and technically sound.\n- The experimental analysis is abundant.",
            "- The proposed method is fast comparing to previous subgraph based method with last layer embedding after message aggregation, which makes it potentially more scalable comparing to subgraph based enumeration method.\n\nThe proposed method can cover both local and global concept extraction.",
            "- Providing local and global explanations is crucial for understanding predictive outputs at both individual and class levels. Figure 1 offers an insightful perspective on how each instance's prediction compares to class-specific patterns, making it easier to debug predictive outputs. Furthermore, extracting concepts based on subtrees presents a novel approach to mining meaningful patterns.",
            "- The paper made two innovative contributions:\n1. It providing a global explanations of GNNs on the dataset or class level in the format of subgraphs rather than nodes, language rules or prototype embeddings in the previous literature. \n2. Instead of subgraph enumeration or search, the paper proposed an efficient algorithm for subtree extraction, and using the root node embedding as subtree embedding."
        ],
        "weaknesses": [
            "- - The methodological details in the paper are unclear. For example, while it mentions identifying graph substructures based on node embeddings, the specific approach is not adequately detailed, making it challenging to understand.\n - A significant issue is the limited evaluation measures, with insufficient justification provided. Various measures, such as sparsity, fidelity (+ and -), and fidelity\\delta [1], are commonly used and could be considered in the evaluation.\n - More comparisons with existing global XAI methodologies are needed. Methods like D4Explainer [2] and TAGE [3], which also can provide global explanations, would enhance the baseline comparisons.\n\nReferences:\n\n[1] Zheng et al., \"Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks,\" ICLR 2024.\n\n[2] Chen et al., \"D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion,\" NeurIPS 2023.\n\n[3] Xie et al., \"Task-Agnostic Graph Explanations,\" NeurIPS 2022.",
            "- - First, the definitions for different types of GNN explanations are quite vague. This paper claims that GNN explanation methods can be categorized into local and global levels; it claims that instance-level explanation is local-level. Under this definition, the proposed method should be considered as local-level, since it only works in the instance-level graph graph classification task where all the subtrees are extracted from the graph to be predicted. \n- Second, the proposed methods only seem to work with graph-level prediction tasks and do not seem to work with node-level prediction tasks. I did not find the paper explicitly discussing that. The existing popular GNN explainability methods, such as GNN Explainer, can work with both node and graph-level prediction tasks. The paper should clearly emphasize its limitations.\n- A key idea of this paper - last layer node embedding represents the full L-hop subtrees, is a well-known result in the GNN research domain (for example, it has been taught throughout the Stanford CS224W course with millions of views on Youtube). The justifications in Section 4.2 look redundant to me. Furthermore, Theorem 4.2 and its proof are a direct application of the results from the GIN paper, which is also redundant. \n- Overall, I do not find the proposed method offers a new understanding of the explainable GNN domains, especially given that it can only work with graph-level prediction tasks, and the performance is only on par or even worse than existing methods (Table 1). I am happy to defend my opinion further if needed",
            "- The subtree features extracted by last layer embedding might not be able to fully reflect meaningful concept, and could ignore certain patterns that are useful for explainability. See questions for more details.\nThe evaluation is not sufficient, deeper experiments should be done on more architectures (GCN, GraphSAGE, GAN, etc), as different GNN architectures have varying expressive power and aggregation mechanisms. Besides, variations in last layer embedding quality across architectures could influence the interpretability in TreeX.",
            "- W1. The proposed method's heavy reliance on K-means clustering for both local and global concept extraction introduces several inherent weaknesses. Consequently, the proposed method exposes the weaknesses and limitations of clustering such as initialization issues, sub-optimal issues, the sensitivity of hyper-parameter $k$, and quality issues such as under-clustering or over-clustering. Due to this well-known limitation, the assignment of clusters is likely to be sensitive. Thus, explanations based on the clustering method may not be robust. \n\nW2. The process of providing final explanations in Figures 8, 9, 10, and 11 is not clear. Based on class-specific global rules,  connecting concepts and subgraphs in the specific input graph. \n\nW3. Experiments are not thorough in several aspects as below. \n\n(1)  A quantitative evaluation of global explanations is lacking, which is a key contribution of the proposed method. This aspect should be more thoroughly examined.\n\n(2) Global explainers in the graph domain such as XGNN [1] and GNNInterpreter [2] are not considered in the comparison.\n\n(3) No visualization of the local explanations. \n\n(4) AccFidelity is an easy metric to achieve decent performance as it counts the explanation as correct even when $\\hat{y}_i=0.51$ in binary classification tasks. Nevertheless, authors only use $Fid^{-}$, excluding the $Fid^{+}$ [3] which is the most common measurement in many literatures. \n\n(5) The time analysis focuses solely on explanation inference, omitting the process of concept extraction. This process likely incurs significant computational costs due to its use of k-means algorithms in twice and post-processing steps to extract final concepts. A more comprehensive time analysis would provide a clearer picture of the method's efficiency.\n\nMinor typo in line 352, e.g., In the he BA-2Motifs data.\n\nReference\n\n[1] Yuan, Hao, et al. \"Xgnn: Towards model-level explanations of graph neural networks.\"\u00a0*Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining*. 2020.\n\n[2] Wang, Xiaoqi, and Han-Wei Shen. \"Gnninterpreter: A probabilistic generative model-level explanation for graph neural networks.\"\u00a0*arXiv preprint arXiv:2209.07924*\u00a0(2022).\n\n[3] Yuan, Hao, et al. \"Explainability in graph neural networks: A taxonomic survey.\"\u00a0*IEEE transactions on pattern analysis and machine intelligence*\u00a045.5 (2022): 5782-5799.",
            "- I have concern on the global concept extraction method outlined between lines 200-206. How can the method ensure that the spaces of embedding across $D$ graph instances are aligned? The authors should provide a detailed explanation of this process, as without it, I am skeptical about the possibility of clustering the $kD$ local graph concepts, because these concepts originate from $D$ distinct embedding spaces.\n\nIn the \"global rule generation for each class\" (line 220-), I doubt the method can always work in general situations. The global rules are generated through frequency-based analysis, and it may be challenging to ensure that these frequency-based features always possess the necessary discriminative power to distinguish the differences between classes.\n\nThe innovation of this paper is not significant. Please compare the proposed method with \n- Motif-driven Subgraph Structure Learning for Graph Classification (https://arxiv.org/abs/2406.08897)\n- STExplainer: Global Explainability of GNNs via Frequent SubTree Mining (https://openreview.net/forum?id=HgSfV6sGIn)"
        ]
    },
    "yarlMUJePB": {
        "venue": "ICLR 2025",
        "title": "Energy-Based Discrete Mask Approximation for 3D Molecular Graph Explanation",
        "link": "https://openreview.net/forum?id=yarlMUJePB",
        "abstract": "In recent years, Graph Neural Networks (GNNs) have become a powerful tool for modeling molecular data. To enhance their reliability and interpretability, various explanation methods have been developed to identify key molecular substructures, specifically a set of edges, in the decision-making process. Early work with 2D GNNs represented molecules as graphs with atoms as nodes and bonds as edges, neglecting 3D geometric configurations. While existing explanation methods perform well on 2D GNNs, there is a pressing need for 3D explanation methods tailored for 3D GNNs, which outperform 2D GNNs in many tasks. Current explanation methods struggle with 3D GNNs due to the construction of edges based on cut-off distances in 3D GNNs, resulting in an exponentially large number of edges. We identify the sources of errors in explanations and decompose them into two components based on a derived upper bound between the optimized masks and the actual explanatory subgraph. This gap can be significant, especially for 3D GNNs because of the large number of edges. To achieve optimal explanation fidelity, our method aims to bridge this gap by assigning two energy values to each atom based on its contribution to the prediction: one energy reflects the scenario where this node is important in making the decision, while the other represents the scenario where it is unimportant. In analogy to physics, lower energy values indicate greater stability in the prediction, and thus, we are more confident about the scenario with which it is associated. Our approach strives to push up and down the energies, respectively, to distinguish these two scenarios to simultaneously minimize both components of the derived upper bound of error, enabling us to identify a stable subgraph that maintains high explanation fidelity. Experiments conducted on backbone networks and the QM9 dataset demonstrate the effectiveness of our method in providing accurate and reliable explanations for 3D graphs.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- This model adds a regularization constraint to the soft mask, encouraging the mask values to be closer to 0 or 1, thereby reducing the uncertainty in the output of effective subgraphs. This  sound good.",
            "- The paper is well-written, and the authors effectively introduce and motivate their topic and work. The notation is clear and (mostly) correct, while the figures are well-designed and help clarify the content.\n\nThe experiments include both qualitative and quantitative results, as well as an ablation study to assess whether their proposed extra loss term truly affects the method's performance.\n\nThe topic is relevant, as explainability remains a significant challenge, especially in domains like chemistry where even domain experts often don't know the ground truth.\n\nThe paper offers some novelty, although in my opinion it's rather incremental.",
            "- The author identified a key challenge in explaining 3D GNNs: the discrepancy between soft and discrete masks. To address this, the proposed method introduces a novel approach using an energy-based function to represent the importance probability.",
            "- 1. This paper uses the energy-based model to provide a constraint rather than the loss function used in GNNExplainer\n\n2. This paper is well-organized, which makes it easy to follow the main idea.",
            "- - 3D graphs are likely to have unique features that need to be addressed with property techniques, also from an explainability perspective\n- The presentation and the writing are good\n- **Touches several overlooked aspects in many explainers**, such as the discreteness of the mask, and the difference between edge vs node masks\n- The paper focuses on regression explanation, which **is often overlooked in the XAI literature**."
        ],
        "weaknesses": [
            "- The main drawbacks of the paper are three: the motivation is not very clear, the contribution is insufficient, and the experimental section is insufficient. The specific reasons for each are as follows:\n\n1\uff09The motivation is not very clear. First, regarding the motivation of the paper, the authors try to address the Graph Explanation task for 3D molecular graphs. However, both 3D and 2D molecular graphs are essentially graphs; 3D molecular graphs may contain more edges, making the explanation slightly more challenging. Therefore, could the authors provide specific molecular examples or quantitative evidence to demonstrate how existing 2D interpretation methods are inadequate when applied to 3D graphs? Besides, based on the visualization results in Figure 4, the authors are still focused on actual bonds and atoms, which can also be achieved with 2D graph explanation. Interpreting extra edges in a 3D molecular graph defined by a cutoff radius (those not representing actual chemical bonds) is also not particularly meaningful. Therefore, could the authors explain how the proposed 3D molecular explanation method handles extra edges that do not represent actual chemical bonds, and why it is necessary to interpret them?\n\n2\uff09The contribution is not enough. First, using regularization to enhance algorithm performance is a very common approach [1,2]. Furthermore, the authors do not explain the advantages of using the EBM function to design the regularization term. For example, directly applying L1 regularization to the mask in Equation 7 could also make the mask values closer to 0 and 1. Could the authors compare their proposed method with directly applying L1 regularization to the mask and clarify the advantages of using the EBM function for the regularization term? Secondly, the purpose of explanation should be to leverage atom contribution to improve property prediction performance, rather than predicting properties using a smaller subgraph. In the experimental section, the MAE for property prediction using subgraphs is often higher than for the original graph. If you cannot achieve better results than the original graph, then what is the purpose of predicting properties using the optimal subgraph? Thus, the suitability of applying Equations 5 and 1 to molecular property prediction is worth considering. If the goal is simply to explain which parts of the molecule contribute more, the hard mask, as defined in traditional Graph Explanation, may not serve this purpose effectively.\n\n3\uff09Finally, the experiment is insufficient. Molecular research experiments typically involve at least two datasets (such as QM9 and GEOM-Drugs [3]) or more, yet this paper only uses the QM9 dataset, making the experimentation inadequate. Therefore, I suggest that the authors include experiments on the GEOM-Drugs dataset. Additionally, the baselines are outdated, with the latest comparison algorithm from 2022. In the experimental section, the authors need to demonstrate that their 3D interpretation method outperforms 2D interpretation methods from 2024, such as [2,4-9]; otherwise, the proposed 3D interpretation method lacks justification. Therefore, I suggest that the authors include at least one recent graph explanation methods like [2,4,5,7,9] as baselines.\n\n[1] Shan C, Shen Y, Zhang Y, et al. Reinforcement learning enhanced explainer for graph neural networks[J]. Advances in Neural Information Processing Systems, 2021, 34: 22523-22533.  \n[2] Zhang W, Li X, Nejdl W. Adversarial Mask Explainer for Graph Neural Networks[C]//Proceedings of the ACM on Web Conference 2024. 2024: 861-869.  \n[3] Axelrod S, Gomez-Bombarelli R. GEOM, energy-annotated molecular conformations for property prediction and molecular generation[J]. Scientific Data, 2022, 9(1): 185.  \n[4] Huang R, Shirani F, Luo D. Factorized explainer for graph neural networks[C]//Proceedings of the AAAI conference on artificial intelligence. 2024, 38(11): 12626-12634.  \n[5] Chen T, Qiu D, Wu Y, et al. View-based explanations for graph neural networks[J]. Proceedings of the ACM on Management of Data, 2024, 2(1): 1-27.  \n[6] Bui N, Nguyen H T, Nguyen V A, et al. Explaining Graph Neural Networks via Structure-aware Interaction Index[J]. arXiv preprint arXiv:2405.14352, 2024.  \n[7] Liu X, Ma Y, Chen D, et al. Towards Embedding Ambiguity-Sensitive Graph Neural Network Explainability[J]. IEEE Transactions on Fuzzy Systems, 2024.  \n[8] Homberg S K R, Modlich M L, Menke J, et al. Interpreting Graph Neural Networks with Myerson Values for Cheminformatics Approaches[J]. 2024.  \n[9] Chen Y, Bian Y, Han B, et al. Interpretable and Generalizable Graph Learning via Subgraph Multilinear Extension[C]//ICLR 2024 Workshop on Machine Learning for Genomics Explorations.",
            "- My main concern is the novelty of the approach (more specific questions about details can be found below in the question section). As I understand it, the authors add a loss term regulating the \"discreteness\" of the masks. These types of losses are ubiquitous in deep learning; for instance, it's common when learning a permutation matrix to learn a soft-relaxed one and enforce bistochasticity during training. \n\nFurthermore, the interpretation of their method as energy-based, while legitimate, seems rather superficial (every exponential distribution can be viewed as an energy function from a statistical mechanics perspective) and appears aimed at aligning the paper with the current popularity of (generative) energy-based models.\n\nI also find that restricting the evaluation to QM9 is somewhat limited nowadays, as larger datasets like Geom-Drugs and QMugs are becoming standard for assessing new methods. This limitation is particularly notable since the authors claim their method is especially useful for scaling to larger graphs, and QM9 contains only small compounds.",
            "- Overall, the proposed method is poorly presented, lacking clear definitions and containing errors in the equations. More detailed explanations and proofs are needed to demonstrate how the two identified problems are effectively addressed. While the use of an energy-based function is new, the method\u2019s novelty is limited, as it primarily introduces a parameterized explainer without significant advancements beyond that.",
            "- 1. Some core ideas are confusing. For example, the difference between 2D GNNs and 3D GNNs is not clear, even though this paper addresses explanations for 3D GNNs. In Section 3.1, Challenge 1 claims that the random graph is not suitable for 3D GNNs, but it is unclear what kind of assumption should be used. Challenge 2, the dense adjacency matrix, is not a problem in 2D graphs with size regularization.\n\n2. Some expressions are confusing. For example, in Figure 1, row b, the meaning of the red dashed lines and why they are needed is not clear.\n\n3. The metric differs from the mainstream definition, according to [1,2]. The experiments are limited to older methods. It would be better to compare it with more recent methods.\n\n\nReference:\n\n[1] Explainability in graph neural networks: A taxonomic survey, TPAMI, 2022\n\n[2] Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks, ICLR, 2024",
            "- - The differences outlined in Section 3.1 **do not seem to be convincing enough to justify ad-hoc approaches for 3D graphs**. Specifically, *Point 1* hinges on the unsuitability of the edge independence assumption in 2D explanations. While I agree that edge independence is oftentimes a very limiting assumption, I acknowledge that it is unsuitable for many 2D tasks as well (consider for example the case of social interactions, 2D molecules, or transportation networks), therefore not constituting a major difference between 2D and 3D graphs. *Point 2*, instead, highlights that 2D graphs are typically sparse. This is however not always the case, as there might be many scenarios in which also 2D graphs are dense (for example in social interactions, transaction records, or image regions in images). \n\n- Similar to above, lines 253-256 already hold for many 2D settings, making this argument not solid enough to motivate the need for 3D-specific methods.\n\n- The authors claim that previous methods do not account for the discreteness of the explanation mask. This is however oftentimes addressed by the common design choice of using a Gumbel distribution over the explanation scores (PGExplainer, LRI), which naturally pushes activations close to either 0 or 1. Therefore, **the authors should at least show that their proposed strategy to stimulate discreteness is more effective than using a Gumbel activation**.\n\n- **Only Fidelity- is used to evaluate explanation quality**, whereas other metrics are also available. The authors should at least provide both Fidelity- and Fidelity+, as they are complementary [1,2,3]. \n\n\n[1] Explaining the Explainers in Graph Neural Networks: a Comparative Study. Longa et al. 2024. ACM Comput. Surv.\n\n[2] GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks. Amara et al. 2022. LoG\n\n[3] Explainability in Graph Neural Networks: A Taxonomic Survey. Yuan et al. 2023. IEEE"
        ]
    },
    "wgnMdxS2nZ": {
        "venue": "ICLR 2025",
        "title": "MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption",
        "link": "https://openreview.net/forum?id=wgnMdxS2nZ",
        "abstract": "The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5,
            3
        ],
        "strengths": [
            "- The manuscript is well written in general. The integration of quantum federated learning to address performance degradation of FHE scheme is interesting. Experiments, although not detailed enough, seem to support the claim.",
            "- - The paper is easy to follow and I commend the authors for showing results on different types of datasets. \n- The results are intriguing\n   - Even though quantum layers perform worse than classical training in centralized setting and is only on par in federated setup quantum layers + FHE outperforms federated training in some cases.",
            "- The authors introduce the research background of quantum computing, privacy protection, and multimodality in federated learning. They propose a framework that combines quantum federated learning and homomorphic encryption and apply it to multimodal quantum federated learning. Through ablation experiments, they demonstrate the specific role of each module.",
            "- The paper provides a novel integration of quantum computing and fully homomorphic encryption within the federated learning context, which is an innovative contribution to improving data privacy without sacrificing model performance.\n\ufeff\nThe proposed MQMoE architecture effectively handles diverse multimodal data, achieving enhanced representation learning and mitigating the performance degradation associated with homomorphic encryption.\n\ufeff\nThe work is well-supported by experimental results, which illustrate improvements in both data privacy and model accuracy across diverse datasets, particularly in sensitive areas like genomics and MRI scans.",
            "- The experimental results show that quantum neural networks (QNN) is more robust to be trained in in federated settings. Specifically, when comparing centralized versus federated learning with FHE, classical models show a larger drop in performance than quantum models."
        ],
        "weaknesses": [
            "- - I do not have any experience with fully homomorphic encryption nor quantum federated learning. However, to me, this work is understood as a proof of concept, with many non-trivial tasks abstracted away. Please refer to the Questions section for more detailed comments.\n- Given that all quantum experiments are carried out with Pennylane, it is hard to conclude that the proposed method indeed is beneficial; on the other hand, this work seems to assume client as well as server in FL has access to quantum computer, which seems very farfetched.\n- It\u2019s a bit hard to follow without any equation numbers, for instance third point in the questions.",
            "- - While the empirical results with QC+FHE+FL are better, the motivation for using QC unclear. What is the main intuition? It is further not clear why QC works in some cases and doesn't work in other cases. Authors should investigate this more. \n- The experimental setup is quite vague. It is unclear what is the distribution of and size of the training datasets for each client. One of the key issues in FL is heterogenous setups, that is, different clients may have different distribution and different sizes of training dataset. Authors should evaluate the approach in heterogenous settings.\n- The baselines are trivial or ablation of the main method. It is unclear how the proposed method QC+FHE perform, compared to other more advanced methods, say CreamFL + FHE? It would be nice to see some more relevant baselines. \n\n- While paper is easy to follow, certain parts of the paper are unclear or incoherent. See questions.\n\nOverall, I think the experimental results in the paper are nice but the setting is limited. The motivation for the proposed approach is not clear to me. I would like authors to improve on these aspects in future.",
            "- 1. The paper lacks an in-depth exploration of the integration between homomorphic encryption, quantum computing, and federated learning. There is insufficient discussion on how these technologies work together and how their respective advantages are reflected within the framework.\n2. The argumentation regarding homomorphic encryption technology is insufficient. The paper employs the CKKS scheme but lacks a thorough discussion on security analysis and threat models, including potential attack methods and countermeasures, which may weaken the paper's discourse on privacy protection. For example, references such as \"Remark on the Security of CKKS Scheme in Practice\" by Jung Hee Cheon, Seungwan Hong, and Duhyeong Kim (2020) and \"On the Security of Homomorphic Encryption on Approximate Numbers\" by Li, B., Micciancio, D. (2021) should be considered.\n3. In the experimental section, the paper lacks information on the parameter configuration of the CKKS scheme and its corresponding security levels, which may affect the reproducibility and practicality of the experimental results.\n4. Additionally, the paper lacks technical innovation, merely combining homomorphic encryption and quantum computing for use in federated learning.\n5. The layout of some figures in the paper could also be improved, as some images are too small to read comfortably.",
            "- The paper lacks sufficient detail on the practical implementation challenges associated with deploying the proposed quantum-enhanced federated learning framework.\n\nThe paper discusses the impact of homomorphic encryption on model accuracy in the introduction and related works. However, the discussion and citations related to this topic should be expanded to provide a more comprehensive context. Additionally, the experiments could better reflect this aspect by including different hyperparameters or model structures to illustrate the effects more thoroughly.",
            "- 1- In Algorithm 1, the decryption seems to happen on the server side, which is problematic. If the server has access to the secret key, it can decrypt each client's model parameters individually, making the encrypted aggregation redundant, so the purpose of the encryption should be clarified.\n2- The simulation (dashboard_src/client.py) the implementation differs from Algorithm 1 and shows that only clients have access to the secret key. Clients send encrypted model updates to the server, where aggregation occurs on encrypted parameters, and the aggregate value is returned to each client for decryption. However, this scenario has a critical weakness: for the server to aggregate encrypted parameters, all clients must share the same secret key. The paper doesn't specify how clients share this key without server knowledge, particularly if all communication goes through the server. If secret sharing is used, there should be additional communication channels specified; otherwise, the purpose of FHE in this scenario is questionable.\n3- The paper lacks theoretical proof demonstrating why quantum models should perform better under FL+FHE compared to classical approaches. Additionally, some simulation results appear inconsistent with theoretical expectations mentioning in point 5.\n4- The application of QNN in this work lacks clarity. It appears disconnected from the FL logic and CKKS encryption mechanism, serving only as a comparison between implementing FL+FHE with classical models versus QNN+FL+FHE .\n5- Under ideal conditions (perfect communication, infinite rounds), FL can at best match centralized learning's performance. It is unclear why QFL achieves higher accuracy compared to the centralized version (as shown in Tables 3 and 4 for DNA, MRI, PCOS, and multimodal datasets). Also, QFL+FHE outperforms both QFL and centralized quantum approaches in DNA and multimodal cases which is not compatible with FL concept.\n6- The communication cost for FHE implementation is not addressed. The paper should explain both the communication and computation complexity of adding FHE to the FL process."
        ]
    },
    "zyGrziIVdE": {
        "venue": "ICLR 2025",
        "title": "Exploration by Running Away from the Past",
        "link": "https://openreview.net/forum?id=zyGrziIVdE",
        "abstract": "The ability to explore efficiently and effectively is a central challenge of reinforcement learning.\nIn this work, we consider exploration through the lens of information theory.\nSpecifically, we cast exploration as a problem of maximizing the Shannon entropy of the state occupation measure.\nThis is done by maximizing a sequence of divergences between distributions representing an agent's past behavior and its current behavior.\nIntuitively, this encourages the agent to explore new behaviors that are distinct from past behaviors.\nHence, we call our method RAMP, for ``$\\textbf{R}$unning $\\textbf{A}$way fro$\\textbf{m}$ the $\\textbf{P}$ast.''\nA fundamental question of this method is the quantification of the distribution change over time.\nWe consider both the Kullback-Leibler divergence and the Wasserstein distance to quantify divergence between successive state occupation measures, and explain why the former might lead to undesirable exploratory behaviors in some tasks. \nWe demonstrate that by encouraging the agent to explore by actively distancing itself from past experiences, it can effectively explore mazes and a wide range of behaviors on robotic manipulation and locomotion tasks.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            5,
            3
        ],
        "strengths": [
            "- Though the problem of state space exploration is very extensively covered in the RL literature, the proposed RAMP method provides what appears to be a novel approach to accelerating state space coverage. Due to its strategy of choosing policies maximizing divergence of state space coverage from that achieved by previous policies, it makes sense that RAMP will be more effective at rapidly exploring the state space than existing unsupervised RL methods (e.g., APT, SMM, Proto-RL) that simply maximize state occupancy measure entropy, and the experiments provide some support to this. Moreover, though the actual learning procedure used in RAMP is essentially a combination of existing techniques ([Eysenbach et al., 2020] for $r_{KL}$, [Durugkar et al., 2021] for $r_{W}$, and SAC [Haarnoja et al., 2018]), the combined approach detailed in Sec. 3.4 and Algorithm 1 appears to be novel and is interesting, and the fact that both KL-divergence and Wasserstein distance versions of RAMP are provided adds to its flexibility and significance. For these reasons, RAMP is likely of interest to the community and definitely merits further investigation.",
            "- 1. The problem addressed is important to the community.\n2. The new objective function is theoretically motivated and provides new insights to compute good exploration policies.",
            "- The strength of the paper lies in a fairly clear presentation of the motivation and methodology. The idea of \"running away from the past\" is not strictly novel but the paper proposes an algorithmically viable way to instantiate such an idea. The paper presents a fairly clear math formulation and has carried out ablations on choices of the algorithmic designs. The experimental ablation also seems fairly comprehensive.",
            "- The paper is written well, and the proposed intrinsic exploration objective is novel. The use of Wasserstein distance instead of the typical KL divergence is an interesting/novel choice."
        ],
        "weaknesses": [
            "- Despite the strengths discussed above, I have concerns about the experimental evaluation and theoretical results:\n1. Most importantly, the \"state coverage\" performance metric upon which the comparisons of Sections 5.2 and A.1 rely is insufficiently justified as a good proxy for measuring exploration and for making fair comparisons between the algorithms considered. As described in the third paragraph of Sec. 5.2, this metric is obtained by discretizing the space of Euclidean (x-y or x-y-z) coordinates of the agent's state space, recording whether each grid cell has been visited or not during training, then returning the percentage of the grid cells that have been visited. There are two main issues with using this notion of state coverage as a proxy for exploration. First, the state space dimensions in most of the environments are far larger than 2 or 3 (e.g., 18 for HalfCheetah, 113 for Ant), and, for many of these environments, pose information other than location in Euclidean space (e.g., joint angles, velocities) is far more important for learning to operate within the environment and for specific downstream tasks. Second, recording only whether a grid cell has been visited or not ignores more complex visitation behavior, such as the empirical state visitation frequency defined at the beginning of Sec. 2. To render the state coverage metric used more meaningful, it would be helpful to include ablations over the other dimensions of $S$ or comparison with other coverage notions, such as Shannon entropy of the empirical state visitation frequency.\n2. Implementation details for the RAMP algorithm, the algorithms compared with, the discretization used in the state coverage metric, and other aspects of the experiments are not provided. The experimental results are therefore not reproducible in their current form. In addition, across all experiments, the lack of implementation details makes it difficult to assess the fairness of comparison with existing methods and even the comparisons between $RAMP_{KL}$ and $RAMP_{W}$. This makes it difficult to evaluate the significance of the experimental results, weakening the overall contribution. To remedy these issues, a thorough description of the implementation details is needed.\n3. The qualitative results in Sec. 5.1 are difficult to understand, leaving the practical differences between $r_{KL}$ and $r_W$ unclear. See the questions below for specific concerns.\n4. The connection between Theorems 2 and 3 and the rest of the paper is unclear, and the assumptions made are so strong as to immediately imply the results. For the former concern, a description of what $\\pi$ and $\\pi'$ of Theorems 2 and 3 correspond to in the RAMP method is missing, making it unclear how the results are meant to be applied. Regarding the second concern, it is assumed variously that $|| \\rho^{\\pi} - \\rho^{\\pi'} || \\leq \\varepsilon_0$, $|| \\hat{r} - r^{\\pi} || \\leq \\varepsilon_1$, and that the average reward $J_{\\hat{r}}(\\pi') = \\langle \\rho^{\\pi'}, \\hat{r} \\rangle$ is sufficiently larger than $J_{\\hat{r}}(\\pi) = \\langle \\rho^{\\pi}, \\hat{r} \\rangle$ to ensure that the desired inequalities hold. Under these assumptions, the proofs follow with some straightforward manipulation of inequalities. To make the results more consequential, it would be helpful to clarify how they are meant to be applied in the context of the paper, then weaken the assumptions accordingly.",
            "- 1. In Section 2, different justifications for introducing the learning objective pursued by the agent are wrong or weak in several aspects:\n\na. The justification line 108 for going from equation (1) to equation (2) is in my opinion wrong. Using the entropy of the policy as proxy to the entropy of the state distribution is a huge approximation. Maximizing the entropy of the policy does not provide a good state coverage in general nor in most practical cases. Note that if it was sufficient to maximize the entropy of the policy to get a uniform distribution of states, it would not be necessary to introduce a complex algorithm as the authors do.\n\nb. Line 128 authors justify to use the Wasserstein distance instead of the KL-divergence as the KL does not account for a potential geometry in the state space. This fact result from the original choice to define as exploration objective the entropy over the state space, which does not account for a potential geometry of the state space. So by choosing to maximize the Wasserstein distance instead of the KL, the authors change the original hypothesis that that the objective is to have high state entropy. While it can be discussed that it is a potential better framework to account for some geometry, it makes most of the previous mathematical justifications irrelevant.\n\n2. The authors claim in Section 3.4 that it is sufficient to optimize with any RL algorithm the reward model from Section 3.2 or Section 3.3 to maximize the objective equation (2) or equation (4). It is equivalent to neglecting the entropy of the policy. Authors, nevertheless, eventually use SAC, which is an algorithm that regularizes the MDP rewards with the log-likelihood of actions. This should be clarified.\n\n3. Only the final values are reported in the experimental section. From my personal experience, complex exploration methods may be unstable, and the learning curves provide important insights. Adding them in the paper would make the results more trustworthy.\n\n4. In the experiments, there is no statistical evidence that the method at hand outperforms the concurrent methods. Most confidence intervals overlap.\n\n5. I think that the related work should include [1, 2], and probably other, more recent, works.",
            "- The idea of \"running away from the past\" is not strictly novel. From a theoretical standpoint, running away from old trajectories might not always be optimal and it is not clear theoretically what is gained by adopting such an approach. From an empirical standpoint, the ablations are carried out on the continuous control tasks, most of which do not seem to require extensive exploration to solve. It is not very clear if the claimed gains are really due to the exploration bonus, or some other unknown side effect.",
            "- 1) Prior Work/Baselines: The paper misses several crucial works on intrinsic exploration (c.f., 1, 2, 3 for a survey). Particularly, there are works that use the model epistemic uncertainty/disagreement as an intrinsic reward which works well in practice and also scales favorably (4., 5., 6.). \n2) Theory: In particular, the model epistemic uncertainty is theoretically a well-studied objective (7., 8.). In 8, the authors derive a connection between maximizing the model epistemic uncertainty and maximizing information gain/conditional entropy of the trajectories, while also showing convergence for sufficiently smooth dynamics. \n3) Unclear motivation: Given the theoretical and experimental strengths of the method discussed above, its unclear to me what particular gap the authors are trying to address with their intrinsic reward. I'd appreciate the authors elaborating further on this. Furthermore, I think all the aforementioned works should be discussed in the paper and in particular one of the baselines should use the model epistemic uncertainty as the intrinsic reward. Perhaps one weakness the authors might raise is that the aforementioned works are computationally more expensive as they have to learn an ensemble of networks to quantify disagreement. However, this should also be empirically shown in the experiments (as the proposed method also learns a model to estimate the intrinsic reward). \n4) Hyperparameters are not provided in the paper, which makes it difficult for me to assess how sensitive the results are to the choice of hyperparams. In particular, I am curious about how $\\beta$ affects the performance of the algorithm. How can we appropriately select $\\beta$? Furthermore, doesn't the method suffer from sample inefficiency for large values for $\\beta$, i.e., when lots of data from the buffer is discarded?\n5) Scalability: Its unclear to me whether the proposed method would scale reasonably well to more high-dimensional settings such as POMDPs/visual-control tasks (note that 5, 6 also work for POMDPs). Could the authors elaborate further on this?\n\nI am happy to raise my score if my concerns above are addressed. \n\n1. https://arxiv.org/abs/2109.00157\n2. https://www.sciencedirect.com/science/article/pii/S1566253522000288?casa_token=ScYOIGv6D2wAAAAA:buNFoXMZLqPiWzo0CLpe3K-ac_nxundN5855FT0QwSnE6jhpm6VwPFS0UHyt1E9WXJePruqZsg\n3. https://www.mdpi.com/1099-4300/25/2/327\n4. https://arxiv.org/pdf/1906.04161\n5. https://arxiv.org/abs/2005.05960\n6. https://arxiv.org/abs/2110.09514\n7. https://arxiv.org/pdf/2006.10277\n8. https://arxiv.org/pdf/2306.12371"
        ]
    },
    "zweyouirw7": {
        "venue": "ICLR 2025",
        "title": "Spiking Transformer-CNN for Event-based Object Detection",
        "link": "https://openreview.net/forum?id=zweyouirw7",
        "abstract": "Spiking Neural Networks (SNNs) enable energy-efficient computation through event-driven computing and multiplication-free inference, making them well-suited for processing sparse events. Recently, deep Spiking Convolutional Neural Networks (CNNs) have shown energy efficiency advantages on event-based object detection. However, spiking CNNs have been limited to local and single-scale features, making it challenging for them to achieve better detection accuracy. To address this challenge, we propose a hierarchical Spiking Transformer-CNN (i.e., Spike-TransCNN) architecture, which is the first attempt to leverage the global information extraction capabilities of Spiking Transformers and the local information capture abilities of Spiking CNNs for event-based object detection. Technically, we first propose using the Spiking Transformer to extract global features and employ a multi-scale local feature extraction CNN module to complement the Spiking Transformers in local feature extraction. Then, we design intra-stage and inter-stage feature fusion modules to integrate global and multi-scale local features within the network architecture. Experimental results demonstrate that our Spike-TransCNN significantly outperforms existing SNN-based object detectors on the Gen1 dataset, achieving higher detection accuracy (mAP 0.336 vs. 0.321) with lower energy consumption (5.49 mJ vs. 7.26 mJ). Our code can be available in the supplementary materials.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- This paper proposes a novel hybrid architecture of Transformer and CNN based on pulse neurons, which has achieved good results in event object detection tasks. Among them, the author has made a reasonable design for the Transformer branch and CNN branch under the spikng neuron, and designed a feature fusion method that conforms to the hybrid architecture. The experimental part is relatively complete, with reasonable and clear composition.",
            "- 1. This work is the first attempt to combine Spiking-Transformer and Spiking-CNN architectures for event-based object detection.\n2. Spike-TransCNN achieves competitive performance on the Gen1 dataset, whose mAP is 0.336 and energy consumption is 5.49 mJ.\n3. The visualization and graphical presentation are of high quality and clarity.",
            "- 1. The hierarchical integration of Spiking Transformer and CNN blocks is well-motivated and effectively leverages the strengths of each architecture, leading to improvements in both accuracy and energy efficiency.\n2. The paper provides robust experimental validation, including comparisons with state-of-the-art methods, and energy efficiency analyses, showcasing the advantages of Spike-TransCNN over conventional ANN-based methods.\n3. The focus on energy-efficient event-based object detection aligns with the needs of edge-computing applications, and the results demonstrate significant energy savings, which is a major contribution in neuromorphic computing.",
            "- 1. The paper proposes the Spike-TransCNN model, successfully integrating global information from Spiking Transformer with local information from Spiking CNNs, which is beneficial for future development in this field.\n2. The paper introduces several interesting blocks, such as STS and SCB, which effectively improve the model's performance."
        ],
        "weaknesses": [
            "- The introduction of this article does not highlight the innovative points. The combination of Transformer and CNN has been proven effective and maturely applied in ANN. The use of LIF neurons for Spiking is not an important innovation point. Furthermore, in the theoretical section, it is evident that both the Transformer architecture and CNN module were designed with reference to previous tasks, which does not constitute an important innovation point to support the paper. The subsequent feature fusion module described two types separately, but the differences between types and the issues addressed were not elaborated in detail. Reading through the methods section, the innovative points and logical description of the methods are similar to the module stacking without highlighting the author's own analysis.",
            "- 1. The performance on the 1Mpx dataset (0.250) is significantly inferior to existing methods (0.483), without adequate explanation or analysis.\n2. Lack of comprehensive comparisons with recent state-of-the-art SNN detection methods, such as \"Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection.\"\n3. The whole architecture primarily transplants the established Transformer-CNN paradigm into the SNN domain, with limited Innovation.\n4. The paper exhibits an overreliance on descriptive language while lacking theoretical analysis for the performance improvements of the proposed architecture.",
            "- 1. Although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. Including pseudo-code or flow diagrams might enhance the reader\u2019s understanding of the model\u2019s operation.\n2. While the results on the Gen1 dataset are compelling, it would strengthen the paper to evaluate the model on more diverse datasets, particularly larger or more complex event-based datasets, to demonstrate generalizability.\n3. The paper could benefit from a discussion on how Spike-TransCNN compares with hybrid SNN-ANN models, given their potential to balance energy efficiency and performance. This would contextualize the performance and energy efficiency gains of Spike-TransCNN more effectively.\n4. While there is an ablation study on some components, further exploration on the impact of various hyperparameters (e.g., number of time steps, membrane potential thresholds) could provide insights into optimizing the architecture for different applications.\n5. Consistent terminology, particularly around spiking mechanisms and attention mechanisms, would improve readability. Some abbreviations and terms could be clarified for non-specialist readers.\n6. The paper includes visualization results, but providing side-by-side comparisons with other models on challenging scenarios could offer a clearer view of the model\u2019s strengths in handling occlusions and motion.",
            "- 1. The integration of global information from Spiking Transformer with local information from Spiking CNNs was first proposed in [1], rather than in this work.\n2. The used or proposed modules in this paper, including SSA, SCB, and both intra-stage and inter-stage spike feature fusion modules, fail to preserve the spiking characteristics of SNNs due to their incorporation of non-spiking computations. Consequently, Spike-TransCNN would be more accurately categorized as a hybrid ANN-SNN model rather than a pure SNN.\n3. The paper omits comparisons with other pure SNN models (such as SpikeYOLO [2]) and hybrid models (like EAS-SNN [3] and SpikingViT [4]). Furthermore, the model's mAP performance on GEN1 and 1Mpx datasets is substantially inferior to these state-of-the-art approaches.\n4. The motivation for proposing STS is unclear - why is it used in shallow layers instead of SSA?\n5. The motivation for proposing SCB is also inadequately explained - on lines 260 and 263, it merely states that it \"leverages the local feature extraction capabilities\" and \"captures local multiscale information\". However, this raises questions: Couldn't standard 3x3 and 5x5 convolutions achieve the same objective?\n6. The reported energy consumption raises significant concerns. Given that Spike-TransCNN has double the parameters of SFOD with similar firing rates, and extensively employs SPIKE-ELEMENT-WISE ADDITION operations (introducing non-spiking computations) across its modules, the claimed lower energy consumption compared to SFOD requires further justification.\n7. The paper contains formatting issues, specifically on line 382 where the Firing Rate and Energy (mJ) are overlapping.\n\n[1] Yao M, Hu J K, Hu T, et al. Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips[C]//The Twelfth International Conference on Learning Representations.\n[2] Luo X, Yao M, Chou Y, et al. Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection[J]. arXiv preprint arXiv:2407.20708, 2024.\n[3] Wang Z, Wang Z, Li H, et al. EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks[J]. arXiv preprint arXiv:2403.12574, 2024.\n[4] Yu L, Chen H, Wang Z, et al. Spikingvit: a multi-scale spiking vision transformer model for event-based object detection[J]. IEEE Transactions on Cognitive and Developmental Systems, 2024."
        ]
    },
    "ziB549CQ30": {
        "venue": "ICLR 2025",
        "title": "Solving the Fuzzy Job Shop Scheduling Problem via Learning Approaches",
        "link": "https://openreview.net/forum?id=ziB549CQ30",
        "abstract": "The fuzzy job shop scheduling problem (FJSSP) emerges as an innovative extension to the conventional job shop scheduling problem (JSSP), incorporating a layer of uncertainty that aligns the model more closely with the complexities of real-world manufacturing environments. This enhancement, while enhancing its applicability, concurrently escalates the computational complexity of deriving solutions. In the domain of traditional scheduling, neural combinatorial optimization (NCO) has recently demonstrated remarkable efficacy. However, its application to the realm of fuzzy scheduling has been relatively unexplored. This paper aims to bridge this gap by investigating the feasibility of employing neural networks to assimilate and process fuzzy information for the resolution of FJSSP, thereby leveraging the advancements in NCO to enhance fuzzy scheduling methodologies. To this end, we present a self-supervised algorithm for the FJSSP (SS-FJSSP). This algorithm employs an iterative mechanism to refine pseudo-labels, progressively transitioning from suboptimal to optimal solutions. This innovative approach adeptly circumvents the significant challenge of procuring true labels, a common challenge in NCO frameworks. Experiments demonstrate that our SS-FJSSP algorithm yields results on a par with the state-of-the-art methods while achieving a remarkable reduction in computational time, specifically being two orders of magnitude faster.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1.A new problem of JSSP is investigated and solved, considering the fuzzy processing time.\n\n2. The proposed method performs better than the compared method.\n\n3. The proposed method can achieve self-learning by using pseudo labels for training.\n\n4. The idea is good and the paper is well-written.",
            "- The paper organized well, but contains some really strange text at times. I find it hard to believe the line \"The surge in deep learning has catalyzed the emergence of neural combinatorial optimization (NCO) sparking a burgeoning interest in leveraging...\" was not generated by an LLM (note that I do not believe the paper was written by an LLM, just this part). Regardless of where it comes from, it is not so good. The document is overall understandable, the typos do not impact this. It would be best, though, if the authors used GPT to support fixing typos, or just watch the spellchecker (e.g., 'algotirhm' on page 6)\n\nThe main novelty is the application of the semi-supervised learning approach from Corsini et al. (2024) (note that this paper is cited with its arxiv version, but actually it is accepted to NeurIPS 24) to the FJSSP with a few twists. There are some interesting ideas in here. The semi-supervised approach is particularly interesting, although the novelty in this paper is (1) only a small advancement over the previous work of Corsini et al. and (2) not examined in an ablation analysis at all (see below). The second claim to novelty about a \"refinement process\" seems to me to just be the Corsini paper again, so I am not sure what is meant here -- the perturbation technique?",
            "- The paper proposed a deep network to address addresses this gap by examining the potential of neural networks to process fuzzy information for solving FJSSP.",
            "- The integration of neural networks with fuzzy scheduling and self-supervised learning seems to be interesting and important.\n\nThe paper is generally well-structured, and the explanations of both the problem and the proposed solution are clear.\n\nThe proposed solution addresses a critical issue in fuzzy scheduling by improving computational efficiency while maintaining accuracy."
        ],
        "weaknesses": [
            "- 1. The experiments are not sufficient to show the contributions of the proposed method without comparing other methods.\n\n2. The novelty is limited since the fuzzy job shop scheduling problem has been investigated and the paper looks like an extension of existing work by adding the fuzzy processing time.",
            "- There are two major issues with the paper that right now prevent me from being more positive about it.\n\n1. The new perturbation strategy is not experimentally validated at all. First note that it is said that \"Corsini et al. (2024) ... select the optimal solution as the pseudo-label\". My understanding of the Corsini paper is that they use the best solution from the sample. I assume that this is just a typo though. More importantly, where does P_p come from and who sets it? Why is this value not experimentally validated? The experimental section says that it is set to 0.05 and that's all. First, turning it off is an important ablation step, and showing the sensitivity is also very interesting. I also wonder why the authors do not try this technique out on the original JSSP? It would improve the generalizability of this paper, which is rather sharply focused on the FJSSP right now and, in all honesty, ICLR is not an applications conference. The reviewers need to at least believe there is a chance of generalization here, but right now there is no evidence.\n2. The experimental results are not complete. They compare the approach with a CP model, mostly lose, and then claim victory because it is faster. First, CP is proving optimality and this method is not, so it is a completely apples to oranges comparison. Second, where do these results even come from in the Afsar et al. (2023) paper? I do not see these runtimes; on the contrary, Table 5 in the Afsar paper indicate that S6.1-4 only need 0.1 seconds to solve to optimality and the heuristic approach HTS only 0.2 seconds. On S10.1-4 on average 76 seconds are needed for the CP model and 1.0 for HTS. This brings me to a question: where is HTS in this work? How does HTS compare to the SS-FJSSP approach? I note that this comparison is quite important as SS-FJSSP has many domain specific components in it, so the method is not interesting in and of itself -- it ought to be as good as the current techniques available. One more note here: the authors make note that they do not re-implement the CP model because the source code is not available... but we are talking about a model with 5 constraints, this would not be much of a challenge...\n\nI note that I am also a bit wary about the FJSSP in general. The fuzzy literature makes rather simplifying assumptions (triangular distribution), thus avoiding any of the hard parts of modeling stochastic optimization problems while still claiming to be more realistic. While I agree that it is more realistic than a standard JSSP, the question is whether it is really worth the trouble compared to just modeling a two-stage, robust or chance constrained problem. My opinion is that it is not because the triangular distribution is unrealistic for a variety of reasons (real distributions are often shifted, can have long tails, etc.). Thus, I think the authors are betting all their money here on a rather weak application that is not at the level of top conferences.\n\n\nSome other notes about the paper for the authors to improve:\n1. The references in the introduction about the FJSSP are not terribly convincing that it is an important problem. For a paper at a top conference, I would expect papers at top conferences (or an argument why those venues have overlooked this important problem)\n2. The second paragraph of the introduction is not well written; it is very long and just meanders through the literature.\n3. Page 3: Additional should be addition\n4. The explanation about the arrow operator in 2.2 is unclear (namely i->x or x->j) -- I think this can be interpreted (incorrectly) as banning chains of operations.\n5. The mathematical model ought to have a min before the max so it is more clear. Also the math model does not use the max operator in the constraints, which seems like it ought to be necessary? I admit I am not so familiar with modeling with fuzzy variables, so maybe there is a convention there I am not aware of.\n6. The jobs numbers ought to be more clear in Figure 1 so it is easier to understand the scheduling scheme.",
            "- The authors just compared their method with one method, which is not enough to verify the effectiveness of the proposed method. There are state-of-the-art evolutionary computation algorithms for FJSSP. What is the advantage of the proposed method over them? The comparison between the proposed method and them should be implemented to analyze the difference.  Meanwhile, the proposed algorithm is not very competitive in items of FMS, and its performance is poorer than that of the comparison algorithm in some problems. \nThe paper lacks parameter analysis and ablation study, making a gap between the results and the conclusion.",
            "- My primary concern about this paper is regarding its novelty. While using neural combinatorial optimization (NCO) techniques to solve FJSSP problems might be novel, the high-level graph representation of the FJSSP problem adopted in this paper is very similar to many existing studies. It seems that the key difference is the introduction of fuzzy model related features and feature vectors. However, the technical novelty associated with these features has not been properly highlighted and justified.\n\nMeanwhile, the training strategy does not seem to be new. Perhaps random solution selection is a novel element of the training algorithm. Nevertheless, the effectiveness of this selection mechanism is only discussed intuitively. More thorough theoretical and empirical analysis may be necessary to clearly quantify its importance for the newly proposed NCO system to find near-optimal solutions. To my understanding, the analogy with genetic algorithms does not reveal the true advantages of using random solution selection techniques, since the proposed NCO system is not closely related to genetic algorithms.\n\nThe baseline algorithms adopted in this paper were published about 8 to 9 years ago. It is important to include more recent baselines in the experimental comparison to truly understand the technical advancement introduced by the new approach. In particular, to clearly demonstrate the advantages of the newly developed encoder-decoder network architecture, several existing approaches using different GNN architecture designs should be experimentally examined and compared in this paper."
        ]
    },
    "zgs450VzkU": {
        "venue": "ICLR 2025",
        "title": "Test-Time RAG: Enhancing Long Context Understanding in LLMs with Retrieval-Augmented Mechanisms",
        "link": "https://openreview.net/forum?id=zgs450VzkU",
        "abstract": "Large Language Models (LLMs) are becoming increasingly pivotal in applications that depend on extensive personalized context, such as conversational agents and specialized task-oriented systems. In these scenarios, effective long-context handling is essential to support agentic tasks and enhance in-context learning capabilities. To address this challenge, we propose a novel integration of Retrieval Augmented Generation (RAG) techniques with LLMs, designed to enhance their ability to effectively manage and utilize large contextual information only available at test time. Our methodology, Test-Time RAG (TTRAG), enriches LLMs by dynamically generating novel conditional embeddings coupled with query rewriting and utilizing semantic search to retrieve the most relevant document chunks at test time. This process preserves the context\u2019s meaning and enhances the model\u2019s responsiveness and accuracy in knowledge-intensive Question Answering (QA) tasks. Our evaluations demonstrate our system\u2019s ability synthesize and retrieve information across extensive texts: HotpotQA (+17.29%), QASPER (+4.39%), and Natural Questions (+8.73%), demonstrating the effectiveness of TTRAG across varied context lengths from 1 million to 9.6 million tokens.",
        "decision": "Reject",
        "review scores": [
            3,
            5,
            3,
            3
        ],
        "strengths": [
            "- The authors propose to iteratively rewrite the query to improve search results for extensive personalized context, which is a timely problem of interest, with a significant body of literature.",
            "- The paper handles an important problem around long-context RAG. Their approach of using a conditional compute is important and shows the versatility of tasks in a question answering system and how a LLM prompt may not be the best way to do all queries. Query rewriting based on iterative search is an interesting idea too. The problem has practical implications in both research and industry.",
            "- The starting idea is insteresting: dealing with streaming and changing long context with tailored RAG.",
            "- 1. The author conducts experiments on 3 RAG datasets with 2 different LLMs and 2 different embedding models to demonstrate the advantage of test-time RAG."
        ],
        "weaknesses": [
            "- Overall confused by the claims of the objective of the work and the actual paper body, it is not clear how the work relates to \u201clong context understanding\u201d, \u201cpersonalised context\u201d, or \u201cspecialised task-oriented systems\u201d.  The authors proposed a rewriting query scheme with iterative embedding, with substantial details missing (and therefore, conclusiveness), see below. \n\n- There is a substantial amount of work on query rewriting and agentic RAG which is not mentioned at all. There is no comprehensive literature review, with the \u201cmotivation\u201d section being almost unrelated. Not able to compare to other related papers like \u201cQuery Rewriting for Retrieval-Augmented Large Language Models\u201d, among many others. \n- \u201cTest-time\u201d term is misleading. Similarly, the authors didn\u2019t clearly show benefits of the approach specifically for \u201clong context\u201d, as claimed. The examples do not illustrate long context, only short context.\n- Benchmarks do not seem well suited for the specific problem(s) claimed by the authors. Also, BM25 is not an embedding model.\n- Motivation for the conditional compute not clear, context with long context understanding missing. Seems ad-hoc and not reproducible.\n- A significant number of question arise, which is caused partially by the lack of reference to state of the art methodologies: how does this work compare to reranking methodologies? What is the latency of the pipeline? When does it stop the iteration loop (what are the stop criterion)? What is the effect of these stop criterion? How significant are the small increments in performance claimed by the authors? \n- Lack of overall reproducibility for different datasets",
            "- The paper presents an interesting new perspective on long context retrieval. However the work has some limitations worth considering\n\n1.  Section 3.1 introduces conditional embeddings however the authors do not state they are using pretrained embeddings for the same in this section. It reads like this is a custom embedding. Much later, in Table 1 we understand that the authors have used GPT3.5 embeddings and llama embeddings for the same. This makes reading section 3.1 hard without forward referencing\n2. It is not clear why an average across the unmasked tokens in CE embeddings is better than taking a  weighted average (with learned weights)\n3. Scalability of solution - how does this solution scale with respect to large datasets of documents ?\n4. I do not understand on how you prevent Algorithm 1 to create irrelevant questions if the retrieval is of poor quality. The approach runs the risk of run away poor results on questions which do not have good answers in the database\n5. Lack of clarity in what is CE and what is \"CE (Filtering only)\" in Table 1.\n6. Lack of clarify that the model is a LLM model used only for question answering\n7. How is F1 score computed? Over how many documents i.e. what is your retrieved k?\n8. Whilst I appreciate the principle behind the conditional compute, it's relevance to the rest of the paper is relatively weak. Also how is the LLM leverage For example for count-based queries the map-reduce paradigm that has been recommended, does it mean the LLM generates the code for map-reduce? if not how will this be implemented?\n9. The example in Figure 6 is a good example of my earlier comment on Conditional Rewrite. If the \"Call me Manana\" is not present in the document repository the query rewrite may tag it to some random group (based on what is retrieved) in the way the algorithm is designed/presented. it is not clear how this run-away is prevented to attach it to something random in what has been retrieved. Similar comment on fig 7/8\n\n\nOverall, while interesting the paper has many unanswered questions. First, it is not clear on the cost vs. benefit analysis for the CE approach. Second, the conditional rewrite has a risk of runaway poor results which need to be addressed. Finally conditional compute is explained but how are some of it implemented is vague. In view of this, this paper in the current form leaves many questions unanswered on its applicability. More clarity on writing is preferable as there is a need to read further to understand the methodology.",
            "- 1. **Poor Writing and Organization**: The presentation of the paper is confusing and difficult to follow due to ambiguous and hard-to-read sentences, poor organization, and unclear structuring of key arguments.\n\n2. **Inconsistent Focus**: The introduction starts by discussing personalized and trustworthy AI, but the subsequent sections focus on long-context benchmarks without sufficiently connecting to the original arguments.\n\n3. **Strange Motivation for Test-time Processing**: The rationale for processing context at test time appears weak, especially given that user history or dynamically changing contexts could be indexed incrementally.\n\n4. **Lack of Component Integration**: The paper presents three distinct components (Conditional Embeddings, Iterative Query Rewriting, Conditional Compute) but does not adequately explain their interrelationship. The experiments are also conducted individually rather than for the overall method, which leaves the effectiveness of the integrated system unclear.\n\n5. **Inefficient Indexing**: The experiments on Conditional Embedding involve incorporating the query into the document indexing process, which makes it impractical because the indexed embeddings cannot be reused efficiently.\n\n6. **Weak Experimental Validation**: The experiments provided are not comprehensive or robust enough to fully validate the effectiveness of the proposed techniques.\n\n7. **Lack of Discussion or Justification**: There is minimal discussion on why or how the proposed techniques improve the results, leaving the reader with questions about the validity and significance of the approach.",
            "- 1. Poor writing\n\nThe author\u2019s presentation is hard to understand. I cannot understand the description of the methods even after reading 10 times, such as the sentence \u201cutilize encoder-decoder variants which jointly condition context on text appearing before and after a given token in a sequence\u201d (line 164).\n\n2. Impracticable time consumption\n\n\u201cConditional embedding\u201d needs to encode each document in test-time, which will certainly take extremely much more time. Even though the author proposes \u201cconditional computing\u201d to reduce the latency, it can only function on a narrow range of task types such as counting or KV retrieval. But the author did not compare the time latency with that of standard RAG methods on more general problems. This casts doubt on the practical feasibility of this approach in real-world scenarios.\n\n\n3. Query rewriting may introduce more issues\n\nThe author claims that query rewriting can leverage knowledge in LLM\u2019s pretraining data. However, it can be affected by LLM\u2019s hallucination, which is exactly what RAG technology wants to solve. Therefore, in most RAG scenarios, LLMs are allowed to only use knowledge in the retrieved documents instead of their inner knowledge.\n\n4. The resolution of the figures is low. And the sizes and positions of them are also inapt."
        ]
    },
    "zV6D212c7Q": {
        "venue": "ICLR 2025",
        "title": "Masked Cross-attention Adapters Enable the Characterization of Dense Features",
        "link": "https://openreview.net/forum?id=zV6D212c7Q",
        "abstract": "Learning meaningful representations is a core topic of deep learning. Throughout the last decade, many strategies for learning image representations have been proposed involving supervision and self-supervision and various data sources. \nIn most current work, evaluation is focused on classification tasks while neglecting dense prediction tasks, possibly because linear probing is more challenging in the latter case.\nFurthermore, dense prediction heads are often large and come with specific inductive biases that distort performance measurement further.\nIn this work we propose masked cross-attention adapters (MAXA), a minimal adapter method that is capable of dense prediction independent of the size and resolution of the encoder output. This allows us to make dense predictions using a small number of additional parameters ($<0.3 $%) while allowing for fast training using frozen backbones.\nUsing this adapter, we run a comprehensive evaluation assessing instance awareness, local semantics and spatial representation of a diverse set of backbones. \nWe find that DINOv2 outperforms all other backbones tested - including those supervised with masks and language - across all three task categories.  \nCode is available at https://to.be.released.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- The ability to perform a meaningful cost-effective evaluation of backbones for their dense prediction capabilities is currently lacking, and as a result, only a limited number of studies offer meaningful evaluations. The proposed method is well-motivated and clearly described, making it easy to follow. The authors provide a solid introduction to the problem and include an informative related work section. Additionally, the ablation studies demonstrate that the advantages of certain design choices are consistent across multiple tasks and different pretrained backbones.",
            "- - Comprehensive Benchmark for Dense Vision Tasks: The paper makes a valuable contribution by introducing a comprehensive benchmark specifically designed for evaluating the dense prediction capabilities of pre-trained vision encoders, addressing a notable gap in the current landscape of benchmarks that primarily focus on classification tasks. \u00a0 \n- Methodological Rigor: The authors employ a rigorous methodology, including the use of masked cross-attention adapters (MAXA) to enable fair comparisons across different encoders. The choice of MAXA is well-justified, as it allows for fast training and evaluation. \u00a0 \n- Clarity and Insightful Presentation: The paper presents a clear and well-organized set of experiments, covering a diverse range of pre-trained models and dense tasks. The results are presented in a readily understandable and comparable manner, providing valuable insights into the relative strengths and weaknesses of different encoders for dense prediction tasks.",
            "- - MAXA can adapt to downstream tasks with only a small number of additional parameters (less than 0.3%).\n- Since the backbone network is frozen during training, MAXA achieves faster training speeds.",
            "- - (1) The idea is simple and easy to read.\n    \n- (2) This research has potential to become a standard for evaluating the dense awareness of frozen features.\n    \n- (3) The authors explore various aspects of the capabilities of frozen features, providing a solid basis for evaluating the adapter's effectiveness."
        ],
        "weaknesses": [
            "- The relevance of a cost-effective dense prediction evaluation largely depends on its high correlation with currently optimal but more resource-intensive evaluation techniques. However, the experiments provided are insufficient to establish confidence in this correlation. While the authors evaluate MAXA across multiple tasks and backbones, they do not present a statistical analysis of its correlation with fine-tuned results, nor do they quantify the trade-off between training cost and performance compared to state-of-the-art techniques. Such context is necessary to make the presented evaluations meaningful.\n\nThe choice to follow simple FPNs and rely solely on information from the last layer appears questionable. Although Vision Transformers (ViTs) maintain the same resolution across all layers, it is doubtful that the final layer alone contains all the necessary information without fine-tuning the backbone. For instance, MAE demonstrated that linear probing performance of ViTs is not always a reliable indicator of fine-tuning performance. Furthermore, [1] showed that employing cross-attention readouts from every layer leads to significant performance improvements compared to using simple FPNs.\n\nminor: Lines 260-270 contain some incomplete sentences, such as \"SAM2 also good.\".\n\n[1] Chen, Zhe, et al. \"Vision Transformer Adapter for Dense Predictions.\" The Eleventh International Conference on Learning Representations.",
            "- - Limited Insight into Learned Representations: While the benchmark effectively compares the performance of different encoders, it lacks deep analysis regarding the specific representations learned by each encoder. Simply stating that \"DINOv2\" achieves the highest numbers isn't sufficient; the paper would benefit from a more in-depth investigation into the characteristics of the learned representations that contribute to performance differences. \u00a0 \n- Overlooking Architectural Biases: The paper does not explicitly address how architectural biases in different encoders might contribute to their performance on dense tasks. A discussion on this aspect would be valuable, as it could help disentangle the effects of pre-training from those inherent to the encoder architectures. \u00a0 \n- Potential Bias from MAXA: Although the authors justify the use of MAXA, the paper could be strengthened by exploring whether the findings hold consistent with other adapter methods or lightweight dense heads. This would provide further validation of the results and address potential concerns about biases introduced by the specific choice of MAXA. \u00a0 \n- Missing Comparisons with Key Adapter Methods: The paper lacks a direct comparison with other relevant adapter methods, such as ViT-Adapter and FeatUp. Including these methods in the evaluation would offer a more complete picture of the adapter landscape for dense prediction tasks.",
            "- - Although MAXA has been evaluated across three main task categories, these categories may not fully cover all possible visual tasks.\n- There is a lack of comparisons with other fine-tuning methods, such as Adapters Strike Back.\n- The novelty is relatively weak.",
            "- Major Weaknesses\n\n- (1) The literature cited is outdated. For instance, the authors state, \u201cAt the other end of the spectrum, using complex dense task heads, for example, Faster R-CNN (Ren et al., 2015) for object detection, adds a large number of parameters and introduces its own inductive biases.\u201d However, Faster R-CNN is nearly a decade old. The authors should clearly differentiate their approach from more recent works like ViTDet [1], ViT-Adapter [2], and Segmenter [3] in both the introduction and related work sections, as these studies also focus on developing lightweight dense task heads. Although ViTDet is briefly mentioned in the \u201cExperiment Design\u201d section, this reference is insufficient for establishing the distinction.\n    \n- (2) Lack comparison with zero-shot dense prediction using frozen features. Zero-shot segmentation using frozen features [4, 5] \u00a0from foundation models has been extensively studied. These models [4, 5] demonstrate strong segmentation performance with training-free dense heads.\n    \n- (3) The experimental comparison is insufficient. In the CLIP setting, the authors focus solely on ViT-based architectures (SigLIP for example), whereas ConvNet-based or hybrid architectures might be more appropriate for dense tasks. It is highly recommended that the authors include experiments with Hybrid CNN-Transformer architecture like ViTamin [6] and ConvNet architecture like CLIP-ConvNeXt [7]. These additional experiments are crucial, and I would consider raising the score if they are incorporated during the rebuttal phase.  \n    \n\n\nMinor Weaknesses\n\n- (1) In Figure 1, it is unclear how the M(q, \\sigma) is generated.\n    \n- (2) Section 3 mentions that \u201cspatial queries Q of size (HQWQ, 16)\u201d. It is unclear why the query dimension is chosen to be 16.\n    \n- (3) Concerns regarding reproducibility arise due to the unclear descriptions. Section 3 mentions that \u201cThis is realized through a small CNN operating on the output of all queries using transposed convolutions to increase spatial size.\u201d. Please specify the details of \"small CNN\". CNN usually refer to Convolutional Neural Network consisting with convolutional layer and non-linear activation layer. Please specify the type and number of layers, and the kernal size of each convolution operator.\n\n\n[1] Li Y, Mao H, Girshick R, et al. Exploring plain vision transformer backbones for object detection[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 280-296.\n    \n[2] Strudel R, Garcia R, Laptev I, et al. Segmenter: Transformer for semantic segmentation[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 7262-7272.\n    \n[3] Chen Z, Duan Y, Wang W, et al. Vision transformer adapter for dense predictions[J]. ICLR, 2023.\n    \n[4] Sun S, Li R, Torr P, et al. Clip as rnn: Segment countless visual concepts without training endeavor[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 13171-13182.\n    \n[5] Wang F, Mei J, Yuille A. Sclip: Rethinking self-attention for dense vision-language inference[C]//European Conference on Computer Vision. Springer, Cham, 2025: 315-332.\n    \n[6] Chen J, Yu Q, Shen X, et al. ViTamin: Designing Scalable Vision Models in the Vision-Language Era. CVPR. 2024. https://huggingface.co/jienengchen/ViTamin-XL-384px\n    \n[7] https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"
        ]
    },
    "zV2cgXk2aY": {
        "venue": "ICLR 2025",
        "title": "Sentinel: Multi-Patch Transformer with Temporal and Channel Attention for Time Series Forecasting",
        "link": "https://openreview.net/forum?id=zV2cgXk2aY",
        "abstract": "Transformer-based time series forecasting has recently gained strong interest  due to the ability of transformers to model sequential data. Most of the state-of-the-art architectures exploit either temporal or inter-channel dependencies, limiting their effectiveness in multivariate time-series forecasting where both types of dependencies are crucial. We propose Sentinel, a full transformer-based architecture composed of an encoder able to extract contextual information from the channel dimension, and a decoder designed to capture causal relations and dependencies across the temporal dimension. Additionally, we introduce a multi-patch attention mechanism, which leverages the patching process to structure the input sequence in a way that can be naturally integrated into the transformer architecture, replacing the multi-head splitting process. Extensive experiments on standard benchmarks demonstrate that Sentinel, because of its ability to ``monitor\" both the temporal and the inter-channel dimension, achieves better or comparable performance with respect to state-of-the-art approaches.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. The paper is well-written, with a clear presentation of both methodology and results.\n2. Experimental results are competitive and close to state-of-the-art (SOTA) performance.\n3. The code is made available, which facilitates reproducibility.\n4. Targeted Design for Temporal and Inter-Channel Dependency Modeling: The focus on addressing both temporal and inter-channel dependencies in time series data highlights a well-considered model design.",
            "- 1. its ability to capture both temporal and inter-channel dependencies crucial for multivariate time series forecasting.\n2. by utilizing a patching process, Sentinel introduces a new attention mechanism that replaces traditional multi-head attention splitting. This shifts the focus from \"heads\" to \"patches,\" integrating more naturally into the Transformer architecture.\n3. through ablation studies, the paper demonstrates the contribution of the proposed components to the overall predictive performance.",
            "- - This paper considers both channel and temporal dimensions in the modeling of time series.\n\n- This paper proposes a multi-patch attention mechanism to replace the standard Transformer multi-head attention by focusing on patch-level information in time series.",
            "- 1. A useful insight is that the authors link multi-head splitting to multi-patch splitting, seamlessly introducing their new attention mechanism.\n2. I agree that the ablation study is reasonable.\n3. The paper includes comparisons with numerous baselines."
        ],
        "weaknesses": [
            "- 1. Novelty: The contributions appear to be incremental. The multi-patch attention mechanism provides a marginal improvement over existing architectures, and the modeling of both temporal and inter-channel dimensions in time series is also not a new idea.\n   \n2. Justification of Multi-Patch Attention: The rationale behind why multi-patch attention performs better than traditional multi-head attention is not fully explained. An analysis of the root causes for its effectiveness, ideally with theoretical insights or visualizations, would strengthen the contribution.\n\n3. Performance Compared to SOTA: The proposed model does not achieve state-of-the-art performance compared to the provided baselines.",
            "- The main weaknesses of the article lie in its novelty and results.\n* The methods used in the article mostly have already been proposed by others, and the simple stitching together of ideas makes the article lack novelty.\n* The experimental results of the article are mostly the second.\n* The main experiments in the article are limited. It might be worth considering adding short-term experiments and incorporating new datasets. For example, there are many new datasets available here: https://huggingface.co/datasets/Salesforce/lotsa_data.\n* Lack of sensitivity analysis of parameters.",
            "- The motivation of the method lacks clarity, and its novelty is limited. Furthermore, the performance of the proposed method does not show a clear advantage over other baselines, and there is a lack of validation of the rationale behind the design of individual model components.",
            "- 1. The main concern is that the proposed method does not demonstrate sufficient power, as evident from the experimental results. CARD clearly outperforms the proposed method on numerous datasets. This undermines the authors\u2019 claim, as Sentinel introduces more complexity than CARD with added components\u2014such as \u201cspecializing the encoder in capturing contextual information through the channel dimension.\u201d As a result, the contribution may not be sufficient, as integrating inter-channel information effectively could likely boost independent Transformers (PatchTST).\n\n2. Although the authors review many recent papers, they fail to clearly explain their model\u2019s advantage over existing approaches.\n\n\n3. Compared to similar ICLR papers, the experiments seem insufficient. For instance, CARD, iTransformer, and PatchTST conducted more extensive experiments, either across more datasets or with more comprehensive experimental settings."
        ]
    },
    "zSHoaTNlmA": {
        "venue": "ICLR 2025",
        "title": "Segmentation using efficient residual networks with attention-fusion modules",
        "link": "https://openreview.net/forum?id=zSHoaTNlmA",
        "abstract": "Fusing global and local semantic information in segmentation networks remains\nchallenging due to computational costs and the need for effective long-range\nrecognition. Based on the recent success of transformers and attention mechanisms,\nthis research applies attention-based methods of attention-boosting modules\nand attention-fusion networks in enhancing the performance of state-of-the-art\nsegmentation networks, such as InternImage and SERNet-Former, addressing\nthese challenges. Integrating attention-boosting modules into residual networks\ngenerates baseline architectures like Efficient-ResNet, enabling them to extract\nglobal context feature maps in the encoder while minimizing computational costs.\nAttention-based algorithms can also be applied to networks utilizing vision transformers\nand convolutional layers, such as InternImage, to improve the existing\nresults of state-of-the-art networks. In this research, SERNet-Former is deployed\non the challenging benchmarking datasets such as ADE20K, BDD100K, CamVid,\nand Cityscapes by depending on the attention-based methods with new implementations\nof the network, SERNet-Former v2. Our methods have also been implemented\nfor InternImage-XL and improved the test performance of the network on\nthe Cityscapes dataset (85.1 % mean IoU). Respectively, the results of the selected\nnetworks developed by our methods on the challenging benchmarking datasets are\nfound worth considering: 85.1 % mean IoU on the Cityscapes test dataset, 59.35\n% mean IoU on ADE20K validation dataset, 67.42 % mean IoU on BDD100K\nvalidation dataset, and 84.62 % mean IoU on the CamVid dataset.",
        "decision": "Reject",
        "review scores": [
            3,
            5,
            1,
            5
        ],
        "strengths": [
            "- The proposed method improves the segmentation results based on the transformer architecture, which is lightweight.",
            "- - **Innovative Approach**: The paper presents a novel method, SERNet-Former, which combines Efficient-ResNet with attention mechanisms, demonstrating a promising advancement in segmentation tasks.\n- **Comprehensive Experiments**: The authors conduct extensive experiments across various datasets, showcasing the effectiveness of their approach and providing a thorough comparison with existing methods.\n- **Clear Presentation**: The manuscript is well-organized, with a logical flow that makes the methodology and results easy to follow, enhancing the overall readability.",
            "- -",
            "- 1.This paper has made useful explorations in the fusion of global and local information, bringing some inspiration to this field.\n2.Experimental results show that this method has some advantages"
        ],
        "weaknesses": [
            "- 1. It should be noted that IEEE CVMI has accepted the manuscript \"SERNet-Former: Segmentation by Efficient-ResNet with Attention-Boosting Gates and Attention-Fusion Networks.\" Although CVMI has not yet provided the official version of the accepted paper, the author has provided a GitHub repository, which indicates that CVMI has accepted the paper. Furthermore, the figures and experimental results in the GitHub repository with arXiv and CVPRW versions are the same as those in the paper submitted to ICLR. The author should clarify this.\n\n2. Apart from the above point, I find that this paper's presentation is of low quality. It lacks the motivation to propose a new method of fusing local and global semantics, which has been well-known for improving semantic segmentation performance. I suppose this motivation is presented in the introduction, which is missed in every part of the paper. Though the performances have been compared in the experimental section, I still cannot figure out why the proposed method yields better results. Furthermore, the presentation of the method lacks the necessary information. The critical Figure 2 fails to provide a clear illustration of the method. The relationship between Figure 2 and the equations is also unclear. This fact further disallows the reader to understand the insight behind the method.\n\n3. Though the proposed method improves the segmentation results, it still lags behind other methods on important datasets (see test set on Cityscapes in Tab 4).\n\nBased on the above points, I believe this submission fails to meet the ICLR standard and recommend its rejection.",
            "- - **Limited Discussion on Limitations**: The paper could benefit from a more in-depth discussion of the limitations of the proposed method, particularly in relation to different types of data or specific segmentation challenges.\n- **Insufficient Detail in Ablation Studies**: While the authors present some ablation studies, additional detail on the impact of each component in the network would strengthen the understanding of their contributions.\n- **Comparative Analysis**: The comparison with state-of-the-art methods could be more robust, particularly by including more recent benchmarks to provide a clearer context for the performance claims.",
            "- Considering that this paper has already been accepted by IEEE CVMI, I think it should be rejected.",
            "- 1.The writing of this article is poor. Many sentences are not clear and not easy to understand. Some sentences are too long and difficult to understand. e.g. line 125: The multi-scale problem in computer vision can be described as the discrepancy in integrating the different sizes of spatial and channel-based semantic information of an object acquired from the global and local contexts of segmentation networks.\nline 203\uff1aIt is aimed at developing an encoder-decoder architecture with additional attention mechanisms to get efficient segmentation networks fusing semantic information from different contexts by regarding the multi-scale problem.\n\n2.The method in this article lacks insight, and many designs are tricky.  e.g. Why are there two consecutive layers (AbM4, AbM5) in H/8 and W/8 resolutions? For another question, please refer to Question 2.\n\n3.The experimental analysis is not enough, and the ablation experiment is not very sufficient. More details can refer to Question 4.\n\n4.This paper seems to have multiple submissions\uff0cwhich was accepted by IEEE CVMI previously."
        ]
    },
    "z3vplLsIve": {
        "venue": "ICLR 2025",
        "title": "Learn to Synthesize Compact Datasets by Matching Effects",
        "link": "https://openreview.net/forum?id=z3vplLsIve",
        "abstract": "The emerging field of data distillation aims to compress large datasets by aligning synthetic and real data representations to create a highly informative dataset. The optimization objectives of data distillation focus on aligning representations by using process alignment methods such as trajectory and gradient matching. However, this approach is limited by the strict alignment of intermediate quantities between synthetic and real data and the mismatch between their optimization trajectories. To address these limitations, a new data distillation method called effect alignment is proposed, which aims to only push for the consistency of endpoint training results. The approach uses classification tasks to estimate the impact of replacing real training samples with synthetic data, which helps to learn a synthetic dataset that can replace the real dataset and achieve effect alignment. The method is efficient and does not require costly mechanisms, and satisfactory results have been achieved through experiments.",
        "decision": "Reject",
        "review scores": [
            5,
            5,
            1,
            3
        ],
        "strengths": [
            "- Originality\nThe concept of effect alignment in dataset distillation is innovative, focusing on endpoint effects rather than intermediate training states.\nTheoretical Foundation\nThe method is grounded in theory, with error approximation guarantees that lend robustness to the approach.\nExperimental Results\nThe method demonstrates strong performance in several datasets, showing robustness in handling biases.",
            "- 1. This method is novel, which is a combination of MTT [1] and DD [2].\n2. Good writing, easy to follow.\n\n\n[1] Dataset Distillation by Matching Training Trajectories, cvpr 2022.\n\n[2] Dataset Distillation, 2018.",
            "- The idea of effect alignment for dataset distillation is reasonable.",
            "- 1. The paper is well-written. I am able to fully follow the method and the experiments.\n2. The proposed formulation is novel in the literature of dataset distillation."
        ],
        "weaknesses": [
            "- Scope of Experiments\nThe paper only evaluates the method on classification tasks, which might limit its applicability to other machine learning tasks such as regression.\nDataset Diversity\nThe experiments are conducted on a limited number of datasets, which raises questions about the method's generalizability to other data distributions.\nComputational Complexity\nWhile more efficient than some alternatives, the method\u02bcs computational cost could still be a concern for large-scale datasets or real-time applications.\n\npls add comparisons with more recent methods",
            "- 1. Instead of directly matching training trajectories, this method is proposed to match the 'effect', which is measured by the differences between probability distribution predicted by models trained on synthetic data and real data. This means that naturally, this method will have worse performance than matching training trajectories (MTT) [1]. Because MTT directly minimizes the differences between parameters of models trained on synthetic data and real data, where models' predictions will be the same ideally (which is the optimization goal of the method proposed in this paper). Coinciding with this, TESLA [2] (following work of MTT), which also uses soft labels, always performs better than this method.\n\n2. I notice this method outperforms TESLA in large IPC cases, is it because this method uses the difficulty alignment trick (control matching range) proposed by DATM [3]? The author should report the hyper-parameters to improve clarity.\n\n3. What are the benefits of replacing matching parameters with 'effects'? Being more efficient? Have better generalizability? The paper only reports one comparison, I think more comprehensive comparisons can improve the quality of this paper.\n\n\n[1]. Dataset Distillation by Matching Training Trajectories, CVPR 2022.\n\n[2]. Scaling up dataset distillation to imagenet-1k with constant memory. ICML 2023.\n\n[3]. Towards lossless dataset distillation via difficulty-aligned trajectory matching. ICLR 2024.",
            "- 1. It seems that this paper is not finished. There is only one incomplete table in the experiment. Although the proposed method performs better than the other methods on digital datasets with IPC=1/10, it performs worse on the other datasets. So, the experiments cannot demonstrate the superiority of the proposed method.\n2. An ablation study on the hyper-parameters is required.\n3. The summarised contributions are not matched to the method.\n4. For Eq.(6), reducing the steps of network optimization $T$ can help to close the gap of  approximation error. However, a smaller $T$  usually means a sub-optimal performance of a network.",
            "- 1. Although the proposed formulation is novel in the literature on dataset distillation, I cannot get the motivation of the proposed objective against the original formulation of BPTT. Specifically, BPTT wants to minimize the error on real data for models trained by synthetic data, i.e., $\\mathcal{L}(P,\\theta^*_{A})$, while the matching effect is to minimize $|\\mathcal{L}(P,\\theta^*_{D})-\\mathcal{L}(P,\\theta^*_{D-G+A})|=|\\mathcal{L}(P,\\theta^*_{D})-\\mathcal{L}(P,\\theta^*_{A})|$, given that we would like to replace all the real data with synthetic data and thus we can assume $D=G$. \n   1. I do not get why the authors believe the latter can be better than the former.\n   2. In the practical cases of dataset distillation, since the synthetic dataset is small, the error on the real data of models trained by the synthetic dataset is usually larger than that trained by the real dataset, in most cases $|\\mathcal{L}(P,\\theta^*_{D})-\\mathcal{L}(P,\\theta^*_{A})|=\\mathcal{L}(P,\\theta^*_{A})-\\mathcal{L}(P,\\theta^*_{D})$, which is equivalent to the original formulation because the term $\\mathcal{L}(P,\\theta^*_{D})$\u200b is not relevant to optimization. From this point of view, the proposed method can conduct some rectification for the opposite case. But I am not sure if this is how the method works in fact and how we could benefit from this rectification. In summary, a more comparative analysis is necessary.\n   3. It seems that the above analysis is also applicable to the proposed approximation, which can be viewed as a variant of the previous gradient matching scheme.\n2. From Eq. 6, it seems that the approximated error is quite large because it is dominated by the farthest distance the neural network parameters move away from their initial state during training when any subset is used as the training set. The authors can provide some analysis on whether the bound is tight. If it is indeed tight, I am not sure whether it is useful in practice. The authors are encouraged to provide some toy experiments to illustrate this approximation.\n3. Accordingly, in the experiments, I recommend the authors provide more ablation studies to compare the proposed method with the original formulation, i.e., BPTT and gradient matching while maintaining other factors the same. Given the current results in Tab. 1, which are not evidently strong, we cannot state that the proposed method is better. More analysis is encouraged to figure out in what cases the method is superior and in what cases it is not."
        ]
    },
    "ysQiaWhnCN": {
        "venue": "ICLR 2025",
        "title": "Autoverse: an Evolvable Game Language for Learning Robust Embodied Agents",
        "link": "https://openreview.net/forum?id=ysQiaWhnCN",
        "abstract": "We introduce Autoverse, an evolvable, domain-specific language for single-player 2D grid-based games, and demonstrate its use as a scalable training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses cellular-automaton-like rewrite rules to describe game mechanics, allowing it to express various game environments (e.g. mazes, dungeons, sokoban puzzles) that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite rule can be expressed as a series of simple convolutions, allowing for environments to be parallelized on the GPU, thereby drastically accelerating RL training. Using Autoverse, we propose jump-starting open-ended learning by imitation learning from search. In such an approach, we first evolve Autoverse environments (their rules and initial map topology) to maximize the number of iterations required by greedy tree search to discover a new best solution, producing a curriculum of increasingly complex environments and playtraces. We then distill these expert playtraces into a neural-network-based policy using imitation learning. Finally, we use the learned policy as a starting point for open-ended RL, where new training environments are continually evolved to maximize the RL player agent's value function error (a proxy for its regret, or the learnability of generated environments), finding that this approach improves the performance and generality of resultant player agents.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- - The design of Autoverse combines a domain-specific language with cellular-automaton-like rewrite rules and achieves efficient computation through convolutions, providing new perspectives and methods for game environment generation and agent training.\n- Through the evolution of the environment, the complexity of the environment can be gradually increased according to the search ability of the agent, effectively avoiding the agent from prematurely falling into local optimal solutions and also providing a curriculum learning from simple to complex for the agent.",
            "- - The idea of using CA-based environments is both original and interesting. The fact that practically limitless environments can be created by modifying the initial state and the update rules is very interesting from an open-ended learning perspective.\n\n- I also find CA environments relevant for research on foundation models for RL. This type of environment could be very valuable for generating training data for these types of models.\n\n- I think that the presentation of Autoverse (Section 2.1) is clear and can be easily understood, whereas Figure 1 also helps to visualize the types of environments that are generated by the evolution process.",
            "- Autoverse offers efficiency by leveraging GPU-based batch processing. Its rewrite rule framework enables the creation of a vast array of dynamic, grid-based game environments, enhancing agent adaptability and preventing overfitting to static setups. The method's progressive curriculum, which evolves increasingly complex environments, allows agents to improve incrementally, while the integration of imitation learning with RL provides agents with a solid starting foundation. These strengths make Autoverse an innovative and robust tool for advancing research in open-ended learning.",
            "- 1, Originality: The paper presents Autoverse, a new environment for open-ended learning, which allows more complex environment dynamics and much more environmental diversity than other open-ended learning environments.\n2, Quality: The use of JAX for implementing cellular-automaton rules allows efficient parallelization on GPUs, and at least an order of magnitude speedup.\n3, Significance: The use of imitation learning followed by reinforcement learning provides a structured way for agents to learn from expert play traces and then further refine their behavior, leading to better generalization."
        ],
        "weaknesses": [
            "- 1. Overall, the paper has limitations in terms of innovation. The framework of combining imitation learning and reinforcement learning used is not novel and has been involved in many current related research fields.\n2. Although the rewrite rules are highly expressive, they are difficult to understand and interpret, which may limit their promotion and further development in practical applications, especially when manual intervention of the rules is required.\n3. In the process of environment evolution, only mutation operations are involved, and crossover operations are not included. This may limit the exploration range of the diversity of environment generation to some extent.\n4. The main experiment is lacking. There is a lack of direct comparison experiments with other advanced methods, making it difficult to accurately evaluate the advantages and disadvantages of Autoverse and clearly define its competitiveness in this field.\n5. The long-term training effect experiment is insufficient. The experimental results mainly present the immediate performance data under specific settings. The performance change trend after a significant increase in the training cycle, the stability of the agent's strategy after a large number of environmental changes, and the evolution law of the long-term adaptability to new environments are not provided, making it difficult to judge the long-term comprehensive ability development.\n6. The coherence of the chapter structure is poor in some parts. For example, the transition from the method description to the experimental results is not natural, affecting the reader's understanding of the logical relationship of the paper.",
            "- Although I think that the main idea (evolvable CA environments) is very interesting and could be promising for areas as foundation models for RL, UED, and open-ended learning. I have important concerns on several aspects of the paper:\n\n- Although most ideas are clearly explained, the overall presentation and soundness of the paper are poor. \n    + The introduction section makes many (non-trivial) statements with no references. The introduction has no references, which I found surprising. Some examples of such sentences missing references and evidence/support are:\n        - L31: \"The idea of open-ended learning in virtual environments is [...]\"\n        - L32: \"This idea comes in many forms, but what unites them all is that [...]\"\n       - L38:  \"There have been interesting results, but learning generally stops at a rather low capability ceiling.\"\n       - L40: \"It has been observed that the complexity of the behavior of a living being, [...]\"\n\n- The ability to endlessly evolve and generate new environments is compelling, but as mentioned multiple times in the paper (e.g.,   L71, L413) evolutive process generates very unstable environments in most cases. I have serious concerns about the ratio of unusable and usable environments generated by the evolutive process. Authors claim scalability, but how much computational resources are needed to generate a fair amount of actually usable scenarios? I think that an exhaustive analysis of this topic is crucial and missing in the current version of the paper.  \n\n- I think that the paper misses many experimentation details. For instance, experiments shown in Table1 and 2 are missing information on the number of evaluations, the number of generated test/train environments, the number of repetitions, etc. Seems that this information is also missing in the appendix. Moreover, none of the appendix sections are referenced in the main paper. Please, consider providing as many details as possible on the experimentation to ensure transparency and reproducibility. Moreover, I strongly believe that all sections of the appendix should be referenced at least once in the main paper.\n\n-   I strongly believe that experimentation should be improved. The authors propose a \"warm-start\" method for open-ended RL, but they do not provide any evidence for why this method is relevant for the research community. I won't ask for a comparison with tens of methods from previous literature, but proposing a new method at least requires an exhaustive analysis of it. Some examples of how I think this could be improved:\n    - Reward values in Tables 1 and 2 are missing interpretability. I can see that the results of some settings are better than others, but how much? Having some sort of reference value could help (e.g., results of a random agent or some well-known baselines).\n    - Why is this method interesting? Does it obtain better results? Does warm-staring help to improve the results? Many experiments could be presented to answer these questions.   \n\n- The paper claims computational efficiency but no evidence is provided. Experiments/benchmarks on computational performance are missing if such claims are made. \n\n- I think that the presentation has room for improvement. Some examples:\n     - Figure 1 is presented is located in page 2, but is referenced on page 8 for the first time. \n     - Figure 3 holds an entire page but some of its text (Fig 3.a) is **extremely** small. There is plenty of blank space on Page 3 to increase some parts of the figure or rearrange them to improve visibility.\n    - The format of tables 1 and 2 is not aligned with the ICLR style. Please carefully read the style PDF before submitting the paper.\n    - There are many incorrect usages of parenthesis in references. For example, parenthesis in Earle et al. 2023 L271 should be removed, and Section 4 is missing parentheses in most of its references: L470-471, L476, L482...",
            "- First, there are presentation issues, as figures (like 3 and 4) lack clarity\u2014small text sizes, ambiguous elements, and layout issues reduce readability. Second, empirical evidence demonstrating scalability is lacking; additional experiments or metrics require to solidify this claims.  The paper fail to do performance comparisons of Autoverse\u2019s combined imitation and RL approach against using either method alone. While it states that Autoverse\u2019s environments are more complex and diverse than others, this claim lacks citations or direct comparisons with other open-ended learning benchmarks. Finally, the paper does not report the performance achieved after the behavior cloning stage, leaving its contribution to the final results ambiguous. Addressing these points could greatly enhance the paper\u2019s clarity, rigor, and persuasive power.",
            "- 1, Presentation Improvement: Provide more detailed feedback, such as clarifying ambiguous elements like the meaning of the gray box in Figure 4 and addressing layout issues with full-page figures such as figure 3 and figure 4. And the texts in figure 3 are too small.\n2, Scalability Evidence: Request empirical evidence or additional explanations to demonstrate scalability, including experiments or metrics.\n3, Justification for Greedy Tree Search: Maybe adding references in related work and experimental validation to justify the choice of greedy tree search.\n4, Comparison of Methods: Recommend including a comparison between the proposed approach and using only imitation learning or reinforcement learning.\n5, Performance Results: The paper mentioned \"Once the behavior cloning algorithm has converged, we continue training the agent with reinforcement learning\", but there is no results show that what kind of performance can this method reach when behavior cloning algorithm has converged and what is the final performance compared to that stage.\n6, Lack explanation of comparing with other environments: In the paper \"Autoverse stands out for allowing more complex environment dynamics and much more environmental diversity than other open-ended learning environments.\" But I did not see any citation or detailed comparison with other open-ended learning environments. Further discussion will make the arguments more convincible."
        ]
    },
    "ys3eqxzkeN": {
        "venue": "ICLR 2025",
        "title": "Efficient Gun Detection in Real-World Videos: Challenges and Solutions",
        "link": "https://openreview.net/forum?id=ys3eqxzkeN",
        "abstract": "Object detection in videos is a crucial task in the computer vision domain. Existing methods have explored different approaches to detect objects and classify the videos. However, detecting tiny objects (e.g., gun) in videos has always been a challenging and rigorous task. Moreover, the existing video analysis (detection and classification) models may not achieve high accuracy for gun detection in videos in real-world scenarios due to the lack of a large amount of labeled data. Thus, it is imperative to develop an efficient method to capture the features of tiny objects and train models that can perform accurate gun detection. To address this challenge, we make three contributions. First, we perform an empirical study of several existing video classification methods to identify the presence of guns in videos. Our extensive analysis shows that these methods may not achieve high accuracy in detecting guns in videos. Second, we propose a novel gun detection method with image-augmented training and evaluate the technique in real-world settings with different evaluation metrics. Third, our experimental results demonstrate that our proposed domain-specific method can achieve significant performance improvements in real-world settings compared to the other popular methods. We also discuss emerging challenges and critical aspects of detecting tiny objects, e.g., guns, using existing computer vision techniques, their limitations, and future research opportunities.",
        "decision": "Reject",
        "review scores": [
            3,
            5,
            3,
            3
        ],
        "strengths": [
            "- 1. The empirical study highlights the limitations of existing video classification methods, providing a solid foundation for the proposed approach. Additionally, the two-stage methodology enables the model to capture both spatial features of guns and temporal dependencies across frames, contributing to its effectiveness. \n2. Given the real-world importance of detecting firearms in video data, the paper has notable significance. By addressing limitations in current methods and proposing a targeted approach, it contributes meaningful insights and methods that could inform future research for high-stakes applications.",
            "- 1. I think gun detection is an important topic. However, the academia seems underrated this task and there are not many research on this direction. Gun detection has big potential to make our world safer. However, the exsiting methods have too many false positives to avoid its wide applications. Thus, I am glad to see that this work tries to contribute this important research direction. \n2. The paper studies a valuable gun detection problem and provides a detailed motivation explanation.\n3. The results show the improvements of the proposed methods.",
            "- 1. The method is clear and straightforward.",
            "- 1) The research topic for gun detection from videos is interesting and valuable.\n\n2) The authors did an empirical study of existing video classification methods to detect the presence of guns in videos.\n\n3) The authors also investigate the challenges of real-world gun video detection."
        ],
        "weaknesses": [
            "- 1. Although the abstract highlights the challenges of limited labels and tiny object detection, the contributions do not directly address these points. Establishing a clearer link between the identified challenges and the proposed solutions would strengthen the study\u2019s coherence and impact.\n2. This paper does not clearly specify whether the focus is on object detection or action recognition, which may confuse readers regarding the study\u2019s objective. A clearer explanation of the task would help define the scope and purpose of the proposed method.\n3. The contributions are presented in an abstract manner, lacking concrete results or specific findings to support them. Adding more details about the outcomes or unique aspects of the contributions would make the study\u2019s significance more evident.\n4. The motivation section is overly lengthy and could be more concise. Streamlining this part would improve readability and make the core motivations clearer.\n5. Section 4.1 contains excessive background details, including algorithmic descriptions of existing methods, which may not be necessary. Providing only the essential comparative results would make this section more concise and focused.\n6. The proposed methodology in Section 4.2 is fairly basic, with limited technical depth, few formulas, and minimal emphasis on handling label scarcity or tiny object detection. Expanding this section to include more detailed techniques targeting these specific issues would add depth to the work.\n7. Table 4 presents results on a dataset where all methods achieve 100% accuracy, making it an unnecessary addition. Removing or replacing it with a more challenging dataset would make the evaluation more meaningful.\n8. The study does not compare against recent state-of-the-art methods and its variants from ablation studies, which limits the relevance and verification of the results. Including newer comparison methods would provide a stronger benchmark for the proposed approach.\n9. The paper lacks qualitative visualizations, such as feature maps, example detections, or interpretability-focused analyses. Adding these would provide deeper insights into the method\u2019s strengths and its effectiveness in handling complex detection scenarios.\n\n**It is unfortunate that the authors, apart from a few commitments to revisions, have largely not addressed my concerns directly.**",
            "- 1. The method proposed in the paper is a simple combination of existing techniques, such as common data augmentations like flipping and rotating.\n2. The application of data augmentations is one of the core ideas of this paper, but it lacks specific details, such as how to select and adjust data augmentations.\n3. The paper mentions using GRU and Transformer for temporal modeling, but it does not provide detailed descriptions of the configurations and optimization processes for these two models.\n4. The paper implements a combined model of VGG and Transformer, but does not elaborate on how these models are integrated. For example, does it use a simple concatenation or parallel approach?\n5. More importantly, gun detection typically requires a quick response in real-time monitoring systems, yet the paper does not address the model's real-time performance in practical applications.",
            "- 1. The paper does not contain novel approaches. Typically, the authors only use transfer learning to extract image features and adopt LSTM / Transformers for video classification. The contribution is limited.\n2. As mentioned in the paper, the gun is usually a small object in real-world videos. However, frame-level feature extraction is not an efficient way to get fine-grained-level features of guns.\n3. In Table 4, it seems 100% accuracy has been achieved. I do not think it is a challenging task as the authors mentioned.",
            "- 1) Very out-of-dated methods and comparisons. The authors should compare some recent video classification and detection algorithms.\n\n2) limited novelty, this paper just combines existing algorithms together, such as Video MAE and different network backbones (LSTM and Transformer). Very old backbones and baselines cannot illustrate the superior performance of the existing work.\n\n3) The motivation part is too redundant. The authors should shorten this part.\n\n4) It is not convinced to include Transfer Learning in this paper since this paper did not perform corresponding experiments."
        ]
    },
    "ydw2l8zgUB": {
        "venue": "ICLR 2025",
        "title": "EEGTrans: Transformer-Driven Generative Models for EEG Synthesis",
        "link": "https://openreview.net/forum?id=ydw2l8zgUB",
        "abstract": "Recent advancements in Large Language Models (LLMs) have been significant, largely due to improvements in network architecture, particularly the transformer model. With access to large training datasets, LLMs can train in an unsupervised manner and still achieve impressive results in generating coherent output. This study introduces a transformer-based generative model, EEGTrans, designed for sequentially generating synthetic electroencephalogram (EEG) signals. Given the inherent noise in EEG data, we employ a quantized autoencoder that compresses these signals into discrete codes, effectively capturing their temporal features and enabling generalization across diverse datasets. The encoder of EEGTrans processes EEG signals as input, while its decoder autoregressively generates discrete codes. We evaluate our method in a motor imagery Brain-Computer Interface (BCI) application, where merging data across datasets is particularly challenging due to experimental differences. Our results demonstrate that the synthetic EEG data effectively captures temporal patterns while maintaining the complexity and power spectrum of the original signals. Moreover, classification results show that incorporating synthetic data improves performance and even surpasses that of models based on Generative Adversarial Networks. These findings highlight the potential of transformer-based generative models to generalize effectively across multiple datasets and produce high-quality synthetic EEG signals.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            5,
            3
        ],
        "strengths": [
            "- * applies commonly used, yet fairly recentnovel deep learning methods from other fields to EEG, where they have not been used as much\n* supplies code\n* overall approach probably makes sense (even though have not fully understood it)",
            "- The paper used Transformer architecture to generate synthetic EEG. The idea is not novel as there have been previous attempts on this, however the problem is not well studied. Thus utilising transformers seems a reasonable attempt at testing the capability. \nThey Validated on multiple datasets.",
            "- 1. The general idea is taken from the language model, thus the overall methodology is reasonably backed up with empirical results, and is still novel in the EEG synth task. \n2. The experiments demonstrate their superior performance compared to a classic method CycleGAN. \n3. The overall presentation is good. The authors clearly conveys their ideas.",
            "- 1. the structure of the paper is good and clear\n2. the method section was clearly described and the figure 2 is informative.\n3. the source codes are provided to facilitate reproduction"
        ],
        "weaknesses": [
            "- I found the way how actually the synthetic data is generated hard to understand and may still not have fully understood it. This should be more clearly described early on I am still confused about it.\n\nSome of the writing I found vague and therefore hard to read, e.g. first sentences \"Large language models (LLMs) have been extensively utilized across various scenarios due to their powerful model characteristic: the generative models. These models are not restricted to producing specific forms of output; instead, they can generate output in any form\" I found these sentences mostly confusing.\n\nIt is unclear to me how the authors split the data into training and test, I think they did not follow official train/test splits of the datasets? Did not see this information, maybe I missed it.\n\nAlso, there are of course a lot of existing works on those data that should be compared to, and for this it is also necessary to check and align the train/test split with those works and to also mention their performances.\n\nAverage power spectra (e.g., for all trials fo one subject) for synthetic and generated data should be shown to assess quality of generated EEG.",
            "- There have been other methods where authors used GPT-based architecture to claim a foundation Model: \"Neuro-GPT: Towards A Foundation Model for EEG\". There is another paper \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" for time series data -  The authors should have evaluated that.\n\nThere should be more details on architecture, and optimisation. The authors explained for EEGTrans but for CycleGAN approach, I couldn't find the information on layers/number of parameters/etc. There is some information in the appendix but still not complete.\nI would encourage authors to include maybe clear architecture diagram for both in the main text, so it is easier to compare and understand the implementation.\n\nAdditionally, There is an extreme weakness in the paper related to evaluating synthetic data. The authors didn't perform standard evaluations like PSD, STFT, activation in alpha and beta bands, or source analysis. Averaging all the subjects doesn't distinguish activations.\n\nThere are 5 datasets for evaluation: Only 1 dataset is there with 1 subject and 128 Hz sampling. Why use that dataset to bring the other 4 datasets with higher resolution and number of subjects to 128 Hz? Maybe let go of that dataset and deal with 250 Hz, as we have tried over the years to have higher sampling and temporal resolution for these datasets \n\nIn paper, I have seen repetitive occurrences stating the same stuff, authors can reduce that, to add more information in main text. ex:\n\"Visual inspection indicates that EEGTrans produces higher-quality synthetic data compared to CycleGAN. It is evident from the\nvisual comparison that EEGTrans\u2019s generated data is significantly superior\". \n\nanother Line [493]The BCI Competition IV Dataset 2a was collected some time ago] - \"some time ago\" really? We can say, 2a was collected in 2008 and HGD was in 2017. However the may statement is speculative in nature, so authors can point towards noise/interference due to room/environment. \n\nLine [534] : \"This method produces high-quality synthetic data that enhances downstream classification tasks.\" - If you only intended for classification (only one downstream), just train a classification model utilizing transformers?",
            "- First of all, I have to mention that the key research problem of the EEG synthesis is not \"what is the best method to generate the synthetic EEG data\", but \"why do we need to generate the synthetic EEG data\". The paper answers this question in the most common way, which is to use these data as augmented data to improve the performance on a specific downstream task (and in this paper the BCI motor imagery task). From this point of view, the scope of the contribution of this paper will be limited to getting improved synthetic EEG data for data augmentation rather than extending our understanding for this area. \n\nThe above statement is actually not the weakness of this paper. I like paper that won't overclaim but focus more on concrete things. However, starting from it, there will be the following concerns about the paper. \n1. It's unclear to me whether the label needs to be generated together with the EEG data. If so, the usage of the synthetic dataset seems to be limited to highly related tasks and the claimed cross-dataset advantage will be weakened.\n2. If the synthetic data is used to boost the performance of downstream tasks, then how does it perform compared to more common but simple data augmentation methods like cropping and concatenating EEG fragments, jittering, etc.? It lacks such justification. \n3. If the synthetic data is claimed to integrate information from different datasets, then how should we compare this paradigm to a more common paradigm where model such as LaBraM will integrate information in the hidden representation space? This alternative paradigm can boost the performance of downstream tasks with unsupervised training on unlabeled EEG data, which can integrate information from more diverse dataset. \n\nThere are also some comments on the methodology: \n1. EEGTrans doesn't take the relationship among channels into consideration, and doesn't evaluate the cross-channel reconstruction quality.\n2. The role that the next-token prediction plays in the proposed method is unclear and unverified. See the questions for more details.",
            "- 1. The model was trained for next code prediction which means the proposed method is more of a EEG signal 'forecasting' or 'prediction' rather than generation for data augmentation purpose. Therefore, the introduction and problem setup is miss aligned.\n2. For synthetic data generation to facilitate training in a new dataset, both the reality and the diversity of generated signals are important. In a typical GAN or Diffusion setting, new signals can be generated or sampled from a random vector sampled from the normal distribution during inference to achieve diversity and variety. However, in the proposed method, it seems like there is no sampling process for diverse signal generation, so how can the proposed method augment the target dataset?\n3. There are GAN-based method for EEG that can directly generate the raw signal which should be added into comparison. For example, EEG-GAN.\n4.The language used in section 4.7 is very vague, for example, 'Dataset 2a was collected some time ago', 'the synthetic data likely does not retain subject-specific information', 'the classifier should yield very similar output'. These claims need to be justified better for example, providing a tSNE plot to show the generated data vs the real data, and the target classes vs the subject to demonstrate there is no subject-specific information. \n5. Why not use MSE as a performance metric since the method is 'predicting' the future segment?\n6. The use of GAN to replace the Transformer module as baseline for next coder prediction task should be justified better."
        ]
    },
    "yCr55EjC1d": {
        "venue": "ICLR 2025",
        "title": "Node Duplication Improves Cold-start Link Prediction",
        "link": "https://openreview.net/forum?id=yCr55EjC1d",
        "abstract": "Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ``multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied on existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.",
        "decision": "Reject",
        "review scores": [
            5,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1. **Comprehensive Experimental Evaluation**: \n\n   The paper is well-supported by extensive experimental results, covering a wide range of datasets and comparisons. This thorough empirical analysis demonstrates NODEDUP\u2019s efficacy in addressing the cold-start problem, adding substantial credibility and depth to the findings.\n\n2. **Clear Presentation and Visualization**: \n\n   The paper is well-organized and clearly written, with visual aids that effectively communicate the experimental results. The figures and tables are particularly helpful in understanding NODEDUP\u2019s impact across different settings, making the paper accessible and easy to follow.",
            "- The proposed method stands out for its simplicity and ease of integration with existing GNN architectures. By requiring only the duplication of low-degree nodes and the establishment of connections between these duplicates, it provides a straightforward augmentation technique that can be seamlessly applied to various GNN models.\n\nThe paper has extensive experiments to demonstrate the effectiveness and superiority of the proposed method. By evaluating the approach across multiple benchmark datasets, the authors provide strong empirical evidence supporting their claims.",
            "- 1. The paper is clearly written and easy to understand.\n\n2. The authors claim that the proposed method demonstrates significant improvements in performance.",
            "- S1: This paper focuses on addressing the cold-start problem in link prediction tasks, a significant issue with numerous practical applications in real-world scenarios.\n\nS2: The authors conduct extensive experiments to demonstrate the effectiveness of their method compared to traditional GNNs, cold-start GNNs, and other data augmentation techniques.\n\nS3: The proposed method is simple and can be easily applied to different GNNs."
        ],
        "weaknesses": [
            "- While NODEDUP demonstrates effectiveness in addressing the cold-start link prediction problem, several critical limitations should be considered:\n\n1. **Limited Scalability**: \n\n   NODEDUP's design focuses heavily on the \"duplication of cold nodes,\" making it overly specialized for the cold-start problem and potentially too simple for broader link prediction tasks. The technique lacks flexibility to handle other challenges, such as heterophilic graphs or highly dynamic networks, which require adaptable augmentation strategies. This limits NODEDUP\u2019s versatility as a universal approach in graph contrastive learning.\n\n2. **Lack of Theoretical Foundation**: \n\n   Despite extensive empirical results, the paper does not provide a theoretical explanation for why NODEDUP improves cold-node representation. A theoretical analysis would strengthen the validity and interpretability of the method, clarifying how and why node duplication facilitates performance gains for cold nodes in GNNs.\n\n3. **Outdated Baselines**: \n\n   Although the authors claim that NODEDUP does not compromise overall performance while addressing the cold-start problem, they compare it only against older baselines rather than the latest advancements in graph contrastive learning. This raises questions about how NODEDUP\u2019s performance aligns with state-of-the-art methods and whether it may lag behind the latest graph contrastive learning approaches in overall link prediction performance.\n\n4. **High Dependency on Hyperparameters**: \n\n   The proposed NODEDUP method relies heavily on the selection of the hyperparameter \u03b4, which determines the distinction between cold and warm nodes. Although the authors provide a heuristic method and a hyperparameter search experiment, they do not offer a clear rationale for selecting a specific value of \u03b4. This raises concerns that an optimal \u03b4 may need to be tuned individually for each dataset, potentially limiting the method\u2019s practical applicability and generalizability.",
            "- The paper appears to lack sufficient novelty in its contributions to the field of link prediction. The underlying idea of augmenting data through duplication has been explored in various contexts. Additionally, the paper does not convincingly demonstrate how NodeDup outperforms or fundamentally enhances prior methods, leading to concerns about the overall impact of the contribution.\n\nAlthough the paper provides some justification for the proposed method, it could delve deeper into the theoretical foundations of why node duplication is particularly effective for cold-start link prediction. More comprehensive discussions could help solidify the rationale behind the approach.\n\nThe paper lacks a thorough investigation into the node degree distribution of the graphs used in the experiments, which can significantly impact the efficiency of the proposed method. In cases where the graph exhibits a highly skewed degree distribution, the additional complexity introduced by duplicating cold-start nodes could potentially lead to a doubling of the original computational complexity.",
            "- 1. The problem addressed in this paper does not align with the experimental datasets used. The cold start issue is a significant challenge in recommendation systems, and while the authors frequently mention the intent to tackle this problem, they rely on publicly available general graph datasets, such as citation networks, for their experiments. Crucially, they do not utilize any recommendation system datasets, such as Movielens-1M or Amazon-Books.\n\n2. The work lacks theoretical guarantees. Although the authors attempt to explain in Section 3.2 how node duplication aids cold start link prediction, I would prefer to see definitive theorems or lemmas presented in the paper. This would provide a more solid theoretical foundation for the work.\n\n3. In Table 18, GCN+NodeDup shows a decline in overall metrics compared to GCN. What is the reason for this phenomenon?\n\n4. There are several works designed to solve link prediction problems using GNN methods, such as references [1], [2], and [3]. However, the authors do not compare NodeDup against these methods in their experiments.\n\n5. The method description lacks clarity. In the NodeDup approach, determining which nodes are cold starts is a crucial step. However, in the core algorithm (Algorithm 1), the authors do not demonstrate how to deterministically obtain the set of cold start nodes V_{cold}.\n\n6. The authors should provide a direct comparison with adding self-loops as an augmentation baseline is missing, which could help clarify the advantages of NodeDup and NodeDup(L) over simpler alternatives.\n\n7. The authors state that OGB datasets were not used because they ``lack a substantial number of isolated or low-degree nodes'' (Lines 892-893). However, even though OGB datasets may primarily include high-degree nodes, they still contain a subset of lower-degree nodes, which would allow for a relevant evaluation of the proposed method's performance across both node types. Additionally, testing on these datasets could strengthen the claim that the method does not compromise warm node performance, as OGB is a widely recognized benchmark.\n\n[1] Bai Q, et al. HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction. WWW-23.\n\n[2] Zhu Z, et al. Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction. NeurIPS-21.\n\n[3] Cai L, et al. Line Graph Neural Networks for Link Prediction. TPAMI-21.",
            "- **W1**: The primary concern of this paper is its limited novelty. The main idea of duplicating cold nodes is not a novel strategy within existing GNN frameworks. Additionally, the distinction between NodeDup and the common data pre-processing step of adding self-loops is not clear. When adopting general message passing GNNs, isolated nodes can also treat themselves as neighbors and update node representations using both $\\boldsymbol{W}\\_1$ and $\\boldsymbol{W}\\_2$. Without a clear differentiation, it is challenging to justify the novelty of NodeDup over existing techniques.\n\n**W2**: It is unclear why such a simple strategy significantly enhances overall GNN performance on warm nodes, particularly on the *citeseer* dataset, which contains a large number of isolated nodes with zero degrees. Consider an example in online social networks, where a node $u$ represents a famous individual with millions of followers, and most of $u$\u2019s followers may have few connections with other users, serving as *cold nodes*. In this scenario, adding self-loops using NodeDup(L) on these cold nodes may introduce additional noise in predicting the *warm node* $u$. This could potentially degrade the model's performance rather than improve it. \n\n**W3**: Additional theoretical analysis from a spectral perspective, such as examining changes in the graph Laplacian after self-augmentation, could enhance the understanding of the self-augmentation strategy. Such analysis might provide deeper insights into why and how the self-augmentation strategy works, potentially revealing underlying mechanisms that contribute to its effectiveness or limitations.\n\n**W4**: More experiments on larger datasets, such as collab, ppa, and citation2 from the OGB benchmark datasets [R1], should be included in the main text. The current experimental setup may not be sufficient to generalize the findings across different types of graphs and scales. Including these larger datasets would provide a more comprehensive evaluation of the method scalability and robustness. Additionally, a comparison between NODEDUP and more recent link prediction models, such as MPLP [R2], is necessary.\n\n---\n\n[R1] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta and J. Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. 2020. NeurIPS(33): 22118-22133.\n\n[R2] K. Dong, Z. Guo, N. V. Chawla. Pure Message Passing Can Estimate Common Neighbor for Link Prediction. arXiv:2309.00976."
        ]
    },
    "y59zhBNKGZ": {
        "venue": "ICLR 2025",
        "title": "Towards Making Linear Attention Usable",
        "link": "https://openreview.net/forum?id=y59zhBNKGZ",
        "abstract": "The original Transformer attention mechanism, based on Softmax, has time and memory complexities of $O(N^2D)$ and $O(N^2)$, where $N$ is the number of tokens and $D$ the dimension per attention head. As current LLM applications trend towards processing larger token sequences, and Transformers gain popularity in image, video, and audio processing, addressing this quadratic cost becomes imperative. Since the introduction of Transformers, numerous approaches have been proposed to linearize this scaling. One such method is Linear Attention, which captures all-to-all token pair attention in $O(ND^2)$ time. However, its drawback lies in its high memory footprint of $O(ND^2)$. While Linear Attention has shown promise in small-scale benchmarks, the high memory demand has prevented Linear Attention to be studied in context of large benchmarks and practical use cases. In this work, we demonstrate how to reduce the memory complexity to $O(ND)$ by approaching calculations from a novel perspective. Additionally, since Linear Attention does not compute the attention matrix directly, it precludes the use of traditional dropout. To address this, we introduce an alternative dropout mechanism. Our study confirms linear scaling in both wall-clock time and memory usage. We also compare our method with Flash Attention and conduct an ablation study on our proposed dropout alternative.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- - The paper is well written and easy to follow.\n- Interesting topic -- making linear attention efficient is of great interest to the research community.",
            "- In terms of practicality, reducing memory cost is very crucial. If this work maintains comparable performance or minimizes the performance drop, it has impactful potential.",
            "- - The Motivation is clear.\n\n- The theoretical derivation is detailed.",
            "- 1. The topic of saving the memory footprint of linear attention is interesting. This is important while it seems few researcher have noticed.\n\n2. The forward and backward derivation is also very clean.\n\n3. An alternative for dropout is proposed, and the experiments in LRA and Tiny Stories datasets verifies its effectiveness."
        ],
        "weaknesses": [
            "- - The results are somewhat unsatisfactory, with only limited to a very small model. More experiments and analysis are needed to justify the effectiveness of the proposed method.\n- What about practical usage? Can this method start from the pre-trained LLMs and directly convert them to efficient linear attention? Also, can it start from a LLM with linear attention and make it efficient through little amount of training or finetuning?\n- Can this method scale to large models? Say, billion scale models. Authors at least perform experiments using 1-3B models.\n- What about hardware efficiency of the proposed changes to the linear attention?",
            "- ### Lacks of benchmarks\nAlthough it shows memory and latency experiments by increasing token length, it didn't show common LLM benchmarks such as MMLU and GLUE or long-context LLM benchmarks such as n streaming books (PG19), Long Context Understanding (LongBench), and book summarization (Booksum).",
            "- Overall, this manuscript is not ready for publication. My concerns are listed as follows:\n\n- The technical novelty is limited. The first proposed method, matrix multiplication multiplication, is a standard operation in linear attention methods [1]. I see some novelty in the second dropout technique, but its effectiveness is validated only in small-scale experiments.\n\n- The experiments are not convincing enough. One small model (Pythia-14M) is adopted, and only a training curve is presented. To make the results more convincing, it is recommended to conduct experiments on larger-scale models (both language and vision models should be included) and well-known benchmarks. The comparison should be the about the trade-off between efficiency and performance.\n\n- The presentation needs substantial improvements. Some examples are listed as follows:.\n  - In the introduction, the authors categorize linear attention methods into two lines of work:  Sparsified/localized Attention, and Kernel Separation. However, no references are cited here.\n  - In the equations (e.g. eq1), the index subscriptions $i,n,j$ should not have bold fonts.\n  - In Sec. 3.1, the FLOPs calculation example could be given by normal matrix presentations instead of a 3x3 toy example.\n  - The derivaton in Sections 3&4 are too detailed and might distract readers from the main method. It is recommended to put detailed derivation process in the appendix, and present the major method in the main text.\n\n\n[1] Flatten transformer: Vision transformer using focused linear attention. ICCV, 2023.",
            "- (1) After the forward and backward modification, althought the computational seems equivalent compared with linear attention, will the proposed method achieve the same performance is still unclear. The authors can check the RepVGG paper, which shows that equivalent computational may not produce the same performance.\n\n(2) The paper only change is computing order of Linear Attn, and proposed a new dropout alternative. I think the novelty is limited.\n\n(3) Lack of experiment.\n\n    (3.1) for the time and memory scaling experiments (sec. 5.1). The authors only show one fixed number of head H, fixed head dim D, varying token length result experiment, and, one fixed token length N, varying head dim D experiment. I think the author should show more results on the H, D choices, since this experiment is training free, and whether the experiment results would show the same trend. Additionally, the only chosen one is weird. Since in ViT-Base H=12 and D=64, in DiT-S H=6 and D=64, I cannot find a popular model with H=16 and D=32.\n\n    (3.2) For LLM experiment in Sec. 5.2, the author does not express the reason why they choose this model and this dataset. And they only show the loss curve (also the orange curve is not trained as long as the blue one is). No evaluation metric provided.\n\n    (3.3) For the ablation study in Sec. 5.3, I do not get the point why the author do not provide these accuracy results in main results, but put them in ablation. The training hypermeter is also not clear. I have also check the LRA results in other papers (e.g. S4, by Albert Gu in ICLR 2022, https://arxiv.org/pdf/2111.00396). The results of LRA in the submission are far lower than this 2 years ago paper. And both the Softmax result and Linear(alt. drop.) results have different numerical range compared with the results in S4 paper.\n\n(4) The experiment hyper-parameter is not described. That may be the reason why it cannot match the results in S4 paper. I think the authors should follow others setting, or explain why they choose a different setting.\n\n(5)Minors:\n\n    (5.1) wrong formula in line 90: f(x) = exp(q \\cdot k / root(D)), while there is no x in the expression.\n\n    (5.2) a latex bug in appendix D.1 (line 860) have not be fixed."
        ]
    },
    "y15LAM4u0A": {
        "venue": "ICLR 2025",
        "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
        "link": "https://openreview.net/forum?id=y15LAM4u0A",
        "abstract": "Embodied artificial intelligence (EmbodiedAI) emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on  EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties. The executable program of this platform is available for download, and we have also released an easy-to-use Python library and detailed tutorial documents. All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this anonymous website: https://embodied-ai.city.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- - The platform's integration with Unreal Engine and AirSim, along with the provision of a Python SDK, significantly lowers the barrier for use and promotes flexible, scalable experimentation for researchers.\n- The benchmark includes evaluations of popular large language models (e.g., GPT-4, Claude 3) across tasks, providing a well-rounded quantitative baseline for the embodied intelligence community.\n- The open structure allows future expansions, such as multi-agent collaboration and adaptability, fostering an extensible environment for advanced research in embodied AI.",
            "- 1. The authors introduce a new urban simulator for simulating pedestrians and traffic states of a city.\n2. This work provides the resources of a large digital city district, which is quite scarce in this field.\n3. This study evaluates several state-of-the-art large multimodal models (LMMs) against the proposed benchmark to assess their effectiveness in addressing embodied tasks from multiple perspectives. The results largely align with findings from other LMM benchmarks, which partially support the validity of the proposed benchmark.",
            "- The proposed simulator and environment covers a large area.\n\nThe authors create various tasks in the simulator.\n\nThe authors evaluate various current VLMs on their proposed tasks.",
            "- 1. The paper constructs a detailed 3D environment based on real-world urban settings in Beijing, improving on previous fictional models.\n2. The paper establishes a diverse set of evaluation tasks that assess various dimensions of embodied intelligence.\n3. The paper provides accessible input and output interfaces for easy interaction and performance evaluation of embodied agents."
        ],
        "weaknesses": [
            "- 1. While the paper addresses the city layout aspect of the sim-to-real gap, it does not extend to other critical factors impacting real-world applicability. Additionally, no experiments are conducted to quantify the sim-to-real benefits derived from using a real-world city layout, leaving the practical advantages of this choice unclear.\n2. The shadows and lighting in Figure 3 appear less realistic, which may limit the benchmark's effectiveness in simulating real-world visual conditions.\n3. The benchmark predominantly focuses on drone-related tasks, with limited discussion on tasks relevant to autonomous vehicle planning. Definitions, metrics, and methodologies for evaluating embodied tasks in autonomous driving contexts, particularly for planning, are not included.\n4. The tasks are largely oriented toward language-based interactions, with an emphasis on using large language models. Metrics like BLEU and ROUGE, which primarily measure text quality, may not fully capture the performance of embodied AI tasks, raising questions about the suitability of these metrics for this benchmark.\n5. The paper does not specify a license for the assets used. Given that some assets are sourced from Unreal Engine, Baidu Maps, and Amap, it remains unclear whether these assets are freely distributable under their original licenses. Clarification on the licensing terms for these assets would strengthen the transparency and accessibility of the benchmark.",
            "- 1. Some metrics presented in Table 1 appear to be subjective and potentially incorrect. For instance, regarding visual realism, the rendering quality in Figure 1 is noticeably less convincing compared to GRUtopia. The images appear to be produced by a rasterization renderer rather than a ray tracing or path tracing renderer, revealing a significant disparity between the quality of human-crafted assets and actual buildings. Furthermore, from an embodiment perspective, the platform seems to primarily incorporate drones and vehicles, lacking support for widely-used embodiments such as humanoid and quadruped robots, despite the authors' claim in Table 1 that all these embodiments are supported.\n2. The diversity of the QA templates illustrated in Figures 8 and 9 appears to be quite limited. A broader range of templates would enhance the comprehensiveness of the evaluation.\n3. While the authors assert that the scene is crafted from real city maps, they do not clarify the benefits of this approach. The quality of the assets and rendered images does not seem realistic enough to justify this claim. Additionally, the authors have not demonstrated the sim-to-real potential of the proposed dataset, which is crucial for its application.\n4. Although the report includes scores based on several metrics, there is a lack of intuitive illustrations to showcase what the large multimodal models (LMM)-agents excel at solving. The results presented do not clearly reveal the main challenges of the proposed tasks.\n5. The rationale for incorporating dynamic pedestrians and vehicles into this platform is not clearly articulated. There appears to be no strong connection between the proposed tasks and the roles of pedestrians and vehicles, which raises questions about their necessity in the framework.\n6. Details regarding the LMM agents are insufficiently described. It remains unclear how these agents handle sequential egocentric observations, which is essential for understanding their operational effectiveness.\n7. The usefulness of the proposed benchmark is not adequately established. The absence of learnable baselines to validate the dataset\u2019s rationale potentially limits the significance and impact of this work.\n8. The authors do not justify the running efficiency of the platform, which is critical for scaling training within the environment. A discussion of performance metrics or benchmarks would be beneficial.\n9. The authors have not conducted experiments to explore the impact of different embodiments on task performance. Such investigations could provide valuable insights into the effectiveness of various embodiment strategies.\n10. The metrics for Evaluative Question Answering (EQA) rely on conventional reference-based NLP metrics, which may not directly demonstrate the correctness of the answers provided. It would be more effective for the authors to utilize a large language model (LLM) to assess the correctness of answers in relation to the ground truth.\n\nTypos:\n1. In the caption of Table 6, \"vision-and-navigation\" should be corrected to \"vision-and-language navigation.\"",
            "- Visuals. The paper advertises high quality visuals, and rates their visuals 3 out of 3 stars. To the reviewers, the visuals do not look better than things rated 2 out of 3 stars, such as CARLA.\n\nEvaluation metrics. Evaluating Embodied QA, Embodied Dialogue, and Embodied Task Planning with captioning and translation metrics, BLUE, CIDEr, etc, seems like a poor choice. I encourage the authors to define a notion of success for each task that evaluates if the agent did the task correctly. Such as, for the Embodied QA and Dialogue tasks, making questions with ambiguous answers multiple choice, or using something like LLM-Match (https://open-eqa.github.io). Questions without ambiguous answers can be evaluated directly. This would lead to a more meaningful and interpretable metric.\n\nMissing References. This paper is missing a very large number of references. For example, the authors mention, by name, the tasks Vision-and-Language Navigation (VLN) (https://arxiv.org/abs/1711.07280) and Embodied QA (https://arxiv.org/abs/1711.11543), but do not cite either work. They also do not cite the paper that proposed SPL (https://arxiv.org/abs/1807.06757). Overall, the space of EmbodiedAI has seen considerable interest and work but the paper cites very little of the work in this area.",
            "- 1. The motivation behind this paper aligns with the principles of ELM [1], focusing on embodied understanding in driving scenarios. A detailed explanation of the differences between the two approaches is necessary.\n2. Most of the evaluation tasks already exist in current literature. Providing a detailed explanation to distinguish these tasks from those in other works is important.\n\n[1] Embodied Understanding of Driving Scenarios"
        ]
    },
    "xom3YUQfbK": {
        "venue": "ICLR 2025",
        "title": "A Language Model based Model Manager",
        "link": "https://openreview.net/forum?id=xom3YUQfbK",
        "abstract": "In the current landscape of machine learning, we face a \u201cmodel lake\u201d phenomenon: a proliferation of deployed models often lacking adequate documentation. This presents significant challenges for model users attempting to navigate, differentiate, and select appropriate models for their needs. To address the issue of differentiation, we introduce Model Manager, a framework designed to facilitate easy comparison among existing models. Our approach leverages a large language model (LLM) to generate verbalizations of two models' differences by sampling from two models. We use a novel protocol that makes it possible to quantify the informativeness of the verbalizations. We also assemble a suite with a diverse set of commonly used models: Logistic Regression, Decision Trees, and K-Nearest Neighbors. We additionally performed ablation studies on crucial design decisions of the Model Managers. Our analysis yields pronounced results. For a pair of logistic regression models with a 20-25\\% performance difference on the blood dataset, the Model Manager effectively verbalizes their variations with up to 80\\% accuracy. The Model Manager framework opens up new research avenues for improving the transparency and comparability of machine learning models in a post-hoc manner.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            5
        ],
        "strengths": [
            "- The develops an LLM based model manager that could be a viable way to get at model differences in a human understandable way. So the core idea is not without merit.",
            "- The main strengths of the paper are:\n\n- The paper addresses an interesting problem that is becoming more important in the community. \n- The methodology proposed is original and promising. \n- The proposed approach is clearly presented and described.\n- The evaluation methodology is interesting and facilitates large scale evaluation of text generation capabilities. \n- Based on the evaluation performed the method performs well.\n- Some interesting discussion of the performance of the approach for different kinds of machine leanring models is provided.",
            "- 1. The task is novel and interesting.\n2. The evaluation approach and metrics are sound.\n3. The paper is well-written and includes enough details of the evaluation.",
            "- The authors tackle a common yet understudied problem and develop a unique solution. The paper was generally clear and easy to follow. With expanded experimentation and detail around methodology, I believe it could make a strong contribution."
        ],
        "weaknesses": [
            "- A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.\n\nThe execution of the key idea is fraught on several fronts. Primarily the choice of the difference levels is entirely unrealistic. Generally, one does not consider models that are this far apart in decision outcomes. I'd expect that the accuracies are also minimally 10 percentage points different (performance measures of the baseline models should be reported as well). A better experiment would be if models differ in the order of a few percentage points - instead of artificially changing models to be so far apart.\n\nWhile much is said about verbalisation, the paper does not provide a single example of verbal outcomes that are human-reviewable. The paper should do more to describe the verbalisations and perhaps even consider a user study on the efficacy of these to understand model differences.\n\nThe evaluation is entirely focused on label outcomes from the verbalisation and $M2$. This is surrogate measure, as there is no guarantee that the verbalisation is reflective of the decision logic. Second by translating to a classification task for the LLM, you induce several artifacts like position bias etc (as you are using the LLM as a classifier). Attributing this to the quality of verbalisation is tenuous.",
            "- The main weaknesses of the paper are:\n\n- No examples of the verbalizations generated by the approach are provided. This is frustrating for a reader as the value of the approach relies on these being useful to a reader and a small set of examples would easily demonstrate this (or not). \n\n- The motivation of the work is to provide model explanations, but the technique developed generates explanations of differences between models. The authors never explain how this approach achieves the overall aim as a reference model of some kind would always be needed. \n\n- The title is somewhat misleading. Model Manager has a specific meaning in the MLOps community (e.g. SAS Model Manager, Siemens AI Model Manager) which is quite different to what is being presented in this paper. This paper is quite specifically about a technique to generate text explanations of the differences between models and a title more reflective of this would be better. \n\n- The work focuses on explaining non-neural network models (e.g, logistic regression and decision trees) but there is a mismatch with the related work covered, which all focuses on neural network models. \n\n- The presentation and discussion of the results is not clear or consistent - e.g. Why are the Diabetes dataset, Overall Accuracy and k-NNs  missing from Figure 2? Also not clear if the results table (Table 2, 3, and 4) are part of the main body of the paper (referenced as such but then introduced later as additional results).",
            "- 1. While the task is described clearly, it is not clear whether it is sufficient to be called a Model Manager.\n2. The LLM-based in-context learning approach is not novel as one can imagine any text based task be formulated that way with some reasonable performance.\n3. Given the small set of experiments with simple datasets and model families, it is hard to see how this would generalize to more complex real world tasks and models.\n4. The code and instructions provided do not seem complete/right. For example, the README says `python lm_manager.py --llm [LM_name] --subject [subject_name]` is the command to run one of the experiments. However neither llm_manager.py nor main.py look aligned with this.\n\nMinor writing issues:\n1. spacing near line 166\n2. \"While our experiments do not focus on classification tasks, we include the feature names to improve interpretability\" - confusing as the experiments do seem to focus on classification tasks.",
            "- I see a few high-level limitations (L1-L3) and minor limitations (L4-L6) with the experimentation.\n\nL1. The presented experiments are limited to models that are not typically associated with the \u201cmodel lake\u201d phenomenon. I\u2019m also not sure that results describing model differences using logistic regressions and decision trees immediately generalize to deep learning models, since the space of verbalizations is likely much more complex with the latter. \n\nL2. The prompt implies there exists some verbalization that explains differences in model predictions. But sometimes, model disagreements could be due to random chance, rather than some systematic, explainable difference. The same could happen if the verbalization dataset is too small. I\u2019d be interested to see the verbalizations for two models which exhibit the same behavior, but produce slightly different predictions because of some other source of randomness (e,g. being trained on two distinct datasets). Is this a case the proposed approach handles? What do verbalizations look like in this setting?\n\nL3. The absence of baselines made it difficult to contextualize results. You could replace LLM_{verb} with an interpretable-by-design model; for example, what if you trained a logistic regression to predict Model 2\u2019s predictions on an example given the example\u2019s features and Model 1\u2019s predictions? How does that do in comparison to reconstructing Model 2\u2019s predictions given the verbalization using LLM_{verb}?\n\nL4. The reliance on feature names makes it difficult to generalize the methodology to settings in which models make use of high-dimensional inputs with no natural feature names (e.g. images).\n\nL5. Changing both LLM_{verb} and LLM_{eval} at the same time makes it difficult to assess whether results are different because a certain LLM is a better verbalizer or a better evaluator. I\u2019m not as familiar with the biases of passing one LLM\u2019s outputs as input to another, but it may still be useful to hold LLM_{eval} constant to isolate the performance of each LLM as a verbalizer.\n\nL6. It\u2019s worth clarifying early on that the examples used to produce a verbalization are distinct from the examples used to train each model. I was a bit confused by \u201cIt does so by serializing a representative sample of input instances (from the dataset) and the corresponding model outputs in a JSON format.\u201c in the introduction, but later clarified my confusion when reading the problem setting. I\u2019d move this information so that it appears earlier.\n\nThere are a few citations you might consider including; the following seem relevant:\n\n1. https://arxiv.org/abs/2201.12323 \u2013 a method to describe differences in text distributions. Seems like such a method could be used to describe differences between LLMs applied to a given task.\n2. https://arxiv.org/abs/2110.10545 - early work on choosing the \u201cbest\u201d model from a pre-trained model hub\n3. https://arxiv.org/pdf/2404.04326 - the authors here use an LLM to generate hypotheses; one could frame the task LLM_{verb} attempts to solve as hypothesis generation, where the model is developing a natural language hypothesis to describe Model 2\u2019s predictions given the features and Model 1\u2019s predictions.\n4. https://arxiv.org/pdf/2410.13609 - also focuses on model selection, but under limited labeled data"
        ]
    }
}