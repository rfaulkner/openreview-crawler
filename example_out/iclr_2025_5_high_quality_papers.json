{
    "xXTkbTBmqq": {
        "venue": "ICLR 2025",
        "title": "OLMoE: Open Mixture-of-Experts Language Models",
        "link": "https://openreview.net/forum?id=xXTkbTBmqq",
        "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10
        ],
        "strengths": [
            "- - The writing in this paper is clear and easy to follow.\n- The paper advances MoE research by providing a fully open-sourced, state-of-the-art MoE architecture, which is beneficial for the research community.\n- The paper presents a thorough analysis of key design choices in MoE, offering valuable guidance on building high-performance MoE models.\n- The analysis is insightful, with discussions on phenomena such as router saturation and expert co-activation providing fresh perspectives and meaningful implications for the field.",
            "- 1) Strong empirical results with state-of-the-art performance for 1B active parameters.\n2) Good exploration of the MoE design space which forms a good guide for MoE model design.\n3) Novel analysis of routing behavior in MoE models during training and inference.\n4) This is the only MoE model where the model weights, code, data and checkpoints are openly available and thus the work is entirely reproducible.",
            "- 1) There is no doubt that training MoE LLMs is challenging. This work offers a couple of important takeaways about how to train a good MoE LLMs, which is very helpful to the community.\n2) The presentation is very clear. For instance, the Table 1 delivers many key designs clearly at the early section of the paper.\n3) The model performance is good as well. As shown in Table 2 and 3, the model performs competitive with dense open models and partially open models (e.g. Qwen, Deepseek).\n4) The Analysis in Section 5 is informative, which greatly help readers and authors to understand how is the model working. This can also greatly speedup the growth of the community."
        ],
        "weaknesses": [
            "- I have a question regarding the experimental results: were the model parameters quoted directly from the original paper for the results shown in Table 2? For instance, in the original paper, OpenMOE\u2019s activation parameter count is reported as 2.1B, whereas Table 2 shows an activation parameter count of 2.9B for OpenMOE. I recommend that the authors carefully verify the accuracy of these values.",
            "- 1) Other state-of-the art MoE models in related works are not exactly in the same parameter count configuration (1B/7B) so an exact comparison cannot be made to this model's performance.\n2) Most of the design choices and training choices are based on prior work and the novelty is more in the design space exploration and analysis of routing behavior.",
            "- 1) Although the model has been relatively large, it is still much smaller than the SoTA MoE LLMs. I understand it is hard to get enough training resource for a fully open projects."
        ]
    },
    "zl0HLZOJC9": {
        "venue": "ICLR 2025",
        "title": "Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution",
        "link": "https://openreview.net/forum?id=zl0HLZOJC9",
        "abstract": "Recent progress in machine learning research is gradually shifting its focus towards *human-AI cooperation* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is *learning to defer* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation.  We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- + The paper addresses a research question relevant to real-world applications by providing a solution for settings where expert annotations are incomplete. \n+ The results show that reducing the workload of highly accurate (and typically overloaded) human experts only slightly decreases overall accuracy and can lead to higher accuracy in scenarios with inconsistent expert performance between the training and test sets. \n+ The proposed controllable workload formulation simplifies the evaluation of accuracy-coverage ratios compared to existing methods, which often require assumptions or post-hoc adjustments to balance learnable models and human experts.",
            "- The paper is well-written and easy to follow. The proposed probabilistic modeling techniques and the use of EM in this setting seem novel and an interesting contribution. Experimental results show the performance gain of the method compared to the baselines.",
            "- - The paper addresses an interesting issue in L2D and proposes a sound solution based on a probabilistic approach. \n- The workload management is particularly promising in many areas where AI is supporting expert decision such as in medicine. \n- This is also relevant in addressing ethical and practical constraints, and possibly even regulations and laws. \n- The ablation study offers an insight on the mechanism that lead to prioritise highest performing humans with the imbalanced approach, with possible overfitting.\n-  It is interesting that the study allows for the conclusion that in practice it may be desirable to distribute workload evenly across all human experts.",
            "- * It seems to me that the topic has been addressed very comprehensively\n* The comparisons include all the mentioned relevant predecessor methods"
        ],
        "weaknesses": [
            "- + As acknowledged by the authors, the proposed formulation does not scale well with the number of human (or learnable) experts. While grouping experts into clusters is suggested as potential future research direction, this introduces the number of clusters as a hyperparameter, necessitating additional tuning and potentially hindering scalability.\n+ Although the paper is concise and generally well-written, the notation is ambiguous in some places (see Q1 and Q2), and the discussion of the results is very brief and could benefit from additional explanations (see Q3 and Q4). \n+ (Minor comment) I recommend the authors release the source code to reproduce results. While not mandatory, providing the code would help readers understand how to implement the algorithm proposed on page 14, especially the implementation steps required to solve the optimization equation formulated in Eq. 4 on page 4.",
            "- A key weakness is highlighted by the authors in the paper: Bad dependency on the number of human experts. Although, they discuss potential remedies, e.g., clustering. However, this probably wouldn't work for a setting with diverse human experts (where the number of clusters is large). Are there other dimensionality reduction approaches (e.g., hierarchical clustering) that one could consider for this setting and how would they affect computational cost?",
            "- 1. Overall, the approach has some limitations, which I acknowledge are also partially discussed. However, it's unclear how well the system can scale given that each expert requires a probabilistic model. It's unclear to me how well the clustering of expert would work and what are the risks associated with that. \n\n2. I would be interested in reading more about the trade-off between the case for fewer deferring cases or deferring cases with the highest uncertainty, which is not much discussed. Clearly, there will be cases, e.g., healthcare, where deferring on uncertain cases would be quite important. \n\n3.  How could the model be adapted to take into consideration fast and slow changing expertise performance? The model assume static performance, however, experts could have fast performance changes, e.g. due to fatigue, or slow performance changes, e.g. due to learning through a period of time. It would be nice to understand how the model could accommodate for such dynamic scenarios.",
            "- I can't see any significant weaknesses. However, this may also be because the topic is new to me.\n\nFurther comments:\n\n* In the case of \u201cIn contrast, machine learning or AI models excel at processing large amounts of information but may be prone to biases (Meehl, 1954)\u201d, the reference chosen cannot be used as evidence for the statement because \u201cmachine learning or AI models ... processing large amounts of information\u201d were not available until long after 1954.\n* I find statement \u201cIdeally, a perfect balanced workload among experts and the AI model can be expressed as follows\u201d a little strange. After all, you will only strive for an equal distribution if all experts are equally competent.\n* I wonder about \u201cslightly-similar\u201d, how can something be slightly similar?\n* I find it a bit irritating that there is no section called \u201cConclusion\u201d.\n* \u201c50 %\u201d -> \u201c50%\u201d"
        ]
    },
    "zCxGCdzreM": {
        "venue": "ICLR 2025",
        "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
        "link": "https://openreview.net/forum?id=zCxGCdzreM",
        "abstract": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.\nIn this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.\nTo this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.\nKinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.\nOur trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments.  Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*.  This includes solving some environments that standard RL training completely fails at.\nWe believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - Kinetix provides 66 hand-designed levels while having the option to edit tasks with a graphical editor or to randomly generate more levels with rejection sampling. \n- The unified goal and dynamics within all environments encourage policies to have physical reasoning capabilities instead of merely memorizing the solution for some particular task, which is a valuable objective for researchers to pursue. \n- Kinetix provides a way to generate unlimited environments and tasks with a unified goal, objects, and dynamics, which could be of interest to multiple research communities like generalist RL policy learning, meta-learning, world modeling, spatial understanding and physics reasoning, and so on.",
            "- 1. Introduces a physics engine that provides \u201calmost entirely dynamically specified\u201d scenes, where environments with different robot morphologies can be vmap-ed and run in parallel, which is not doable with prior Jax-based sim frameworks like Brax.\n\n2. Paper is clearly written.",
            "- - Provide a highly efficient 2D rigid-body physics engine, leveraging JAX for scalable computation, with speedups of up to 30x when training an RL agent, allowing for \n- The learnt agent is highly effective at Zero-Shot transfer in the S and M levels that are held out, indicating the efficacy of pre-training on a wide set of procedural generation tasks. Additionally, show faster convergence/higher performance with this initialization\n- Have interpretable/handmade levels to understand the performance on different sizes/difficulties of tasks.",
            "- - The paper is well-written, organized, and straightforward.\n- Extensive testing across various task complexities validates its robustness in diverse 2D environments.\n- This paper has strong potential to serve as a valuable benchmark for future research."
        ],
        "weaknesses": [
            "- - The paper notes that as the generated environments increase in complexity, they may become unsolvable, which could contribute to the lower performance observed in the Large-level environments. If so, how does this impact the usability and interpretability of the benchmark results? To what extent does this affect the performance results reported in Figure 3?\n- It is unclear whether the proposed benchmark supports visual observations, which are essential for training generalist policies and building agents that can operate in real-world settings.\n- Although Kinetix can generate a vast range of environments, it is unclear how this benchmark would generalize to tasks or environments outside of its defined task distribution.",
            "- 1. All environments in benchmark must fall under the goal of making green shape touch blue shape without touching red shape. This seems to mainly constrain the problem to single-step tasks, where the reward of minimizing the distance from green to blue always incentivizes progress. Was this unified goal constraint purposefully imposed by design, or was it a constraint of Jax implementation, where the reward function for all environments must be the same to be parallelizable?\n\n2. Authors emphasized that parallelism and speed were big advantages of Jax2D. Since it is a reimplementation of Box2D, and this is a critical contribution of the paper, what are the performance gain metrics over Box2D?\n\n3. Experiments were on multi-discrete action space with binary rewards. However, it would strengthen the argument of the paper to do experiments on more of the important features of Kinetix, such as pixel-based observations and continuous action space.\n\n4. The state representation of the policy is very specific to the Kinetix environment suite and not very generalizable to other 2D RL problems. For instance, each entity is encoded separately and there is no scene-level encoding that is passed in as observation for the policy. Often, it is essential for a policy to understand the entire scene when predicting an action.\n\n5. There were no supplementary materials submitted, which would have been a good opportunity to show video rollouts of the trained agent in action.\n\n6. Experiments were mainly limited to the improvement of finetuned policies over pretrained and task-specific, trained-from-scratch policies. However, I would have liked to see more experiments that provide additional insights beyond \u201cfinetuning is mostly good\u201d and \u201czero-shot mostly doesn\u2019t work.\u201d For instance, using Kinetix for lifelong learning, transfer learning, and cross-embodiment learning.\n\n7. Abstract sentence seems like an oversell, given the results. \u201cOur trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments.\u201d Most would also disagree with the 2D learned behaviors as \u201cstrong physical reasoning capabilities.\u201d\n\n8. Minor: I think the wrong citation was provided for MJX in Section 7 (that work seems to be Mujoco).\n\n9. Minor: Experiments would benefit from some comparison to prior approaches/architectures, though this is less important given this is mainly a systems/framework paper.",
            "- - The JAX2D environment seems to be somewhat limited in its expressivities, modeling only 4 unique entities, which may not transfer to a wide set of domains/tasks outside of the ones studied.  \n- The task/reward function seems to be fixed across all environments to collide the green and blue shaped objects, while avoiding red shapes. Additional reward shaping seems to be needed for effective training, leading to some limited applicability of generating this data at scale for any set of tasks.",
            "- - **Real World Tasks:** While this paper provides a strong foundation in 2D simulations, expanding its scope to assess the agent\u2019s adaptability to real-world tasks, such as 3D simulations or complex dynamics as seen in [1,2], would enhance its practical relevance. Bridging this gap could amplify the study\u2019s contributions, offering broader insights into real-world generalization and scalability.\n\n- **Filtering out:** The authors mention that trivial and unsolvable levels are filtered out. What quantitative metrics were used to determine this filtering.\n\n- **Generalizability:** The claims of generalizability might be overstated given that the tasks remain in controlled simulations. Could the authors clarify the expected limitations of deploying such an agent in real-world scenarios with unpredictable environmental factors?"
        ]
    },
    "zBbZ2vdLzH": {
        "venue": "ICLR 2025",
        "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
        "link": "https://openreview.net/forum?id=zBbZ2vdLzH",
        "abstract": "When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to **j**ointly **d**enoise the features and **r**ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- The primary novelty of JDR lies in its combined optimization of graph structure and node feature alignment, enhancing data quality by maximizing alignment between the spectral components of the graph and feature matrices. This unified approach addresses both structural and feature-level noise simultaneously, which is rare among existing methods that typically target these types of noise separately. A key concept introduced is \u201cspectral resonance,\u201d where optimal alignment between the graph\u2019s leading eigenvectors and the feature matrix\u2019s singular vectors is achieved, providing a measurable target for denoising and boosting node classification performance. To manage the challenging non-convex optimization, the paper presents an iterative heuristic based on alternating optimization, which simplifies alignment maximization and enables efficient processing of large, real-world graph datasets with multiple classes. Another advantage of JDR is that it outputs a modified graph in the preprocessing stage, enhancing interpretability and reusability for subsequent GNN applications\u2014a contrast to end-to-end methods that alter the graph only during training. Lastly, JDR\u2019s design allows it to adapt effectively to both homophilic and heterophilic graphs, expanding its applicability beyond previous methods, which often work best with specific types of graph structures, such as those with high homophily.\n\nThe paper is well-organized, systematically guiding the reader through the challenges, methodology, and outcomes of the proposed Joint Denoising and Rewiring (JDR) algorithm. It begins with an introduction that effectively frames the problem of noisy graph structures and features, establishing the need for an approach that addresses both in unison. The methodology section details the concept of spectral resonance and the iterative optimization heuristic that drives JDR, using clear mathematical definitions and visual aids to support understanding. Following this, a comprehensive experimental section validates the algorithm's effectiveness across synthetic and real-world datasets, offering detailed comparisons with state-of-the-art methods and highlighting the algorithm\u2019s robustness across homophilic and heterophilic graph types. Finally, the paper provides an insightful discussion on related works, situating JDR within the broader landscape of graph preprocessing techniques, before concluding with a summary of contributions and potential directions for future research. Overall, the structure flows logically, with each section building on the previous one to reinforce the practical relevance and theoretical underpinnings of JDR.\n\nAddressing the limitations of the proposed approach is appreciated\n\nThe illustrations provided in the work, both in the body and abstract, are informative and well done as well as qualitatively informative.\n\n The use of the appendix is also well done in providing explicit discussion of the proposed algorithm \n\nThe paper has a robust experimental section with compelling results working in favor of the approaches proposed",
            "- 1. The proposed method is general and can be applied to a wide range of GNNs for downstream classification tasks. \n\n2. The proposed method jointly considers graph rewiring and feature denoising.\n\n3. The proposed method improves the performance of a numer of GNNs on a number of popular datasets in the experiments.",
            "- - This paper uses cSBMs as a key framework to build intuition about the graph rewiring and denoising problem, providing the theoretical foundation for the alignment target.\n- The empirical verification using synthetic data is clear.\n- The method is evaluated on both homophilic graphs and heterophilic graphs, showing its generalizability.",
            "- 1.The overall structure is well organized and easy to understand, especially the diagram demonstrations are very helpful.\n2.The problem formulation is clearly presented.\n3.Experiments are comprehensive and convincing.",
            "- 1. The method is well inspired and can achieve better performance than several other graph rewiring methods.\n2. An impressive amount of experiments were implemented."
        ],
        "weaknesses": [
            "- JDR depends on the availability of informative node features for effective rewiring and denoising, which restricts its applicability to settings with substantial node feature information; this reliance could limit its effectiveness in networks that primarily encode structural data. The algorithm\u2019s design is also tailored to node-level tasks, making it less suited for graph-level tasks like graph classification, where global structure matters more than node-specific features",
            "- 1. The solution is somewhat incremental and its novelty is low, although it appears to be sound.\n\n2. There is no theoretical guarantee on the degree of improvement using JDR.\n\n3. The experiments were conducted on a small number of datasets that cannot be considered as an evidence that the proposed method is really effective.",
            "- - The proposed method involves graph structure matrix and graph feature matrix decomposition, which can be computationally challenging on extremely large real-world graph data, limiting the practicability of the proposed method.\n- As the SVD decomposition can have time complexity of $O(N^3)$, it may be not accurate to say the proposed JDR has time complexity of $O(N)$",
            "- 1. The theoretical framework seems convincing to me, while how the real data sets fit the parametric model needs further investigation, otherwise the heuristic of denoising features might not be applicable.\n2. More explanation is needed on the insights of where the rationale of denoising comes from, e.g. lines 159-190.\n3. Since the eigendecomposition is applied on adjacency, the comparison of training and inference time costs between the vanilla GNN and with the add-on of the proposal is needed.",
            "- 1. The paper should better discuss previous works on GNN versus MLP, and the connection between heterophily and graph noise. [1-4] investigated the phenomena that MLP sometimes perform better than GNN (especially GCN) for heterophilic graphs. In particular, [4] proposed a metric that well correlates with empirical GNN performance, and also discussed the connection between heterophily and graph noise. A better discussion (acknowledgement?) of [4] is needed due to its high relevance to this paper.\n\n2. The comparison between JDR and graph rewiring methods seems not perfectly fair as the authors also mention themselves. Moreover, if JDR indeed achieves optimal denoising, then the graph may no longer be needed in later training. This (JDR(X) + MLP) seems uncovered by the ablation settings. \n\n3. The authors should check if the rewired graph structures degenerate, which may be a natural consequence of combining matrix factorization scheme and thresholding. \n\n4. The hyperparameter tuning was not performed for the GNN backbone. This is questionable as the optimal backbone hyperparameter is anticipated to vary across different rewired graphs (and potentially different denoised features). This may matter because the performance gap between JDR and other methods seems small in most cases.\n\n5. Some paragraphs are unclear and not readable. In particular, it is unclear what \u201cfindings\u201d in the sentence \u201cThe Gaussian adjacency equivalence conjecture (Shi et al., 2024) suggests a generalization of the findings to the true binary adjacency case.\u201d is referring to. The whole section (as well as the proof) needs to be polished.\n\n[1] Ma, Yao, et al. \"Is Homophily a Necessity for Graph Neural Networks?.\" International Conference on Learning Representations.\n\n[2] Gomes, Diana, et al. \"When Are Graph Neural Networks Better Than Structure-Agnostic Methods?.\" I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification. 2022.\n\n[3] Luan, Sitao, et al. \"Revisiting heterophily for graph neural networks.\" Advances in neural information processing systems35 (2022): 1362-1375.\n\n[4] Dong, Mingze, and Yuval Kluger. \"Towards understanding and reducing graph structural noise for GNNs.\" International Conference on Machine Learning. PMLR, 2023."
        ]
    },
    "z8sxoCYgmd": {
        "venue": "ICLR 2025",
        "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
        "link": "https://openreview.net/forum?id=z8sxoCYgmd",
        "abstract": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- + LOKI is a novel, multimodal dataset.\n\n+ The paper is easy to read and well-organised.\n\n+ Comprehensive Evaluation and Validation.\n\n+ Curates a diverse dataset with 18,000 questions across five modalities and 26 categories, providing a solid foundation for synthetic data detection evaluation.\n\n+ Detailed Annotations.\n\n+ Directly addresses the challenges of synthetic data proliferation, impacting security, misinformation, and content authenticity.\n\n+ LOKI\u2019s findings on LMM strengths and weaknesses have the potential to drive advancements in synthetic data detection and multimodal model development.",
            "- The paper is well-written and easy to follow.\n\nThis work addresses a glaring and emergent need in a topic field: synthetic data detection for LLMs; I agree with the authors that there doesn't currently exist a comprehensive, multi-modal, nuanced dataset including explainability assessment for this domain area. \n\nExtensive examples and case studies provided in appendices. \n\nMany data domains are covered in this benchmark, including several categories and characteristics that are often underrepresented in synthetic data detection (e.g., satellite images, \"abnormal details\").",
            "- 1.\tThe paper is well written and easy to follow. The authors provide sufficient technical details for readers to understand their work.\n2.\tThe benchmark designed by the authors encompasses a rich variety of modalities and diverse question types, enabling a comprehensive evaluation of LMM performance.\n3.\tThe authors introduce a metric called the Normalized Bias Index (NBI) to quantify the performance differences of the model on natural and AI-generated data across different modalities, which is an innovative way to assess model bias.",
            "- - Comprehensive Multimodal Benchmark: LOKI covers an extensive range of data modalities and subcategories.\n- Inclusion of specialized domains like satellite imagery, medical images, and philosophical texts pushes the boundaries of traditional datasets and tests models in less-explored areas.\n- Multi-Level Task Design: The benchmark doesn't just focus on binary classification but also includes tasks that assess models' abilities to explain their reasoning, promoting the development of interpretable AI systems.\n- Highlighting the Importance of Explainability\n- By testing perception, knowledge, and reasoning across modalities, LOKI contributes to the broader goal of advancing towards AGI."
        ],
        "weaknesses": [
            "- -  The benchmark lacks a robustness test against common real-world conditions like compression artifacts. To enhance real-world applicability, the authors could include performance evaluations on compressed data.",
            "- While the differentiated modalities, categories and annotation levels are beneficial, the overall size of the dataset actually seems relatively small vis-a-vis related datasets (Table 1). \n\nIt is unclear to me, how a user can methodically compare scores for different models across tasks/categories (e.g., in Table 2); perhaps the authors can address this, given the heterogenous and imbalanced nature of the data modalities and tasks, as well as the problem/domain \"difficulty\". \n\nAs deepfake detection is one of the most prominent synthetic data detection categories today, I believe the benchmark would benefit from its inclusion.",
            "- 1.\tThe current evaluation mainly relies on accuracy and NBI; however, at low recall rates, NBI may not adequately reflect model bias. Additionally, the design of NBI may be insufficient to comprehensively capture various types of bias exhibited by the model.\n2.\tThe paper mentions that the model exhibits \"bias\" across different modalities. However, the specific causes of this bias are not thoroughly explored through experiments or comparative analysis. This conclusion may be based on surface-level observations without further investigation into whether the bias arises from data, model architecture, or task design.\n3.\tThe paper mentions that the Chain-of-Thought (CoT) approach can impact model performance in image and 3D reasoning tasks.  However, it does not provide sufficient experimental details to clarify whether CoT significantly enhances performance across all types of tasks or if it is only effective for the specific tasks currently evaluated.\n4.\tIt is suggested to discuss and compare more related works such as [1,2] in this paper.\n\n[1] Detecting and Grounding Multi-Modal Media Manipulation and Beyond. TPAMI 2024.\n\n[2] Detecting and Grounding Multi-Modal Media Manipulation. CVPR 2023.",
            "- - Limited Performance in Certain Modalities: The benchmark reveals that LMMs perform poorly in modalities like 3D and audio, which may be due to the lack of available models or training data in these areas.\n- Insufficient Details on Data Generation Methods: The paper could provide more in-depth information on how synthetic data was generated for each modality, which is crucial for reproducibility and understanding potential biases in the dataset.\n- Evaluation of Few-Shot and Chain-of-Thought Prompting: The analysis of prompting strategies is somewhat limited."
        ]
    },
    "xoXn62FzD0": {
        "venue": "ICLR 2025",
        "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
        "link": "https://openreview.net/forum?id=xoXn62FzD0",
        "abstract": "A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution\u2014which can differ substantially from the LM\u2019s base distribution\u2014is generally intractable. In this work,\nwe develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis\u2014we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\\times$ larger, as well as closed-source, fine-tuned ones. \nIn support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. \n[Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- This work solves one important problem with some of the widely adopted constrained/structured generation such as Guidance, SGLang and Outlines: that is, these framework achieves control by masking out next-tokens that would violate the constraint, leading to biased sampling (compared to the ground-truth conditional distribution). By leveraging sequential Monte Carlo, the proposed technique is able to approximate unbiased sampling in a relatively practical/scalable way. Empirical evaluations demonstrate strong performance on challenging real-world problems.",
            "- The authors propose adapting SMC methods to novel semantic parsing tasks, resulting in notable performance improvements. \n\nThe author conduct a interesting analysis and shows that resampling improves the approximation of the global product-of-experts distribution and approximation quality are consistent with those observed in downstream accuracy evaluation.",
            "- Originality\n- To my knowledge, the extension of sequential Monte-Carlo to the task settings in the paper, and the specific generation receipe (re-weighting, resampling) are new. However, I am not closely familiar with [Lew et al 2023] or its subsequent papers (which are mentioned several times by the authors). Therefore, my evaluation of novelty may be slightly off. \n- Placing ideas such as token-masking, filtering out partial sequences, and selecting partial sequences to explore next in a probabilistic framework is a nice contribution (with the same caveats in the point above).\n\nQuality\n- The experimental evaluation presents a controlled study that ablates each component in the model.\n- Derivations and the divergence analysis seem to be of high quality.\n\nClarity\n- Once the reader becomes familiar with the terminology, the paper is written clearly and precisely. \n\nSignificance\n- The method could potentially be useful in settings where token-level and partial-sequence level constraint functions are available (e.g., those in the experiments). This has some generality (though could also be viewed as a limiting factor).\n- Placing more domains and settings into the probabilistic framing from [Lew et al 2023] helps to further the probabilistic perspective on sequence generation.",
            "- -Good motivation from analysis of weight formulations for importance sampling.\n\n-Benchmarks validate claims that the proposed algorithmic components improve downstream performance. Additionally, authors chose a sensible set of benchmarks.\n\n-Weight correction and resampling seem to be novel components."
        ],
        "weaknesses": [
            "- Some detailed analysis/case study on the sample complexity of SMC would provide more insights, especially how much better SMC is compared to naive importance sampling.",
            "- In terms of experiments: \n- The authors do not emphasize their unique algorithmic contributions within the experiments. The authors could also report the performance of LM with grammar constraint, weight correction and resampling as a regular SMC baseline to further show the effectiveness of semantic potential. Additionally, the authors lack a detailed comparison between their method and the highly relevant SMC method in https://arxiv.org/pdf/2306.03081, and should report it as a baseline, e.g., including without-replacement resampling. \n- For ablation studies, how the number of particles will affect the final performance should be analyzed.\n\nThere is no mention of the computation cost, it would be very useful if the authors could evaluate the efficiency of the proposed algorithm.",
            "- My primary concerns were on the experimental validation. The paper performs a self-contained, controlled experiment using one Llama 3 model on a set of tasks. As a result, it was unclear how the findings generalize to other models, or how they compare in terms of performance to other methods in the literature.\n\n1) For example, taking the example of DS-1000, the absolute numbers are quite low: the DS-1000 paper reports up to 39.2 performance (with Codex-002) versus 28.5% here (with Llama 3). These are *not* comparable since they use different models, but it would be nice to see how this method performs for models closer to the state of the art. Similarly, Lever [1] reports numbers on Spider from previous work that range from 67.0% to 81.9% [1]. The reason this is important is that the exact experiment setup can lead to different conclusions on the performance of methods, so it was concerning that the absolute numbers seemed low. However, the authors could potentially clarify this.\n\n2) It was also unclear why 10 particles was selected, since in these sampling methods the number of samples can impact performance, and we often want to understand how performance varies with the sampling budget. How does the method vary as the number of particles varies? Is there a sample-and-rerank approach that could outperform this method if it drew a large number of samples?\n\n[1] LEVER: Learning to Verify Language-to-Code Generation with Execution, Ni et al ICML 2023",
            "- -The method was not benchmarked against alternative methods. While the ablation study is useful, how does the method compare against other SMC-based techniques such as the ones cited in the related works section that are particularly relevant to this work? E.g. comparisons against the method in Lew et al. and Zhao et al. would be beneficial. There are other non-SMC-based methods that could also be benchmarked against.\n\n-Only Llama 3.1 8-B was evaluated. The manuscript would benefit from benchmarks on additional LLMs to see if results are consistent across similar sized LLMs. I would be curious to see if the benefits are as substantial on larger models, but I understand the authors may have limited computational resources for such analyses.\n\n-There is a lack of theoretical grounding as to the benefits of the components. E.g., a theorem rigorously showing the reduction in KL-Divergence shown in Figure 2 would strengthen the manuscript.\n\n-Notation can be difficult to follow at times. Exposition can be a bit drawn out in certain places, e.g. section 2. I appreciate the authors trying to point out the inefficiencies in each component of MC in order to justify their approach, but I think the exposition would benefit from a condensed explanation of, e.g., the computational burdens of IS."
        ]
    },
    "xDrFWUmCne": {
        "venue": "ICLR 2025",
        "title": "Learning to Discretize Denoising Diffusion ODEs",
        "link": "https://openreview.net/forum?id=xDrFWUmCne",
        "abstract": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at https://github.com/vinhsuhi/LD3.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- - The paper is well-written and easy to follow.  \n- It presents an easy solution to the sampling problem of diffusion models that only requires limited training time while obtaining. \n- The soft teacher loss is effective and simple to implement. \n- The evaluation is thorough and includes multiple models, multiple datasets, and multiple sampling strategies.\n\nIn general, I liked the paper and I lean toward acceptance. However, since this is not my area of expertise, I would wait for the discussion with the authors and other reviewers to increase the score to Accept and recommend borderline Accept for now.",
            "- - The LD3 algorithm is extremely lightweight, requiring only 100 samples and less than 1 hour on a single GPU to learn optimized sampling schedules.\n- The method is evaluated on a comprehensive set of pretrained models and compared against several baseline, showing improved quality in the majority of cases\n- A proper ablation study is done on the various choices/hyperparameters.",
            "- - The paper includes proofs of soundness of their proposed minimization objectives, going beyond purely empirical contribution.\n- The number of experiments is substantial across both datasets, baselines from prior work, and choice of pretrained models.\n- The experiments conducted include notoriously difficult datasets in the literature of diffusion step reduction like ImageNet, and shows improvement in more complex settings such as a text to image model.\n- The objective is cheap to train compared to prior work; the key being that a very low batch size of 2 is permissible to use.\n- Ablation studies demonstrate the importance of the proposed changes separately.\n- The samples presented qualitatively look very reasonable and show clear improvement over the usual hand-crafted timestep schedules, and they fix the random seed so the same samples can be comapred."
        ],
        "weaknesses": [
            "- Although I liked the paper, there are some concerns that, if addressed, would improve the paper. In the following paragraphs, I describe my concerns in detail:\n\n- In the table with the main results, sometimes it is not clear what the metrics are computed against. I suppose the metrics in table 2, 3, 4, and 5 are computed against random samples of the model using the accurate estimation of the ODE. However, if this metric is computed against the true distribution, the performance of the teacher with the accurate computation of the ODE should be shown (1000 steps). I think the evaluation protocol needs to be more clearly defined.\n\n- In a similar direction, Table 6 shows the performance of a teacher model using 8 steps. Why only 8 steps are used here? Would not the teacher use a higher number of samples?\n\n- The model used is quite simple being only composed of a single vector (or two in the decoupled version). From the results in Table 7, increasing the number of parameters leads to better results. Would increasing the complexity of the model lead to better results?\n\n- In the limitations section I found missing that the proposed method needs to be retrained for the target number of sampling steps. One model trained to generate images with 2 samples, would not be useful for 3 and a new model would need to be trained. This might be a problem since different images might necessitate a different number of steps to achieve good quality.",
            "- - There are several typos in the paper. See some examples below:\n    - Algorithm 1 Line 6: $x'_T \u2190 x'_T + ...$  must be $x'_T \u2190 x_T + ...$\n    - Line 251: $x'T \\rightarrow x'_T$\n    - Line 251: $\\Psi*(x_T) \\rightarrow \\Psi_*(x_T)$\n- Theorem 1 requires more explanation on its invertibility assumption. Specifically, if the NFE is small, functions $\\Psi_*, \\Psi_\\xi$ invertibility is a non-trivial fact which requires some justification on its assumption. \n- The method relies on a learned perceptual distance (LPIPS) to achieve optimal results, as shown by the significant quality drop in Table 7 when switching to a standard Euclidean loss. This raises questions about how well the method might generalize to other data types beyond images.",
            "- There are two main areas around which the paper could be much stronger. The first is in comparisons to distillation methods, which are among the strongest in the literature. The paper includes a comparison to progressive distillation and consistency distillation in Table 9, but it is really difficult to compare these methods apples-to-apples. There are details missing (please correct me if I missed these e.g. in the supplementary material) such as what models were compared and where are the baseline scores taken from; ideally the same model should be post-trained with the different techniques. The number of forward passes across methods also doesn't match, making it difficult to draw any conclusions. One conclusion that can be drawn, however, is that progressive distillation remains better than LD3 in FID score at NFE=8, albeit requiring much more compute to distill.\n\nThe other major weakness is the lack of careful qualitative comparisons to other step reduction methods. The vast majority of the qualitative samples are compared to hand-crafted schedules, which are the weakest baselines. This is really important, especially because prior work has shown that very low FID scores can be achieved somewhat adversarially, resulting in strange samples (e.g., consider the CIFAR10 samples in the GGDM paper), so quantitative results are insufficient to truly demonstrate that LD3 improves over all prior work. Careful side-by-side comparisons of different step reduction methods, derived from the same pre-trained model and using the same initial noise and matching NFE would be significantly more convincing.\n\nOverall, the work is strong, and the quantitative results already put this paper as a valuable contribution to the literature that should be accepted at the conference. I am opting for a weak accept, because the comparisons to distillation methods seem improper and incomplete, and the qualitative comparisons require more care. But even if so, due to the very low cost of the proposed technique and the achieved scores, the work already has intrinsic value. I strongly encourage the authors to address the concerns outlined above as it would make the work excellent."
        ]
    },
    "xByvdb3DCm": {
        "venue": "ICLR 2025",
        "title": "When Selection Meets Intervention: Additional Complexities in Causal Discovery",
        "link": "https://openreview.net/forum?id=xByvdb3DCm",
        "abstract": "We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - The authors have found an interesting flaw in previous methods in the presence of selection bias\n- The paper is well motivated with clear examples. Some examples can be made clearer (see below)",
            "- 1. The selection bias in interventional experiments is an under-explored but crucial issue in causal discovery, and the authors use two clear examples to illustrate why this problem matters and why the existing methods and simply augmenting DAG fail.\n\n2. The authors provide a solid theoretical foundation in this paper by (1) rigorously defining the interventional twin graph and characterizing its Markov properties and (2) proving the soundness of the proposed algorithm.\n\n3. Synthetic experiments show that the proposed method outperforms baselines in handling selection bias and remains robust as the number of variables increases. It also uncovers novel causal relationships in real-world applications.",
            "- 1. The problem of selection bias is important yet often overlooked in existing interventional causal discovery. The setting is general.\n2. This paper is technically sound, with clear formulation of the causal DAG and Markov properties. \n3. The illustrative examples enhance the paper's clarity, helping readers better understand the concepts.",
            "- The paper clearly lays out the problem of selection bias in causal discovery and why certain natural approaches to the problem are not sufficient. The paper also puts forward a very general solution to the problem and considers its consequences. Overall the paper is well-written, despite being notation-heavy.",
            "- - This paper studies a relevant and interesting problem - both mathematically and philosophically. It considers the question: \"What does selection bias actually mean\" and proposes a sound answer and the necessary mathematical framework to deal with such situations.\n- Based on the framework to treat selection bias, a sound and complete causal discovery algorithm is proposed.\n- The method is evaluated not only on synthetic data, but on real-world examples as well. This provides some confidence that it may be useful in practical applications."
        ],
        "weaknesses": [
            "- Some bits of the exposition are unclear (see below).",
            "- 1. While the introduction of the interventional twin graph and its Markov properties is rigorous, it may be challenging for readers to grasp at first glance due to its complexity. Providing a high-level explanation to offer an intuitive understanding would greatly benefit readers.\n\n2. The interventional twin graph is more complex than a simple DAG and involves additional nodes. It would be helpful if the authors discussed the computational cost of the proposed model compared to the simpler DAG, including an analysis of the algorithm's computational complexity under the new graphical model.\n\n3. The authors did not address the identifiability guarantees of the proposed method. It would be useful to know if the method can reliably identify the selection variables and under what conditions the true interventional twin graph can be identified.\n\n4. Minor typos:\n    * Line 48: \"We show that existing existing graphical representation paradigms\" --> \"We show that existing graphical representation paradigms\"\n    * At the end of line 169: \"models a completely different scenario\" needs to be revised",
            "- More comprehensive results and explanations of the empirical studies would be beneficial to support the effectiveness of the proposed algorithm. For example:\n1. For the simulation, can you report the proportion of true causal edges estimated as directed edges by the algorithm? Additionally, a comparison of the output graphs with the ground truth would illustrate how the new algorithm perform differently from other methods under selection bias. \n2. For the gene application, can you provide more comprehensive analysis of the result? \n3. For the education dataset, can you explain why the pre-intervention selection bias is a potential issue? Highlighting and interpreting key information of the resulting graphs would be helpful, as the current graph and variable names are difficult to follow.\n\nMinor comments about clarity:\n- The notation in this setting is dense and improvements of readability can be helpful for readers less familiar with the area. For example, \"CI\" in line 93 and different types of arrows in Example 7 can be clarified before their first appearance.\n- Typos: line 48 \"existing\", line 295 \"false\".",
            "- One thing that was unclear to me is how complete is the paper's characterization of Markov equivalence classes in the given model. I would think that the Markov equivalence class would encode all DAGs with the same conditional independence structure (i.e. the right-hand side of the implications in Theorem 1). However, the equivalence structure is defined with respect to the left-hand side of the implications in Theorem 1. This would seem to imply that the equivalency classes are not as fine-grained as they potentially could be.\n\nThe algorithm provided also suffers from this issue, in that the authors point out that it may not be complete. It is not clear to me how useful it is to have a causal discovery algorithm that is sound but not complete. The trivial algorithm that says there are no causal relationships is sound but not useful.\n\nThe simulation study not reporting any information on completeness is disheartening. While I understand that the paper does not contain any guarantees on completeness, in the simulations there is access to the ground truth. So it is hard to see how there is no way to evaluate the ability to discover some fraction of those relationships.\n\nAt a higher level, I'm not sure how much of the framing of the paper is specific to the selection problem. It seems like the approach of the paper is tackling the more general problem of causal discovery with unobserved latent variables. If that is not the case, then the paper should explain how their methods do not generalize to the latent variable setting.",
            "- The biggest weakness I see is the presentation of the paper. The first two sections are dense, but give a good introduction and motivation to the problem, based on good illustrations in Examples 1 and 2.\n\nHowever, Section 3 is the most painful piece of text I have read in a while. It relies mostly on mathematical notation to bring across the main points and lacks the contextualization in prose. I appreciate that examples are given in Section 3, but even those are a bit cryptic and fail to provide an accessible intuitive understanding. I suppose there are three main reasons for this: (1) Writing about complex SCMs is inherently difficult and a certain level of formalism is necessary - not much you can do here. (2) The amount of content in the main paper, given the page limit might a bit too much. Some of the more technical parts could be relegated to the appendix and exchanged for more contextualization. (3) The text could consider the reader's state of mind more. Some examples:\n\n- L211f: introducing the functions $f^*$ uses the mathematical symbols for the corresponding variables in the counterfactual basal world to introduce them, but does not use the word \"counterfactual\". That means as a reader, I either have it in working memory, or I have to go back to the definition and jump back again to the sentence to parse it.\n- As far as I can tell, abbreviations like \"CI\" and \"MAG\" aren't defined, or used before they are defined, e.g. \"PAG\".\n\nSuch presentation choices add unnecessary mental effort for understanding, and I would think twice if I'd go back to this paper and build on it for future work - not because it's wrong, but because of the mental effort to access the information."
        ]
    },
    "wg1PCg3CUP": {
        "venue": "ICLR 2025",
        "title": "Scaling Laws for Precision",
        "link": "https://openreview.net/forum?id=wg1PCg3CUP",
        "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- Strengths:\n\n1. The paper tackles an important issue with the introduction of a bit precision scaling law. While this topic has been explored before, the theoretical scaling law presented in this work offers valuable guidance for the efficient deployment of models in real-world applications. The implications of this work could be transformative for the field.\n\n2. The authors have provided a wealth of experimental results that not only validate the existing scaling laws across different model sizes but also demonstrate the generalizability of previously unseen scenarios. This thorough experimental section strengthens the paper's contributions and is persuasive.\n\n3. The manuscript is particularly strong in its methodological rigor, with a clear articulation of the scaling laws and their implications for precision in deep learning models.",
            "- (1) The paper studies a meaningful topic, the scaling laws of precision, which is a new topic following the scaling law of data and parameters.\n\n(2) The paper gives a good presentation. I especially appreciate the introduction to quantization. I'm not familiar with how quantization works in detail, so it helps a lot.\n\n(3) The paper shows interesting findings in Sec. 3.1 Fig. 2: more pretraining tokens result in lower performance for post-train quantization with a high quantization rate.\n\n(4) The paper shows interesting findings in Sec. 4.1 Fig. 3: KV cache is more sensitive to the change of precision when precision is low, but when precision is high, KV cache is more robust to the change of precision compared with weights and activations.\n\n(5) The paper shows interesting findings in Sec. 4.3 Fig. 6: there would be cases where training in low precision leads to better evaluation loss.\n\n(6) The paper generally shows that the proposed scaling law works well in the experimental setting of the paper.",
            "- - The paper introduces a new dimension to the well-established scaling laws by incorporating precision as a critical factor. This is an important contribution because most prior work focused on model size and dataset size without considering precision, which is increasingly relevant due to hardware advancements supporting lower-precision computations. By doing so, the authors offer a more comprehensive framework for understanding and optimizing model performance under different training and inference conditions.\n\n- The authors fit on over 465 pretraining runs across different precisions (3-bit to 16-bit) and sizes (up to 1.7 billion parameters), providing a robust dataset to validate their proposed scaling laws. The empirical results are consistent with the theoretical predictions, achieving high R^2 values (e.g., R^2 = 0.97 for post-training quantization degradation). \n\n- The paper offers actionable insights into how low-precision training can be compute-optimal, particularly in scenarios where hardware constraints or cost considerations are paramount. For example, it shows that training larger models at lower precision can sometimes be more efficient than using higher precision, which is a valuable insight for practitioners looking to optimize both performance and computational costs.",
            "- 1.\tThe proposed scaling law unify the post train quantization and quantized training into a single functional form.\n2.\tThe finding in the section 4.3 is inspired and the conclusions are consistent with usual experience and give a theoretical explanation.\n3.\tThe experiment is adequate and reasonable and the paper is well written."
        ],
        "weaknesses": [
            "- no clear weakness.",
            "- (1) The paper uses the dataset Dolma for experiments. Though it's hard, it would be interesting to see how pretraining data affects this law.\n\n(2) The paper uses the OLMo-style models for experiments. It would be great to give a general introduction to OLMo-style. Are they transformer-based model? While the abstract states the scaling law for language models, there would be other types of language models other than OLMo-style models, such as SSM.",
            "- - While the paper focuses extensively on integer-type precisions (e.g., 3-bit, 8-bit), it does not explore floating-point types like FP8 or BF16 in as much depth. Given that floating-point formats are widely used in modern hardwares, this omission limits the generalizability of the findings to real-world applications where floating-point precision is common. This could limit the applicability of the scaling laws in environments where floating-point precision dominates, potentially requiring further research to adapt these findings.\n\n- The experiments are conducted on specific hardware setups that support low-precision computations, such as GPUs optimized for integer-type operations. The fitted constants and trends may not generalize well across different hardware architectures or future technologies that handle precision differently. This may reduce the long-term relevance of the paper\u2019s findings as hardware evolves.\n\n- Maybe I'm missing this, but the paper suggests that compute-optimal precision is around 8 bits but does not deeply explore scenarios where precision drops below 4 bits (e.g., binary or ternary quantization). Given that future hardware may support even lower precisions, this limits the scope of the findings.\n\n- While pretraining cost optimization is thoroughly explored, inference-time costs -- especially in real-time or latency-sensitive applications -- are not given as much attention. In many practical deployments, inference-time efficiency is more critical than pretraining cost savings. This imbalance might limit the practical applicability of some of the findings in scenarios where inference-time efficiency is more important than pretraining considerations.",
            "- 1.\tThe paper use the $N(1-e^{P_{w}/\\gamma_{w}})$ to fit the left in the figure 3. But I think the power law is the most commonly used in all kinds of scaling law form. I suggest the author could compare the exponential with power law like $N(1- A*P_{w}^{\\alpha})$."
        ]
    },
    "weM4YBicIP": {
        "venue": "ICLR 2025",
        "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "link": "https://openreview.net/forum?id=weM4YBicIP",
        "abstract": "With the introduction of video diffusion model, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals such as movement regions to stabilize movements, which compromise the naturalness and freedom of motion. To address this issue, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed two key modules: an inter- and intra-clip temporal module and an audio-to-latents module. These enable the model to better utilize long-term motion dependencies and establish a stronger audio-portrait movement correlation. Consequently, the model can generate more natural and stable portrait videos with subtle facial expressions, without the need for manually setting movement constraints. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios. Video samples are available at https://loopyavataranony.github.io/",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The proposed method is solid, with enough technical contributions to address the long-term dependency between motions and audio conditions.\n\n2. The experiment results are strong enough compared to prior works and baselines, in particular on FVD metrics and DExp metrics.\n\n3. Both qualitative results and the demos shown in the supplementary webpage are appealing and convincing enough, where the long-term dependencies and correlations between audio and portrait motions are consistently maintained.\n\n4. Overall, the paper is well-written and easy to follow, albeit having many technical details.\n\n5. The human study results clearly show that the proposed method perceptually outperforms other baselines and prior arts.",
            "- 1. The paper introduces an end-to-end audio-only conditioned video diffusion model, which moves beyond traditional methods that rely on spatial constraints for motion stabilization.\n\n2. The proposed novel modules like inter- and intra-clip temporal modules and audio-to-latents module are well-designed, resulting in more natural and consistent portrait movements and leading to better synchronization and more expressive facial movements.\n\n3. The paper includes extensive experiments that demonstrate Loopy\u2019s superiority over other audio-driven portrait diffusion models both quantitatively and qualitatively, with evidence of more lifelike and stable video outputs in the supplemental website.\n\n4. The paper is well-written, the proposed components and architecture are described clearly.",
            "- 1. The motivation is clear. The authors focus on the weak correlation between audio and portrait motion in end-to-end audio-driven methods.\n2. Overall, this paper is easy to follow. The proposed TSM module is technically sound in its design, and the experimental validation is effective.\n3. Many synchronized and vivid portrait videos are generated.",
            "- 1. The results are good. \n2. The introduction of two modules (Temporal and Audio) is reasonable and interesting. Ablation study supports the benefits of these modules."
        ],
        "weaknesses": [
            "- 1. For audio-to-latent module, why replacing it with cross-attention module leads to largest performance drop as seen in Table 3. What are missing from cross-attention that makes it fail to perform as good.\n\n2. During inference, audio ratio and ref ratio are manually set for classifier guidance, an ablation study is suggested to their impact on the final quality of generated video to have some insights about this weighting scheme.\n\n3. Could the proposed method be further optimized and adapted to real-time settings, where the audio is being played and video follows interactively?\n\n4. What are limitations of the proposed method and what could be improved? Are there failure cases where the generated motions cannot follow the audio closely?",
            "- 1. While the audio-to-latents module improves the audio-motion correlation, there is no mention of how different audio characteristics (e.g., background noise, varying loudness) might impact the model\u2019s performance, which could be critical for real-world applications.\n\n2. The paper lacks a detailed analysis of potential failure modes or scenarios where Loopy may struggle. Highlighting these cases would provide a more balanced view of the model's robustness and limitations.",
            "- 1. In the A2L module, the effects of Movement and Expression on the method have not been thoroughly validated. The audio inputs shown in Fig. 4 are somewhat confusing. I assume they refer to audio features from wav2vec. \n2. Human expressions are closely related to many facial details, but the implementation in the paper is rather trivial. \n    1) the detected landmarks are too sparse and not accurate enough (DWPose), which makes it difficult to capture a person's expression accurately. \n    2) using the variance of keypoints to calculate head movement and expression changes presents several practical issues, \nsuch as the entanglement of head movement and camera movement. Why not use FLAME coefficients or results from other emotion estimation methods? \n3. The TSM module needs a deeper discussion on its impact on overall computational efficiency.\n4. In Tables 1 and 2, the methods perform worse than others on some metrics, especially those related to Glo and Exp. The authors do not provide detailed analysis or discussion on this.\n5. The paper has several writing issues. Some symbols and abbreviations are introduced without explanation, such as TSM in Fig. 2. Additionally, some text in the figures is too small to read, such as \"other computational layers\" in Fig. 3. The main paper does not reference Table 2. There are also some typos, such as in Line 302, where there is an error with punctuation.\n6. The paper does not include a discussion of the limitations of the proposed method.",
            "- 1. Lack of ablation of stand-alone intra- / inter-temporal model. Is both of them necessary or only the inter-clip temporal layer is enough?\n2.  The functionality of the Temporal Segment Model is unclear. Is it for capturing the appearance of the character under different expressions? If so, why (L478) longer motion frames lead to worse results?\n3. Similar to the above issue. I watched the video samples of the ablated model. Seems to me the ablation of either part leads to similar degradations \u2014 lack of head pose variance and subtle expression. This makes me unclear about the different roles of the two proposed modules."
        ]
    },
    "vzItLaEoDa": {
        "venue": "ICLR 2025",
        "title": "Open-World Reinforcement Learning over Long Short-Term Imagination",
        "link": "https://openreview.net/forum?id=vzItLaEoDa",
        "abstract": "Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be \u201cshort-sighted\u201d, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The jumpy prediction technique within the long-term imagination framework is innovative as it departs from the fixed interval approach prevalent in previous work, offering increased flexibility in jumpy prediction\n2. The paper is well-organized and clearly written.",
            "- - The paper is mostly well written. \n- The method proposed is novel and the results are promising comparing to the baselines.",
            "- - **Significance**\n  - Long-horizon world modeling and reinforcement learning in open-world environments are important problems.\n  - The proposed approach is insightful and successfully addresses these problems.\n- **Originality**\n  - The proposed approach involves the combination of multiple novel and inisightful components.\n- **Quality**\n  - Overall the quality of the paper is relatively high, with the method reasonably clearly explained and analyzed.",
            "- 1. It introduces a method for generating target states in MineDojo (or possibly in other 3D-RPG games).\n2. It demonstrates the feasibility of training a world model with jumping transitions and optimizing policies over such transitions.\n3. The illustration is clear and the writing organization is good."
        ],
        "weaknesses": [
            "- 1. The proposed method employs a hierarchical structure, yet the baseline comparisons are made with flat learning methods. Including comparisons with hierarchical MBRL methods like Director[1] could greatly strengthen the paper.\n2. Equation 9 appears to have an inconsistency in the time indexing; should the bootstrapping term $R^\\lambda_{t+1}$ be $R^\\lambda_{t+\\hat{\\Delta}_{t+1}+1}$ ? \n3. The use of\u00a0 $\\lambda$ -return in evaluating the policy might introduce bias since it should be evaluated with on-policy data,, but the predicted jumpy state, $\\hat{z}_{t+1}$, might not aligned with the learning policy.\n4. The paper focuses on Harvest tasks. Including results from other complex, long-horizon tasks, such as the Tech Tree task group from MineDojo, would better demonstrate the framework\u2019s effectiveness.\n\n[1]: Hafner, Danijar, et al. \"Deep hierarchical planning from pixels.\" Advances in Neural Information Processing Systems 35 (2022): 26091-26104.",
            "- - Although the high-level idea is straight-forward, the implementation is overcomplicated. \n- The method feels very ad-hoc to the Minecraft tasks studied in this paper. It doesn't come into my mind about any other relevant tasks other than Minecraft where the proposed method can be applied.",
            "- - **Clarity**\n  - Some aspects of the paper are not particularly clear. The main one is the use of the word 'jumpy' throughout the paper. The meaning of this word is assumed, but is not defined in the paper or standard usage as far as I'm aware, and is relatively unscientific, so I feel it is not the right word to use. 'Multi-step' state transitions seems more appropriate. If the authors were attempting to highlight that the number of steps can vary, then 'variable-step' transitions would be better. At the very least, 'jumpy' should be properly defined at the beginning of the paper.\n  - Similarly affordance maps may not be familiar to all readers and the exact meaning of this term can vary. For example, a short clarification early in the paper such as ''...affordance maps, that elucidate which parts of an observation may be associated with reward, ...\" would be helpful.\n  - Some other unclear aspects/minor mistakes include:\n    - L326: \"employs *an* actor-critic algorithm to learn behavior *from* the latent state sequences...\"\n    - L351: Grammar is slightly wrong and confusing, should be: \"Notably, since long-term imagination does *not* involve actions, we do not optimize the the actor when long-term imagnation is adopted.\" \n    - L354: Also worth highlighting the difference with the DreamerV3 loss. \"The loss of the actor is therefore equivalent to DreamerV3, with an additional factor of $(1-\\hat j_t)$ to ignore updates from long-term imagination:\"\n    - L361: \"on the chellenging...\"\n    - L500: Doesn't make sense. Maybe \"Our work is also related to affordance maps for robotics.\" is sufficient?\n    - \"*Learning to Move with Affordance Maps*\" [1] should likely also be compared and cited here.\n\n- **Limitations**\n\n  - This approach has important limitations that are not mentioned. In particular, the approach is limited to embodied agents navigating a 3D environment in which there are objects associated for which reward is obtained by approaching them. Therefore the approach assumes, for example:\n    - Observations are of a 3D environment\n    - Actions are navigation actions of an embodied agent\n    - Rewards are assoiated with identifying or moving towards objects\n    - A reward model is available to identify high reward regions of observations\n  - The experiments are limited to Minecraft for which these assumptions hold. This approach would likely not work as well even in Crafter [2] for example, which provides a 2D 'open-world' analogue of Minecraft, since objects do not become larger as you move towards them.\n  - The approach also relies on both the long-term and short-term models being used, given only the short-term model is able to update the actor. While the thresholding of $P_{jump}$ can partially be used to address this, this is not particularly robust, and still requires some close-up objects in initial exploration for the standard one-step world model to be used, so the approach may not work as well in very sparse environments.\n  - There is still significant value of the approach despite these limitations, and the paper is reasonably scoped, but they should be included in the limitations at the end of the paper, which are currently overly brief and narrow.\n\n  **References:**\n\n  [1] \"*Learning to Move with Affordance Maps*\", Qi et al., 2020\n\n  [2] \"*Benchmarking the Spectrum of Agent Capabilities*\", Hafner, 2021",
            "- 1. Since MineCLIP is an important tool for this work, I suggest the author include a brief introduction of what MineCLIP can do in section 3.2/3.2.1 or an appropriate position. This would help readers who are not familiar with research on MineDojo to understand this paper.\n\n2. In the abstract, \"We argue that the primary obstacle in open-world decision-making is improving the efficiency of off-policy exploration across an extensive state space.\" This seems not closely connected to the main contribution of this paper. I suggest paraphrasing it to highlight \"across a long horizon\" or something that is more related to the topic.\n\n3. Though the method sounds promising for solving MineDojo tasks, it may not be a general method for all kinds of open-world games. Such as in 2D games or fixed camera control tasks.\n\n    (a)  Before seeing the target for the first time in the training process, there won't be a reasonable goal-directed reward or jumping option, the exploration still requires extensive enumerates.\n\n    (b) The crop and resize operation (or assumption) is only useful for 3D visual navigation tasks.\n\n    (c) When the target is occluded, the world model still needs to perform step-by-step imagination.\n\nIf these are true, I suggest the authors include a sentence or so in the limitation section to clarify it."
        ]
    },
    "vf5aUZT0Fz": {
        "venue": "ICLR 2025",
        "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
        "link": "https://openreview.net/forum?id=vf5aUZT0Fz",
        "abstract": "Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The authors tackle an important problem. The usage of data mixtures during pre-training is not well understood but is an essential part of modern foundation models.\n2. While the idea of using model averaging after an inner loop of training on dedicated subsets of data is not particularly novel, it might have a big impact on pre-training, given the encouraging results.",
            "- - The setup proposed in this paper looks very satisfying, and it seems to solve several problems both in the industry and in research labs.\n- The value proposition seems clear to me.\n- The deployed methodology appears novel.\n- The literature research looks satisfactory to me, given the scope of the paper.",
            "- - The paper is well-written and easy to follow.\n\n- The idea of decoupling embedding matrix and transformer block in pre-training within the federated learning framework is novel.\n\n- The authors answer the raised research questions with meaningful and extensive experiments.\n\n- The results generally confirm that DEPT can improve the generalization and plasticity of the models."
        ],
        "weaknesses": [
            "- 1. Writing can be improved or misses important information. For example, for the experimental setup, I struggle to understand L205-311, and information on software/hardware, such as how many FLOPS  or hours training took, is missing.\n2. Some claims are overstated: M-T outperforms DEPT in 5/11 datasets in Table 2. I am not convinced that Trim and Glob perform identically (L377).\n3. An important additional baseline would be models trained on individual data sets. This would give insights into the advantages/disadvantages of model averaging.",
            "- - [addressed] The paper's form is well below the required writing standards. To address this, I'd suggest specific improvements, such as:\n  - Standardizing method names throughout the paper and tables (SPED vs SPEC, GlOB vs GLOB vs Glob, ...)\n  - Clearly defining the performance metrics used and specifying explicitly whether lower or higher values are better\n  - Adding a reference to Table 1 in the main text\n  - Improving table readability by adding summary statistics (averages...), using bold or color highlighting, or splitting into multiple tables (moving some languages to an Appendix).\n- [addressed] Not enough arguments are brought forward to justify the issue with diverging models during training. I have myself never experienced this phenomenon in similar setups. As such, it's difficult to rule out that it might be the result of bugs in the training code or poor hyperparameter choices, rather a general phenomenon. \n  - A better description of the exact training methodology and the hyperparameter search would help alleviate concerns, here.\n  - An ablation study or an explanatory paragraph isolating factors that contribute to divergence would also help.\n- [addressed] The lack of comparisons with baselines not trained by the authors is worrying. \n  - I would prefer for external baselines to be added, even if some added context is necessary to explain away unfair comparisons (could be an appendix).\n- [addressed] Without devising a clear methodology to perform inference on SPEC-type models, the paper feels a bit incomplete. \n  - I'd suggest to the authors to briefly outline a proposed inference methodology for SPEC models, and to discuss the challenges and potential approaches for inference with these models in more detail.",
            "- - The data sources are not always clear given a dataset. The proposed pipeline only works if the domains are known. Otherwise, some manual or automatic clustering has to be used to create different sets of data.\n\n- The multi-domain data is almost only in English. But for the multilingual data, the data of each language should also contain various domains. Therefore there are confounding variables. A natural question would be whether the model can generalize to the same domains across different languages.\n\n- No downstream tasks in natural language understanding or generation are evaluated on the resulting models. But such further evaluation is important."
        ]
    },
    "v593OaNePQ": {
        "venue": "ICLR 2025",
        "title": "Learning to Search from Demonstration Sequences",
        "link": "https://openreview.net/forum?id=v593OaNePQ",
        "abstract": "Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. The code is available at https://github.com/dixantmittal/differentiable-tree-search-network.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            6
        ],
        "strengths": [
            "- 1. Novel Architecture: The paper proposes a novel neural network architecture, D-TSN, which embeds the inductive bias of a best-first search algorithm, allowing for the end-to-end learning of planning components from demonstration sequences. \n\n2. Joint Learning of Planning Components: D-TSN jointly learns the encoder, value function, and world model. This is advantageous when the world model is not given but is needed to be learned from data. \n\n3. Variance Reduction Technique: Authors use an effective variance reduction technique using a telescoping sum in the REINFORCE algorithm to addresses the high variance associated with policy gradient methods. \n\n4. Comprehensive Experiments: The method is applied to a wide variety of tasks, such as reasoning problems, navigation, and game environments, supporting the claim that it is versatile and effective across domains. \n\n5. Improved Performance: The authors show that D-TSN outperforms baselines, showing its problem solving performance in challenging tasks with limited supervision, especially in jointly learned world model settings.",
            "- - Builds on TreeQN and improves a significant limitation of the prior work, i.e., having a fixed tree structure. In reality, search algorithms should attempt to filter large action spaces and focus computation on promising variations in the tree. The proposed work gets around this limitation by sampling from the action space, and using REINFORCE to differentiate through the discontinuity of sampling.\n- Strong empirical evidence that the proposed method improves on TreeQN, and having the modules trained separately.\n- Strong ablation results showing the effectiveness of the proposed method.",
            "- + This paper introduces a differentiable treee seach and planning network to alleviate the suboptimal search results that arise when using only action and observation sequences to learn to plan in many modern data-driven plannning problems. The tree search basically synergizes submodules in the form of an encoder, value, reward and transition functions from a network by inducing the bias of a best first search into the neural architecture.\n\n+ I love the motivation stated for constructing an search tree expansion policy that is stochastic in nature. But the justification for why it ensures the continuity of the loss function when the search tree is not fixed is missing. I am referring to lines 55-56.\n\n+ I love the conceptualization and the synergy of REINFORCE, mathematical mechanisms to reduce variance in the REINFORCE Loss owing to possibly biased estimates, the continuity proof (though I have questions hanging over the proof of Lemma B.3 to be fully satisfied with this poposition) .\n\n+ I love that the conclusion section meticulously summarizes the problem, contributions, and shortcomings. Kudos to the authors.",
            "- i) This work introduces the integration of the algorithmic inductive bias from the best-first search algorithm into a differentiable network, enabling automatic guidance of the tree search process through gradient backpropagation.\n\nii) This work underscores the significance of maintaining continuity of both the parameter space and the loss function which are dependent on the tree structure. To address this, the authors advocate for the adoption of a stochastic expansion policy to fulfill these prerequisites. \n\niii) The experiment results are compelling. And it is particularly noteworthy to see the success achieved in tasks involving LLM."
        ],
        "weaknesses": [
            "- 1. Limited to Deterministic Environments: The current implementation is restricted to deterministic decision-making problems with discrete action spaces.\n\n2. Computational Complexity: The computational complexity for the approach  might be high, because it consists of constructing search trees and performing REINFORCE updates. This can be a problem especially when applying for deeper trees or larger action spaces.\n\n3. Scalability: scalability is not thoroughly analyzed for longe-horizon tasks or higher dimensional state space.",
            "- - Seeing how the approach handles large action spaces remains an empirical question, since currently, there is no policy that directly outputs a distribution over actions, instead the method requires the application of the transition network and the reward network for every action, which is not scalable to settings like, say, Go.\n- The proposed method is mostly applicable to discrete action spaces with deterministic environments. Improving on this remains a future empirical question.",
            "- While I do love the mathematical contributions of the paper, I think there are essentials that are missing in the logic, organization, and flow of arguments that require a thorough review before this paper makes it into an acceptance. A principal one is the following (mentioned in the summary box but repeated here. The authors would do well to alleviate my concerns): Arguing that for a slight perturbation in network parameters, the implementation of the loss function in equation (2) could generate a tree structure that causes the loss function to become noisy, the authors equated this to a lack of continuity in the loss function space. I think they mistook stochasticity in gradient propagation with discontinuity. The whole premise of the contribution of the paper is based on this assumption that is barely proven to be true or false before the authors dived into a host of mathematical analysis that resulted in the loss function on Line 272 (please number all your equations to allow for easy referencing in the future). \n\n+ The claim in the last paragraph of Theorem 3.1 that slight changes in the network parameters could cause discontinuity in the tree structure seems anecdotal and not backed up by a solid proof. I would love to see a concrete reasoning (analytical proof or abundant empirical  proofs) behind this claim that warrants section 3.5 and Appendix C.\n\n+ Grammatical errors fly out of the page hither and yon throughout the paper; the uthors would do well to carefully organize their arguments, present their logic convincingly throughout the paper, and punctuate and label every equation appropriately!\n\n+ The logic in the paper could use a more thoughful presentation. Here is an example critique:\n     - In the \"introduction\", it is stated in the first paragraph that constructing a search tree from expert data is infeasible due to lack of practicality or scarcity. The authors make an assumption that a search tree is a principal prerequisite for information retrieval (IR) without any justification as to why it may be better than alternative IR methods. Then in the second paragraph, they mentioned how search and planning could be better executed in the presence of a simulator or world mode. While I find this premise alluring, I find it disingenious that the authors claimed that the search could be incomplete because the search process may visit regions unexplored during training. I think the reasoning here is incomplete and should be revisited by the authors.",
            "- i) Previous methods have introduced diverse differential network architectures to integrate different search algorithms into networks, as mentioned in the related works. It is unsurprising that integrating the best-first algorithm into the network has also yielded success. Thus, it would be beneficial to compare the architectural variances between this method and previous methodologies.\n\n\nii) This work trains the overall network using an offline dataset. However, as extensively deliberated in preceding offline RL studies, this paradigm may get stuck when facing out-of-distribution states or actions. Thus, a comparative analysis between online training and offline training for the newly proposed network architecture could provide valuable insights."
        ]
    },
    "wPMRwmytZe": {
        "venue": "ICLR 2025",
        "title": "Progressive distillation induces an implicit curriculum",
        "link": "https://openreview.net/forum?id=wPMRwmytZe",
        "abstract": "Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several \u201cintermediate\u201d teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the student\u2019s learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            6,
            8,
            8,
            8
        ],
        "strengths": [
            "- Originality - The paper presents a novel perspective on progressive distillation by identifying and formalizing the concept of an \"implicit curriculum.\" While prior work has explored progressive distillation empirically, this study delves deeper into the underlying mechanisms and provides theoretical grounding for its efficacy. The connection between intermediate teacher checkpoints and an implicit curriculum is a fresh insight that contributes to a better understanding of knowledge distillation.  \u00a0 \n\nQuality - The research is technically sound, employing rigorous methodology and comprehensive experiments. The authors combine theoretical analysis with empirical validation, drawing on diverse tasks like sparse parity, PCFGs, and natural language modeling. The use of multiple progress measures to quantify the implicit curriculum further strengthens the quality of their analysis. The study is well-designed, and the results are convincing.  \u00a0 \n\nClarity -The paper is clearly written and well-organized. The authors present their ideas in a logical progression, starting with a clear motivation and gradually building up their analysis. The concepts are well-explained, and the figures effectively illustrate key findings. The paper is accessible to readers with a background in knowledge distillation and deep learning.\n\n\nSignificance - The findings have significant implications for the field of knowledge distillation. By elucidating the role of an implicit curriculum in progressive distillation, the study provides valuable insights for designing more effective distillation strategies. The theoretical results on sample complexity offer a deeper understanding of the optimization benefits of progressive distillation. The practical implications of this work are substantial, particularly for training efficient and capable models in resource-constrained settings.",
            "- 1. The paper is well written, and despite the complexity of the narrative, it is generally easy to follow, and I enjoyed the reading.\n2. Though only applied to a simple use case, the mathematical analysis does provide useful insight about sample efficiency of progressive distillation.\n3. The metrics selected in the analysis such as $\\mathcal{M}_{robust}$ is quite useful to understand the feature learning aspect of the method.\n4. The authors run experiments in various settings including three datasets, MLPs, BERT and GPT-2 (in the appendix) to show the gains can be generalized.",
            "- * The paper is well-written and easy to read.\n* The paper includes results on tasks across different complexity levels - going from a toy setting of sparse parity to PCFGs and then to a non-synthetic task of natural language modeling.\n* Authors also run experiments across multiple model architectures, name MLPs and transformers of different sizes.\n* The induced curriculum is discussed from a human interpretability point of view (i.e. showing the correlation between degree 1 monomials and the logits of the intermediate teacher checkpoint in the sparse parity task, and drawing similar analogy in the PCFG task).\n* The paper (more specifically; the appendix) includes further extensive experimentation on the settings discussed in the main paper.",
            "- I liked the spirit of this paper, which I found complete, well-written, carefully designed and executed. Overall, I enjoyed the following strengths:\n\n- **Experimental completeness:** The paper is generous in providing extensive empirical evidence across synthetic tasks (sparse parity, PCFGs) and real-world datasets (Wikipedia, Books).\n\n- **Theoretical Analysis:** Offers mathematical proofs demonstrating sample complexity benefits in specific cases like sparse parity learning. \n\n- **Memorable Observations:** Beyond the idea that checkpointed learning leads to faster optimization, it identifies phase transition in the teacher's training where intermediate checkpoints can be most beneficial to the student. These correspond to acquring new skills of increasing complexity.\n\n- **Impact and Practical and actionable implications**",
            "- Strengths\n* **Intuitive Motivation and Theoretical Foundation:** The paper is well-motivated, addressing a significant challenge in the effective distillation from large to small models. The theoretical underpinnings are well grounded, with rigorous proofs using the sparse parity example demonstrating why progressive distillation is effective.\n* **Empirical Validation Across Diverse Tasks:** The authors conduct comprehensive experiments on both synthetic tasks (sparse parity and PCFGs) and realistic settings (training BERT on Wikipedia and Books datasets). This breadth of evaluation underscores the generalizability of their findings."
        ],
        "weaknesses": [
            "- Some possible improvements I can see are the following :-\n\na) more investigation of impact of temeprature on knowledge distillation. this seems to be a bit missing in the main sections of the paper\nb) analysis of how implicit curriculum learning varies across model layers, across datasets, trainign objecives etc\n\nc) exploring more tasks and architectures\n\nd ) explore interaction of optimization algorithms, batch size etc with curriculum learning",
            "- Despite the strength, I think the paper can be improved.\n1. I understand the necessity to use a toy use case (sparse parity) to show rigorous mathematical analysis, but the following experiments can be more practical in order to provide stronger empirical evidence of the effectiveness of progressive distillation.\n- Instead of masked token prediction, can run experiments in challenging NLP tasks such as QA, summarization and long-form generation.\n- Can also experiment with more recent LLMs - GPT-2 in the appendix is a good start, but it is still a very outdated model.\n2. Some writing (particularly Theorem 3.2) can be made even clearer. For example, L291-292, I didn't quite follow where higher degree odd polynomials come from. L296-297 \"This gap allows .. to grow quickly with only ... samples.\" This statement isn't clear if I don't read the entire proof in the appendix. Please consider writing it in a more intuitive way in the main text.\n3. Probably unfair as this is more of an analysis paper, but the overall contribution appears to be limited considering the scope of its experiments.",
            "- * There is a typo in Definition 4.3: I believe it should be \"boundary of span(n^{(i)})\" instead of boundary of n^{(i)}\n* Discussion about how the relative sizes of teacher and student models were decided is missing. It would be interesting to see a study of how the performance is affected w.r.t the size of the student models\n* Empirical analysis on tasks in the vision domain and with other model architectures such as CNNs and recurrent networks would strengthen the paper significantly.",
            "- - **Alternative intepretations, e.g. winning subnetworks and the Lottery Ticket Hypothesis:**\n\nThe first thing I thought about, as a possible intepretation of the empirical (and theoretical) findings of the paper is the lottery ticket hypothesis (LTH), which could alternatively explain the benefits observed in progressive distillation. The hypothesis posits that within a large neural network, there exist smaller subnetworks (winning tickets) that can learn the tasks efficiently. Searching for these subnetworks takes long, but once they are identified, training only on them makes the learning faster. \n\nSo while the paper frames the intermediate checkpoints as providing an implicit curriculum of increasing task complexity, this could be reinterpreted as the teacher progressively revealing parts of the winning subnetwork, narrowing the student's search space at each step. Hence a crucial question is: could the novelty in using intermediate checkpoints be just an operationalization of the lottery ticket hypothesis in a distillation context (i.e. Guided Search) rather than a fundamentally new concept (implicit curriculum)? \nThere, the student model is being guided to explore progressively smaller regions of the solution space, as per LTH. These correspond to learning features of increasing complexity as the paper points out, but that's because that's how networks progress in their learning of the best parameters to solve a problem. By emphasizing the curriculum aspect, the paper might divert the \"reader\" from other factors contributing to accelerated learning, such as the inherent properties of the optimization landscape or the effects of network pruning (implicit or explicit).\n\nAnd both interpretations would be aligned with the empirical and theoretical findings. \n\nFirst, the fact that not all checkpoints provide good performance could be  due to the model learning more and more complex skills as it discovers the winning tickets, and not because the tasks are more and more complex (curriculum). Matter of fact, the training data is not going from simple tasks to complex ones, what goes from simple to complex is what the model has learned, not what is was trained on. Said differently, the discovery of progressively complex features could be the result/consequence of a guided search to efficient parameter configurations.  \n\nSecond, looking at the sample complexity improvement that the authors prove:\n\"the total sample complexity needed for the student to reach \u03f5-loss using progressive distillation with 2 checkpoints is \u00d5(2^k d^2 \u03f5^\u22122 + k^3). However, one-shot distillation requires at least \u03a9(d^k\u22121, \u03f5^\u22122) samples\"\ncould perhaps be reinterpreted as: The student benefits from getting direct signal about which features are important (winning tickets), rather than having to discover them from scratch like in one-shot distillation.\nSame goes for the theorem about intermediate checkpoints having stronger correlations with degree-1 monomials: could be read as a curriculum of increasing task complexity, or about how the teacher naturally learns (simple correlations first) and why intermediate checkpoints help (they provide clearer signal about important features).\n\nSeen this way, calling this an \"implicit curriculum\" might be misleading because the task complexity is constant, and what is changing is the model's internal learning progression. \n\n-  **Other minors**\n\nThe specific temperature values, as well as the choice of dramatically different temperatures (10^-4 vs 10^-20) based on vocabulary, could benefit from more rigorous exploration. Ok, the authors still acknowledge this as a limitation they defer to future work.",
            "- **Weaknesses**\n\n- **Scalability to Larger Models and Diverse Architectures**: While the experiments include models like BERT, the paper does not thoroughly investigate how progressive distillation scales with increasingly large models (in the main paper).\n\n- **Effectiveness in GPT Models**: In Appendix E, the paper examines autoregressive training with GPT-2. Although progressive distillation accelerates learning speed, the performance plateaus around 8,000 training steps. It remains unclear why this convergence occurs and what the outcomes would be if training continued beyond 8,000 steps (e.g., up to 20,000 steps).\n\n- **Potential for Degenerate Features**: While progressive distillation leverages an implicit curriculum to identify key patterns, there is a concern that this curriculum might lead to degenerate features that could hinder long-term generalization. This issue could be further investigated by extending the training duration in GPT-2 to ensure that no negative consequences arise from prolonged training."
        ]
    },
    "vaEPihQsAA": {
        "venue": "ICLR 2025",
        "title": "CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation",
        "link": "https://openreview.net/forum?id=vaEPihQsAA",
        "abstract": "Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. While breakthroughs have been made in driving human animation through various modalities for portraits, most of current solutions for human body animation still focus on video-driven methods, leaving audio-driven taking body generation relatively underexplored. In this paper, we introduce CyberHost, a one-stage audio-driven talking body generation framework that addresses common synthesis degradations in half-body animation, including hand integrity, identity consistency, and natural motion.\nCyberHost's key designs are twofold. Firstly, the Region Attention Module (RAM) maintains a set of learnable, implicit, identity-agnostic latent features and combines them with identity-specific local visual features to enhance the synthesis of critical local regions. Secondly, the Human-Prior-Guided Conditions introduce more human structural priors into the model, reducing uncertainty in generated motion patterns and thereby improving the stability of the generated videos.\nTo our knowledge, CyberHost is the first one-stage audio-driven human diffusion model capable of zero-shot video generation for the human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects. CyberHost can also be extended to video-driven and audio-video hybrid-driven scenarios, achieving similarly satisfactory results.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            6,
            6,
            10,
            8
        ],
        "strengths": [
            "- 1. CyberHost introduces the first one-stage approach for audio-driven talking body generation, avoiding the complexity and inefficiencies of multi-stage systems that rely on intermediate representations.\n\n2. The proposed Region Attention Module component effectively enhances critical areas such as hands and faces, improving the quality of local details and maintaining identity consistency.\n\n3. By integrating motion constraints and structural priors via human-prior-guided conditions, the model mitigates the challenge of motion uncertainty, resulting in more stable and natural body animations.\n\n4. The qualitative results in the supplementary materials are impressive. Also, compared to the previous state-of-the-art audio-driven half-body generation method, VLOGGER, CyberHost produces visibly superior results.\n\n5. The paper is well-written and clearly presents its objectives, methodology, and findings.",
            "- 1. Cyberhost can generate cospeech videos with very natural motions and clear hand/body structures.\n2. It employs various control training methods, including codebook , hand clarity, pose-aligned reference, and also key point supervision. Experimental results indicate that these methods effectively enhance the clarity of hands and the correctness of body structures in the generated objects.",
            "- 1.\tThe proposed method demonstrates a certain degree of generalization, allowing it to adapt to multiple tasks, such as video-driven generation or multimodal-driven generation, while also enabling open-set generation.\n2.\tBased on the experimental results, the proposed method surpasses both the baseline and state-of-the-art methods across multiple metrics.",
            "- 1. This paper addresses two important and challenging problems in the body animation field, and the proposed approaches are novel and effective.\n2. The proposed method supports multi-modalities driving\n3. The driving results show really good rendering quality and natural motion fidelity.\n4. The paper is well-organized and well-written.",
            "- The authors aimed to tackle two significant challenges in audio-driven body generation and achieved progress in:\n1. Improving the synthesis quality of critical regions (hands and face)\n2. Reducing motion uncertainty caused by weak correlations.\n\nSpecifically, this paper successfully addresses the challenge of generating high-quality hand and facial features using proposed modules including RAM.\n\nIn addition, comprehensive experiments were conducted. Comparisons were made to evaluate not only audio-to-body generation methods but also video-to-video and audio-to-face methods, demonstrating its expandability."
        ],
        "weaknesses": [
            "- 1. Detailed Failure Analysis: The paper would benefit from a discussion of failure cases or limitations where CyberHost struggles, such as specific types of input audio or complex poses. This would provide a more balanced view of the model's capabilities.\n\n2. Scalability to Full-Body Generation: The paper focuses on half-body animation, but it does not discuss how well the architecture scales to full-body animation or if there are significant challenges in extending the framework.\n\n3. Lack of User Study for Subjective Evaluation: The paper does not include user studies or subjective evaluations to gather feedback on the perceived naturalness and quality of the generated videos. Such evaluations would provide valuable insights into how well the model meets human expectations for lifelike animation.",
            "- 1. The generated videos exhibit insufficient facial clarity and detail, resulting in a noticeable discrepancy between the generated object and the characteristic features of the person in the reference image.\n2. Unlike the codebook in VQ-VAE, which is specifically used for the reconstruction of designated features, the codebook in Cyberhost lacks supervisory signals during training, making it unable to ensure that the codebook effectively guides the model to generate correct hand shapes and facial features.\n3. It would be good to visualize the ablation study for the two main contribution components: \u201cMotion codebook\u201d and \"ID Descriptor\".",
            "- 1.\tAlthough the proposed method achieves promising results overall, it introduces many components. As shown in Table 1, there are nine components, but the experiments lack in-depth analysis of these. For example, the impact of the size of the latent bank in RAM. The results of using alternatives in the Region Attention Module (RAM), such as not using spatial latents, were not examined. Additionally, the effect of not decoupling the latent bank into spatial and temporal latents\u2014instead using a single 3D latent bank\u2014was not investigated. Furthermore, it remains unclear what specific aspects of video information are captured by the spatial and temporal latents, lacking justification and explanation.\n3.\tThe use of the Laplacian operator to compute the hand clarity score requires justification, as the rationale behind this choice is not explicitly discussed. Additionally, the influence of the hand clarity score on the experimental results is not demonstrated in the experiments. It is essential to clarify whether this score is necessary and how it contributes to the overall performance of the proposed method.\n4.\tThe method [1] is also a one-stage audio-driven half-body video generation model, but this paper does not discuss or compare it.\n\n5.\tThe dataset used in [2] was not employed in experiments for comparison with previous methods. Additionally, the beat consistency metric [3] was not reported in the experiments.\n\n6.\tSome typos, such as in line 313 feference -> reference\n\nreference:\n\n[1] Liu X, Wu Q, Zhou H, Du Y, Wu W, Lin D, Liu Z. Audio-driven co-speech gesture video generation.\n\n[2] Qian S, Tu Z, Zhi Y, Liu W, Gao S. Speech drives templates: Co-speech gesture synthesis with learned templates.\n\n[3] Li R, Yang S, Ross DA, Kanazawa A. Ai choreographer: Music conditioned 3d dance generation with aist++",
            "- 1. Some details are not provided:  \na) Which specific layers of Wav2Vec features were used? (line 191)  \nb) How to constrain the basis vectors of the latents bank to be orthogonal? (line 242)  \nc) There is a lack of loss description for regional mask predictor. (line 260)  \n2. Authors claim that the hand clarity score can enhance the model's robustness to blurry hands during training and enable control over the clarity of hand images during inference. They conducted ablations on hand clarity, but they did not demonstrate to what extent this score can control hand clarity during inference. I would like to know this result.\n3. The explanation of how the proposed 'Pose-aligned Reference Feature' works has not convinced me for two reasons:  \na) Although the ablation on pose-aligned ref shows a lower HKC score compared with Cyberhost, this method was proposed to solve the case of challenging initial poses, and the authors did not demonstrate its effectiveness in that scenario.  \nb) The authors claimed that the skeleton map provides topological structure information, which improves the quality of hand generation. However, they did not explain how this structural information actually contributes to generating higher-quality hand images.  \n4. Some spelling mistakes: 'feference' should be corrected to 'reference' in line 313.",
            "- While most parts are understandable, some details and explanations are missing. The questions regarding the missing information are listed under \"Questions.\" Additionally, as the methods utilize many well-known architectures and frameworks while introducing several modules\u2014including the Latent Bank, Pose Encoder, Heatmap Estimator, and Mask Predictor\u2014some missing information limits the paper\u2019s reproducibility and clarity of the paper. If the concerns or questions listed on \"Questions\" are addressed, this paper would be worthy of a higher rating."
        ]
    },
    "z5uVAKwmjf": {
        "venue": "ICLR 2025",
        "title": "AFlow: Automating Agentic Workflow Generation",
        "link": "https://openreview.net/forum?id=z5uVAKwmjf",
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFLOW, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFLOW's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            6,
            8
        ],
        "strengths": [
            "- Overall, this paper is clear, well-motivated and provides a new framework for automatic workflow optimization, which has significant potential impact on agent design and workflow optimization for the broader machine learning community. It proposes a novel, original approach to model the workflow as a sequence of LLM-invoking nodes in a graph structure, with prompts, operators, and code-represented edges in the search space. By leveraging MCTS, the paper reaches SOTA performance on major workflow benchmarks and shows the potential of enabling smaller, cheaper models reaching similar performances as large models. The documentations of the experiment setup, code representation, case studies and results are clear and technically sound, and this paper can provide great inspiration for other researchers in the domain of agent workflow optmization.",
            "- Novel Approach: AFLOW\u2019s integration of MCTS with code-represented workflows introduces a new direction in automating LLM workflows. This reduces the reliance on manual design and allows efficient workflow discovery and optimization.\n\nComprehensive Problem Formulation: The paper formalizes workflow optimization with a general mathematical framework, effectively unifying prior approaches and broadening the potential for future applications.\n\nDetailed Critique of Prior Work: The authors present an insightful analysis of existing methods, identifying the limitations of prior frameworks like ADAS in handling information accumulation and search efficiency. This sets a strong foundation for AFLOW\u2019s proposed contributions.\n\nEmpirical Validation: AFLOW is rigorously evaluated across diverse benchmark tasks, with quantitative comparisons to multiple baselines. Ablation studies further illustrate the impact of different operators, and cost analysis demonstrates AFLOW\u2019s efficiency.",
            "- The methodology is interesting, and the experimental setup is convincing in showing that AFLow enables smaller models to achieve superior performance to larger models. This lifts the cost/accuracy Pareto front. Given these results I am appreciative of this work.",
            "- 1. Originality\n\nThis paper pioneered a new definition of workflow optimization. By redefining the workflow process, it transformed the problem of automatically generating workflows into a Monte Carlo tree search problem in the search space. It also made up for the shortcomings of the previous problem, transformed the previous work into special cases, and provided a unified framework for subsequent researchers.\n\n2. Quality\n\nThe AFlow framework proposed in the paper has shown strong performance in experimental evaluation, outperforming existing methods by an average of 5.7% on benchmark datasets. Additionally, AFLOW can achieve better performance than larger models using smaller LLM models, which is of great importance for practical applications.\n\n3. Clarity\n\nThe paper provides detailed descriptions and explanations of the key components of the AFLOW framework, such as the MCTS algorithm, node selection strategy, and LLM-driven node expansion, making the entire implementation process clear and understandable. At the same time, the paper also describes the experimental setup and experimental process in great detail, and the explanation of the experimental results is also clear. The entire paper is logically rigorous and well-organized.\n\n4. Significance\n\nThe AFlow proposed in this paper not only has a significant improvement in performance, but also proposes a unified framework in the field of automatic generation and optimization of workflows. The new definition better explains this task and provides a new framework and optimization direction for future research."
        ],
        "weaknesses": [
            "- The paper could benefit from discussions with regards to the following points:\n1. To reduce the search space, the paper focuses on custom prompts, operators and code-represented edges by fixing parameters such as model choice, temperature and output format - which is a sound choice. Could there be more discussion on the potential effect of these parameters on model performance?\n2. The authors mention some of the parameters used in MCTS in the appendix (e.g. $\\lambda = 0.4$ used to balance exploration vs. exploitation), but not in the main paper. It would be helpful to include key parameter values and brief discussions about the choices. \n3. Similarly, a quick discussion around why models are chosen for specific parts (executor vs. optimizer) would be helpful for context as well.",
            "- Limited Scope and Generalizability: The paper primarily demonstrates AFLOW on benchmark tasks with clear success metrics, which raises questions about its applicability to more open-ended tasks, such as document generation or creative exploration. There is limited discussion on how the standardized prompts used in AFLOW would generalize to tasks without clear success criteria. The current prompts seem tailored to test-taking scenarios and may lack the flexibility required for tasks that demand creative or exploratory outputs. A clearer strategy for adapting or evolving these prompts to support open-ended workflows would strengthen the paper\u2019s position on generalizability.\n\nImplementation Details for Reproducibility: While the automated workflow design is a strength, the paper lacks sufficient details on prompt handling, tool calling, and how workflows evolve with execution feedback. Specific examples of workflow changes during optimization and consistency maintenance across components would improve reproducibility.\n\nLimited Comparative Analysis: Although the paper provides a robust critique of ADAS, the treatment of DSPy is brief, and Tree of Thoughts is only briefly mentioned despite its relevance. A more detailed comparison with these approaches would clarify AFLOW\u2019s unique contributions and limitations.\n\nTheoretical Analysis: The paper lacks a theoretical analysis on the convergence of the MCTS optimization process, completeness of the search space, and performance bounds, which are important for understanding the robustness and scalability of the method.",
            "- While the methodology and results are nice, I do however believe that the presentation of the paper requires improvement. The description of the AFLow methodology lacks precision and can at times even be called handwavy (details below), which makes the paper hard to read. In case authors are able to address those issues I may be willing to increase my score. Some concrete examples (more examples in the questions):\n- There is a tree structure involved in the MCTS search process, but there is also a graph and nodes involved in the search space of workflows. This is a potential point of confusion and many parts of the paper do not make explicit which of those two graphs they are talking about.\n- The exact definition of the search space is not made clear anywhere in the paper. It is clear that a node is a tuple consisting of a node, prompt, temperature, and output format. What is then less clear is what it means to have an edge between two nodes. Does this mean that we first made the first LLM invocation (of the first node), and then sequentially following that, we make the second LLM invocation (of the second node)? And what does it then mean if our node has two outgoing edges (is this a decision point where we invoke one or the other, or do we execute both next nodes in parallel)? What is also less clear is how the output of the first LLM invocation is used in the LLM calls for later nodes (if at all). Is there a root node? These all point to a broader issue of a lack of precision in the specification of AFlow.",
            "- This paper has achieved good results in both method and experiment. In terms of method, the paper innovatively formulated the automatic workflow optimization problem, establishing a foundational structure for future research. In terms of experiment, it not only achieved good results, but also conducted a lot of relevant analysis. However, this paper has some weaknesses in the following aspects:\n\n1. The experimental part of this paper lacks the cost analysis of the early AFlow search stage. The cost analysis of different methods later in the paper shows the effectiveness and low consumption of the workflow found by AFlow, but the early MCTS search is a huge process, and the execution of nodes will also consume certain resources. This part does not provide experimental explanation.  If the cost of exploring and finding the optimal workflow is huge, then the discussion on cost should include this resource consumption.\n2. This paper argues that different language models require different workflows to achieve their optimal performance. However, there is a lack of sufficient experiments to support this assertion, as the paper only mentions that the workflow identified using DeepSeek-V2.5 performs notably weaker on GPT-4o-mini compared to the workflow found using GPT-4o-mini itself.  At least one more set of comparative experiments should be added, that is, generate a workflow through GPT-4o-mini and then use DeepSeek-V2.5 and GPT-4o-mini respectively to see the experimental results. It would be best if more comparative experiments of other types of models could be added, such as adding another LLama series model, and comparing the three models. This is an interesting assertion, but more sufficient experiments are needed to verify it."
        ]
    },
    "yfW1x7uBS5": {
        "venue": "ICLR 2025",
        "title": "Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI",
        "link": "https://openreview.net/forum?id=yfW1x7uBS5",
        "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.\nIn response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that **all existing protections can be easily bypassed**, leaving artists vulnerable to style mimicry.  We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6,
            8
        ],
        "strengths": [
            "- - The paper is well-written and easy to follow.\n- It works on an important problem and provides critical insights: all protection methods today cannot protect artworks from diffusion-based mimicry. Though works like Glaze have been widely accepted by artists, they actually perform bad.\n- The proposed robust mimicry methods are simple but effective.\n- The authors did extensive experiments to support the claims.",
            "- 1.  While it was anticipated that prominent style protection techniques like Glaze might have weaknesses, the extent of their fragility is striking. The paper demonstrates that these methods fail even against rudimentary attacks, with Glaze unable to withstand a mere change in the fine-tuning script. This revelation underscores a concerning lack of \u201csecurity mindset\u201d within the style protection research community, which is particularly alarming given the recognition Glaze has received, including multiple awards from USENIX Security.\n\n2. The MTurk study is well constructed and the authors have taken pains to ensure that their work adheres to commonly accepted ethical standards.",
            "- 1. The writing is clear and easy to follow.\n2. The topic is interesting and important.",
            "- + S1: The motivation is clear and compelling, effectively serving the paper's purpose: to caution researchers about the limitations of using adversarial perturbations for protection against art mimicry.\n\n+ S2: The paper is well-organized and easy to follow. The experiments conducted are solid.\n\n+ S3: The conclusions drawn from this work are potentially significant for the community and could reshape the landscape of this research field. Researchers are encouraged to reconsider the current paradigms of artwork / copyright data protection in light of their practical effectiveness."
        ],
        "weaknesses": [
            "- Clarification:\n- The title of this paper is 'ADVERSARIAL PERTURBATIONS CANNOT RELIABLY PROTECT ARTISTS FROM GENERATIVE AI', while style-based mimicry is one part of diffusion-based mimicry, there are more basic mimicry e.g. inpainting, style-transfer by diffusion model, which are also tested in previous papers of protection e.g. Mist. Fine-tuning a diffusion model seems to have a more complicated mechanism compared with image-to-image applications of diffusion models. \n- I wonder does the proposed method also work for inpainting/image-to-image SDEdit, if it works, the proposed method becomes more general.\n\nMethods:\n- I noticed that the perturbation used in this paper is quite small, if the noise is scaled up, will the purification be worse\n- While Glaze and Mist are popular, there are many other protection methods that can be studied to get a safer conclusion. e.g. MetaCloak [3] and SDS [4].\n\nRelated Papers:\n[1, 2] are highly related to this paper, [1] also find that the current attacks are vulnerable to purifications, [2] proposed that latent diffusion models can be easily purified by pixel-space diffusion models. \n\n[1] Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?\n\n[2] Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think.\n\n[3] MetaCloak: Preventing Unauthorized Subject-driven Text-to-image Diffusion-based Synthesis via Meta-learning\n\n[4] Toward effective protection against diffusion-based mimicry through score distillation",
            "- 1. The work has limited novelty since it was predictable that style protection would be vulnerable to DiffPure-like purification methods. It would be good if the purification methods evaluated in the paper could be packaged into a standard baseline, say on the lines of AutoAttack.\n\n2. In my opinion, the paper overstates the general case against style protection techniques based on adversarial perturbation. The presented argument makes an assumption that artists have the choice to release their artworks on the internet, and they may choose to withhold their artworks if they believe that AI models may be trained on their artworks. However, digital artists are extremely dependent on the internet to grow their customer pool and advertise their works, so this is likely not a feasible option. The appropriate counterfactual to artists using Glaze-like methods would be not using any protections at all. Also, since these methods seem to be improving against simple attacks at least, it may be enough to use them as a deterrent rather than as fool-rpoof security.\n\n\nI struggled to decide whether to rate this paper as borderline accept or clear accept due to the stated weaknesses. Ultimately, I decided to rate this paper as clear accept as the argument may indeed be persuasive to a small minority of artists who may decide to not publish any of their works if there is no secure style protection mechanism.",
            "- The main weakness is the unreliable evaluation and whether the experimental results reflect real-world cases.\n\n1. The study heavily relies on MTurk workers, who may not be suitable for this task. They might lack the expertise to classify artistic styles or assess whether artwork quality has been degraded. Artists themselves, such as those mentioned in [1], should be included in the main evaluation. Additionally, these workers may not represent potential consumers of artists' works, contrary to what the authors propose (L 338). A more thorough study should report the workers' relevance to the art domain (e.g., how often they purchase art or visit museums).\n2. The effectiveness of the fine-tuning procedure is unclear. As shown in Fig. 3 (especially the bottom row) and Fig. 5, the generated images, even without protection, do not closely resemble the style of the training images. In contrast, previous work like Glaze [1] uses a fine-tuning setting with much stronger resemblance between generated images and training data (see Fig. 2 in [1]). The focus should be on whether adversarial perturbations effectively defend against mimicry once it has been successfully achieved. Even in Appendix K, where the limitations of using MTurk for evaluation are acknowledged, fewer than 50% of cases were rated as better or equal to \u201csuccessful\u201d mimicry.\n    \n    In an extreme case, if fine-tuning involved only one step, adversarial perturbations would likely fail to defend against mimicry. The core issue seems to be underestimating how closely the model need to fit training images, including adversarial perturbations,  to learn the style, which may result in an underestimation of their impact.\n    \n3. Even if the above weaknesses are overlooked, the paper provides no new insights on solving the problem of mimicry. Both pre-processing and adversarial perturbations degrade image quality, raising the question of whether removing adversarial perturbations is worth the cost, given that pre-processing might degrade the image quality in ways that hinder style recognition.\n4. The paper overlooks broader ethical implications, such as the removal of adversarial perturbations used as a watermark. Similar to traditional watermarks (such as a simple icon on the corner) that can be removed but are illegal to do so in many countries, stronger watermarks complicate removal and leave evidence. A discussion on these ethical issues would contribute more meaningfully to the community.\n5. The proposed methods lack novelty, largely offering improved hyper-parameter tuning of existing approaches.\n\n[1] Shan S, Cryan J, Wenger E, et al. Glaze: Protecting artists from style mimicry by {Text-to-Image} models[C]//32nd USENIX Security Symposium (USENIX Security 23). 2023: 2187-2204.",
            "- + Typos\n    1. Line 219, fare -> fail?\n    2. Figure 11, 13, 14, 15 seem to have the wrong label - now they are all labelled as \"gaussian noising\""
        ]
    },
    "yVeNBxwL5W": {
        "venue": "ICLR 2025",
        "title": "MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
        "link": "https://openreview.net/forum?id=yVeNBxwL5W",
        "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MaRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8,
            8
        ],
        "strengths": [
            "- 1). The paper is very well written, and easy to understand especially very coherent even with the math part.\n\n2). The core idea of the paper is very interesting. Especially how the authors have formulated  the core idea.\n\n3). The shown results are very impressive and competitive.\n\n4). Even though the concept behind the \"sampling trajectory\" part is simple, the visualizations obtained through it really helps to understand the core concept and contrast with existing methods. \n\n5). The MRS is plug and play.",
            "- - Addresses a gap in fast sampling for mean-reverting diffusion\n- Provides two alternatives focusing on noise/data prediction\n- Relevant ablation studies for some parameter choices\n- Evaluates performance in image restoration tasks",
            "- - The authors' proposed method demonstrates robust performance across a variety of experiments, consistently showing strong results in both quality and efficiency. Notably, it outperforms posterior-based and Euler-based samplers when using a high number of sampling steps. The performance advantage becomes even more pronounced as the number of steps is reduced, underscoring the method's effectiveness with low-step counts.\n\n- It is particularly encouraging to see that the proposed method performs best across almost all of the 16 tested image perturbations. This breadth of performance suggests that the approach is not only effective but also adaptable to different types of image perturbations.\n\n- The paper is generally well-organized and written. Key concepts and technical choices are clearly explained.\n\n- Overall, this is a well-rounded paper that offers convincing results and provides a method that should be straightforward to integrate with existing frameworks. I also appreciate the authors\u2019 decision to release the code for easy reproducibility.",
            "- The paper introduces a novel approach to sampling in Mean Reverting  Diffusion models, which itself is a relatively new paradigm compared to more conventional diffusion processes like Variance Preserving and Variance Exploding SDEs. The key originality lies in the way the authors combine semi-analytical solutions derived from ODE and SDE solvers with MR Diffusion, enabling faster sampling. This combination is innovative as it offers a fresh perspective on accelerating diffusion models by altering the underlying stochastic differential equation structure rather than focusing solely on the score function as in prior work.\n\nAdditionally, the Mean Reverting SDE framework offers a natural integration of image conditions into the generation process, making it more applicable to tasks that require controllable generation, such as image restoration and inpainting. This is a novel contribution compared to prior work that has typically used Diffusion Schr\u00f6dinger Bridge or Optimal Transport methods for similar purposes. The MR Diffusion model\u2019s ability to address multiple tasks beyond denoising is a creative and valuable extension of existing frameworks.\n\nThe technical depth of the paper is commendable. The authors offer a rigorous derivation of the semi-analytical solution and provide substantial theoretical grounding for their approach. The use of probability flow ODEs (PF-ODEs) and reverse-time SDEs in the MR Diffusion context is clearly explained and well-justified. The semi-analytical solution, which combines an analytical function and a neural network-based integral, reduces computational complexity without compromising sampling quality.\n\nThe experiments are comprehensive and cover ten different image restoration tasks, providing strong empirical support for the proposed method. The use of multiple performance metrics (e.g., FID, LPIPS, PSNR, SSIM) demonstrates that the authors took a thorough approach to evaluating both the quality and speed of the generated samples. The speedups of 10-20x, without a significant drop in sample quality, highlight the robustness and practical value of the proposed method.\n\nThe paper is well-structured, with clear sections on the methodology, theoretical contributions, and experimental validation. The technical content, while complex, is made accessible through the use of visual aids (e.g., Figure 1 comparing qualitative sampling results, and charts showing performance metrics) and clear explanations of the mathematical formulations. The distinction between noise prediction and data prediction models, and the impact of these choices on sampling quality and stability, is clearly delineated and contributes to a deeper understanding of the technique.\n\nThe authors also provide appendices with detailed proofs and further experimental results, ensuring that the methodology is reproducible and the claims are verifiable. Overall, the clarity in presenting a technically complex subject is a strong point of the paper.\n\nThe proposed MR Sampler addresses a significant challenge in the field of diffusion models: accelerating the sampling process without sacrificing quality. The speedups achieved in this work, which range from 10x to 20x across multiple tasks, are substantial and have clear practical implications, particularly in real-time applications such as image restoration. This makes the method highly relevant for use cases that demand controllable and fast generation, such as medical imaging, video processing, and computational photography.\n\nThe method\u2019s plug-and-play nature is another strength. It is adaptable to a variety of existing diffusion models and does not require retraining, making it easy to integrate into different applications. The broad applicability to various image restoration tasks (e.g., dehazing, inpainting, motion-blur reduction) enhances the significance of the work."
        ],
        "weaknesses": [
            "- I believe this is a great paper with solid reasoning and a well-thought-out approach. However, I have some questions regarding the numerical implementation and the comparison methods used.\n\n1). Can the authors elaborate on why only the backward difference method is used? Are there specific benefits to this approach over others? \n\n2). The proposed MR Sampler is only compared with posterior sampling, and Euler-Maruyama discretization. What are the current SOTA methods? How does the proposed approach compare with these, beyond posterior sampling and Euler-Maruyama?\n\n3). I would like to understand why the authors say \"frequently fall outside\" in this part (line304)\n \"Although the standard deviation of this Gaussian noise is set to 1, the values of samples can frequently fall outside the range of [-1,1]\"\n\n4). Could the authors specify the neural network architecture they used? \n\nPlease address these, and I'm leaning towards acceptance.",
            "- - No ablation study on n, k\n- Wall clock time not reported, only NFE improvement is discussed\n- No comparison to standard SDE fast samplers \n- The whole description is for unconditional sampling, but all results are for image restoration. How is the guidance incorporated?",
            "- - The choice of the number of sampling steps is not entirely clear. While it seems that setting this value to 20 yields acceptable results, this may not fully leverage the acceleration benefits of the proposed method. Further guidance on selecting an optimal number of steps or a discussion on its trade-offs would make this aspect more transparent.\n- There is some ambiguity regarding the number of function evaluations (NFEs) required for MR Diffusion. The authors mention that MR Diffusion typically requires hundreds of NFEs, but in the current paper, this value is set to 100, consistent with the original paper. This raises questions about whether the maximum performance of the Posterior and Euler-based sampling methods could be higher if a larger number of steps were used. Given the performance gains depicted in Figure 2, it would be helpful to clarify if the efficiency improvements observed are partly due to these optimized sampling steps.\n- Table 15 in the appendix contains incorrect highlight",
            "- A notable limitation is that the paper places more emphasis on the speed of sampling rather than the quality. While the authors claim comparable quality in terms of metrics such as FID and LPIPS, they do not provide an in-depth analysis of the trade-off between sampling speed and output quality. This is a  gap, as accelerating sampling without sacrificing quality is one of the central challenges in diffusion models. The authors could have benefited from comparing their approach with other speed-quality trade-off techniques, such as those mentioned in prior work (e.g., the 'Come-Closer-Diffuse-Faster' approach). Although these methods might be older, they are relevant in establishing a clear benchmark and providing a deeper understanding of the trade-offs at play. (like https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf  and https://arxiv.org/abs/2108.01073 )\n\nThe experiments demonstrate a clear focus on speedups, and while they are rigorous and cover a variety of tasks (e.g., image dehazing, inpainting), the lack of comparison with more state-of-the-art methods suggests that the method may not yet be positioned as a new state of the art but rather as a faster alternative with comparable performance under specific conditions.\n\nOverall, the introduction of the Mean Reverting SDE into the posterior sampling stage with ODE solvers is a fresh and promising perspective. However, the paper would benefit from addressing the broader implications of their method, particularly how the mean-reverting approach impacts the balance between speed and quality. Including more comprehensive comparisons with established trade-off techniques would further strengthen the contribution but overall this is a good paper, potentially worth discussing at ICLR."
        ]
    },
    "y5einmJ0Yx": {
        "venue": "ICLR 2025",
        "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
        "link": "https://openreview.net/forum?id=y5einmJ0Yx",
        "abstract": "Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            6,
            8,
            8,
            8
        ],
        "strengths": [
            "- - *No OOD Data Required*: Synthesizes pseudo-OOD data through adversarial training, alleviating the need for real OOD samples.\n- *Implementation Flexibility*: Supports both LDM and VAE variants, offering trade-offs between performance and computational efficiency.\n- *Strong Empirical Performance*: Outperforms non-OOD methods and matches/exceeds methods using real OOD data.",
            "- 1. The paper presents a method with notable novelty, particularly in how it handles the generation of OOD (Out-of-Distribution) samples based on ID (In-Distribution) data. This approach demonstrates creative problem-solving and provides a potentially valuable contribution to OOD detection research.\n2. The experimental section is comprehensive, covering multiple datasets and providing a range of performance metrics. This thorough evaluation supports the robustness of the method and suggests that it may perform effectively across diverse scenarios.",
            "- - GOLD\u2019s adversarial latent generation is a novel approach that synthesizes pseudo-OOD data without auxiliary datasets, making it efficient and broadly applicable.\n\n- GOLD\u2019s effectiveness is demonstrated through comprehensive experiments on five benchmark datasets, showing its robustness across various graph types and OOD scenarios.\n\n-  The implicit adversarial objective and energy-based detection approach lead to a clear divergence between ID and OOD embeddings, validated by experimental visualizations.\n\n- This paper is well-structured.",
            "- 1. The idea of implicit adversarial learning is novel.\n2. The experiments are solid and comprehensive, especially the improvements achieved on the FPR95 dataset is impressive.\n3. The paper is easy to follow and well-structured. The writing is good. The theoritical proofs are provided."
        ],
        "weaknesses": [
            "- - Using generative models to generate samples for downstream tasks has been widely adopted in previous methods. For example, generated samples are commonly used in continual learning for experience replay. It seems that the use of generators in GOLD applies the same concept to different tasks. Energy-based detection is also widely used. The major contribution seems to lie in a new divergence regularisation (Eq.12).\n- Further ablative studies on divergence regularisations may be needed to better reflect their effectiveness.\n- The presentation needs to be improved.",
            "- 1. Certain issues in the manuscript reduce its overall clarity and precision. For example, Figure 1(d) appears to be blank, which may hinder understanding and interpretation of the paper\u2019s content.",
            "- This paper has no obvious weaknesses except for the training computational cost induced by the pseudo-OOD data. However, this cost increase is acceptable. And the authors have also discusses this issue in POTENTIAL LIMITATIONS section.",
            "- I didn't see obvious weaknesses. Here I provide some suggestions:\n\nCould you provide a more in-depth analysis of the performance achievements on the FPR95 dataset, including an analysis from the perspective of the dataset's characteristics? Additionally, could you explain why similar significant improvements were not observed on other datasets (although the improvements on other datasets are also quite good)?"
        ]
    },
    "xsx3Fpo3UD": {
        "venue": "ICLR 2025",
        "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models",
        "link": "https://openreview.net/forum?id=xsx3Fpo3UD",
        "abstract": "Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at https://github.com/SLIT-AI/ADPA .",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6
        ],
        "strengths": [
            "- - The two methods proposed in the paper combine RLHF with distillation, providing insights into the preference alignment of small language models and solving problems that previous methods had not addressed. \n- The experimental section is well-designed, with numerous comparative and ablation experiments to verify the performance improvements of the methods. \n- The paper is well-written, with detailed descriptions of the two methods, including algorithm steps and formulas, which are easy to understand.",
            "- 1. This paper investigates the challenge of aligning small language models, the proposed methods are well-motivated and principal.\n\n2. The ADPA method considers the distribution-level reward signal, which is an advancement compared to the previous KD methods.\n\n3. The KD baselines are comprehensively compared, and the studied teacher-student settings are representative.\n\n4. This paper is well organized and the presentation is decent.",
            "- 1. The idea of improving the performance of small model in the preference alignment stage is interesting.\n2. Advantage-guided distillation from the preference-aligned teacher model to the student model is novel for knowledge distillation.\n3. The experiments are detailed and the presentation is good.",
            "- 1. How to leverage the preference signals for KD is an important but under-explored problem.\n2. The methods are concise and easy to implement.\n3.  The empirical results of ADPA is strong."
        ],
        "weaknesses": [
            "- - ADPA provides more nuanced guidance signals, but the additional computations introduced by fine-grained signals may increase the computational overhead, and the paper seems to lack specific quantitative metrics on this point.\n- The experimental section utilizes a rather singular evaluation dataset, without providing results on a broader range of models, domains, and data. Additionally, there is a lack of experiments assessing the impact of teachers of varying proficiency levels on the results.",
            "- 1. Limited baseline of alignment methods. In Table 1, while the KD baselines are comprehensively compared, the alignment baseline only includes DPO.\n\n2. Computation overhead of the KD methods. It is not clear whether the proposed methods (and other baseline KD methods) consume more training or inference resources compared to directly applying alignment techniques to the student models.",
            "- This is a good work. The proposed method is simple yet effective. I do not have additional concerns for it.",
            "- 1. Although ADPA seems effective empirically, it is still unclear how the improvement is related to the motivation of the method. From Section 3.3 and Algorithm 1, it seems ADPA does not need the preference labels: only the prompts and the grouth truth reponses works for ADPA. How is ADPA related to preference alignment?\n2. From Table 2, it seems that the reference teacher model is critical for the effectiveness of ADPA. It would be better to add more explanation on why the difference between $\\pi_{dpo}$ and $\\pi_{ref}$ should be considered, rather than $\\pi_{dpo}$. Furthermore, what if $\\pi_{dpo}$ is removed from Equation (11)? (to show the effect of $-\\log \\pi_{sft}$ alone)"
        ]
    },
    "xsELpEPn4A": {
        "venue": "ICLR 2025",
        "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
        "link": "https://openreview.net/forum?id=xsELpEPn4A",
        "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8,
            8
        ],
        "strengths": [
            "- - The problem setting of building cheaper, scalable LLM judges is an important problem now that LLM-driven evaluation is becoming standard, and the provided benchmark will be incredibly valuable to the community\n- This paper is easy to follow and provides a comprehensive analysis comparing JudgeLM to existing LLM judges on both accuracy and efficiency\n- Improvements over previous LLM judges is impressive and provides a promising alternative to expensive closed source model judges",
            "- The paper demonstrates a novel approach to scaling LLM evaluation by fine-tuning models as judges, creating a comprehensive system that addresses biases inherent in model evaluations. This creative combination of fine-tuning techniques with practical augmentation methods (swap, reference support, reference drop) removes limitations from prior works that struggled with consistent, scalable evaluation in open-ended tasks. \n\nThe quality of the work is solid, backed by a large-scale, carefully curated dataset, including GPT-4 judgments and human validation, which strengthens the empirical basis of the results. In terms of clarity, the paper effectively communicates its methodology and contributions. \n\nGiven the increasing role of LLMs across various fields, there is pressing need for scalable, unbiased evaluation frameworks. JudgeLM\u2019s seems a valuable tool in AI evaluation, with potential impact on future benchmarks and research in LLM development.",
            "- - JudgeLM\u2019s ability to process up to 5K samples in 3 minutes on an 8-GPU system is impressive, and it supports multiple use cases including single-answer and multimodal evaluations.\n- The swap augmentation and reference-based adjustments offer a nice way to mitigate biases that impact LLM judgments, contributing to more reliable scoring across scenarios.\n- The JudgeLM dataset, with its human-verified judgments and optional reference answers, is a notable contribution.",
            "- 1. **Strong Motivation**: Training a judge LLM for efficient, effective evaluation of LLMs in open-ended benchmarks is highly valuable, as it enhances privacy, consistency, and cost-effectiveness.\n2. **Thorough Exploration of Key Research Questions**: The paper addresses significant questions around agreement, consistency, efficiency, scalability, and generalization for LLM-as-judge models.\n3. **Solid Experiments**: A convincing ablation study supports the effectiveness of data augmentation strategies, and the \u201cgrading, judging, and reasoning\u201d pattern retains significant agreement and consistency benefits while improving efficiency.\n4. **Open-Sourced Code and Dataset**: The code and dataset are readily accessible and user-friendly, enabling further research and reproducibility."
        ],
        "weaknesses": [
            "- - Knowledge bias is not a bias but rather a limitation of the LLM, and the proposed solution to this seems to be providing that knowledge via a reference (AKA making the out of distribution task in distribution). While I am not opposed to this solution, I would argue this is not a bias addressed but rather a universally known failure case of the model that the authors try to mitigate by training on more knowledge via reference examples - a universally known solution to address model knowledge gaps.\n- While the validation set was manually checked and corrected by the authors, it does still rely on GPT generated outputs. This provides somewhat of an unfair evaluation as JudgeLM is trained on GPT generated judgements as well. Even with the human validation, there is a reasonable chance that if this dataset where annotated by a different LLM and produced different judgements, humans checking responses would also consider them reasonable. An unbiased way of annotating is for humans to provide judgements *without* knowing what the GPT judgement is. If the agreement between humans and the GPT judgements are similar, than I would consider this evaluation relatively fair across all judge models. \n- The authors did not provide clear evidence that this model is able to maintain good performance across tasks not in the training set. I suspect that the comparison to the PandaLM test set is showing this to some extent, but I did not see any prose on *how* these two datasets differ. What tasks are seen in PandaLM that arent seen in the JudgeLM dataset? If the authors can show that the task distribution is significantly different from the training set I would be satisfied",
            "- The improvements seem The dataset, although extensive, primarily relies on GPT-4 for initial judgments, which may inadvertently transfer GPT-4\u2019s specific limitations to JudgeLM. A more diverse range of teacher models such as Claude could minimize over-reliance on any single model\u2019s limitations, making JudgeLM\u2019s judgments more adaptable.",
            "- - The model's robustness under varied task complexities or unseen domains is not extensively tested (e.g. math/coding/reasoning task). Additional benchmarks or diverse human annotations would reinforce its generalizability.\n- The study acknowledges that scaling up the judge dataset is costly and currently relies on GPT-4 outputs. Exploring alternative sources or synthetic judgment data could be beneficial.",
            "- 1. **Assumption of GPT-4 Judgments as Ground Truth**: Although GPT-4 is a common choice for cost-effective labeling, it would be beneficial if the authors could further substantiate its reliability before using it as a standard.\n2. **Suboptimal Training Query Distribution**: While the authors highlight the diversity and quality of the training data by statistics, further optimization of the ideal data distribution for training judge LLMs would add depth. What constitutes a more ideal queries distribution? For example, the distribution of user queries collected in Chatbot Arena reflects a large volume of real user queries, which is one of the reasons Chatbot Arena has become such a well-recognized benchmark. If we align the queries distribution in the training data more closely with the distribution of actual user queries, the resulting Judge LLM would evaluate samples more fairly,  drawing on the philosophy behind MixEval/MixEval-X paper.\n3. **Potential Overclaiming**:\n    - Bias Analysis: In Section 4, while the paper discusses \u201cposition bias\u201d and \u201cknowledge bias,\u201d these concepts are well-established in prior literature. The novel contribution here lies in addressing \u201cformat bias,\u201d so the phrase \u201cshed light on three key biases\u201d could overstate the novelty. It might be more precise to note that this work builds on previous discussions of position and knowledge biases while introducing and addressing format bias as a new focus.\n    - Scalability Comparison: In Appendix 4, the authors mention JudgeLM\u2019s scalability as a differentiator from PandaLM. While PandaLM also explores different model sizes (up to 70B), JudgeLM offers a more granular analysis of scalability. It would clarify the novelty here to acknowledge PandaLM\u2019s scalability exploration but emphasize JudgeLM\u2019s finer scalability insights.\n4. **Paper Structure**: Despite the solid research content, the paper\u2019s organization could benefit from a clearer structure, such as arranging sections by key research questions."
        ]
    },
    "xoIeVdFO7U": {
        "venue": "ICLR 2025",
        "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning",
        "link": "https://openreview.net/forum?id=xoIeVdFO7U",
        "abstract": "Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).\nOur analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            6,
            8
        ],
        "strengths": [
            "- The paper provides a thorough theoretical analysis of METRA, reinterpreting it within the mutual information skill learning (MISL) framework. This helps demystify the method and connects it to well-established concepts like contrastive learning and information bottlenecks.\n\nThe presentation is clear and to the point. The writing is excellent, I did not find typos or mistakes.\n\nThe paper includes extensive empirical evaluations, comparing CSF with existing methods across various tasks. This robust experimental setup strengthens the validity of the proposed method.",
            "- - **[Technical soundness and novelty]** The technical soundness is robust; this work provides a thorough in-depth analysis of the METRA method, finding approximate equivalences with contrastive objectives and the information bottleneck. The analysis leads to a novel method that simplifies METRA, and I found no technical flaws; the method is both novel and solid.\n\n\n- **[Evaluation]** The empirical evaluation effectively validates the hypotheses and theoretical analysis, enhancing the overall persuasiveness of the work.\n\n- **[Presentation]** The presentation is clear and easy to follow.",
            "- The paper is easy to follow and well-written, the analysis is sound and the experiments are relevant.",
            "- This paper is incredibly well-written. The message/purpose is clear. There is an extensive literature survey, and the authors' discussion of the relevant material and methodology is informative. Unlike some papers that resort to mathematics unnecessarily, all components seem necessary, and are meaningfully explained in text. The authors perform a relatively large set of experiments to both show the performance of their algorithm, but also to back up other claims (such as the properties of representations learned). I also commend the authors for the extended information in the appendices, and also for providing code (I checked these briefly but not extensively)."
        ],
        "weaknesses": [
            "- While I appreciate that the paper is presented as an improvement on METRA, I'd have enjoyed more a reading that was presenting a new method that is then shown to be equivalent to METRA under certain conditions.\n\nGiven that the presented method performs are par with METRA, it would also be nice to show where (if anywhere) one fails when the other succeeds. Perhaps partially observed MPDs, more interactive objects or discrete actions spaces would be key in identifying where exactly both methods stand.",
            "- - **[About performances]** A significant question arises especially in the Quadruped experiments, where performance still shows room for improvement compared to METRA. Given that the proposed framework has a similar objective function, fewer hyperparameters, and avoids complex min-max optimization, why does the empirical performance (or at least the rate of convergence) not exceed that of METRA? Any discussion on this would be beneficial.\n- **[About demonstrations]** It would be advantageous to include demonstrations or other forms of visualization for the skills learned, as I did not find this in the appendix code.",
            "- The final model is very close to previous work and do not present substantial improvements on the different environment compared to METRA. This is overall acknowledged by the authors.",
            "- I do not believe there are any substantive weaknesses of this work, but there are some questions the authors could address. As acknowledged by the authors in their own Limitations section, it is unclear how well these algorithms scale beyond the relatively simple MuJoCo benchmarks, but the authors have performed a significant amount of experiments on these domains."
        ]
    },
    "xaYlO03tIk": {
        "venue": "ICLR 2025",
        "title": "Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion",
        "link": "https://openreview.net/forum?id=xaYlO03tIk",
        "abstract": "Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose ***Stem-OB*** that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. *Stem-OB* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. *Stem-OB* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of **22.2%** in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            6,
            8,
            8,
            8
        ],
        "strengths": [
            "- * This paper focuses on enhancing the robustness of visual imitation learning, addressing a practical and impactful topic. The approach holds significant potential for advancing research in the field of robotics.\n\n* The idea of using diffusion inversion to remove low-level visual variations while preserving high-level scene structures is both novel and intriguing.\n\n* The evaluations are thorough, with experiments conducted in both simulated environments and on real-world robots. The real-world experiments highlight the method's strong generalization capabilities.",
            "- - The idea is, as the authors write, 'simple yet effective' and the authors manage to neatly package a quite theoretical idea and a very practically result measured in real-world robotic experiments all in 1 paper\n- While I lack the background to assess the theoretical grounding, the idea itself is well explained\n- The authors combine experiments on simulated datasets with actual real-world tests. Real-world tests are incredibly important for visual methods, yet in case of robotic applications very hard to make reproducible. The combination of both is a commendable experimental design. [**update after discussion phase** the extend and rigour of experiments has even further improved during the discussion phase]\n- Similarly, the authors conduct real-world user studies to investigate their hypothesis that diffusion inversion progresses along semantic hierarchies.",
            "- 1. The paper is well-written, with clear motivation and solid experimental validation, including both real-world and simulated experiments.\n\n2. The central idea resonates well: robotic observations often include excessive low-level details, while effective scene understanding requires capturing high-level structural information. This paper draws on an intriguing insight from Yue et al. (2024): \"Rather than uniformly removing information from different semantic hierarchies, the process brings structurally similar images closer in the early stages of inversion.\" Building on this, Stem-OB leverages this observation to project data into a space that prioritizes inherent structural features.",
            "- * The paper relies on a central observation from a previous paper (Yue et al. 2024), which found that during the inversion process of a diffusion model, fine-grained (or high-frequency) details of an image are destroyed first before the low-level semantic concepts. This paper builds on this basic concept and proposes a new preprocessing step that uses this theoretical property of diffusion models as a preprocessing step, which, in the process, improves the generalization of the IL algorithm. \n* The technical principle seems well established at this point, but this paper takes these theoretical findings and applies them to a new problem. While the whole approach is very simple from a technical standpoint, the paper re-introduces the necessary background in section 2 before developing the reader's intuition as to why this preprocessing might work in section 4.2 through theoretical explanation and small experiments. I believe that this paper warrants publication because it shows that diffusion model inversion can be useful for additional tasks. \n* The presentation of the paper is clear and easy to follow overall. \n* The results nicely highlight the drawbacks of previous methods and that the proposed method offers much better generalization capabilities. \n* I appreciated the detailed appendix, with theoretical derivations, additional details for reproducibility, and additional experiments."
        ],
        "weaknesses": [
            "- * I found the structure of this paper somewhat difficult to follow, as certain sections lacked clarity. For instance, in the preliminaries, the definition of diffusion inversion seemed to be a combination of both forward and backward diffusion. But in Figure 2, it seems that proposed method uses the noised observations as the policy input. If the proposed method is trained on the inversion-altered space, why using the noised version of the observation as input can improve the performance? How can the authors guarantee that applying forward diffusion to the image improves generalization?\n\n* There seems to be a trade-off when choosing the inversion step, which can not be either too large or too small. Is there any explanation about this phenomenon? \n\n* The theoretical analysis section was also challenging to understand. The authors discuss a loss between two latent variables,  $x_0$ and $y_0$. What  is the intuition of calculating the loss of two different images? From my understanding, the attribute loss here should refer to the inversed data $\\hat{x}_0$ and its original version $x_0$. Could the authors also clarify the statement, \u201cimages with fine-grained attribute changes tend to become indistinguishable sooner than those with coarse-grained modifications under identical diffusion schedules\u201d? \n\n* Section 4.1 reminded me of another study [1], which employs diffusion models to purify noise within noisy demonstrations while preserving the optimal structure. Are there any conceptual similarities between that approach and the proposed method?\n\n* Typos: Line 242: $er$f should be $erf$.\n\n[1] Imitation Learning from Purified Demonstrations, ICML 2024.",
            "- - [**update after discussion: this point was fixed**] I find the derivation in Section 4.1 very hard to follow. Only the first sentence of Section 4.2 makes it clear that the whole derivation is based on the assumption that semantically similar images are closer in latent space. That is a HUGE assumption and the following experiments on intra- and inter-class confusion do not show this over multiple levels of semantic hierarchies, but only for the semantic leafs. The introduction (lines 60-65) however argues that much more than leaf classes, abstract semantic hierarchies are important to generalize over perceptual differences.\nIn the empirical study on diffusion inversion, one might argue that out of the 5 empirical examples, \"bowl\" and \"cup\" are most similar. However, Table 1 shows that the latent representation of 'cup' samples is actually on average closer to any of {'drawer', 'duck', 'faucet'} than to 'bowl'. To me this makes the assumption 'semantically similar images are closer in latent space' quite unbelievable. For sure, this assumption and therefore the formulation of semantic overlap as overlap of latent gaussians is anything but 'intuitive' (line 270) in presence of this data.\n- [**update after discussion: this point was fixed**] In relation to the above, I miss a clear definition of what kind of variation should be compensated through diffusion inversion. The abstract and introduction repeatedly claims that diffusion inversion can extract the 'high-level structure' of the scene, suggesting generalization over different object instances, shapes, appearances, relative placements, and robot arms. Section 4.1 considers 'variation', 'fine-grained attribute change', 'coarse-grained modifications', and 'semantic overlap' without defining any of these. The investigation in Section 4.2 is focused on variations within a semantic object class, i.e. a demonstration with 1 cup should be repeated with a different cup. The experiments then consider lighting change and a limited set of object appearance change of sometimes multiple object instances, while locations are fully fixed. The problem is that all of this currently does not fully fit together and it would be good if the authors can define more accurately what kind of variations they expect diffusion inversion to abstract / generalize over, and then design experiments accordingly to show improvement with exactly these variations.\n- There are a couple of odd aspects about the simulation experiments that raise questions about the soundness of the results:\n  - For the benchmarks from ManiSkill and MimicGen, why were not all tasks evaluated? For ManiSkill, plug-charger seems to be specifically excluded and for MimicGen 4 out of 12 tasks were picked without any explanation.  \n[ **update after discussion: now there are 8 out of 12 tasks from MimicGen. It is still a subset, but this indeed makes the results more thorough**]\n  - While I did not quickly find comparable numbers for ManiSkill, the MimicGen paper reports much higher success rates for the investigated tasks. E.g. for Threading, MimicGen reports around 19% sucess from just 10 videos and 98% success from 1000 videos. For the 500 videos used in the experiments here, the success is below 19% for 3 out of 4 variants, including the proposed method. Why are the achieved success rates so low? And can the proposed method actually improve anything in a more state-of-the-art setting? [**update after discussion: This concern partially remains. While the authors show that their success rates for 500 trials are mostly between the baseline MimicGen results of 10 and 1000 trials, there is no case where the authors can demonstrate to improve over these prior results. If one expects the relation between number of trials and success rate to follow a saturation trend rather than a linear trend, their results are still a bit below those of prior works.**]\n  - Why is the RO baseline excluded from the simulation experiments? [**update from discussion: this was resolved**]\n- In all experiments (real and simulated), most of the differences between methods are within the standard deviation, so it is very hard to say if any conclusion can be drawn. Why is the standard deviation so high? [**update from discussion: this has been thoroughly addressed by the authors. The updated results show, similarly to the question about success rates above, that Stem-OB does not push the state-of-the-art further than where it is. But it does more importantly also not appear to be much worse, while the method in some sense is simpler.**]",
            "- Stem-OB enhances the generalization capabilities of robotic systems by using diffusion inversion to map observations into a shared representation. However, another promising line of research\u2014self-supervised representation learning\u2014also aims to unify raw observations into a common representation space, reducing low-level visual discrepancies. I suggest that the authors consider benchmarking Stem-OB against these approaches, particularly methods that leverage self-supervised learning (SSL) for robotic representations, to offer a more comprehensive comparison.",
            "- I believe that the paper's technical novelty is small, as the main concept has already been introduced in other papers. That being said, I also believe that the paper is worth publishing, as it adds to the growing body of literature showing the usefulness of diffusion model inversion for various tasks, and the empirical validation is well executed.\n\nI have additional suggestions to improve the paper's readability and make it easier for the reader to understand.\n1. Section 2.2 can be cut (title), and the paragraph can be integrated into Section 3.1. \n2. As a reader, I would really like a short problem formulation that properly introduces the problem the paper addresses. This should properly define the inputs and outputs. I would also suggest rearranging the sections/subsections of the paper to something like this:\n* Introduction\n* Related Work\n* Problem Definition\n* Preliminaries\n* Method\nThis would greatly improve the flow of the paper. \n3. While the paper is generally easy to follow, some sentences are should be simplified or run through a writing program. I have added some examples below:\n* Line 85: \"To be specific, our method is as simple as inverting the image for reasonable steps before\" What is reasonable? Maybe rephrase?\n* Line 226: \"Intuitively, given a source image and its variation, distinguishing between them becomes increasingly difficult\" This sentence does not make a lot of sense (although I can guess what is meant).\nThere are many more of these convoluted sentences. Cleaning up the writing a bit would go a long way to improve the paper.\n\nThere"
        ]
    },
    "xQBRrtQM8u": {
        "venue": "ICLR 2025",
        "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control",
        "link": "https://openreview.net/forum?id=xQBRrtQM8u",
        "abstract": "Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific *memoryless* noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named *Adjoint Matching* which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8,
            8
        ],
        "strengths": [
            "- This paper is theorectically well written, with detailed introduction to the literature, explanation of motivations, several interesting theorems and propositions, and provide theorectically-driven approaches for diffusion models fine-tuning. Diffusion Models fine-tuning or RLHF for diffusion models are an important direction which already contributes to improving the performance to SOTA diffusion models. This paper indeed provides a novel model-based SOC method for diffusion models alignment, which is theorectically sound and also yields good performance for tuning Flow Matching based models.",
            "- This paper provides a theoretically sound framework for the reward fine-tuning problem, viewing it as a stochastic optimal control problem. The observation of the value function bias problem in previous approaches and proposal of using \u201cmemoryless\u201d noise schedules are based on this view.\n\nThe proposed Adjoint Matching algorithm for SOC, casting it as a least-squares regression problem, is novel and effective.\n\nThis paper is well-written, clearly structured and easy to follow.",
            "- - The paper is well-written and structured.\n- Strengths of memory-less noise schedule:\n    - The proposed approach is a less complex and provides an arguably more elegant solution to the initial value function bias compared to the work by Uehara et. al. [1].\n    - The proposed approach is compatible with both DDPM and Flow/Bridge-Matching models.\n- Strengths of (lean) adjoint matching:\n    - Simple regression-based objective with circumvents memory problems associated with the discrete adjoint method\n    - Simulating the adjoint ODE does not require control evaluations, making it more scalable than the continuous adjoint method.\n    - Compatible with general SOC problems.\n- Numerical evaluations show that the proposed approach outperforms competing methods on a variety of evaluation criteria.",
            "- The concept of using stochastic optimal control (SOC) to fine-tune diffusion-based generative models is not new, as [1] formulates fine-tuning for diffusion models (specifically forward-reverse models) through Doob's h-transform, which involves optimally controlled diffusion processes. The novelty lies in:\n\n* Reformulating fine-tuning of flow-matching based generative models as an SOC problem, the authors introduce a suitable cost functional for the control policy parameterized by a training neural network. Additionally, they incorporate a memoryless noise schedule to ensure factorizability, thereby eliminating inherent bias.\n* Proposing an adjoint-matching objective to solve the above SOC problem. It is well known that solving SOC problems is computationally challenging due to the need for continuous gradient graph caching. Extending the adjoint methods commonly used in dynamical learning as a dynamic solver for SOC objectives is, in my view, a significant contribution.\n\n\n```\n[1] Denker et al., Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised h-transform.\n```"
        ],
        "weaknesses": [
            "- The main weakness is that the paper can be benefited from more comparisons with baseline methods empirically, specifically, there lacks a baseline in experiments in directly optimizing objective (17) (though theorectically there is value bias as shown in the paper), using stochastic control methods like adjoint matching proposed in this paper. There is comparison for this in the synthetic examples in Figure 2, but more practical downstream tasks evaluations are needed to show that the noise schedule proposed in this paper can indeed yield better performance. \n\n**Minors**:\n\n1\uff09On line 229-230, the expression \"Dividing (14) by (15)\" is odd as (14) is not an equality, \"Plug in normalization constant (15) to (14)\" might be better.\n\n2\uff09The adjoint method in Section 4 needs more introduction or discussion, or an earlier pointer to the part in appendix, including more clarification on what is adjoint on line 350-352, what is this loss on Equation (21); The reference in Appendix should be referred earlier instead of being deferred to until line 422.\n\n3\uff09In proposition 2, it is better to first define earlier of the adjoint matching objective instead of combining the definition with the proposition.",
            "- The main paper presented experimental results on fine-tuning a Flow Matching model, and provided pseudo-code for fine-tuning denoising diffusion models; it would be more convincing if results on denoising diffusion models are provided.\n\nThe experiments with classifier-free guidance do not seem comprehensive. It would be better if there are similar quantitative comparisons with other baselines than selected DRaFT-1.",
            "- - Weaknesses of the memory-less noise schedule:\n    - The statement \u201chowever, there does not yet exist a simple approach which actually provably generates from the tilted distribution\u201d is, to the best of my knowledge not true: Take for example the approach in [2, 3] and use the base measure as reference process. Then, we have the terminal cost\n    \n    $$\n    g(X_1) =p^{\\text{base}}(X_1)/p_{\\text{target}}(X_1) = p^{\\text{base}}(X_1)/(p^{\\text{base}}(X_1)\\exp(r(X_1)) = 1/\\exp(r(X_1)) \n    $$\n    \n    which es ensures that \n    \n    $$\n    p_{\\text{target}}(X_1) = \\exp(r(X_1)).\n    $$\n    \n    The \u2018memoryless\u2019 property thus eludes to the fact, that we need a time-reversal of the base process. A more general discussion compared to [2, 3] can be found in [4, 5].\n    \n- Weaknesses of (lean) adjoint matching:\n    - It is not clear (at least to me) which objective function lean adjoint matching is optimizing. \n    - Lean/Continous Adjoint-matching requires simulating another differential equation which may result in computational overhead.\n- Code is not public",
            "- * While I may have missed it in the appendix, it appears that the experiments in this paper primarily address the quality of fine-tuning, without providing quantitative results on aspects of the proposed adjoint objective, such as convergence plots or memory usage statistics comparing the adjoint matching loss to traditional SOC objectives, as shown in studies like [2, 3]. From a theoretical standpoint, although the author suggests that the approach could be sufficiently applied, it would strengthen the claim to include experiments validating the numerical effectiveness of the new SOC objective.\n\n\n```\n[2] N\u00fcsken and Ritcher, Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space.\n[3] Domingo-Enrich et al., Stochastic Optimal Control Matching.\n```"
        ]
    },
    "xGs7Ch3Vyo": {
        "venue": "ICLR 2025",
        "title": "Better autoregressive regression with LLMs via regression-aware fine-tuning",
        "link": "https://openreview.net/forum?id=xGs7Ch3Vyo",
        "abstract": "Decoder-based large language models (LLMs) have proven highly versatile, with remarkable successes even on problems ostensibly removed from traditional language generation.  One such example is solving regression problems, where the targets are real numbers rather than textual tokens.  A common approach to use LLMs on such problems is to perform fine-tuning based on the cross-entropy loss, and use autoregressive sampling at inference time. Another approach relies on fine-tuning a separate predictive head with a suitable loss such as squared error. While each approach has had success, there has been limited study on principled ways of using decoder LLMs for regression. In this work, we compare different prior works under a unified view, and introduce regression-aware fine-tuning(RAFT), a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6
        ],
        "strengths": [
            "- The paper is well-organized, with a clearly defined objective and comprehensive experimental comparisons, making it accessible and straightforward to follow. I enjoyed reading the paper.",
            "- - Paper (in general) is well-written and easy to follow (even though there are still small things that require further clarification)\n- Simple and effective idea \n- Shown results indicate improvements across different tasks\n- Additional analysis on several aspects (grid, pre-training, masking)",
            "- - The research question is relevant and interesting.\n - The presentation of MALI (previous work) is clear.\n- The counter-example of Lemma 1 is good. Since it's short, I would include it in the main body if possible.\n- The experiment on using semantically unrelated tokens for the regression task (Table 11) is interesting.",
            "- 1. This paper provides a well-rounded formulation for LLM-based regression approaches.\n2. It presents thorough evaluations across diverse tasks (e.g., Wireless, Music, Personal Care) and models (e.g., Gemma and PaLM).\n3. The proposed RAFT method demonstrates effectiveness across these benchmarks."
        ],
        "weaknesses": [
            "- 1. **Performance Drop on Alternative Grids (Lines 902\u2013903 and L531)**: You note a performance drop when using a grid of letters a\u2013e, yet Table 11 shows similar MSE to the numerical grid. Could you clarify this inconsistency?\n\n2. **Lemma 1 (Line 187)**: Based on the appendix, does the norm in Lemma 1 refer to an $L_1$ norm rather than $L_2$? The crafted example in the appendix (where a probability mass of $\\epsilon / 2$ is positioned at 0 and $y_\\text{max}$ implies that the $L_1$ loss should satisfy $E_x [ \\| P(. | x)  - p(.|x)  \\| ] <= \\epsilon $ (the norm being L1), given that line 747 defines $ | \\mathbb{P}(. | x) - p(.|x) |_{1} < \\epsilon $.\n\n3. **Token Granularity and Grid Complexity**:\n   In the RAFT experiments, was the grid composed solely of single tokens? If so, does this also apply to the months-named grid?\n   If the pretraining hypothesis holds that a simple grid (like 1\u20135) leverages pretrained information effectively, would a more complex grid (e.g., values such as 0.97, 2.3, 3.001, etc.) yield similar performance? I think this would be a very important question that can substantially strengthen the paper.\n4. **Dataset Examples**: Including a table of dataset examples and inputs/outputs of the RAFT model would further help with clarity. It would help in verifying dataset characteristics too.",
            "- The predictive head approach closely follows the RAFT; however, there is no extended discussion of what sets RAFT apart.\n\nNote that the below weaknesses are mostly minor.\n\nThe authors highlight the issues of the decoder-based LLMs. However, there is no baseline comparison (i.e., it is not clear what the potential upper bound is with current technologies). In the literature, performance for datasets like SSTB is often reported as Pierson/Spearman correlation\n- Since some things are still unclear to me in the experiment setup (please see question), I am hesitant to assign a score",
            "- - On a first read, I misunderstood the implications of Lemma 1. I think it would be worth slightly rewriting the paragraph after Lemma 1. In particular, I would replace the sentence \n> Intuitively, log-perplexity fine-tuning treats all \u201cwrong\u201d predictions the same, *and does not account for the magnitude of their differences from the ground-truth target*.\n\nby\n\n> Intuitively, log-perplexity fine-tuning treats all \u201cwrong\u201d predictions the same, *as it is unaware of difference in magnitude of the numerical values represented by the tokens. For example, assuming that \"100\" and \"1000\" are represented with a single token, placing $\\epsilon$ too much mass on tokens representing \"100\" is penalized similarly as placing  $\\epsilon$ too much mass on the token \"1000\".\n\nIf you have a better rephrasing, you can of course use something else, but I think the aforementioned sentence can be improved (e.g. using my example).\n\n- On the line 371, you are explaining that you are experimenting with a reduced training set for the STSB benchmark (using 1k examples instead of the whole training set). I understand that it is interesting to vary the amount of training data, but why are you only reducing the training set size on STSB and not on the other datasets?\n\n- **Main weakness**: While your paper studies regression tasks using autoregressive models, you observe on lines 465-467 that when training from scratch, the predictive head methods work best. Since encoder-only models such as BERT are widely used for tasks mentioned in the introduction (e.g. sentiment analysis, regression, ranking), it would be valuable to include a baseline of a large fine-tuned RoBERTa model on the same tasks that you present in the paper. Your findings would remain interesting and relevant in case the RoBERTa baseline was beating RAFT, but I think knowing how RAFT compares to fine-tuning RoBERTa is very relevant to practitioners. Therefore, I would suggest adding the following experiments (with the same hyperparameter tuning as RAFT):\n    - Prediction head regression over a mean-pooling of RoBERTa representation (over sequence length)\n    - Prediction head regression over the output for the CLS token of RoBERTa model\n    - Both variants with either frozen RoBERTa weights, as well as unfrozen weights\n\n- There could be more practical details on the fine-tuning process. Are you updating all the parameters of the model or did you freeze some of them (eg. update only the last linear layer)? Which optimizer are you using? If you are using Adam, what values of the betas, weight decay, and epsilon are you using? What batch size are you using? How long does one training run takes on average? How many machines did you need? The answers to these questions are important to help other researchers reproduce your results.",
            "- 1. While this paper presents a strong approach, it addresses a relatively niche problem. In my view, the choice between autoregressive sampling and a predictive head largely depends on the specific task, making it less crucial to claim that your method is universally superior to both across all tasks.\n2. The proposed method does not show a significant improvement over the predictive head. For instance, in Table 9, RAFT performs worse than the predictive head on the Music dataset."
        ]
    },
    "wkHcXDv7cv": {
        "venue": "ICLR 2025",
        "title": "Tuning Frequency Bias of State Space Models",
        "link": "https://openreview.net/forum?id=wkHcXDv7cv",
        "abstract": "State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an $88.26\\\\%$ accuracy on the Long-Range Arena (LRA) benchmark tasks.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8,
            8
        ],
        "strengths": [
            "- This paper presents a remarkable analysis of the frequency bias in state-space models (SSMs). It has been commonly observed that neural networks tend to fit low-frequency information first, often filtering out high-frequency data in many cases. This paper critically emphasizes the importance of initialization and introduces a strategy to mitigate this problem using a Sobolev filter. The figures presented in the experiments are highly inspiring. As a theoretical work, this paper provides sufficient experimental results, demonstrating strong performance.",
            "- 1. The organization and presentation of this paper is smooth and clear, and it provides a better understanding on the training mechanisms of state space models on sequence modeling.\n\n2. I find the paper to be well-written and easy to follow. The overall topic of state space models is important.",
            "- The paper is very clearly written, and the motivation and implications of the theoretical results are clearly explained. The identification of the frequency bias is useful for problems where high frequency components need to be extracted. The proposed methods to mitigate the frequency bias appear simple to implement, and are principled.",
            "- Well-written with effective illustrative explanations.\n\nProvides a theoretically sound analysis of a complex problem.\n\nPractical relevance is high."
        ],
        "weaknesses": [
            "- The weaknesses of this paper mostly come from two parts. \n\n1. The analysis is based on SSM which is potentially hard to be generalized on general neural networks. \n\n2. Even though the reason is getting clearer, these strategies of this paper could still be suboptimal for practitioners.",
            "- 1. In the end of Section 2, the authors state that the larger the total variation of the transfer function is, the better an LTI system is at distinguishing the Fourier modes. This claim is not intuitive for me, and the total variation cannot distinguish the following two different cases: the first one is large amplitude with low frequency and the second one is small amplitude with high frequency. For example, $G$ is an impulse response vs $G$ is a sinusoidal wave with a small amplitude. From my understanding, these two LTI systems have different ability on distinguishing the Fourier modes.\n\n2. The statement after Lemma 1 that small $|y_i|$ induces small total variation seems to be wrong. From the upper bounds in Lemma 1, if $|y_i|$ decreases, the the upper bounds increase, which means that the total variation will be large.\n\n3. The initialization method in Corollary1 is not a commonly used method. For S4D, the initialization methods for $a$ are mainly S4D-Legs and S4D-Lin. Why not choose these two initialization methods instead?",
            "- 1. The paper can be improved by clarifying some notational and definition issues, and these are detailed in the questions below.\n2. Notational suggestion: the notation of using $y_j$ to denote imaginary part of the diagonal of $A$ can be changed to avoid confusion with the output of the network, also called $y$.\n3. Since the SSM transfer function is differentiable, it may be simpler to define the total variation as the integral of its derivatives to avoid unncessary complications.",
            "- The analysis is limited to diagonal systems.\n\nTuning the additional hyperparameters, \u03b1 and \u03b2, may pose challenges in practice."
        ]
    },
    "wM2sfVgMDH": {
        "venue": "ICLR 2025",
        "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
        "link": "https://openreview.net/forum?id=wM2sfVgMDH",
        "abstract": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.",
        "decision": "Accept (Oral)",
        "review scores": [
            6,
            8,
            8,
            8
        ],
        "strengths": [
            "- S1. Reduces complexity  issue by collectively considering the status of key participants in the driving scenario and jointly modeling the motion prediction and closed-loop planning tasks as a future trajectory generation task.\n\nS2. Integrating closed-loop planning with a diffusion model is an effective approach, and the use of the architecture is clearly articulated.",
            "- The strengths of the paper are threefold.\n\n1. The use of diffusion models in the autonomous driving planning task is novel. The authors effectively address the limitations of existing learning-based planning methods, such as handling multi-modal behaviors and out-of-distribution scenarios.\n\n2. The paper provides a thorough explanation of the Diffusion Planner\u2019s architecture and how it integrates prediction and planning tasks. The classifier guidance mechanism for adaptable planning behaviors is particularly well-explained.\n\n3. The evaluations on the nuPlan benchmark and the delivery-vehicle dataset demonstrate impressive closed-loop performance, surpassing both learning-based and rule-based baselines. The method shows robust transferability, highlighting its potential for real-world applications.",
            "- 1. A novel diffusion-based framework in solving the motion planning task. Intutive DiT-enabled framework for integrated prediction and planning with costs guidance. \n\n2. Strong planning results delivered against state-of-the-art baselines in nuPlan.",
            "- The proposed model is carefully designed and extensively tailored for practical application in autonomous driving. The experiments are comprehensive, with multiple baselines and detailed ablation studies. It achieves better performance than the current state-of-the-art methods on the nuPlan leaderboard. Detailed descriptions of the model implementation and experiment design are provided. This work provides useful lessons for the community on achieving good closed-loop planning performance with diffusion-based motion planners for practical autonomous driving applications."
        ],
        "weaknesses": [
            "- W1. Though PLUTO (hybrid method) performs better than Diffusion Planner in the NuPlan dataset, no comparison with PLUTO w or w/o refine is shown for the delivery-vehicle driving dataset.\n\nW2. The paper would benefit from a more explicit and detailed statement of contributions, perhaps in a dedicated paragraph near the end of the introduction. This should clearly outline how the Diffusion Planner addresses each of the limitations mentioned and what specific novel aspects it introduces.\n\nMinor Nitpicks\n\nN1. A legend should be added to Appendix A, Figure 8\nN2. Line 195: Conditions C could be mathematically defined",
            "- The overall quality of this paper is strong. One minor area for improvement is the quantity of simulation benchmarks used. The paper evaluates planner performance solely based on the Test14 random benchmark, which consists of approximately 280 closed-loop scenarios. It would be beneficial to include additional simulation benchmarks from nuPlan, such as the Val14 benchmark and the Test14 hard benchmark, to provide a more comprehensive demonstration of the advantages of the proposed method over the baselines.",
            "- 1. Insufficient benchmark and metric comparison: 1) Additional results in other popular benchmark, such as Val14 and Test14-Hard are required to manifest the planning results under more diversed / challenging scenarios. 2) Other settings, such as closed loop reactive simulation, and open loop results are not verified.\n\n2. Insufficient baseline comparison. Motion planning / trajectory simulation leveraging diffuision are not novel. Hence, the planning results using other diffusion strategies ([1] [2] for instance) seems needed.\n\nRef:\n\n[1] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" The International Journal of Robotics Research (2023): 02783649241273668.\n\n[2] Zhong, Ziyuan, et al. \"Guided conditional diffusion for controllable traffic simulation.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.",
            "- 1. While I believe this work has good practical values for the autonomous driving community, diffusion models have been explored in several works for motion prediction, closed-loop simulation, and motion planning in autonomous driving. The authors attempted to differentiate their work from existing works by emphasizing that the diffusion model does not drive performance improvements in these cases but relies on rule-based refinement or LLM, and they claimed that they are the first to fully harness the potential of diffusion models to enhance closed-loop planning performance in autonomous driving. However, this statement is quite vague, and it is unclear what novel technical contributions the authors have made in this work. My current impression is that the proposed model combines existing techniques investigated in the literature on diffusion models and learning-based motion planning for autonomous driving (e.g., transformer architecture, classifier guidance with manually designed cost functions, data augmentation, etc.). To help the audience better appreciate their novelty and contributions, the authors may list their novel contributions at the end of the Introduction. \n\n2. Related to the first point, the authors should provide a more extensive review of the related literature on diffusion models for autonomous driving. The authors currently give a concise discussion in Sec. 3.2. It should be moved to the related work section and extended to be more comprehensive. In particular, the current review misses an essential line of research on diffusion models for closed-loop simulation (e.g., [1], [2], [3]). Also, some works have developed diffusion-based motion planners for nuPlan (e.g., [4], [5], and [6]; the authors cited [4][5], but they are not included as baselines). If possible, they should be included as baselines for comparison. \n\n[1] Zhong, Ziyuan, et al. \"Guided conditional diffusion for controllable traffic simulation.\" ICRA 2023.\n[2] Zhong, Ziyuan, et al. \"Language-guided traffic simulation via scene-level diffusion.\" CoRL 2023. \n[3] Chang, Wei-Jer, et al. \"Safe-sim: Safety-critical closed-loop traffic simulation with diffusion-controllable adversaries.\" ECCV 2024.\n[4] Yang, Brian, et al. \"Diffusion-es: Gradient-free planning with diffusion for autonomous driving and zero-shot instruction following.\" arXiv preprint arXiv:2402.06559 (2024).\n[5] Hu, Yihan, et al. \"Solving motion planning tasks with a scalable generative model.\" ECCV 2024. \n[6] Sun, Qiao, et al. \"Large Trajectory Models are Scalable Motion Predictors and Planners.\" arXiv preprint arXiv:2310.19620 (2023)."
        ]
    },
    "wJv4AIt4sK": {
        "venue": "ICLR 2025",
        "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
        "link": "https://openreview.net/forum?id=wJv4AIt4sK",
        "abstract": "The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6
        ],
        "strengths": [
            "- - The paper covers an interesting and timely topic. Given the increasing size of parameters in pre-trained models, there is growing interest in techniques such as quantization and sparsity. Providing both analytical and empirical insights into the relationship between these techniques is valuable, especially as they are often studied separately.The findings in this paper, such as the optimal order for applying sparsity and quantization and the established upper bounds, can offer practical guidance for researchers in this area.  \n- The paper effectively demonstrates the non-orthogonality of sparsity and quantization, determining the optimal sequence for applying these transformations through theoretical analysis, supported by empirical studies on large, modern networks.  \n- The work is well-written, easy to follow, and enjoyable to read.",
            "- - The paper makes significant theoretical contributions by proving the non-orthogonality of sparsity and quantization and deriving the optimal $S\\to Q$ order. These insights challenge conventional assumptions and provide valuable guidance for model compression. \n- The mathematical analysis is rigorous and comprehensive, covering tensor-level and dot product-level errors. \n- The experimental results are extensive, spanning diverse models (OPT, LLama, ResNet, ViT) and settings.\n- The orthogonality bound metric seems like a useful tool for practitioners. \n- Overall the paper is well-structured, with clear definitions, detailed appendices, and informative tables.",
            "- * A timely and important topic as sparsity and quantization are promising compression strategies for the large model scales popular today.\n* The paper includes a comprehensive summary of relevant literature.\n* The proofs are relatively easy to follow and explained in an intuitive manner by the authors in the main text.\n* Empirical results generally appear to support the theoretical findings.\n* While many works have studied the combination of sparsity and quantization, this is the first that I am aware of to rigorously consider the interplay between these methods in detail. \n* Empirical experiments include both LLMs and vision models. \n* Extensive supplementary info includes an analysis of several leading SOTA methods from LLM pruning and quantization literature.",
            "- - Notation, definitions, and theorems in Section 3 are generally clear and their significance is adequately articulated.\n- The authors have addressed an issue that has gone overlooked in the pruning/quantization literature through both theoretical proofs and derivations as well as empirical studies that further solidify their claims."
        ],
        "weaknesses": [
            "- - In the experiments section, the results appear promising and generally align with the theoretical findings. However, it is unclear whether the reported results represent averages of multiple runs or single-run outcomes. If they are averages, what are the standard deviations?  \n- Additionally, I believe the related work section should remain in the main body of the paper, particularly since there is available space before reaching the 10-page limit. Moving it to the appendix could diminish its visibility and importance.",
            "- - While the experiments cover a range of models and settings, the datasets used (WikiText2, ImageNet1k) are relatively small and few. Evaluating on larger, more challenging datasets would further strengthen the findings. \n- The paper does not explore the impact of different sparsity patterns (e.g., block-wise sparsity) or more advanced quantization schemes.",
            "- Overall I am leaning towards accept; however, some concerns regarding the empirical experimental design causes me to doubt the applicability of the results to more general settings:\n\n* The primary metrics considered in the empirical results are perplexity or cross-entropy loss. While these are certainly reasonable proxies for downstream task performance, they are not perfectly correlated. While some accuracy metric for CV models was included in the appendices, it would be beneficial to extend this to downstream tasks for LLMs such as the OpenLLM v1 leaderboard evaluation tasks. It has been shown previously that PPL and CE can be particularly misleading metrics for quantized and sparse models [1]. \n* The experimental design for Section 4.1 is potentially concerning. If I understand the described process correctly, in the Q->S case the pretrained models are pruned and quantized before each forward pass (i.e., instantaneous masking and quantizing). Are the parameters themselves stored as dense fp32 tensors during this process and quantization is simulated similar to QAT approaches? Are the optimizer states left in fp32? The authors note issues with training dynamics in the Q->S case in Appendix A and my concern is that this could be related to numerical precision issues during fine-tuning rather than providing a reliable comparison on the order of compression. Adding a more detailed summary of the fine-tuning approaches in the appendix would potentially clear up any misunderstandings on this point. \n* In the Q->S case quantized activations are used but in the S->Q case it appears the full precision activations are used. It's unclear to me if the dramatic difference in performance is caused by the quantized activations during fine-tuning rather than the specific order of compression for the weights. \n\n\n[1] A. Jaiswal, Z. Gan, X. Du, B. Zhang, Z. Wang, and Y. Yang, \u201cCompressing LLMs: The Truth is Rarely Pure and Never Simple,\u201d Oct. 02, 2023, arXiv: arXiv:2310.01382. doi: 10.48550/arXiv.2310.01382.\n\n\n### Suggestions / Typos:\n* Defining \u201ctensor and dot-product levels\u201d earlier in the text would improve the reader's understanding. Specifically it may be worthwhile to relate these terms to \u201cweights\u201d and \u201cactivations\u201d respectively.  I note that activations / dot-products are also represented as tensors. \n* On L68, the authors refer to the challenge of quantizing LLMs due to outliers in \u201ctensor distributions\u201d and reference the smoothquant paper. This should be corrected to \u201cdot-product outliers\u201d as the challenge typically arises from outliers in the activations, not the weights (which instead follow a more gaussian-like distribution typically). \n* I suggest separating references for fine-grained (N:M and similar) and structured (neuron-level or larger NN components) sparsity in the related work discussion on L115. In particular, it would be beneficial to introduce N:M sparsity before it appears in section 3. \n* L469: state-of-the-arts -> state-of-the-art",
            "- - The discussion following Theorem 3.9 is very hard to digest for a reader who hasn\u2019t spent as much time as the authors thinking about this problem. I\u2019d encourage the authors to prune the text, retaining only the essential message (which presumably is what\u2019s written in italics) and moving other information to the Appendix.\n- Overall, the theoretical claims and experiments are not astonishing as one would perhaps expect that pruning should precede quantization.\n- The theoretical contribution is quite limited as it only holds for magnitude-based pruning (without fine tuning) and block-wise quantization. Importantly, magnitude pruning has gone out of fashion in the context of LLMs because it requires costly fine-tuning to recover model performance and is outperformed by methods like SparseGPT and WANDA when fine-tuning is not performed. The authors mention in the Appendix that, empirically, the order had less of an impact for WANDA and SparseGPT. \n- The experiments seem to be quite orthogonal to the theoretical results. By employing fine-tuning for all the experiments, the authors are making their original theoretical proofs/derivations inapplicable in the context of the experiments as the derivations are based on errors calculated when no fine-tuning is applied. \n- Proof of Theorem 3.5: Only show equality is attained for L1 norm and not all norms. Is it clear that this implies that equality is also achieved for all other norms? Statement of Theorem or proof should be modified to address this. \n- Proof of Theorem 3.6 is only a counter-example for the L1 norm. Is it immediate that the theorem is true in general for norms beyond the L1 norm? Either the statement of the theorem or the proof should be modified to address this.\n- Throughout the paper, some statements are true for all norms, others are only shown for the L1 norm, and then the empirical experiments utilize the L2 norm for measuring errors. \n- The generalization of orthogonality in Definition 3.8 is not clear to me as functions are now being applied coordinate-wise. Is the composition only permitted to happen in one coordinate (similar to in Theorem 3.9). It might be worth it to explicitly write out the definition as the lack of an explicit definition of orthogonality also makes the statement of Theorem 3.9 confusing."
        ]
    },
    "wHebuIb6IH": {
        "venue": "ICLR 2025",
        "title": "VLMaterial: Procedural Material Generation with Large Vision-Language Models",
        "link": "https://openreview.net/forum?id=wHebuIb6IH",
        "abstract": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another pre-trained large language model (LLM). Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6
        ],
        "strengths": [
            "- - Personally, I always enjoy seeing papers that show \"foundation models work on X\". Even if there is minimal contribution architecture and algorithm wise, it still provide me with insight about what kind of tasks can benefit from large models. This paper adds even more evidence about the viability of applying LLM/VLMs to procedural generation (and graphics in general), and I find this message important.\n- Beyond the rather straightforward application of LLM + finetuning, considerable works as also being done here for creating the finetuning data. There are valuable insights on how data is cleaned and augmented, and the LLM based approach for creating new samples from two examples seem generally applicable for anything that involve visual programs. The dataset itself can also be immensely useful, especially when the alternative is not publicly/easily accessible.\n- Solid insights on how to work with procedural material graphs that a not differentiable. The MCMC based approach is reasonable, appears to be working from the ablations and examples. It is also nice (and a bit nolstalgic) to add some exposure to a classic set of optimization methods and show that they are very viable in certain cases.\n- The results look good. I do have a good amount of issues with some results but overall, it does appear to be that there are cases where the proposed method works clearly better than alternatives. Ablation is also solid and effective.",
            "- * The paper is well-written and motivated. All things are explained clearly. \n* The work aims to address a significant issue of directly generating a material graph from an input image as this is highly desriable in CG pipelines. Compared to existing work such as MatFormer and Conditional MatFormer, the results are superior. This is also proven quantitaively in Table 1 against baselines.\n* The method creates a large datasset by performing augmentation using an LLM which picks 2 sample programs and then creates a distinct program. \n* The VLM in the loop to generate a dataset and also synthesize programs after fine tuning is simple but novel in material generation. \n* The ablation results are strong. A test for program correctness is also included.",
            "- + This paper proposes a straightforward pipeline for procedural material generation. It trains a domain-specific Vision-Language Model (VLM) through meticulous data collection, processing, and fine-tuning, followed by effective post-processing techniques to address the problem.\n+ Both data augmentation and MCMC-based post-processing are validated through qualitative and quantitative results. \n+ Using VLM to tackle graphics problems is a promising and intriguing area for exploration, with potential applications across a wide range of domains. \n+ The results appear robust, surpassing all baselines, including MatFormer and LLM-based approaches, in both quantitative and qualitative evaluations.",
            "- 1. Recasting procedural material generation as a vision-language task, effectively combining visual perception with the generation of structured, editable material programs.\n2. Fine-tuning a VLM specifically for procedural material generation, a domain that was previously not widely explored in the vision-language field.\n3. Introducing a dataset of 550,000 procedural material samples, which includes not only real-world data but also creatively synthesized samples generated via program-level and parameter-level augmentations. This contribution provides a foundational dataset for further research in this area.\n4. Evaluation on both synthetic and real-world images shows that VLMaterial outperforms baselines like BlenderAlchemy and Conditional MatFormer in metrics such as style loss, SWD, and program correctness, demonstrating improvements in visual fidelity and program accuracy."
        ],
        "weaknesses": [
            "- - Also I do appreciate the message that \"VLM works for procedural materials\", arguably the novelty is limited. This is one of the lower hanging fruits, and a good amount of work is engineering-centric, making the impact of this work potentially limited. It probably also makes this paper less suitable for a venue like ICLR, since there isn't too much contribution learning-wise. I would definitely love to see this at a graphics venue a bit more.\n- Following the previous point - even for paper that's mostly \"apply VLM to X\", the amount of insights in the paper is still on the relatively low end. Most of the discussion is around the data. They are important, and I do consider the data portion a strength. However, I feel that there's missed opporunities here in further invesgitating how to best apply VLMs to this problem, especially since the supervision is entirely token level, without visual feedback (comparing the generated material to the input). E.g. there are many known limitations with VLM/LLMs that make them not perfectly suitable for directly outputing complex visual programs with lots of parameters that are non-trivial to interpret on a purely language level. How to design something that alleviate such issues? Does fine tuning take care of most of it or do we need to more carefully design/modify the language and the prompts? What part of the output do the model struggle the most with? Is there a more intuitive explanation of why failure cases like those in Figure 7 happen, and what part of the output contribute the most to the discrepancy between the output material and the image (e.g. it does appear that many BSDFs are quite off?)? Discussions along these lines would be very helpful both for people who want to use this approach, or for future researchers that might build upon this.\n- My standard on quality of results is definitely higher on this one, due to the rather limited amount of technical insights. And while the results look good overall, they are still quite far from matching the input image, even among the few qualitative examples provided and after the post-processing step.",
            "- * The method is 90% accurate in terms of program correctness compared to other methods which have guaranteed correctness. Have the authors explored methods to reduce LLM hallucination to improve correctness?\n* It would good to have a reference for the compution time for different methods",
            "- + Adapting VLM as a tool for material code generation may not be entirely reasonable, as LLaVa primarily addresses natural images rather than focusing on code generation. It is important for the network design to account for these biases.\n\n+ Additionally, could you provide a detailed user study for artist? Is it possible for this AI tool to substitute certain steps in the artistic creation process?",
            "- 1. Limited Novelty in Augmentation Techniques: While the paper presents a large dataset with program-level and parameter-level augmentation, the augmentation techniques themselves rely on GPT-based models and parameter variations, which may not fully capture the variety found in real-world material designs. \n2. Despite its strong performance, the model struggles with highly intricate textures, where certain details are either lost or inaccurately represented, as shown in Figure 7. Furthermore, in Table 3, this method underperforms compared to other approaches on out-of-distribution datasets (Substance and real images) when operating under a more constrained sample budget. This limitation may affect its applicability in scenarios that demand high precision.\n3. The paper\u2019s evaluation relies heavily on quantitative metrics and in-distribution and out-of-distribution tests. However, given the subjective and artistic nature of material design, the paper would benefit from user studies or feedback from professional material artists. Expert insights could help assess aspects like the usability, editability, and practical value of the generated materials in real production workflows."
        ]
    },
    "vRvVVb0NAz": {
        "venue": "ICLR 2025",
        "title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
        "link": "https://openreview.net/forum?id=vRvVVb0NAz",
        "abstract": "Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            6,
            8,
            8
        ],
        "strengths": [
            "- - The paper is very well-written and easy to follow.\n- It provides a guideline for when and why task arithmetic works in multi-task learning, machine unlearning, and generalization to new tasks.\n- The discussion of low-rank approximations and magnitude-based pruning of task vectors supports the use of efficient approximation techniques in task arithmetic fine-tuning.\n- This is the first known theoretical generalization analysis of task vector arithmetic in nonlinear Transformer-based models, filling a notable gap in the literature.\n- The theoretical claims are validated through empirical experiments on the Phi-1.5 language model and Colored-MNIST image classification, adding practical credibility to the proposed framework.",
            "- * Very comprehensive mathematical analysis and theoretical proofs\n* Discussion on the task vector is extensive.",
            "- - This paper is the first to theoretically examine the generalization of task vectors, filling a significant gap in current research.\n- Writing is well-written and easy to follow.\n- The theoretical contributions are well-supported by experiments, effectively bridging theory and empirical validation.\n- The theoretical insights align well with intuitive expectations regarding the effects of task vectors across aligned, irrelevant, and contradictory tasks.",
            "- - Very exciting theoretical result that includes very practical aspects (e.g., hyperparameters of the fine-tuning, how to set alpha for each task vectors, etc)\n- Novel characterization of generalization conditions on 1 layer transformer, building up on previous work that uses NTK assumptions\n- Practical scenarios in terms of the relation between tasks (aligned vs irrelevant vs contradictory)\n- Nicely written and relatively accessible to non-theory people. I particularly like the remark after each theorem that explains what the theory is and what does it imply\n- Nice setup on CMNIST that reflects the aligned vs. irrelevant vs. contradictory condition, followed by a nice non-toy experiment."
        ],
        "weaknesses": [
            "- - The theoretical analysis relies on a single-head, one-layer Transformer model, which may limit the applicability of the results to more complex multi-layer Transformer architectures.\n- While the empirical validation includes a large language model and a basic image classification task, the study could benefit from a broader set of tasks, including more complex or structured tasks beyond binary classification.\n- Although the theoretical framework outlines conditions for selecting arithmetic coefficients, more practical guidelines or analyses for tuning these coefficients in real-world applications would be beneficial.\n\ntypos:\n- line 288, \"fine-turning\" -> \"fine-tuning\"\n- line 388, \"are are\" -> \"are\"",
            "- * There are some issues with the paper's writing (Formula 4 and Definition 2 in the Section 3.2 is confusing).\n* In the language generation task, only a model with 1.5B parameters is used, and the experimental results are not fully meet expectations (also a noticeable performance loss in so-called irrelevant task).",
            "- - Although insightful, the data model may be overly simplistic for capturing the complexities of real-world data. For example, even for simple yes/no questions, a negation word in a sentence may flip the relevance of certain words in the sentence, which cannot be captured by the proposed data model. I wonder if some theoretical aspects can be generalized independently of this data model.\n- The analysis is restricted to a one-layer transformer with limited nonlinearity, despite claims in the title and introduction regarding the challenges of analyzing nonlinearity in task vectors.",
            "- N/A -- good paper overall :)"
        ]
    },
    "v9EjwMM55Y": {
        "venue": "ICLR 2025",
        "title": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery",
        "link": "https://openreview.net/forum?id=v9EjwMM55Y",
        "abstract": "Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-\nlearning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels\u2014atoms, substructures, and molecules\u2014via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in \u2206AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6
        ],
        "strengths": [
            "- 1.  UniMatch is the first method that integrates multiple levels of molecular structures, demonstrating outstanding performance across various tasks and sigificantly contributes to the filed of drug discovery.\n2. This paper is clearly-written and accompanied supported by well-designed figures, well done!",
            "- 1. The concept of modeling molecular structures across multiple levels is promising.\n2. The manuscript is well-written and easy to read.\n3. Extensive experiments across diverse datasets demonstrate that the model consistently outperforms baselines.",
            "- 1. The paper tackles a valuable domain-specific problem, which high societal value, with a method that can be reasonably adapted to different domains.\n2. The evaluation is extensive and clearly demonstrates the advantages of the method.\n3. The ablation study is complete and clearly demonstrates the contribution of every component in the method.",
            "- 1. The writing is clear, with well-crafted figures that enhance understanding. It also offers a discussion of limitations and detailed supplementary explanations.\n\n2. The experiments are quite extensive. The research examines the universality of the method across different network structures. It also conducts evaluations on three benchmarks. The results are convincing."
        ],
        "weaknesses": [
            "- 1. I suggest further investigating the interpretability of the model, particularly with respect to the multi-level representations. Some gradient-based methods (e.g. DeepLIFT) could be used to reveal the importance of these features.",
            "- 1. The related work section is somewhat lengthy. It might be more effective to move it to the end of the manuscript or condense it.\n2. While the rationale for modeling hierarchical structures is solid, I have concerns about the use of mean pooling. This approach may lead to a predominantly molecule-level representation, potentially losing crucial atomic and substructural details. What do the authors think?",
            "- Major weaknesses\n---\n1. The technical description of the dual matching mechanism is hard to follow and it is a disservice to an otherwise solid work.\n\n\nMinor weaknesses\n---\n_(These are minor points that do not have a direct bearing in my evaluation of the paper, but I think would improve its quality)_\n\n1. In line 075-077 and Figure 1, one of the examples provided of the importance of the multi-level representation of molecules from atomic, to whole molecule serve their purpose as an illustration, but it is not chemically correct. Hydrogen ions do not exist within the molecule, they would be referred to as hydrogen atoms. Though it is true that Hydrogens ions in a solution determine its acidity. In a molecule, the hydrogen atoms are not the determinants of acidity, but rather the atoms they are attached to. So in the first examples, the acidity of those molecules is not determined by the Hydrogen atoms but by the chlorine and oxygen atoms; and their electronegativity (or ability to hold onto the Hydrogen). Still the illustration is accurate, but the atom that should be highlighted is not the Hydrogen. In the case of the right example, it is arguable that the acidity is not only caused by the oxygen, but rather the whole substructure of H-O-S.\n2. In line 409, when describing the FS-Mol dataset, it is described as \"[a benchmark] for macromolecules (i.e., proteins)\". This is not accurate, the benchmark measures the ability of small molecules (the one that serve as input for this system) to bind to proteins. In other words, the tasks are whether the molecules can bind to specific proteins. This is an important distinction because due to their size, modelling proteins with the approach presented in this paper would be computationally infeasible.",
            "- 1. **Timeliness**:\n - The baselines and related work employed in the paper lack more recent works, especially those of recent 1 or 2 years. \n\n- The pretraining model, Pre-GNN (Hu et al., 2020), is relatively old in the context of molecular pretraining. Recent years have seen the emergence of far more powerful models. I encourage authors to consider integrating more up-to-date pretraining models to showcase the significance and practical usage of their approach. \n\nBy simple searching, I find more recent related works on Hierarchical molecular representation learning methods like \"UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning\" (ICML 2024) and \"Adapting Differential Molecular Representation with Hierarchical Prompts for Multi-label Property Prediction\" (Briefings in Bioinformatics, 2024).\n\n\n\n2. **Novelty**:\n\nThe hierarchical molecular representation extraction concept is commonly seen in the literature. Additionally, the paper employs standard attention and meta-learning methods. I'm concerned that this may not meet the novelty standards of top conferences."
        ]
    },
    "uvHmnahyp1": {
        "venue": "ICLR 2025",
        "title": "SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints",
        "link": "https://openreview.net/forum?id=uvHmnahyp1",
        "abstract": "Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and buyable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6,
            8
        ],
        "strengths": [
            "- * The background and related work section was strong, and covered the previous work in this domain well.\n* The paper, including the appendix, was well-structured and easy to follow.\n* There are more strengths, but I mention them below amongst some of the specific questions I had, to put the questions in context.",
            "- 1. The use of the GFlowNet improves both diversity and high reward modes compared to other optimization methods such as reinforcement learning.\n2. The proposed backward policy ensured that backward-constructed trajectories belong to the GFlowNet MDP. \n3. Various benchmark and ablation studies were performed to ensure the strategies of the proposed framework.",
            "- - Focus on Synthetic Accessibility: SynFlowNet adopts a framework that effectively addresses a key limitation in generative models for drug design by ensuring that generated molecules are synthetically feasible, which is crucial for practical drug development applications.\n- Diverse and Optimized Molecule Sampling: Leveraging GFlowNets, SynFlowNet provides a balanced sampling process that maximizes both reward and structural diversity, effectively mitigating the typical mode collapse encountered in reinforcement learning-based models.\n- Validation through Synthetic Accessibility Metrics: In silico experiments demonstrate SynFlowNet\u2019s advantage over fragment-based approaches, particularly in terms of synthesizability, underscoring the practical benefits of synthesis-based molecule generation for drug discovery.",
            "- 1. Novel Backward Policy of GFlowNet training: SynFlowNet proposes a novel learning strategy for backward policy to ensure the directed acyclic graph and facilitate synthesizable molecules.\n2. Connection to Wet-Lab Experiments: The case study in Section 4.6 provides a clear demonstration of how to connect wet-lab experiments and *in silico* drug design, highlighting the potential real-world applicability of the method.\n3. Comprehensive Ablation Studies: The paper provides ablation studies for many design choices."
        ],
        "weaknesses": [
            "- * I searched throughout the paper and in the submission, and could not find any code to accompany the paper. Given that the authors lamented the lack of publicly-available code for two previous works in their paper, I would have expected a greater emphasis on publishing the code here, as it would also help with the reproducibility of this work. To me this is unacceptable for a computer science conference submission in 2024, so please let me know if I simply missed it, and I can raise my score (I ranked it below the acceptance threshold because of this, and would update my score if the code was made available as it was overall a good paper).\n* Throughout the paper, the authors repeat that one of the key advantages of SynFlowNet is its ability to generate more diverse candidates than other methods (namely, REINVENT and SyntheMol). However, I argue that diversity is only a meaningful metric in molecular generative models if the types of molecules being generated are from the accessible molecular space; if the generated molecules are diverse but not accessible, then the model is not very practically useful. That being said, it was good the the authors evaluated synthesizability using AiZynthFinder, but an \"easy\" experiment that the authors could have also done that would add a lot of value is to consider molecular rediscovery benchmarks. That is, can the model also find known, previously-synthesized binders; celecoxib rediscovery is a widely-used standard benchmark for this, but there are also more challenging targets. Such rediscovery metrics are a nice alternative to experimental validation which is not necessarily possible on the review timeline. To summarize, I would like to know that SynFlowNet can generate diverse and *known* synthesizable molecules, not just diverse *predicted* synthesizable molecules, because at the end of the day CASP tools are not a replacement for experiment.\n* I found the lack of comparison to other methods for synthesizability-constrained molecular generation a weakness. The authors mentioned that the models of Gottipati and Horwood & Noutahi were not publicly available, and thus focused on comparison to SyntheMol and REINVENT. While these comparisons were interesting and certainly well-motivated, I would have also liked to know how SynFlowNet compares to other synthesizability-constrained molecular generation models which *do* have publicly available codebases, like MoleculeChef or SynNet. As it stands, we do not know which model is best, because SyntheMol has not done any comparisons to those models either.\n* In the Appendix, the authors state that 300K oracle calls were used both to train REINVENT and SynFlowNet, however, this is an extreme number of oracle calls; can the authors justify this? REINVENT demonstrates good performance after only 10K oracle calls for most objectives, so are 300K oracle calls simply what is needed for good performance in SynFlowNet? I would like to see the analogous experiments and comparisons/results with only 10K oracle calls, which is the standard number of oracle calls used for comparing molecular generative models.",
            "- 1. The conclusions of some experiments are arguable (see the questions below), which need to be justfied probably with additional experiments for clarity.  \n2. Increasing the number of BBs slows down training linearly that may cause a computational burden when dealing with a large amount of BBs.",
            "- Limited Testing on Optimization Tasks: While SynFlowNet showcases improvements in SeH proxy, its optimization abilities are not fully tested against established molecular optimization benchmarks. Incorporating PMO benchmark tasks and directly comparing results with other generative baselines, such as those in Gao et al. (2022) [1], could provide a more comprehensive evaluation of SynFlowNet\u2019s optimization performance.\n\n### Reference\n[1] Gao, Wenhao, et al. \"Sample efficiency matters: a benchmark for practical molecular optimization.\" Advances in neural information processing systems 35 (2022): 21342-21357.",
            "- **Overall Comment**\n\nI acknowledge the novelty of this work in successfully adapting the GFlowNet framework for synthesizable molecular design, but there are several critical aspects of the current manuscript that need to be addressed.\nI think this manuscript may be suitable for acceptance after revision, and I\u2019m positive about changing the scores during the rebuttal period.\n\n*\\* The issues are sorted in the order they appeared in the manuscript.*\n\n**Issue 1: Lack of novelty / appropriate citations for the major contribution.** Page 2, Line 103, Contribution 3\n\nOne major concern is that the manuscript does not contain appropriate citations for this major contribution.\nTo use 200k building blocks, the authors used action embedding [1], which has already been employed in existing synthesis-oriented molecular generative models [2-4].\nIn particular, Koziarski et al. [4] introduced action embedding into GFlowNet and investigated its effectiveness in large action spaces.\nWhile the authors use a different fingerprint representation (Morgan fingerprints instead of MACCS fingerprints), Gottipati et al. [2] have already evaluated various representations, including both fingerprints.\nAuthors should highlight the novel differences from existing approaches or provide appropriate citations.\n\n**Issue 2: Lack of experimental details.** Section 4.1 and Section 4.5\n- In Section 4.1, what is the rewarding strategy used for multi-objective optimization (FragGFN SA, SynFlowNet SA, SynFlowNet QED)? The possible reward structures could be Multi-objective GFlowNet (MOGFN) [5] or multiplication-based rewards [6].\n- In Section 4.5, I couldn't find the experimental details. The related section is on Page 19, Section A.5, but it does not provide crucial information about this experiment. I suggest to include experimental details and mention it like Section 4.6.\n  - Reward Function: What reward function was used in the ablation studies? Is it sEH?\n  - Average Times: Clarify what *average times* refers to\u2013e.g., runtime per batch or runtime per molecule.\n  - For Figure 7 D-F, entire building block set is used?\n\n**Issue 3: Fairness of evaluation metric.** Page 8, Figure 6\n\nThe improvement in the reward/diversity trade-off is impressive, but I have questions about the metric, **Reward/Novelty trade-off**.\nRegarding the Reward/Novelty trade-off, it is possible that the improvement in novelty is due to the use of the Enamine building blocks rather than the effectiveness of well-structured generative model architecture.\nIn this case, the REINVENT's high similarity to ChEMBL would be greatly reduced by simply retraining it on Enamine REAL or ZINC.\n\nFor a fair comparison in terms of novelty, the comparison should be performed on the same exploration space:\n- Train REINVENT on Enamine REAL.\n- Use ChEMBL-like building blocks, such as blocks obtained from the retrosynthetic decomposition [3] of ChEMBL molecules.\n\n**Issue 4: Limited comparative study.** Sections 4.2 and 4.3\n\nThe comparison between the proposed method and baselines was performed only on proxy models.\nI recognize that proxy models such as sEH [7] are widely used to evaluate the effectiveness of optimization strategies in the optimization community [8], so I think this is a minor issue.\nHowever, I have concerns about the appropriateness of evaluating and comparing the performance of discovering novel molecules based on proxy models trained on a restricted and known chemical space.\n\n**Issue 5: Restricted scalability to a large building block set.** section 4.5\n\nI definitely agree with the argument for employing an extensive building block (BB) set to explore the vast chemical space.\nI believe that it can mitigate the reward-diversity trade-off by expanding the sample space.\nHowever, I concern that the current experimental results do not seem to provide sufficient evidence about this argument and the scalability of SynFlowNet.\n\nThe authors demonstrated that a larger BB set yielded a higher selection of unique BBs (Figure 7B), but this has a negative impact on the average reward (Figure 7A) and speed (Figure 7C).\nConsequently, the current findings seem to emphasize the negative effects of the usage of extensive BB sets on the quality and throughput for generation.\nTo justify the significance of utilization of unique BBs, I suggest including structural diversity metrics for generated samples, such as the number of unique Bemis-Murcko scaffolds or the average pairwise Tanimoto distance.\n\n---\n**Reference**\n1. Dulac-Arnold, Gabriel, et al. \"Deep reinforcement learning in large discrete action spaces.\" arXiv preprint arXiv:1512.07679 (2015).\n2. Gottipati, Sai Krishna, et al. \"Learning to navigate the synthetically accessible chemical space using reinforcement learning.\" International conference on machine learning. PMLR, 2020.\n3. Seo, Seonghwan, Jaechang Lim, and Woo Youn Kim. \"Molecular generative model via retrosynthetically prepared chemical building block assembly.\" Advanced Science 10.8 (2023): 2206674.\n4. Koziarski, Micha\u0142, et al. \"RGFN: Synthesizable Molecular Generation Using GFlowNets.\" ICML'24 Workshop ML for Life and Material Science: From Theory to Industry Applications (2024).\n5. Jain, Moksh, et al. \"Multi-objective gflownets.\" International conference on machine learning. PMLR, 2023.\n6. Lee, Seul, Jaehyeong Jo, and Sung Ju Hwang. \"Exploring chemical space with score-based out-of-distribution generation.\" International Conference on Machine Learning. PMLR, 2023.\n7. Bengio, Emmanuel, et al. \"Flow network based generative models for non-iterative diverse candidate generation.\" Advances in Neural Information Processing Systems 34 (2021): 27381-27394.\n8. Gao, Wenhao, et al. \"Sample efficiency matters: a benchmark for practical molecular optimization.\" Advances in neural information processing systems 35 (2022): 21342-21357."
        ]
    },
    "uuriavczkL": {
        "venue": "ICLR 2025",
        "title": "Counterfactual Realizability",
        "link": "https://openreview.net/forum?id=uuriavczkL",
        "abstract": "It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the *Pearl Causal Hierarchy*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of realizability, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            10,
            6
        ],
        "strengths": [
            "- As the paper points out, the counterfactual randomziation procedure in itself has been alluded to in previous work. But, the main contribution of this paper is a characterization of when a counterfactual distribution can be 'realized'. I liked that the paper operates under this structure for sampling distributions at different levels of the causal hierarchy by the notion of actions. Since counterfactuals are commonplace in many applications and a major hurdle is their non-identifiabilty, this work is certainly significant. The paper is well written and padded with many examples to illustrate the applicability and importance of the procedure.",
            "- The paper tackles a relevant problem, and covers all key bases to provide its key contribution. \n\nSampling from counterfactual distributions is naturally hard, and developing methods and tools to do so is important, as suggested in the paper.",
            "- The authors' contributions are original and significant. Their insights are laid out clearly and limitations and discussion of future work are provided. \n\nThe authors show the advantages of counterfactual experiments with intuitive examples. The authors show that counterfactual fairness measures that are not identified through interventions can be realized (and thus identified) through counterfactual experiments. Furthermore, the authors show that bandit policies which allow for counterfactual mediators dominate policies from the classical causal bandits setting.",
            "- Originality/Quality:\nThe definition of ctf-randomization and the algorithm for checking the counterfactual realizability are new in the field of causality. Moreover, the authors discussed the applications of $\\mathcal{L}_3$-based methods in fairness analysis and bandit problems which might be interesting for people in other fields.\n\nClarity: The writing should be improved as some of the parts need more explanations.\n\nSignificance: Although the concept of counterfactual realizability is new in the field, it is not completely clear to what extent such ctf-randomization can be implemented in reality."
        ],
        "weaknesses": [
            "- As the paper also mentions, I believe the idea for intervening on perceptions was known before and makes multiple informal appearances. A few other references are (S. Geneletti, A.P. David, Defining and identifying the effect of treatment on the treated, 2010). Also, the notion of SWIGs (Richardson and Robins, 2013) is quite similar. I believe a more thorough comparison with previous work is needed to make the paper more complete.",
            "- I am not an expert in this field, so take it with a grain of salt:\n\nAs written by the authors \"representing counterfactual distri- butions, is believed to be inaccessible almost by definition.\"\n\nReferences are provided for e.g. the 'counterfactual randomisation procedure', i.e. attempts at sampling from CFs, but there is not much discussion about why it is 'believed' why CFs are inaccessible by definition.\n\n\"inaccessible almost by definition\" is not precise enough to be useful for a reader, \"almost\" needs to be refined and explicated, ideally both via intuition and formally, even just briefly.\n\nIn the conclusion, this rigour is missing, too, when the authors state: \"Countering prevalent belief\". A reader needs references to be able to follow the thoughts and verify by themselves, that indeed, the venture of CFs sampling is hard (or impossible?)\n\nIf CFs are not in fact \"\"inaccessible almost by definition\"\", then this paper of course would constitute a huge contribution to the field of causal inference in which case this should be highlighted more.",
            "- I do have few substantial weaknesses to discuss. In the Questions section I voice a slight disagreement about the interpretation of ''physically possible'' experiments. Below are some minor comments.\n\n(Lines 105--107) Presumably W_* is a tuple of potential outcomes and w is a vector over the support of W_*. These should be defined formally to avoid confusion. I suspect there is a typo on line 107 when defining P^M---perhaps taking the product over t = 1 to |W_*| would clear this up. \n\n(Lines 120--121) Read (V)^(i) does not ''measure'' the effect of f_V on V, these mechanisms are in general not identified. This should be reworded---perhaps the physical actions should be defined formally.\n\n(Figure 2) Given the discussion in Example 1, it may be more natural to include latent confounding between X and Z in the DAGs.",
            "- The content of the paper is fairly dense and the authors did not have enough space to give a sketch of the proofs or even CTF-REALIZE algorithm is not described very well in the paper (For instance, it is not clear why there is a rejection sampling in line 18 of Algorithm 1). This also applies to the results in Section 4. Although the authors showed in some specific scenarios, $\\mathcal{L}_3$-based methods perform better than $\\mathcal{L}_2$-based methods, there is no intuition given in the paper why this is the case. One might wonder if the causal graphs or the causal mechanism are meticulously engineered to show the advantage of counterfactual reasoning."
        ]
    },
    "ujpAYpFDEA": {
        "venue": "ICLR 2025",
        "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
        "link": "https://openreview.net/forum?id=ujpAYpFDEA",
        "abstract": "Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. \n    However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services.\n    This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, \n    while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. \n    Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6,
            8
        ],
        "strengths": [
            "- 1 This work is the first study on the imperceptibility of watermarked LLMs. \n2 This paper is well organized and written, make it easy to follow.\n3 The experiments are conducted across different LLMs and with different sampling methods and temperature settings. The conclusion and discussion based on the evaluation results are clear.",
            "- 1. It is reasonable to use repeated sampling to detect whether an LLM has been watermarked. \n2. Prompts have been designed to reveal the connection between generated text and watermark keys in a black-box setting.\n3. The evaluation is comprehensive, and many different watermarking methods have been tested. Experimental results also show that the proposed method works.",
            "- - **Accuracy and Robustness**: The experimental results demonstrate the high accuracy and robustness of the Water-Probe algorithm across various LLMs, watermarking methods, and generation settings. The low false positive rate for non-watermarked LLMs further strengthens the algorithm\u2019s reliability.\n- **Practical Solution**: The proposed Water-Bag strategy offers a practical solution to improve the imperceptibility of watermarks, which is a critical concern for LLM providers.",
            "- The paper identifies watermarks in a black-box setting. Also, it proposes an effective method to improve watermark imperceptibility."
        ],
        "weaknesses": [
            "- The threat model should be further described, especially in terms of the prior knowledge  assumptions of the detector.",
            "- 1. Symbol reuse. In Section 3 and Section 4, the symbol $P$ represents both the model distribution in $P_M$ and the prompt in $P_1, P_2, ..., P_N$, which can be confusing. Too many $P$ across line 222 to line 227.\n2. Line 289, Figure 2. \u2018KTH\u2019 does not refer to anything else in other contexts. Is it Exp-Edit or ITS-Edit?",
            "- - **Concept Confusion**: The manuscript mislead concept of watermarking and fingerprinting, the proposed method should be categorized into fingerprinting instead of watermarking. see Question 1.\n- **Lack of Controbution**: The authors inputs prompts to see the response of watermarked LLM and non-watermarked LLM, which is so called identification algorithm. The manuscript just defeines some concept, samples prompt to see similarity of the inspected models. \n- **Limited Scope of Water-Probe**: The Water-Probe algorithm might be limited in its application to only certain types of watermarking schemes. The paper does not sufficiently explore how the algorithm would perform against a wider range of watermarking techniques, especially those that are more sophisticated or less predictable.",
            "- The primary concern I have with the paper is the similarity of distribution differences between prompts in the no-watermark case. First, cosine similarity measures the similarity between two non-zero vectors; thus, the equation Sim(0, 0) in the proof in Appendix B is confusing. Second, if we examine the distances of the elements directly rather than using the Sim function, the distances of the elements in the no-watermark case are approximately zero, suggesting high similarity.\n\nThe selection of prompts appears empirical. The paper could be strengthened by providing guidance on generating prompts that produce similar output distributions."
        ]
    },
    "ztzZDzgfrh": {
        "venue": "ICLR 2025",
        "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
        "link": "https://openreview.net/forum?id=ztzZDzgfrh",
        "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the **Knowledge FFNs** in LLMs overemphasize parametric knowledge in the residual stream, while **Copying Heads** fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose **ReDeEP**, a novel method that detects hallucinations by decoupling LLM\u2019s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6
        ],
        "strengths": [
            "- - Authors provide a straightforward method to detect hallucinations in RAGs that does not require model fine-tuning.\n- Empirical results provided by the authors look good.",
            "- Each step is thoughtfully motivated, with both conceptual reasoning and empirical validations in \u00a73. The detection method shows effective results in Table 1, and the RAG truthfulness improves using AARF, as shown in Figure 6.",
            "- 1. The development of the ECS and PKS metrics to understand the contributions external and internal knowledge have on the LLM's generation is a compelling and novel way to understand LLM outputs. \n\n2. They demonstrated great empirical validation by running extensive experiments across two datasets, three LLMs, and many baseline methods. \n\n3. They also introduce a method to curb hallucinations called AARF - which relates back to the introduced metrics nicely."
        ],
        "weaknesses": [
            "- ## Lack of justification for PKS and ECS\n\n### No PKS justification\nAlthough PKS is correlated with a hallucination label (line 319) there is still no guarantee that it is adding parametric knowledge. Since you do not provide any theoretical justification for this score, at least an empirical justification is needed. You can run a simple experiment: use LogitLensed outputs before FFN as final outputs and check whether it removes the parametric knowledge bias using some of the setups for that, for example, the one from [1] (they study it through the prism of the phenomenon you encounter in RQ3 and Appendix E).\n\n### Questionable ECS justification\nContrary to the PKS the authors provided empirical justification for the ECS measuring model reliance on context, however, I find it not very convincing so far.\n\nFirst of all, I do not see how the ratio of attention head attending vs mis-attending justifies ECS. It would make more sense to me if you provided such a ratio for mulitple different values of ECS and observed that the higher the ECS the more often a model attends.\n\nSecondly, I am not sure that ratio of attending is computed correctly. As far as I understood for LLama-7B you take hallucinated response (which means that it contradicts external context) and the most attended span in external context. Then you ask gpt-4o to evaluate whether this span supports existence of a conflict in response or not. If that is the case, I do not understand why this experiment shows whether the model attends (the attention span contains part of the context needed for the correct answer) or mis-attends. If attention span supports the existence of a conflict in response it might still not be relevant for the correct response itself, which means a conflict exists but we can not call it a hallucination according to your definition (hallucination = response is contradicting the context or is not supported by it - line 72).\n\nPlease correct me if I misunderstood the experiment setting, what is meant by attending, or the way attending and mis-attending is computed.\n\n## Too many hyperparameters\nI am afraid that the proposed hallucination detection method is not applicable in practice as it requires a lot of manual hyperparameter tuning. According to the provided values, they all are different per dataset and model (see Appendix I). They include:\n\n- top k % for ECS \n- top k % for PKS\n- tau threshold for H - page 8 bottom\n- alpha and beta for reweighting page 9 top\n- chunk size for the chunked version of REDEEP\n\nI suggest that the authors discuss strategies for automating hyperparameter selection or provide guidelines for choosing these parameters in real-world applications.\n\n## Insufficient experiments\n\n### Hallucination detection experiment\n- For RagTruth dataset there exist baselines provided by the original paper [2] which perform better than all the baselines considered by you, could you please include them? E.g. Baseline LLama2-13B results fine-tuned on RagTruth have 78.7 F1, see Table 5 in [2] vs yours 78.3 in Table 1. I think the comparison makes a lot of sense since you tune many hyperparams using RagTruth validation dataset while you could simply fine-tune that baseline on the same data instead.\n- Same comes for Dolly dataset, please include results for AlignScore and RepC-LE-nn-n2000-e2000 that have 84 and 86 accuracy correspondigly, while the best method provided by you scored 73.73 (LLama2-7B).\n- Please also provide results for the Noisy Context split from Dolly [3] dataset because it better approximates realistic RAG application scenario. \n\n### Causal experiment\n\n- First of all, I don\u2019t see how a higher NLL difference for the experimental group than for the control group shows a causal relation between hallucinations occurrence and copying heads neglecting necessary knowledge, could you please elaborate?\n- The experiment results are very noisy and it is hard to draw any conclusions from them, for example, boxplot of the experimental group is fully contained within the boxplot of the control group in Figure 5 (b). \n- It is not clear how many heads are within experimental and control groups, it can be the case that loss changes are bigger for the experimental group simply because it intervenes in more heads.\n\n### Hallucination generation experiment\n\nPrompt for truthfulness (Appendix L) creates bias, since GPT-4o knows which answer belongs to the baseline and which to AARF. It can influence its answers since usually in scientific papers named methods outperform baselines, which must have been the case on chatgpt training data as well and possibly created such a bias. \n\nInstead, it would be nice to see the results for prompts that contain anonymous names (e.g. model 1 and model 2 instead of baseline and AARF) to avoid the mentioned naming bias and have a randomly shuffled order of AARF and Baseline inputs before showing to GPT-4o to avoid positional bias.\n\n### Lack of sensitivity experiments\nPlease provide sensitivity experiments to the numerous hyperparameters you introduced (see the section \"Too many hyperparameters\" for the hyperparameters)\n\n## Unclear writing\n- While being core concepts of the paper, Copying Heads (set A) Knowledge FFNs (set F) are not formally defined (line 381). I guess set A is built by taking top-k attention heads after sorting them by ECS while set B is built by taking top-k FFNs after sorting them by PKS, but I could not find it in text.\n- Strange ordering equations, for example, Eq. 2 that defines an important part of ECS has an undefined value \u201ca\u201d which is only introduced in Appendix Eq. 8.\n\n## Typos\n455: REDEPE\n\n## References\n\n[1] Kortukov, E., Rubinstein, A., Nguyen, E., & Oh, S.J. (2024). Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents.\n\n[2] Wu, Y., Zhu, J., Xu, S., Shum, K., Niu, C., Zhong, R., Song, J., & Zhang, T. (2023). RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. Annual Meeting of the Association for Computational Linguistics.\n\n[3] Hu, X., Ru, D., Qiu, L., Guo, Q., Zhang, T., Xu, Y., Luo, Y., Liu, P., Zhang, Y., & Zhang, Z. (2024). RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models.\u00a0ArXiv, abs/2405.14486.",
            "- Figure 3 is problematic. The starting point and flow of the diagram are unclear, with too many arrows, making it hard to identify the main computational paths. An effective graphic would show one main data processing pipeline, which is missing here. Additionally, the quantities computed are not well-defined. Panels (b) and (c) add no extra information and could be removed without loss.\n\nOtherwise, rather minor points:\n- l.281: Please describe the number of hallucinations and non-hallucinations (h = 0 and h = 1) in the evaluation set.\n- Pearson's Correlation in \u00a73: Why measure Pearson\u2019s correlation between ECS and hallucination labels (binary)? It would be more informative to report accuracy at a fixed threshold or detection metrics such as AUROC. Similarly, for PKS and hallucination, detection metrics like AUROC would be preferable.\n- l.465: Could you clarify the criteria for selecting thresholds for accuracy, recall, and F1?\n\nEven more nits:\n- Use full names for FFN, ReDeEP, and AARF, at least in the abstract.\n- In Figure 4(c), clarify what the colour bar values represent.\n- Overall, font sizes in the figures are too small.\n- Structure in \u00a73.2 is difficult to follow. Stick to a standard structure using \\section, \\subsection, \\subsubsection, \\paragraph, etc., rather than introducing new hierarchies (boldface, underline, italics, numbering (1), (2), \u2026).",
            "- 1. Performing this analysis at the token/chunk level might limit its practicality in real time or large scale settings - it would be nice to have a richer discussion of the trade-offs and real world feasibility. \n\n2. The experiments are extensive - however they are all with the LLama family of models - testing (even a much smaller set) on a different model would be informative. \n\n3. While the performance of AARF seems good (Figure 6) - it would be good to see some example outputs - its unclear how this could effect the model\u2019s output in terms of coherence/writing in general."
        ]
    },
    "ws5phQki00": {
        "venue": "ICLR 2025",
        "title": "The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions",
        "link": "https://openreview.net/forum?id=ws5phQki00",
        "abstract": "Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection, while remaining interpretable and aligned with real world data. (ii) Using the synthetic data as a reference, we can improve performance even further by identifying the most informative samples in an unlabelled dataset, i.e., those samples which the stance detection model is most uncertain about and can benefit from the most. By fine-tuning with both synthetic data and the most informative samples, we surpass the performance of the baseline model that is fine-tuned on all true labels, while labelling considerably less data.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8
        ],
        "strengths": [
            "- - The paper leverages the data augmentation capabilities of LLMs to improve transformer based models which are better suited for online deployment as they are more reliable. \n- The presented method can be adapted to other text classification tasks and hence is a significant contribution.\n- It is well written and easy to follow, except for few instances mentioned in the comments.",
            "- - The proposed method is sound. I do not see any major issue with the method.\n- Although the idea of using synthetic data to augment models is not entirely new, it probably has not been widely explored for stance prediction.\n- The authors conducted extensive experiments to evaluate the method, including varying the size of the synthetic dataset, comparing with meaningful baselines, and the further experiments that compare with a LLM zero-shot baseline.",
            "- Clean experimental methodology with proper ablation studies\nGood visualization of how synthetic data aligns with real data distributions\nActually bothered to translate German political content properly instead of using Google Translate\nReasonable baseline comparisons and honest reporting of limitations\nThe SQBC approach is somewhat novel, even if not revolutionary"
        ],
        "weaknesses": [
            "- - It\u2019s possible I\u2019m missing some key context here, but I\u2019m having trouble following the ablation study in Section 5.2. To test whether the performance gains come from dataset size or the generated content itself, the authors \u201cshuffle\u201d instances, apparently misaligning the posed questions with synthetic data. If the synthetic data consists of single text instances with labels, this shuffling wouldn\u2019t seem to affect outcomes. Perhaps the authors mean they\u2019re using different proportions of synthetic data in each run while keeping the total instance count constant, but this explanation feels somewhat unclear.\n- Even though authors acknowledge this as a limitation, fine-tuning a separate model for each question doesnot seem to be a scalable approach, especially when the main motivation for the research was in line with training robust models for online deployment. \n- The X-stance dataset is described as having around 48k annotated comments on various questions. However, an overview of the dataset\u2019s statistics\u2014such as the number of comments per question\u2014would greatly enhance readability. When you mention selecting 10 questions from the test set, it would be helpful to specify how many comments correspond to each question. While I see some statistics are included in the Appendix, a high-level summary within the main text would improve clarity and context for readers.\n- Section 4.2, General setup: Please review this section for more readability. Currently, it is a bit difficult to get a picture of what models are being tested and how the methods differ between them.",
            "- - The experiments are conducted using a German dataset, but translation into and back from English is used in order for the method to work (probably because of limited German language understanding and generation capabilities of the Mistral model that is used?) There is no explanation of why the authors do not evaluate the method using an English dataset.\n- The novelty and impact of the work is still limited. (1) Using synthetic data to augment models is not new. Although applying the idea to stance prediction might be new, it is one of many NLP tasks. The way synthetic data is generated and used during training in this paper is also standard, hence there is limited technical contribution. (2) The idea of using synthetic data for active learning is very interesting and is novel based on my knowledge. However, its effectiveness is limited based on the experiments. Therefore, overall, although the work is very solid in general, its novelty and impact may not meet the standard of this conference.\n- There is room for improvement in terms of presentation. In particular, the active learning method proposed can benefit from first presenting an overview of the high-level intuition behind the method before describing the method itself.",
            "- Limited theoretical justification for why synthetic data helps beyond \"moar data good\"\nDoesn't address the entropy/diversity problem in synthetic data generation\nResults are modest (~2-3% improvements) for considerable computational overhead\nHeavy reliance on a specific dataset (X-Stance) limits generalizability claims\nThe \"active learning with synthetic data\" angle feels like two papers duct-taped together"
        ]
    },
    "wmV4cIbgl6": {
        "venue": "ICLR 2025",
        "title": "CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series",
        "link": "https://openreview.net/forum?id=wmV4cIbgl6",
        "abstract": "Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it.\nDespite this, in-the-wild evaluation of these methods is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions.\nReal-world causal structures, however, are often complex, evolving over time, non-linear, and influenced by unobserved factors, making\nit hard to decide on a proper causal discovery strategy.\nTo bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time-series data to date.\nCausalRivers features an extensive dataset on river discharge that covers the eastern German territory (666 measurement stations) and the state of Bavaria (494  measurement stations).\nIt spans the years 2019 to 2023 with a 15-minute temporal resolution.\nFurther, we provide additional data from a flood around the Elbe River, as an event with a pronounced distributional shift.\nLeveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany).\nThese graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings.\nTo demonstrate the utility of CausalRivers, we evaluate several causal discovery approaches through a set of experiments to identify areas for improvement.\nCausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods.\nBesides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time-series forecasting and anomaly detection.\nBased on this, we hope to push benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6
        ],
        "strengths": [
            "- - The dataset is interesting and improves over existing benchmarks both in terms of size and resolution. \n-  The authors provide a reliable baseline for the causal relations in the dataset. \n-  The paper is well written and easy to follow. \n-  The authors provide an extensive set of experimental baselines along with multiple software tools for analysing and processing the dataset.  \n- The authors commit to publishing the full pipeline used to construct the data set ensuring reproducibility.",
            "- The paper\u2019s main strength is the extensive, realistic testing of causal discovery methods in the wild, which provides valuable insights into performance in a complex problem. CausalRivers supports various graph structures and sampling techniques, making it adaptable for different scenarios. It is obvious that benchmarks are important, and CausalRivers\u2019 advanced methods allow for comprehensive performance comparisons across various scenarios.",
            "- This paper presents a large-scale benchmark dataset for causal discovery to estimate causal relationships between multivariate time series. The dataset provides an unprecedented scale of ground-truth data for causal discovery in time series, and it is expected to contribute to the advancement of research in this area significantly. Furthermore, the benchmark experiments conducted using this dataset revealed that a simple baseline method outperformed many other causal discovery techniques, offering important insights into the field, which has traditionally relied on evaluations using artificial data or simpler real-world datasets."
        ],
        "weaknesses": [
            "- - The authors could have discussed the completeness and reliability of the dataset, such as data quality checks and missing data handling more extensively. \n- Allowing the reviewers access the dataset and software resources would have been beneficial but is understandable given the nature of the article. \n- Although the authors mention that the data is compiled the from multiple sources these are not specified in the paper, providing a list of specific data sources would enhance transparency and reproducibility.",
            "- Although this paper has an intensive assessment of models, its main weakness is that it is a technical benchmark. That is, the dataset\u2019s complexity, along with various methods, may present implementation challenges for causal discovery researchers, and thus, it offers scientific opportunities. However, the scientific insight from the paper is missing (unless ICLR has changed its structure and now accepts also non-research but technical contributions, in which case this paper would be a good fit)",
            "- The benchmark dataset provided in this paper is of unprecedented scale and will undoubtedly contribute significantly to the field of causal discovery. However, the task may be somewhat simplified because the dataset is tied to geographical information. Predicting causal relationships between nearby river basins is relatively easy, meaning that causal discovery might only need to focus on a limited subset of the series. This could slightly diminish the dataset's overall value. The experiments seem to focus only on causal discovery within subsets of the time series. A broader evaluation of causal discovery across all series could lead to a more in-depth discussion.\n\nAdditionally, there are a few minor typographical errors. For instance, in Section 3.2.1, the phrase \"benchmarking kid\" should be corrected to \"benchmarking kit.\" Moreover, variables with upper bars are used in the same section without explanation, and the notation should be clearly defined."
        ]
    },
    "wg3rBImn3O": {
        "venue": "ICLR 2025",
        "title": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling",
        "link": "https://openreview.net/forum?id=wg3rBImn3O",
        "abstract": "Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just $O(n\\log n)$ model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we find that Leverage SHAP achieves an approximately 50% reduction in error compared to the highly optimized implementation of Kernel SHAP in the widely used SHAP library [Lundberg & Lee, 2017].",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8
        ],
        "strengths": [
            "- Shapley values are a basic and important topic in interpretable AI and beyond, finding wide application in practice. The problem of efficiently estimating them well is a very well-motivated one. This paper makes a very nice and useful contribution to this problem. The key theoretical insight of analyzing the form of the leverage scores is simple but very clever and elegant, and allows them to make use of a very well-studied toolbox in statistics (although there is still technical work to be done). It immediately feels like the \"right\" way to solve the problem. The resulting algorithm is theoretically sound, clean, simple, as well as effective in practice. The paper is also very clearly written, with a clear description of all the relevant context as well as clear exposition in general. I did not verify all the proofs in complete detail but they seemed correct to me.",
            "- - Estimating Shapley scores accurately and efficiently is an important problem in explainable machine learning. The paper provides a theoretically principled approach for this problem.\n- The approach seems to outperform Kernel SHAP and optimized Kernel SHAP baselines in the experiments.",
            "- This paper is very well written, introduces the context of their work beautifully, and provides both a theoretical and practical contribution to the field."
        ],
        "weaknesses": [
            "- I do not see any major weaknesses. I do think would be helpful for the authors to discuss the limitations of the Leverage SHAP algorithm a bit more (e.g. does it strictly dominate all prior algorithms?), and provide some context on what still remains open in this space (see below for related questions).",
            "- - The main theoretical result (Theorem 1.1) is somewhat unsatisfactory as it does not directly compare the true and estimated Shapely values. The authors address this via Corollary 4.1, but it has a non-intuitive $\\gamma$ term which is can be large and makes the approximation guarantees weaker. Are there conditions under which $\\gamma$ is guaranteed to be small? This would better help understand the limitations of current theoretical results.\n- The experiments could include more baselines like (Jethani et al., 2021), (Mitchell et al., 2022b), and (Yang & Salman, 2019) for a more comprehensive comparison with the state-of-the-art.\n- The technical novelty in proving the new theoretical results also appears to be limited. The main result seems similar to the active learning guarantee Theorem 1.1 of Shimizu et al. ICLR 2024, and it is not clear what additional technical insights are needed to develop the current result. A discussion of novel technical insights needed to develop the current result would be helpful.\n\nTypos:\n- Line 155, Line192 finte, Line 255 contrained\n- References are incorrectly bracketed",
            "- A weakness is that it might feel niche, but as a non-specialist in interpretable AI, I cannot judge the importance of the Shapley values. If this information is important, then the author's contribution is quite important because it removes some level of heuristic thanks to their theoretical contribution."
        ]
    },
    "wXSshrxlP4": {
        "venue": "ICLR 2025",
        "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
        "link": "https://openreview.net/forum?id=wXSshrxlP4",
        "abstract": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to group 3D\npoints as objects, existing unsupervised methods are usually limited to identifying\nsimple objects like cars or their segmented objects are often inferior due to the\nlack of objectness in pretrained features. In this paper, we propose a new two-\nstage pipeline called GrabS. The core concept of our method is to learn generative\nand discriminative object-centric priors as a foundation from object datasets in the\nfirst stage, and then design an embodied agent to learn to discover multiple ob-\njects by querying against the pretrained generative priors in the second stage. We\nextensively evaluate our method on two real-world datasets and a newly created\nsynthetic dataset, demonstrating remarkable segmentation performance, clearly\nsurpassing all existing unsupervised methods.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6
        ],
        "strengths": [
            "- - The paper addresses an important problem, unsupervised object detection, since in most real-world scenarios labels are not available for training.\n- The paper is well-written and easy to follow.\n- The use of reinforcement learning is novel and an interesting idea that might be a useful tool to solve other problems on 3D scene understanding.",
            "- - The ideas presented in the paper are intuitive and make sense\n- The writing is easy to follow and describes the contributions well\n- The authors examine different contemporary learning mechanisms for their generative prior learning module, not just VAE but also diffusion \n- First teaching a network what an instance should look like, the iteratively searching the 3D space with this sort of filter to identify instances, makes total sense, is quite intriguing and well executed\n- The authors have conducted a good amount of ablations to observe different aspects of their method",
            "- - The paper is well-organized, with clear explanations in both text and diagrams for each section.\n\n- Based on a thorough analysis of prior methods and their limitations, the authors clearly articulate the motivation underlying their proposed approach.\n\n- Each module in the two-stage pipeline is technically sound to improve the unsupervised 3D instance segmentation without relying on large-scale human labels."
        ],
        "weaknesses": [
            "- Although I think some of the ideas presented in the paper might have value for the community, I believe the framing of the paper and the evaluation is not adequate, and important baselines are missing. In the following paragraphs, I list my main concerns in detail:\n\n- The method is presented as an unsupervised method. However, it relies on annotated data to train the generative model. Therefore, the method is not unsupervised, but weakly supervised, and has a greater advantage over other methods such as Unscene3D. In the paper is stated that those methods have an advantage since only the annotations of the correct class are kept, but those methods are designed to detect any object in the scene while the proposed method is trained specifically to detect a single type of object.\n\n- The reinforcement learning search of objects in the scene will stop when an object is found. To find all objects in the scene it will require several starting positions for different searches. In the paper is indicated that several searches in parallel are used during training, however, this hyperparameter is not evaluated in the paper. An ablation study of this parameter and how many initialization are need to find all objects in the scene will help the reader understand the behavior of the method better.\n\n- The reinforcement search will not be able to find all the objects in the scene in many cases. This is solved by using the objects found as pseudo-labels to train an instance segmentation model. However, this step I believe is not used for EFEM which also suffers from missing objects in the scene. This combination should be tested to show the effectiveness of the reinforcement learning algorithm. If not, we could train an instance segmentation model on the output of EFEM.\n\n- The proposed method trains a Mask3D model on the pseudo labels generated by the reinforcement learning algorithm. However, Mask3D relies on superpoints to perform the instance segmentation prediction. The same superpoints are the ones used to annotate the labels in ScanNet, which gives Mask3D an unfair advantage over other methods since Mask3D then uses the perfect boundaries of the objects. Since the proposed method is based on Mask3D, it also has the same unfair advantage, which might explain the big improvement on ScanNet.\n\n- The synthetic dataset is only evaluated against EFEM and not Unscene3D or Part2Object. These baselines should be included.\n\n- The paper fails to cite in the related work a relevant work that also used a search on the scene to perform object detection based on an object pre-trained network:\n\nFinding your (3D) center: 3D object detection using a learned loss\nD Griffiths, J Boehm, T Ritschel\nEuropean Conference on Computer Vision",
            "- - The paper investigates the sensibility of the threshold \u03b4c on 1 dataset, which is fine, but it would be interesting to know if this threshold is general or needs to be tuned individually for each dataset\n- It would be interesting to know how a successful discovery of a mask influences the next iteration of the policy network \n- In Figure 5, it says the scene in cropped and then encoded, while in section 3.3, the authors seem to argue against random cropping. In the beginning, the container-based cropping should also be close to random right? Is the idea here that the cropping will become more targeted as the container is better navigated by the policy network? Please clear up the confusion\n- What puzzles me is how the qualitative results with diffusion prior look better in some cases that the VAE based ones (Fig 6 row 1, Fig 7 row 3), but this is not reflected in the quantitative evaluation. Could you provide an intuition for this is the case? Can you also show failure cases?\n- It would be nice if acronyms like SDF would be introduced. Even though this is an established method, its still also done for acronyms like ViT=Vision Transformer.",
            "- - The experiments for each module are somewhat lacking.\nIn addition to the points mentioned below, it would be helpful to provide experiments that validate the detailed performance of each module.\n1) If the agent is indeed well-trained, it would be better to visualize the regions discovered by the agent or trajectories of the agent during exploration.\n2) While the ultimate goal of GOPS is instance segmentation, the performance of the object-centric network seems to be of significant importance.\nTherefore, providing relevant experimental results for the object-centric network would further solidify the effectiveness of the network.\nFor example, providing visualizations of the input point cloud of the trained object-centric network along with the corresponding recovered full shape would be beneficial. \n\n- The authors conducted training and evaluation solely on the chair class of real-world datasets (ScanNet and S3DIS).\nWhile they evaluate performance on the synthetic dataset with six classes, the model's effectiveness in real-world scenarios, including various instance categories, remains unclear.\nIs it possible to train and evaluate GOPS for six class objects from real-world datasets (ScanNet and S3DIS) using the object-centric network trained for the six class objects used in the synthetic dataset experiments?\nOr train object-centric networks for six class objects in real-world datasets again?\n\n- While GOPS does not require labor-intensive human annotations, training the two-stage GOPS frameworks seems to demand heavy resources. \nIt would be helpful to clarify their implementation details, including the memory and time costs for the training process."
        ]
    },
    "wFD16gwpze": {
        "venue": "ICLR 2025",
        "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
        "link": "https://openreview.net/forum?id=wFD16gwpze",
        "abstract": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.\nFor linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6
        ],
        "strengths": [
            "- This paper studies an important problem, namely the theoretical origin of neural scaling laws, and studies this in nonlinear two layer committee machines. For linear activations, they can obtain very precise learning curves in terms of the spectrum. They obtain power law exponents for both training time $\\alpha$ and number of student features $N$. For nonlinear networks, they predict escape times for the specialization transition in terms of $M,L$.",
            "- The paper is well-written overall. The arguments are well-founded, the text is concise and clear, and the authors skillfully focus on essential points in the main text, leaving detailed calculations for the appendix. All of the derived analytical results are verified by numerical simulations. The contribution, particularly the investigation of the properties of the plateau, appears to be quite solid as well.",
            "- The paper is very well written, results are clearly exposed and connected to related works. Abundant and clear numerical experiments are provided to support the main results.\n\nThe question explored is interesting. To the best of my reading, the main technical contributions are\n- Establishing the rate of decay of linear regression for one-pass SGD, which happens to match previous full-batch results, e.g. Bordelon and Pehlevan (2022), Bahri et al (2024). The model and training are much simpler than in those papers, but I believe this particular case has not yet been covered in the literature, although I am not completely familiar with it.\n- Generalizing the escape time analysis of Biehl (1996) to structured data, again to the best of my understanding of the literature. In particular, the result that the length of the plateau decreases with the number of eigenvalues is an interesting one.\n\nI have a number of concerns, which I detail in the following sections. Overall, I think the paper is sound, although I have not checked the derivations in detail, and am overall in favor of acceptance if my concerns are addressed by the authors."
        ],
        "weaknesses": [
            "- While the paper introduces a very promising approach of utilizing a hierarchy of order parameters to deal with power law structured covariates, most of the closed form theoretical predictions require restricting to the linear activation case. However, these results are also of interest. There are some remaining questions and issues, which if answered/addressed, could cause me to increase my score.",
            "- The main problem with the manuscript is that the setting is quite narrow - exact degeneracy of the covariance eigenvalue, 2nd layer neurons are all untrainable and *identical*, only one nonlinear activation tested, etc. It is not clear how robust these results are to to even slight variation of the setting. IMHO, This is the main point that would determine the impact of this work.\n\nClearly, it is very difficult to extend the analytical results beyond the setting described by the authors, but it would strengthen the manuscript considerably if the authors investigated numerically the robustness of their results. Since the networks are relatively small, this should be easy to do. Concretely, to what extent do the conclusions hold when:\n  - $K\\neq M$?\n - The weights of the 2nd layer are not identical?\n- The covariance is only almost degenerate?\n- Other (and more common) nonlinear activations are used?\n\n(even a subset of these would be useful, but as I wrote above, the experiments are not challenging)",
            "- I have a number of observations and questions. I regroup my main concerns in this section, and leave the more minor points for the next.\n\n- l.191 I have strong doubts about this statement. I believe preactivations are always Gaussian, as linear combination of the Gaussian inputs. The Gaussian Equivalence principle is needed when discussing the post-activations. Please correct me if I'm wrong.\n\n- l. 417. Is a $l$ missing somewhere, why does the variance not depend on it ? Furthermore, it seems to me the variance scales as $1/N$, which does imply self-averaging. I do not understand the author's claim that higher-order overlaps do not self-average, which is linked to the result that $M$ distinct plateaus are present for the dynamics of e.g. the $R$ overlap. Further clarification would be very helpful."
        ]
    },
    "vWR3KuiQur": {
        "venue": "ICLR 2025",
        "title": "SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models",
        "link": "https://openreview.net/forum?id=vWR3KuiQur",
        "abstract": "Diffusion models can effectively generate high-quality images. However, as they scale,  rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where existing post-training quantization methods like smoothing become insufficient. To overcome this limitation, we propose *SVDQuant*, a new 4-bit quantization paradigm. Different from smoothing, which redistributes outliers between weights and activations, our approach *absorbs* these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights. Then, we use a high-precision, low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD), while a low-bit quantized branch handles the residuals. This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine *Nunchaku* that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without re-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5\u00d7, achieving 3.0\u00d7 speedup over the 4-bit weight-only quantization (W4A16) baseline on the 16GB laptop 4090 GPU with INT4 precision. On the latest RTX 5090 desktop with Blackwell architecture, we achieve a 3.1\u00d7 speedup compared to the W4A16 model using NVFP4 precision. Our quantization library and inference engine are available at https://github.com/mit-han-lab/deepcompressor/ and https://github.com/mit-han-lab/nunchaku/, correspondingly.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            6,
            8,
            6
        ],
        "strengths": [
            "- 1-SVDQuant\u2019s strategy to manage outliers using low-rank decomposition offers an improvement over standard smoothing techniques.\n\n2-LoRunner effectively reduces memory access overhead and enhances performance, particularly for GPU inference, addressing practical deployment constraints.\n\n3-Pushing boundaries for low-bit quantization of both weight and activation",
            "- The paper is well-written and clear. Though the idea is generally intuitive, the observation of the low-rank outliers and the fusion from the low-rank branch to low-bit branch makes this work a strong submission. Experiments are conducted on the latest diffusion models, and the results are generally strong.",
            "- 1. The paper introduces a full-precision low-rank adapter and Singular Value Decomposition (SVD) to effectively compensate for quantization errors.\n2. The authors have implemented a novel kernel that efficiently fuses low-rank and low-bit branches, minimizing computational overhead.\n3. The extensive experiments conducted on state-of-the-art diffusion models, such as FLUX, provide strong evidence of the method's effectiveness and robustness.",
            "- - It is an interesting topic to combine the low-bit quantization and low-rank decomposition, previous work including Loft-Q (ICLR 24), et al.\n\n- This work proposes a kernel fusion implementation to speed up on-device inference.\n\n- The method is reasoning and the writing is easy to follow.",
            "- The paper presents clearly the flow of ideas: statements are clear and detailed proofs are moved to the appendix, hence they do not distract from the flow f the key messages.\nThe motivation for the work is clearly described as well as the gaps in the SoA, and the claims of novelty. \nThe reader is led step by step to the final solution: this flow helps understanding the technical motivations of the various steps. \nExperiments are clear and the KPI used are well defined.",
            "- 1. I like the idea of translating the weight quantization into residual quantization to eliminate outliers in weights. \n2. The figures are well illustrated, and the math presentations are insightful. \n3. The model with a dinosaur head is hilarious."
        ],
        "weaknesses": [
            "- 1-The level of contribution is not great. Using outliers and low rank for quantizations is a very well-known technique. The authors need to have a section in related work and highlight the difference/novelty between the technique and all related work. Otherwise, the novelty seems incremental.\n\n2- The paper lacks a theoretical analysis of why the low-rank decomposition approach can consistently outperform other outlier-handling techniques beyond empirical observations.\n\n3- Code is not provided.",
            "- 1. Can the authors include some results on sub 4-bit quantization with the proposed method? It would be good to know the limitations of the method.\n\n2. The authors should include more comparisons in their experimental results. While they compared with MixDQ and ViDiT-Q, some prominent works have been left out, such as Q-Diffusion [1] and QUEST [2].\n\n[1] https://arxiv.org/pdf/2302.04304\n\n[2] https://arxiv.org/pdf/2402.03666v1",
            "- 1. Adding a W8A8 baseline to the latency comparison would provide valuable insights and a clearer performance reference.\n2. Since the authors use group quantization for weights and activations in the 4-bit setting, it would be beneficial to include methods that also use group quantization, such as Atom [1], as baselines.\n\nReferences: \n[1] Zhao et al. \"Atom: Low-bit Quantization for Efficient and Accurate LLM Serving\" in MLSys'24",
            "- - I don't think splitting low-rank and low-bit branches is a good idea to overcome quantization errors. Different from the QLLM (ICLR 24) et al.,  the computation of the two branches can not merge after training, leading to 5~10% overheads in the paper (line 314).\n\n- Experiments parts are not very solid: the baseline quantization methods are kind-of weak. Recent works including SpinQuant [1], AffineQuant [2], et al. achieve much higher performance than the baseline NF4 in the paper and also without the additional branch.\n\n[1] SpinQuant: LLM quantization with learned rotations\n[2] AffineQuant: Affine Transformation Quantization for Large Language Models",
            "- It is not clear ho lambda for the migration of outlayers from activations and weights si computed (it is per-channel - so the array can be quite big). This is in my view a bit of a problem, because lamda migh be impacted by the dataset used for calibration (if it is decided offline) or may be hard to determine efficiently online. \n\nInference time results are missing because the authors have no access to modern gpus (Blackwell) with native 4b support. This is a minor weakness, but it must be noted that it's not fully clear if the merged kernels will benefit from the same speedup claimed for pure 4b kernels. \n\nthe key ideas are not novel per se, but the combination is interesting - the key intuition being the merged low-rank and low-precision kernel to recover speed",
            "- 1. There are a few strong statements in this work with insufficient reasoning. \n2. The method section mainly focuses on justifying the minimization of quantization error but lacking discussion of the computation flow. \n3. The residual quantization approach looks similar to the quantization of error matrix in LoRC."
        ]
    },
    "vQhn4wrQ6j": {
        "venue": "ICLR 2025",
        "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
        "link": "https://openreview.net/forum?id=vQhn4wrQ6j",
        "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8
        ],
        "strengths": [
            "- * Interesting idea and its evaluation on 1 model and 4 languages, with additional experiments\n* Although the setup raises some questions (limited evaluation, why not freeze layers and avoid having to soup the transition layers, etc.), the expanded evaluation on Swahili and the limitations section address most of these\n* excellent writing, justification and presentation",
            "- 1. The proposed methodology is highly practical in scenarios where one might have publicly available task-specific data in a high-resource language and generic instruction data in the low-resource language. The model parameter adjustments being fully post-hoc eliminate any additional computational overhead apart from the initial fine-tuning required to create task and language experts.\n\n2. *layer swapping* with the best configuration consistently outperforms the individual SFT experts, the base LLM Llama 3.1 and the general model souping approach in three (Swahili, Telugu, Bengali) out of the four languages under this study.  \n\n3. The paper is easy to follow. The authors also take the effort to acknowledge the possible limitations to the work, encouraging future exploration.",
            "- - The paper introduces an efficient, innovative layer-swapping method for zero-shot cross-lingual transfer in LLMs, addressing the lack of task-specific data in low-resource languages with simplicity and strong empirical results.\n\n- This technique is particularly notable for its straightforward implementation, allowing effective merging of task and language expertise without complex adjustments, making it a practical alternative to standard methods like model souping. \n\n- Promising experimental gains on math reasoning benchmarks across multiple low resource languages validate the method's effectiveness, showing that layer swapping successfully enhances cross-lingual transfer without in-language task data."
        ],
        "weaknesses": [
            "- * limited evaluation: only 1 model and one set of tasks (math)",
            "- 1. The methodology is evaluated only on Llama 3.1, using MGSM benchmark for 4 selective languages. In my opinion, evaluation of the method on a single model, single benchmark and limited languages makes the conclusion less generalizable. While the languages used in the study are diverse, incorporating more datasets and models (in terms of different architecture or pre-training) can strengthen the conclusion.\n\n2. The assumption of availability of generic instruction data for low-resource languages might not hold for all languages. Task-specific data and generic instruction data in a high-resource language is generally more accessible. An experiment where language expert is fine-tuned using translated instructions would increase the practicality of the work.",
            "- - The method is tested only on math reasoning, leaving it unclear if layer swapping generalizes to other tasks. Additional evaluations on tasks like question-answering or translation would strengthen the claims of broad applicability.\n- While the paper mentions different layer-swapping configurations, it lacks in-depth analysis on which configurations work best and why. A more detailed study of these choices would help to better understand the method make it more robust. For example, provide ablation studies on the number of swapped layers or transition zone sizes, or to analyze how performance changes as these parameters are varied.\n- Comparisons to recent modular fine-tuning techniques, such as adapters or LoRA, are missing. Including these would clarify how layer swapping performs relative to other efficient, cross-lingual methods."
        ]
    },
    "uqWM9hBDAE": {
        "venue": "ICLR 2025",
        "title": "How Much is Unseen Depends Chiefly on Information About the Seen",
        "link": "https://openreview.net/forum?id=uqWM9hBDAE",
        "abstract": "The *missing mass* refers to the proportion of data points in an *unknown* population of classifier inputs that belong to classes *not* present in the classifier's training data, which is assumed to be a random sample from that unknown population.\nWe find that *in expectation* the missing mass is entirely determined by the number $f_k$ of classes that *do* appear in the training data the same number of times *and an exponentially decaying error*.\nWhile this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample.\nIn our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\\% of the Good-Turing estimator's.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            6,
            8,
            8
        ],
        "strengths": [
            "- - The paper clearly states the problem formulation (fitness function and search space) and proposes a numerical algorithm to solve it.\n- The resulting estimator shows a strong performance in terms of MSE and is relatively cheap to compute.\n- The presentation and the figures are very clear and well-explained.",
            "- This paper is significant, since estimating the missing mass is a classic and fundamental problem in statistics with broad practical applications. Making advances over the widely used Good-Turing estimator is important, and such progress has the potential to bring substantial empirical improvements. This paper is also novel in its approach: their analysis do not rely on Poisson approximation, which allows for deeper and more flexible analysis. Additionally, the paper presents a thorough evaluation of the proposed algorithms, including theoretical insights and synthetic experiments that demonstrate the minimal-bias estimator\u2019s substantially lower bias compared to the Good-Turing estimator across various distributions.",
            "- The paper is well-written: the setting discussed in this work, as well as the proposed methodology, is clearly stated, and relevant questions are answered."
        ],
        "weaknesses": [
            "- - The empirical application is $\\textbf{insufficient}$, only one real world dataset was used and the experiments were done only on 50 datapoints. \n- No $\\textbf{theoretical guarantees}$ were given for the minimal MSE estimator of the missing mass probability. \n- Comparing the proposed genetic Aagorithm to some other optimization approaches  as well as studying the properties of the resulting estimators would be helpful.",
            "- No obvious weakness is found.",
            "- I don't feel that the motivation for and usefulness of the problem are sufficiently explained."
        ]
    },
    "vi3DjUhFVm": {
        "venue": "ICLR 2025",
        "title": "Test-time Alignment of Diffusion Models without Reward Over-optimization",
        "link": "https://openreview.net/forum?id=vi3DjUhFVm",
        "abstract": "Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            5,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. DAS does not require additional training, which reduces computational cost.\n2.The use of SMC with tempering is justified through asymptotic properties.\n3. DAS balances reward optimization and diversity, and is demonstrated across single-reward, multi-objective, and online settings.",
            "- 1. This paper is overall well-written and the motivation is clear. It aims to address the trade-off in diffusion models that align them with specific objectives while maintaining their versatility, which is a critical problem in generative modeling.\n\n2. DAS\u2019s effectiveness is comprehensively validated across diverse scenarios, including toy distribution simulation, single-reward, multi-objective, and online black-box optimization tasks.",
            "- - The introduction provides a clear overview of the problem.\n\n- The proposed method appears promising and might be innovative (see Question 5.)",
            "- The paper is clearly written and there is a good discussion of the work involved. Based on the fact that the existing fine-tuning methods lead to the reward overoptimization problem while the guidance methods lead to the under-optimization problem, the authors propose the DAS method to alleviate these deficiencies. In addition, the authors provide a theoretical analysis of the method and give the relevant code, making the work very solid. Figure 1 illustrates the shortcomings of the existing methods as well as the advantages of the proposed method, and the experimental results are visualized by using an example of a mixed Gaussian distribution."
        ],
        "weaknesses": [
            "- 1. While DAS is compared with fine-tuning and guidance methods, comparisons to baselines like STEGANODE or controlled diffusion could have strengthened the evaluation.\n2. DAS assumes differentiable reward functions, which may limit applicability in scenarios involving non-differentiable objectives.\n3. Most experiments use Stable Diffusion v1.5, and additional models would have enhanced the generality of the findings.\n4. The paper can do more image tasks. Currently it emphasizes findings on aesthetic score, which might not generalize well to other tasks.\n5. Limitations: the setup of SMC with tempering, intermediate targets, and backward kernels can be technically demanding. And the effectiveness of DAS depends on the pre-trained model's quality, limiting performance on models with low initial diversity or reward alignment.",
            "- 1. More intuitive explanations of SMC are suggested to add between the motivation and method to make it more consistent and intuitive since the introduction of SMC in supplementary material is a bit abstruse to understand, making the superiority of adopting SMC to address the training problem unclear.\n\n2. How to choose hyperparameters such as $\\gamma, \\alpha$ and particles should be discussed across different scenarios.",
            "- - The choice of finetuning-based RLHF baselines may not be appropriate (see Question 1).\n\n- The paper is sometimes hard to follow due to the delayed definition of new notations. For instance, the symbol $\\gamma$ is used on line 208 but is not defined until line 250.\n\n- The evaluation metrics used in the paper (line 355 and onward) are not explained, making it difficult to assess their relevance and meaning.",
            "- + The models underlying the experiments in this paper have some weaknesses, and the Stable Diffusion (SD) v1.5 model is somewhat outdated now. The Consistency model [1] and Flow model (SD3) [2]  are widely used nowadays, so I suggest the authors to conduct some experiments on the newer model so as to further illustrate the validity of the proposed method.\n\n[1] Consistency models ICML-2024\n\n[2] Scaling Rectified Flow Transformers for High-Resolution Image Synthesis ICML-2024\n\n+ In addition to mixing Gaussian distributions, **Swiss rolls** are also commonly used to visualize whether a distribution has been learned or not, and due to their structural features, which can further reflect the model's ability to fit the distribution, the authors can give some visualizations that further illustrate the strengths of the proposed method."
        ]
    },
    "vVCHWVBsLH": {
        "venue": "ICLR 2025",
        "title": "Decomposition Polyhedra of Piecewise Linear Functions",
        "link": "https://openreview.net/forum?id=vVCHWVBsLH",
        "abstract": "In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27\u201359, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            5,
            8,
            8,
            8
        ],
        "strengths": [
            "- The problem of decomposing CPWL functions as the difference of convex CPWL functions is interesting. No finite procedure is currently known for finding a minimal decomposition. This work provides a new perspective, based on polyhedral geometry, that guarantees finite convergence, but in the special case where the factors have a fixed supporting polyhedral decomposition",
            "- The DC representation of a general CPWL function is an old but fundamental problem with many applications in various engineering fields. This paper proposes an interesting perspective on how to understand and compute the DC components from a given CPWL function. Although I did not have time to check all the proofs in detail, the paper is generally well written, and the results are interesting. In particular, I appreciate the idea of fixing the underlying pieces and the clean characterization of the DC components.",
            "- 1. The paper presents an innovative approach by linking decomposition problems with polyhedral geometry, leading to the concept of decomposition polyhedra. This is a very novel idea and may inspire many interesting future works. \n\n2. The theoretical analysis of the paper is very solid, providing us with a deep understanding of CPWL decomposition problem. \n\n3. The paper is well-written and well-organized, clearly stating the main contributions and their applications.\n\n4. The applications to ReLU activation, submodular optimization, neural network design, are very interesting.",
            "- Paper is well-written. The problem studied is challenging."
        ],
        "weaknesses": [
            "- The assumption that the DC decomposition should be with respect to a fixed polyhedral complex seems restrictive and perhaps unmotivated \n\nI feel that the paper is hard to read for non-experts in the area (e.g. many technical definitions with not many accompanying figures to help the reader-there are few, but mostly in the Appendix).\n\nThe main result in Section 6 (Corollary 6.4) for representing CPWL functions as NNs, is only applicable to CPWL functions that are compatible with a regular polyhedral complex. It is not clear whether this assumption is (1) natural and (2) easy to satisfy.  The existing decomposition results are applicable to any CPWL function.",
            "- While the pieces are assumed to be fixed in advance (which is, of course, a limitation, as also explicitly noted by the authors), I believe this work has great potential to motivate further investigation into both the theoretical and algorithmic aspects of the decomposition problem. \n\nMy comments are as follows:\n* L200, in the definition of $\\mathcal{P}_f^n$, I don't think the set {$x:g_i(x)=\\max_j g_j(x)$} must be full dimensional. This may depend on the representation of $f$ given in L199. Also, $k=q$ may not be true, as claimed in L202.\n* As for regular polyhedra complex, I understand the usage of the existence of convex CPWL in the proof of theorems. I'm curious about the irregular case and it will be very helpful to provide some details or examples to illustrate the existence of irregular polyhedra complex.\n* Some notation are used before defined in the whole paper. For example, in Proposition 3.3, it seems the function $w_f$ is not defined until the proof of Proposition 3.2.\n* L184, in the definition of CPWL functions, I think you need to require $f$ to be continuous.\n* L1283, what is $\\phi$ defined here?\n* In the proof of Proposition 3.3, I suggest providing more details to justify the equivalence claimed in line 1360. Intuitively, this is correct, but in convex geometry, counterintuitive phenomena can occur, so a rigorous and formal argument would be desirable.\n* L752, the function $h$ may not be convex as claimed.\n* L180, add a period.\n* L146, \"wit\" should be \"with\".",
            "- The main weakness of the paper is that as stated in Limitations section of the paper. The paper mainly focuses on the development of theories, but does not provide practical implementations and applications of their results.",
            "- The obvious limitation is that the underlying polyhedral complex is fixed.\nThere are no bounds shown for the number of pieces in the minimal decomposition. Would it be related to the notion of monomial complexity ? A discussion would be interesting."
        ]
    },
    "ykuc5q381b": {
        "venue": "ICLR 2025",
        "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
        "link": "https://openreview.net/forum?id=ykuc5q381b",
        "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.0 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question answering performance by over 6.6 points. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            6,
            8,
            6,
            6,
            10
        ],
        "strengths": [
            "- A strong benchmark paper should satisfy some of the following dimensions (along with some commentary): \n- the task is useful\nDifficult document retrieval is a long- and widely-studied problem. It is both more important in the era of LLMs due to increased reasoning capabilities, but potentially less important as more information is encoded in the parameters (modulo time-sensitive, etc.). Thus, additional motivation could help justify the significance of the work. \n\n- the dataset is large enough and non-trivial to construct\nThis is also mixed; the dataset isn't particularly large and is ostensibly of varying quality between Stack Exchange, Coding, and Math. That being said, it is clearly more complex than many existing benchmarks for at least a subset of the questions. For some questions, the quality is seemingly higher (i.e., more human validation) than existing datasets.\n\n- there are sufficient details regarding the construction of the benchmark\nIncluding the appendices, there are a lot of details -- to the point where I am confident I could replicate most of the results. However, the amount and clarity of the procedure for different data sets (Stack Exchange, Coding, and Math) isn't as detailed for all cases. Also, it isn't clear in general what the human annotation guidelines were, how annotators were recruited, and how they were compensated (unless it is just the authors and volunteers). However, the details are solid overall.\n\n- the tools provided reduce friction for new people to work on this\nCode is provided and was used to run several experiments. I didn't dig through the code and thus do not know how easily it is to conduct experiments. However, I am reasonable confident it is sufficient.\n\n- the baseline models tested on the benchmark are non-trivial\nThe authors conduct several experiments over several different retrieval engines including state-of-the-art systems on related datasets.\n\n- the benchmark answers new questions or enables new solutions\nThe authors did conduct experiments beyond just IR performance and were able to address some of these questions using this dataset. The discussion in these sections could be strengthened, but it is solid in this regard overall.\n\nEvaluating the paper with respect to the stated dimensions,\nOriginality: There are multiple 'hard QA/IR' datasets, but the emphasis here is on IR for reasoning-heavy scenarios -- which is timely and a useful contribution. Many have likely considered such datasets, but the execution here is better than a first attempt.\nQuality: Overall, the work is well-motivated, well-executed, and sufficiently rigorous. My primary concern in this regard is variance in quality between different benchmark types (QA, Math, Coding) and that this is a relatively small dataset.\nClarity: Overall, the paper is easy to understand and has sufficient details, especially when considering the appendices. The figures are helpful. My two suggestions in this regard are a Table comparing Bright to the most related datasets and more discussion regarding the empirical results including specific references to cells in the tables (i.e., I didn't always know which cells I was looking at when validating quantification claims).\nSignificance: I am fairly certain that at least part of this benchmark will be used, but not sure if all parts will be used. Additionally, it would have more potential impact if it was a larger dataset (or there was clear evidence that it covers some expected 'production' distribution)",
            "- - The data collection pipeline is thorough and includes verification by two domain-expert PhD students. \n- Lots has been done to ensure diversity by focusing on several different StackExchange domains, two coding tasks, and an additional effort for including theorem-based questions.\n- The experiments are extensive and cover 13 different retrievers, in addition to two re-rankers.\n- The paper is well-written and easy to follow.",
            "- - The benchmark is challenging and has the potential to drive future research toward developing retrievers that handle difficult queries more effectively.\n- The dataset is human-collected, ensuring authenticity rather than relying on artificially generated data. \n- I liked the comprehensive appendix, which provided valuable insights into the annotation process and the dataset's structure.",
            "- 1. This paper focuses on creating a new dataset to benchmark reasoning-intensive queries, which is a very important type of queries for RAG systems and search engines, and there does not exist such a benchmarking dataset so far.\n\n2. The dataset covers a variety of domains, including economics, psychology, mathematics and coding, which is quite comprehensive.\n\n3. The retrieval process in the experiments incorporated explicit reasoning about the query and achieved an up to 12.2 points improvement, which demonstrated that reasoning is indeed a bottleneck for such types of queries, which aligns very well with the motivation of the datasets.",
            "- 1. A high quality, relevant and challenging benchmark for information retrieval tasks\n2. Comprehensive evaluation on a wide variety of models\n3. Love the section on how chain of thought reasoning helps improve the models -- especially BM25!\n4. Downstream task evaluation is also very helpful -- confirms the documents are relevant"
        ],
        "weaknesses": [
            "- On the other hand, below are some of my concerns regarding this paper (some previously mentioned):\n- I would like to see a statistics-level comparison between Bright and competing datasets in a table (\"Benchmarking retrieval\" (line 104) and \"Benchmarking reasoning\" (line131) in Related Work section)\n- I would like clarification regarding the annotation guidelines, recruiting experts, compensation for lines 248-253.\n- To get the details for coding and math, one pretty much has to read the appendices and from what I can tell, the significance and quality of the Stack Exchange questions is the strongest aspects of the paper.\n- The appendices are more detailed (to the point where they actually seem different from the text). However, still no details regarding annotation guidelines, recruiting experts, and compensation (unless the authors did all of this)\n- The dataset seems relatively small; if I am incorrect, I would recommend a table contrasting this with other datasets (along with other aspects).\n- For reasoning steps, a bit more from the appendix (e.g., StackExchange vs. coding vs. math stratification) would be helpful in the main text with discussion.\n- In general, it isn't clear that ordering matters for RAG settings, so NDCG-based results may not be that useful as in IR settings. I also would recommend rank-biased precision and recall (i.e., evaluations similar to 'needle-in-a-haystack' settings.\n- As implied in other areas, there are a lot of results, so more specific interpretation would be helpful (but I am aware of the page limit).\n- While the authors claim that there are not licensing issues, I wasn't able to verify this. Obviously, if there are licensing issues (for academic research within commercial organizations?).",
            "- - It could have been helpful to add a quantitative analysis in the analysis section. For example, an analysis that examines when models err could be useful for future research (see the Questions section for further discussion and some suggestions).\n\n- There are a few details in the appendix that are not referenced from the main paper (e.g., the Limitations section), and I found the appendix a bit hard to follow. Consider verifying all main sections are referenced, or alternatively adding a small Table of Contents in the beginning of the Appendix.",
            "- - Detailed analysis of results is lacking and some (RAG, reranking) are not surprising.\n- In line 413, how can LLM-reasoning queries enhance the reasoning capabilities of retrievers? If the primary effect is an increase in lexical similarity (BM25's strong performance), should models be specifically trained to leverage this feature to perform well on BRIGHT? Additionally, the results for Coding and Theorem-based datasets (Table 38) appear inconsistent.\n- In line 428, regarding retrieval-augmented generation, the QA performance seems to be already quite high when paired with current retrievers that are reported to perform poorly.",
            "- 1. There is no unified definition of either relevance or required reasoning across data from different domains in this dataset. Though the author wrote different \"relevance\" in different subsections, they are actually not definition of relevance but how the corpus is created.\nIt is acceptable as a dataset, however, there is a lack of deep scientific understanding about what exact (reasoning) ability is required for a retriever model to success on this dataset. Instead what we can learn from this dataset is that the retriever model may need to overfit some ad-hoc collection process.\n\n2. Due to a lack of unified relevance definition, the dataset is more like a collection of different benchmarking applications where different relevant (can be either closely or loosely) information to the query can be helpful. Therefore, rather than using it as a retrieval benchmark to evaluate retrievers, it is more suitable to use it to evaluate the application. For these applications, the retrieval results are just intermediate results and can be difficult to judge whether they can actually help the final results.",
            "- I could not think of any questions I had which were not answered in the paper. I have some observations which are mentioned in the questions"
        ]
    },
    "xak8c9l1nu": {
        "venue": "ICLR 2025",
        "title": "Computational Explorations of Total Variation Distance",
        "link": "https://openreview.net/forum?id=xak8c9l1nu",
        "abstract": "We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance.\nFirst, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.\nThis corresponds to a special case, whereby the TV distance between the two distributions is zero.\nSecond, we prove that unless $\\mathsf{NP} \\subseteq \\mathsf{RP}$ it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            6,
            8,
            6,
            8
        ],
        "strengths": [
            "- The main contribution of this paper is the first result: deciding whether two mixtures of product distributions are the same. Suppose we have $k$ product distributions $P_1, P_2, \\ldots, P_k$, where each $P_i$ is an $n$-dimensional product distribution, i.e., $X \\sim P_i$ is a vector $(X_1, X_2, \\ldots, X_n)$. The paper takes the prefix of $X$, namely $X^{\\leq j} = (X_1, X_2, \\ldots, X_j)$ for $j \\leq n$. This distribution is denoted by $P^{\\leq j}_i$. Then, they consider the mixture of $P^{\\leq j}_1, P^{\\leq j}_2, \\ldots, P^{\\leq j}_k$, denoted by $P^{\\leq j}$. The algorithm decides whether $P^{\\leq j}$ and $Q^{\\leq j}$ are the same for all $j$. The algorithm is based on induction from $j = 1$ to $j = n$. The base case is trivial. The difficult part is that for $P^{\\leq j}$ and $Q^{\\leq j}$, the support of the distribution can be as large as $\\exp(\\Omega(j))$. To reduce the computational cost, the algorithm finds a \"sketch\" of the two distributions. One needs to check whether $P^{\\leq j}(x) = Q^{\\leq j}(x)$ for exponentially many $x \\in \\Sigma^{j}$. For each $x$, the algorithm views $P^{\\leq j}(x) = Q^{\\leq j}(x)$ as a linear equation. Instead of checking an exponential number of linear equations, the algorithm finds a basis of the linear system, and the size of the basis is $\\text{poly}(n)$. Then the algorithm only need to check the equations in the basis.\n\nOverall, the algorithm and the definition of $P^{\\leq j},Q^{\\leq j}$ are simple and clever, and I think deciding whether two mixtures of product distributions are the same is a basic problem in statistics.",
            "- The paper addresses important computational questions about the total variation distance, which is fundamental in probability and statistics.\n\nThe algorithm for equivalence checking of mixtures of product distributions is new and provides a practical solution to a non-trivial problem.This hardness result bridges complexity theory and statistical measures and provides insight into why certain computational tasks are hard.\n\nThe proofs are well written, the results are accessible.",
            "- - Mixtures of products are a fundamental family of probability distributions and checking their equivalence is one of the most basic questions about them.  \n- The algorithm uses an interesting novel idea of keeping track of bases for the solution spaces of certain equations. This idea might find applications for testing equivalence of other classes of distributions.\n- The problem of estimating the total variation distance between two Ising model distributions is quite natural, as Ising models are a very well-studied class of probabilistic models.\n- The hardness result only relies on the assumption that NP is not in RP, which is a very mild complexity assumption.",
            "- The paper give two new results about computing TVD. \nThe polynomial time algorithm for TVD of mixtures of product distribution has its main strength in the simplicity and clarity of the approach.\nThe second result extends the studies on the complexity of dealing with Ising models.",
            "- see above"
        ],
        "weaknesses": [
            "- The hardness result follows from standard results. Proposition 6 provides a self-reduction for the Ising model, which is used in the standard counting-to-sampling reduction. Therefore, the proof of Proposition 6 could be omitted. Proposition 8 essentially states that one can fix the value of a vertex $v$ by adjusting the function $h(v)$, allowing the TV distance to encode the marginal distribution.\n\nThe relationship between Theorem 1 and Theorem 2 is not very strong, as they pertain to different models.",
            "- It would be nicer if the paper could elaborate more on the practical applications of the equivalence checking algorithm with regard to performance on real-world data.\n\nThe hardness result could also be pushed further by thinking about the possibility of approximate algorithms with different complexity assumptions.\n\nIt would be even more applicable and helpful with more examples or case studies.",
            "- - The algorithm for mixtures of product distributions can only check whether P=Q exactly. The paper would be stronger if it gave an algorithm for approximating the distance between P and Q.\n- The paper rules out FPRAS for TV distance between a pair of Ising models, but it seems that there could still be a constant-factor approximation algorithm, and the paper would be stronger if this question was also addressed (i.e. it was shown that this is also hard, or an algorithm was given).",
            "- The paper appears to be a gluing of two minor results with little connection between them. \nThe second result  builds upoon the Jerrum and Sincler previous analogous study. The first result bears more novelty, although it would not be, in my opinion, sufficient for a paper at ICLR.\nI think the main issue is with the specificity (very particular cases) of the two problems solved.",
            "- see above"
        ]
    },
    "wN3KaUXA5X": {
        "venue": "ICLR 2025",
        "title": "Diffusion On Syntax Trees For Program Synthesis",
        "link": "https://openreview.net/forum?id=wN3KaUXA5X",
        "abstract": "Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts \"noise\" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            6,
            6,
            8
        ],
        "strengths": [
            "- The method is simple but non-obvious\nThe more general problem of program synthesis conditioned on desired outputs is very relevant\nThe authors use randomly generated programs as a dataset which sidesteps dataset curation in favor of just a specification of the language\nThe paper is well-written, easy to understand, and has nice and (mostly) clear figures",
            "- - Innovative Approach: The paper presents a novel combination of autoregressive, diffusion, and search methodologies, which, despite being applied to a specific domain, holds potential for broader applications. The reverse mutation path algorithm also provides an efficient way to generate training targets. \n- Clarity and Replicability: The manuscript is well-written and easy to follow, providing sufficient detail to enable replication of the experiments.\n- Comprehensive Ablation Studies: The authors conduct thorough ablation studies on key hyperparameters and the impact of integrating search, enhancing the understanding of their method's efficacy.",
            "- 1. The main strength of this paper is the design of a neurosymbolic framework to evaluate the automated (i.e. diffusion-based) conversion of images into context-free grammar. This formal evaluation ensures that the desired specifications are met through iterative observation of the execution results and verification.\n\n2. The authors extend the approach to accept hand-drawn sketches and illustrate examples in the appendix confirming the applicability of the approach in several real-world settings.\n\n3. The supplementary videos illustrate the overall problem that the authors are attempting to solve and showcases the \"edits\" made by the framework.",
            "- Originality\n-------------\nThe paper takes inspiration from existing ideas and benchmarks, but they are clearly cited, and the novel aspects are well described. For instance, a backward edit path that's better than reversing the corruption path, removing the need of a partial renderer, and relying on beam search rather than full-fledged reinforcement learning.\n\nQuality\n----------\n\nExperiments demonstrate the advantages of the proposed approach, and properly ablate the different aspects and contributions.\n\nClarity\n---------\nThe paper is overall clear and straightforward to follow. With the additional details of the appendix, the approach should be re-implementable by a different team.\n\nSignificance\n-----------------\nUsing ML models to directly manipulate and modify programs, rather than either generate a whole program autoregressively, or emit edition instructions (which could be invalid or result in an invalid program) could make iterative program generation better or easier.\nThe fact that no reinforcement learning is required, but observation of the output of intermediate programs can simply be combined with beam search is also an interesting result.",
            "- - This paper proposed a novel solution to the reverse CG field, to synthesis programs for visual symbolic reasoning. The proposed method address the hard task through the unique lens of syntax tree, and achieves notably better results.\n\n- The idea of permuting on syntax tree allows for more efficient model, with better performance.\n\n- The efforts to make demo video makes the paper easier to understoand and spread."
        ],
        "weaknesses": [
            "- The paper is somewhat limited in scope (simple problem setup) in ways that make it not entirely obvious how the method \"scales\" to more complex relevant tasks like code generation.\n\nSome minor things covered in Questions",
            "- - Literature Coverage: The authors should consider citing \"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\" in the Neural program synthesis section since this work also takes multiple passes of the program and edits the program. \n- The value network (v\u03d5) training and effectiveness aren't thoroughly evaluated. Alternative approaches to edit distance estimation, including direct calculation from syntax trees, are not explored or compared.",
            "- There are three main weaknesses I would like to bring up. The authors are encouraged to rebut and provide legitimate explanations, if any, against these and the review decision may be adjusted accordingly.\n\n1. A claim made by the author states that the proposed method focuses on editing the program synthesized from the image, unlike prior works that autoregressively generate programs that are incrementally better. In doing so, the authors propose adding random noise to modify a base syntax tree generated from CSG2D. Despite the illustrative example shown in Figure 2, enabling the approach to modify node types rather than shape, the base syntax tree structure is governed by the initial generated program. It remains unclear (at least it has not been proven) that diffusion + base tree always yields the optimal syntax tree (a statement regarding suboptimal steps in section 4.3 is thus not justified). An analysis and example to demonstrate this is lacking and should be included.\n\n2. The overall architecture presented in Figure 3 is difficult to understand at first glance. In addition, the descriptions provided in section 3.4 (the model architecture) do not present enough detail to understand Figure 3.  Specifically, it is not clear how replacing the \"(\" denoted by the edit position token and replacing it with the grammar constrained autoregressive decoding yield valid syntax (i.e. are there low-level implementations in play that ensure that entire blocks from \u201c(\u201c to \u201c)\u201d are parsed out during replacing? How are varying input lengths handled? ). Replacing \"(\" with \"(Quad 8...\" seems to break the pairing of parenthesis. In addition, it is not clear what the purpose of \"EOS\" is in this context.\n\n3. The fraction of problems solved by the method trained with \"no reverse path\" is nearly the same as that of the control after about 60 expanded nodes. The control reaches the same performance at about 50 nodes. Is this a \"significant\" efficiency gain when the maximum node expansion budget was two orders of magnitude higher (i.e. 5000)? There are no computational or time-related metrics presented which help put this into context.\n\n4. Overall the presentation of section 3, especially 3.4, requires careful rework.",
            "- Originality\n-------------\nNo major weakness here, the work is in the continuation of previous cited work.\n\nQuality\n----------\nComparison with baselines might have been more extensive, specifically the RL-based algorithms from previous work, which could have better shown how \"brittle\" they were.\n\nClarity\n---------\nA few things were not clearly defined in the experiments and ablation sections (see \"questions\" below).\n\nSignificance\n----------------\nOverall, the CSG2D and TinySVG languages are a small-scale benchmark, but it's unclear whether the proposed approach would scale to large, structured programs in general purpose languages.\nFor instance, it might not be possible to find a sequence of valid programs created by short mutations between two relatively close programs. For instance, going from recursion to a loop, from an implicit lamda to a declared function, or from a for loop to a list comprehension. Even splitting a function into smaller pieces may require either large edits, or intermediate unparseable states.\n\nAfter discussion\n----------------------\nThe authors provided clarifications, other reviewers raised a few additional concerns, overall I'm maintaining my score.",
            "- This is a good paper, with minor weakness points below.\n\n- It is better to mention the size of the decoder model in the architecture section rather than in the appendix, so that readers with LLM background can quickly understand the edge of the model on this task.\n\n- It is better to discuss the number of steps in the diffusion procedure, and the model's potential ability limit in terms of output sequence length or number of symbols.\n\n- Two highly related work should be cited and discussed:\n\n\"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\" in ICML 2023, which explore the possibility of syntax tree to generate code, and via coarse-to-fine multi-round generation approach.\n\n\"Symbolic Visual Reinforcement Learning: A Scalable Framework with Object-Level Abstraction and Differentiable Expression Search\" in TPAMI, which also learns visual symbolic programs, not to depict the image but to interact with the environments. Rainbow environment is also leveraged in their experiments."
        ]
    },
    "zl3pfz4VCV": {
        "venue": "ICLR 2025",
        "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
        "link": "https://openreview.net/forum?id=zl3pfz4VCV",
        "abstract": "Text embeddings are typically evaluated on a narrow set of tasks, limited in terms of languages, domains, and task types. To circumvent this limitation and to provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) -- a large-scale community-driven initiative expanding MTEB to over 500 quality-controlled evaluation tasks across 1,000+ languages. MMTEB includes a wide range of challenging novel tasks such as instruction following, long-document retrieval, and code retrieval, and represents the largest multilingual collection of evaluation tasks for embedding models to date. We use this collection to construct multiple highly multilingual benchmarks. We evaluate a representative set of models on these benchmarks.\nOur findings indicate that, while LLM-based models can achieve state-of-the-art performance on a subset of languages, the best-performing publicly available model across languages is the notably smaller, multilingual-e5-large-instruct.\n\nMassive benchmarks often impose high computational demands, limiting accessibility, particularly for low-resource communities. To address this, we downsample tasks based on inter-task correlation (i.e., selecting only a diverse set of tasks) while preserving relative rankings.\nWe further optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks at a significantly lower computational cost. For instance, we introduce a new zero-shot English benchmark that maintains a similar ordering at a fraction of the cost.",
        "decision": "Accept (Poster)",
        "review scores": [
            8,
            6,
            8,
            6
        ],
        "strengths": [
            "- - The paper is well written and the main points are clearly communicated\n- The dataset is a great extension to the MTEB and would be a good resource to research community towards building largescale multilingual embedding models\n - The coverage of the dataset is great",
            "- - Given that recent embedding models often shows the trends for optimized for MTEB benchmark tasks, it would be valuable to develop larger-scale benchmarks that include a broader range of tasks.\n- Additionally, MMTEB downscales datasets and caches embeddings to help alleviate computational bottlenecks during evaluation.",
            "- 1. I believe the efforts to reduce the computational resources required for evaluation are very meaningful, as they will encourage more researchers from low-resource language regions to use this benchmark. If MMTEB had simply expanded the scale of MTEB, it could be expected that most strong baseline models would originate from commercial companies with high computational resources, which could hinder the rapid development of text embedding research.\n2. Each computational resource optimization strategy is described in detail, and the methods are easy to implement, which facilitates the adaptation of custom datasets.",
            "- (+++) MMTEB exemplifies a remarkable community-driven effort, engaging diverse contributors and fostering inclusivity.\n\n(+++) Introduces computational optimizations like downsampling and hard negative sampling, reducing evaluation costs to 3.11 hours on a 7B model (H100 GPU), making it accessible to low-resource settings.\n\n(++) Covers over 500 tasks across 10 categories in more than 1,000 languages, with a strong focus on low-resource languages and domains. But it lacks enough justification demonstrating the quality and value of each dataset. Provides an open-source, public leaderboard that encourages continuous contributions to advancing multilingual embedding research.\n\n(+) Expands traditional benchmarks by including new task types like instruction following, long-document retrieval."
        ],
        "weaknesses": [
            "- Based on table9, one limitation is that most of the crowd submissions are already based on existing public datasets from multiple language domains and not particularly for this dataset construction effort.",
            "- In general, dataset information (such as sample numbers, multilingual types, etc) and relevant model benchmark numbers are missing. Find more details in questions.",
            "- 1. The depth of analysis across different datasets seems inconsistent. For instance, the \u201cClustering\u201d section in 2.3.1 provides an average Spearman correlation, but the \u201cRetrieval\u201d and \u201cBitext Mining\u201d sections lack similar metrics. Moreover, as seen from the results in Appendix C.1.2, the selection of \u201cRetrieval\u201d strategy is based on analyses from only the NQ and TREC-COVID datasets, which may lead to biased hyperparameter selection. Although the current level of detail is already quite high, given MMTEB\u2019s potential impact, I believe further detail would only be beneficial.\n2. The abstract mentions \u201ca diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval,\u201d but I saw little content related to these datasets in the paper. I think the authors should clearly explain:\n-  **Why were these tasks included in MMTEB?** (This is a benchmark for multilingual text embeddings, yet instruction retrieval is currently available only in one language.)\n- **How were these new tasks integrated into the benchmark?** (I believe directly including long-document retrieval under the \u201cretrieval\u201d category should be done with caution, as it would require researchers to consider incorporating long-document-related datasets in their training data, which to some extent runs counter to the goal of addressing the \u201clow-resource double bind.\u201d)\n- **How will these new tasks impact model performance?** (The context length limitation of models such as multilingual-e5-large-instruct could hinder their performance on tasks like long-document retrieval. In the LongEmbed evaluations [1], it performs worse than the Mistral-based version. Additionally, the results in Table 16 show that the Mistral-based models perform better on MTEB (code). Thus, claiming the exceptional performance of multilingual-e5-large-instruct in the Introduction without further clarification may mislead readers.)\n\n[1] LongEmbed: Extending Embedding Models for Long Context Retrieval. arXiv:2404.12096",
            "- 1. The study lacks a clear articulation of the specific knowledge gap that MMTEB addresses beyond what MTEB has already achieved in evaluating multi-task capabilities of embedding models. The results section suggests that multilingual scores are closely aligned with English, making it difficult to discern the unique value that MMTEB offers. Additional analysis could better exploit the benchmark's value and clarify its unique contributions.\n\n2. While MMTEB aims to include as many relevant datasets as possible, it is unclear how these datasets were constructed or validated. Details on dataset quality, annotation methods (e.g., human vs. model-generated), and statistics (e.g., query-document ratios) would enhance transparency and reliability, especially given some datasets may be model-generated, such as FollowIR.\n\n3. The paper mentions retaining the top 250 ranked documents per query for each dataset and model but does not specify which model(s) were used to select these hard negatives. Clarifying this would help assess the robustness of the benchmark's retrieval tasks.\n\n4. The combination of 132 tasks makes it challenging to interpret a model's performance on specific languages or language families. While geopolitical categorization is helpful, further segmentation by language, domain, or specific capabilities could provide a more systematic and granular view of model performance. Expanding on the existing MTEB language families in Appendix H could offer researchers a clearer understanding of model weaknesses by language or domain."
        ]
    },
    "zg3ec1TdAP": {
        "venue": "ICLR 2025",
        "title": "Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data",
        "link": "https://openreview.net/forum?id=zg3ec1TdAP",
        "abstract": "Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, prior EHR FMs typically have context windows of $<$1k tokens, which prevents them from modeling full patient EHRs which can exceed 10k's of events. For making clinical predictions, both model performance and robustness to the unique properties of EHR data are crucial. Recent advancements in subquadratic long-context architectures (e.g. Mamba) offer a promising solution. However, their application to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data. We find that longer context models improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. Additionally, we measure robustness to three unique, previously underexplored properties of EHR data: (1) the prevalence of ``copy-forwarded\" diagnoses which create artificial token repetition in EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that higher levels of each property correlate negatively with model performance (e.g., a 14% higher Brier loss between the least and most irregular patients), but that longer context models are more robust to more extreme levels of these properties. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study on how to identify and quantify new challenges in modeling sequential data motivated by domains outside of natural language. We release all of our model checkpoints and code.",
        "decision": "Accept (Poster)",
        "review scores": [
            6,
            10,
            6,
            8,
            5
        ],
        "strengths": [
            "- 1. Introduce non-Transformer architectures (such as Mamba) to process medical data.\n\n2. The paper demonstrates the potential of the Mamba model\u2019s long-context capabilities for future clinical applications.\n\n3. The advantage of this paper is that Mamba can perform well with linear complexity.",
            "- 1. The authors define metrics for evaluating the severity of these properties on any dataset.\n\n2. They demonstrate that as the RR metric and irregularity metric increase, model performance decreases (via patient stratification experiments).\n\n3. The demonstration that perplexity does not generally decrease with sequence length is a major deviation from text data where it is well known to decrease with sequence length. This is because future EHR data is less predictable from previous tokens. The assumption that perplexity reduces with context length for EHR \n\n4. The authors demonstrate that for all three properties, increasing sequence length helped improve performance (either via improved brier scores or perplexity).",
            "- 1. The authors benchmark both transformer-based and subquadratic architectures on EHR data\n2. The authors identified and quantified three challenges present in EHR data.\n3. The authors conduct experiments to show the effectiveness of long-context models on EHR data.",
            "- The identified properties of EHR data are convincing. The evaluation of the effects of these properties provides valuable insights into using long-context models to model EHR data. The observations and conclusions in this paper will be helpful for future work to build better foundation models for EHR. \n\nThe authors have released the code and plan to release the model checkpoints later. The release of pre-trained and fine-tuned foundational models will benefit the community, considering the small number of such models currently publicly available.",
            "- 1. The analysis is solid in its technical execution and experimental design\n2. Generally, a rich technical/experimental paper"
        ],
        "weaknesses": [
            "- 1. Both GPT and LLaMA have a maximum context length of only 4096 tokens, so it's not appropriate for the authors to conduct tests with 16K length as a benchmark.\n\n2.The authors should provide additional details about what the 14 tasks in EHRSHOT include to facilitate better comparison.\n\n3. The paper mentions using the standard deviation of time intervals to divide patients into four groups (Q1-Q4). Regarding the groups with the most regular time intervals and the most irregular time intervals, the standards vary across different diseases, and testing of time intervals should be conducted according to specific diseases.\n\n4. The paper mentions achieving an AUROC of 0.807, but it's confusing that the specific content of the 14 tasks is not listed.",
            "- The authors provide in Figure 4 plots of perplexity over tokens for different context-length models. The GPT model has wildly varying perplexity over token positions which is described by the authors as being caused by \"training instability\". I think a more thoughtful analysis of the issue here is required, because it would otherwise look like the cause is a bug.\n\nWhy isn't transfer-learning or few-shot included. A major problem in the evaluation is that representations are obtained by averaging the last L token representations from the transformer (This is a one-liner in the appendix and really should be added to the main paper and be clearly communicated in the limitations section). It would be great to see these results in the few-shot setting. I imagine that the performance improvements as you increase sequence length would be even more extreme.\n\nThis paper should include comparisons to linear attention models that practitioners are interested in.\n\nThis paper does not communicate compute budgets, such as wall-times and the hardware used for these sequence lengths. Could a plot communicating performance vs the compute-load be provided to help justify whether these improvements are worth the added compute time.\n\nSince you trained an autoregressive sequence model, you could perform a zero-shot evaluation where given the past tokens, you autoregressively generate the future tokens and analyze this generated future trajectory for the binary classification task. This paper does not demonstrate whether these results generalize to the zero-shot. I think that analysis is out of scope for this work but should be mentioned in the limitations.",
            "- 1. The design of token repetition measurement is less convincing. \n\t- Are the proposed models applicable to ICU patients? If yes, routine vital signs and regular lab tests can repeat a lot, but they can continuously show patients' health status. It is tricky to determine whether they are informative. \n\n\n2. the comprehensiveness of experimental design is limited\n\t- The investigated methods are limited. They are general architectures for foundation models. However, foundation models designed for EHR, such as [1] and [2], are not included.\n\t- The authors claimed that irregular time gaps hinder the performance of the models. This is reasonable because the time gap is not encoded during tokenization. It could be interesting to see whether encoding time information would be helpful for some stratified groups although the gain may be minimal overall.\n\n3. The experiment result reported is limited for a comprehensive understanding\n\t- Prior SOTA mentioned in the manuscript (CLMBR-t-base) is also a transformer-based model. However, the context length of this model is not discussed. Additionally, it is not trained with variable context lengths.\n\t- Table 2 provides stratified results of the same experiment as Figure 1 (b) and (d). However, it is confusing that CLMBR-t-base and Hyena don't appear in this table. \n\t- The author hypothesizes that some degree of copy-forwarding can be helpful by emphasizing important diagnoses. This is observed from the CLMBR-t-base but cannot be validated by other models. Moreover, the Brier score of the CLMBR-t-base seems smaller than other models.\n\t- Standard deviation is not provided when comparing different models. The authors only conduct statistical tests between short- and long- context counterparts. However, neither standard deviation nor statistical testing is reported when comparing different methods.\t\n\t- (Minor) The impact of the long-context and proposed properties on pretraining is not discussed. The downstream tasks are enough to show the conclusion but it will be better to see if these factors affect training process.\n\n4. There are some typos in the manuscript\n\t- In line 139, there's a corrupted citation\n\t- 3.3.3 title: Diseae -> Disease\n\t- Reference [Yang et al., 2023a] and [Yang et al., 2023b] are the same\nReferences:\n[1] Guo, L. L., Steinberg, E., Fleming, S. L., Posada, J., Lemmon, J., Pfohl, S. R., ... & Sung, L. (2023). EHR foundation models improve robustness in the presence of temporal distribution shift. Scientific Reports, 13(1), 3767.\n[2] Fallahpour, A., Alinoori, M., Afkanpour, A., & Krishnan, A. (2024). EHRMamba: Towards Generalizable and Scalable Foundation Models for Electronic Health Records. arXiv preprint arXiv:2405.14567.",
            "- Although the authors test the performance of different language models, the tokenization strategies of these models remain the same as the one used in EHRSHOT. It would be helpful to see if using other tokenization strategies could improve performance. For example, Section 4.3 indicates that irregular inter-token time intervals are harder to model. This conclusion is based on EHRSHOT\u2019s tokenization, which doesn\u2019t encode time intervals. However, there are other tokenization strategies, such as those used by ExBEHRT and EHRMamba, that do encode time intervals.\n\nThis paper uses only one EHR dataset to evaluate language models, which somewhat limits its conclusions to the EHRSHOT dataset. While I understand that unifying different EHR datasets is highly intensive work, it would be valuable to see whether similar observations consistently appear in other EHR datasets, such as MIMIC.",
            "- 1.\tThe analysis is solid in its technical execution and experimental design; however, it does not introduce any new methods, models, or techniques, which limits its novelty.\n2.\tThe importance of long context is questionable given the low frequency of extremely long contexts in EHRs. It seems somewhat expected that providing more information for each sample would improve results.\n3.\tWhile the paper states that disease progression increases token complexity over time. I am not very convinced how this property can be problematic with shorter input context compared to longer input context models. \n4.\tThe paper suggests that model performance improves with longer contexts; however, this is not consistently reflected in the figures. If this is indeed the case, it stands to reason that having more information about a patient (i.e., longer context) should facilitate easier predictions."
        ]
    }
}