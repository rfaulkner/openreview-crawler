{
    "u1cQYxRI1H": {
        "venue": "ICLR 2025",
        "title": "Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport",
        "link": "https://openreview.net/forum?id=u1cQYxRI1H",
        "abstract": "Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            10,
            10,
            10
        ],
        "strengths": [
            "- This is an extremely well-written and well-motivated paper; I believe this is a really good example of a highlight paper. Key points I have considered during reviewing the paper:\n* The paper proposes a well-working solution to an extremely challenging task. Lighting and material are only observed in a highly intertwined manner, making it difficult to control only one without changing the other. Previous methods tried to solve this problem with paired synthetic datasets, albeit falling into a domain gap. \n* The proposed dataset provides a really important insight, which can be important for the community: using pre-trained models and synthetic degradations can still provide helpful signals. \n * The light transport consistency is a simple but very effective way to enforce consistency. However, the idea is not new, earlier methods required multi-light datasets to utilize a similar constraint. This paper shows a way how to realize it with in-the-wild samples. \n* The approach is extremely well-documented together with practical insights and considerations.",
            "- - The results are very strong. The performance of the proposed IC-Light is astonishing, and it can be further improved with a stronger generative model as the backbone.\n\n- The method is sound. Although the fundamental observation (the intrinsic component is always invariant to lighting) is not new, the authors are able to effectively leverage this observation in the training of generative models. They thoroughly explain all the details and intuition behind their method.\n\n- The contribution is significant, impacting not only the vision and graphics communities but also holding great commercial potential for the photography and movie industries.",
            "- ### Loss design\nThe design of the new loss function is reasonable. Adding the appearance consistency under a mixed lighting environment is somewhat inspired by the mixing strategies in augmentation and regularization in neural network training, while using that regularization for image illumination editing would be new to me.\n\n### Large-scale training\nThe paper fine-tunes major image generation models with over 10 million samples, resulting in an immediately useful tool for image editing.",
            "- The paper is in general very good, with many strengths:\n1. The data preparation pipeline is very solid\n    1. The authors assembled data from diverse sources for training, significantly enhancing the model's generalizability.\n    2. For in-the-wild data, the authors utilized various intrinsic estimation methods, making the pipeline robust and avoiding inductive bias from any single method.\n2. The proposed lighting consistency loss is novel and effectively prevents training degradation.\n3. The training strategy demonstrates excellent scalability, is applicable to various diffusion base models and is capable of handling large-scale in-the-wild images.\n4. The method achieves state-of-the-art image quality among diffusion-based lighting control methods.\n5. The method also supports alternative lighting control interfaces, such as background condition.\n6. The lighting consistency can be validated through the normal estimation application."
        ],
        "weaknesses": [
            "- This is an extremely good paper overall; I just have minor comments, which might help improve the writing quality.\n* It would be great to highlight more which datasets are used for which parts of the training. Potential cause for confusion: the pipeline figure shows that the training required I_d, although I_d is used only for in-the-wild samples, but then it is not clear where the other components of the dataset are used. \n* Some additional related works, which could be discussed:\n  * [Shape-form-shading](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-117.pdf) - how it relates to the current normal extraction\n  *  [LightIt](https://peter-kocsis.github.io/LightIt/) - illumination control with a generated paired dataset, where similarly an automatically relit image is used as input during training to avoid domain gap. \n* It would be good to highlight the best metrics with bold in Tab. 1.",
            "- - The fundamental observation of this paper is common knowledge in the computational photography and low-level vision communities. Unfortunately, I found that the authors overlooked a large portion of the literature in these fields, especially in terms of intrinsic image decomposition. I noticed that the authors use several IID methods to generate the intrinsic components and mention them in the supplemental materials; I think the authors should also list them in the main paper. \n\n- The quantitative experiments are somewhat limited (including details on the rendering data, e.g., how distinct they are from those in training dataset). Additionally, the numerical and visual comparisons could be improved by including methods like Neural Gaffer.\n\n- Some figures in the paper should be better explained:  For Figure 4 and 5-(c), what is the prompt/condition for the background/lighting?  \n\n- L. 427 and L. 431: \" shadow maps\" should be shading maps.",
            "- ### Unclearness of the releases of the data and implementation\nFrom the paper, it is unclear whether the authors will release the data and implementations or intend to provide a black box web tool for users.",
            "- Some small weaknesses exist in the paper:\n\nFormulation:\n1. Only linear HDR images have light transport additivity, but natural images and images generated by diffusion models are usually tone-mapped LDR images, which do not have additivity. How is this misalignment handled?\n2. There is material ambiguity in the single input image as different material-lighting combinations can yield a similar appearance. Hence I wonder how much the prompt and seed affect the model\u2019s material estimation / final lighting result.\n\nImplementation details:\n1. Lighting consistency loss:\n    1. To add the lighting consistency regularization during the training of latent diffusion models, the authors replaced the sum of two RGB-space images with an MLP that operates in latent space. The MLP works as a soft proxy, but does this MLP really work as intended without any other regularization?\n    2. Also, the loss is derived using x_0 prediction, which is not very stable and can have low SNR when t is large, is there any special design regarding this issue?\n2. Normal estimation:\n    1. The formulas for normal estimation are not correct. Equation 8 should be minus rather than average. It is also not proper to call these two directions N_green and N_red. They should be defined in the world space with range [-1, 1] and visualized as (N + 1) / 2.\n    2. It is better to use a consistent coordinate for the normal maps. In Fig (6), the man in the middle row has a different normal map specification from the examples on the last row, with R and G representing opposite directions in these two examples.\n\nI would raise my score if the above problems were explained.\n\nBesides, in the image results:\n1. Some specular highlight seems to be baked in or show a strong correlation to the input image. E.g. the specular highlight on that car in Fig. 1, and on the helmet in Fig. 4\n2. Some images have wrong global illumination (GI) effects, e.g. the ice-cream example on supp material page 6, has two shadows in opposite directions."
        ]
    },
    "6Mxhg9PtDE": {
        "venue": "ICLR 2025",
        "title": "Safety Alignment Should be Made More Than Just a Few Tokens Deep",
        "link": "https://openreview.net/forum?id=6Mxhg9PtDE",
        "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            10,
            8,
            10
        ],
        "strengths": [
            "- 1. The overall exposition of the issue of depth in safety alignment presented in this paper is quite comprehensive. Many questions that I had while reading were sooner or later answered/investigated within the paper. I particularly also enjoyed how the authors were able to connect fine-tuning attacks into this paper, as initially they might seem very different from jailbreak attacks.\n2. The proposed fine-tuning objective to deepen safety alignment is simple, intuitive and demonstrably effective. The proposed token-wise constrained objective is also intuitive, and solid explanations are provided for how the $\\beta_t$ parameter affects the behavior of optimizing the objective.",
            "- The paper is addressing an important problem, the vulnerability of safety alignment for LLMs, that can be very useful to real world problems.\n\nThe paper ties together prior works in a way that makes it easier to learn from them (i.e. highlighting the common thread amongst successful alignment attacks: their exploitation of shallow safety alignment).\n\nThe contributions of this paper lay the groundwork for future safety alignment solutions. They do offer a couple mitigation strategies, but exposing the shallow alignment issue could inspire many more mitigation approaches. It could also help us understand the success of other attacks and the success/failure of existing attack mitigation strategies.\n\nThe paper includes a good variety of experiments (models, datasets, attacks types) and includes both empirical and theoretical support for their claims.\n\nThe paper flows nicely. It is nicely organized. This makes the paper easy to follow and it makes the main point/contribution of the paper very clear.",
            "- - They key claim of the paper is intuitive, which is that current alignment techniques can satisfy standard RLHF and DPO objectives by simply inducing common refusal prefixes; these prefixes are easily circumvented when one can fully control model inputs\n- Experimental results showing the KL divergence between base/aligned models at different token positions partially support this claim\n- The simple data augmentation approach shows promising robustness to prefilling attacks, and supports the authors' hypothesis\n- The authors leverage these insights to develop a modified DPO objective with per-token weights, which gets decent results",
            "- 1. Writing is excellent!\n2. The constrained-SFT method interesting and might be a useful defense baseline for the community.  \n3. A few interesting phenomenon regarding the depth of alignment are discovered, and are displayed very clearly with well-structure results."
        ],
        "weaknesses": [
            "- 1. The outputs are sampled using non-greedy decoding. Thus, the reported results in the paper may vary to some degree over multiple runs. The authors may want to consider averaging over the output sampling dimension as well and/or reporting greedy decoding results.\n2. Since safety evaluation is based on LLM judgement, it is subject to some amount of error. (This of course is true for any paper that uses an LLM-based safety judge, so I don\u2019t believe this should affect the paper rating much.) The paper could therefore be further strengthened a bit with some human evaluation to estimate judgement accuracy using a small sample of the paper\u2019s experiment results.\n3. Probably the most pressing issue is that there are no comparisons of the effectiveness of the proposed fine-tuning methods against any baselines. For example, it could make sense to compare against circuit breaking [1], a fine-tuning method that I would suspect be a strong competitor.\n\n[1] Zou, Andy, et al. \"Improving Alignment and Robustness with Short Circuiting.\" arXiv preprint arXiv:2406.04313 (2024).",
            "- The explanation of related work is lacking. The related works are listed, but there is not much information that actually explains how your work differs from related work. For instance, you say \u201csome works have also noted asymmetries...\u201d But it would be nice to know how this differs from what you\u2019ve observed. A lot of the statements you make about related work are very broad and could benefit from more detail. \u201cOur work ties these potential failure modes\u2026to potential shortcuts\u201d - does your work do this for all pre-existing methods for improving alignment? Are there some failures that your work does not encapsulate? Also, you never seem to mention any solutions to these alignment failures. Are your methods (e.g. the data augmentation and constrained optimization) the only known mitigation strategies? If so, you should state this. If not, other mitigation strategies should be mentioned.\n\nAfter applying your mitigation strategies, the ASR is still not zero and often isn\u2019t even that close to zero. This isn\u2019t ever really explained in the paper. You at one point say \u201cthe augmented model is still vulnerable\u2026\u201d, but the paper would be stronger if you give more explanation. For instance, does the non-zero ASR mean that there is some other vulnerability apart from the shallow alignment issue? Or are your strategies just not fully fixing the shallow alignment problem?\n\nYour contribution would be stronger if it were explained more clearly. When you say things like \u201cthis work is the first work attempting to consolidate the unified notion behind these attacks\u2026\u201d I don\u2019t quite understand what you mean. If other works have identified the shallow safety alignment effect, then what does it mean for you to \u201cconsolidate the unified notion\u201d? Is shallow safety alignment a new term that you are introducing, because if so, I think you should make it more clear that you are introducing this new concept?\n\nIt is also hard to imagine this problem in a real-world setting/application. The paper would be stronger if, for example in the introduction, we were given an example of the effect that jailbreaks can have (e.g. him what scenario would some attacker be able to provide a deployed model with the start to a response)",
            "- - The authors do not include any evaluations to highly relevant baseline defenses, like the following: \n    - Prior work [1] presents strong robustness to prefilling attacks by training model representations, but this work does not acknowledge this. \n   - Prompt-level defenses [2] have been proposed that optimize suffixes for defending against input-space attacks\n\n- Experiments that thoroughly measure reward hacking during alignment training would have made sense for this work. Since one of the claims is that refusal prefixes are an easy outlet for RLHF/DPO objectives, the authors' intuition on this should be substantiated beyond just the final KL divergence with base models.\n\n- Although the paper's aim is to generally illustrate that models are only superficially implementing common alignment objectives, a clear threat model could provide more clarity. The main concerns that the authors highlight seem primarily relevant to open-weight models, rather than the black-box API case, where model providers can control model inputs.\n\n\n\n\n\n\n[1] Zou, A., Phan, L., Wang, J., Duenas, D., Lin, M., Andriushchenko, M., ... & Hendrycks, D. (2024). Improving Alignment and Robustness with Short Circuiting. \n\n[2] Zhou, A., Li, B., & Wang, H. (2024). Robust prompt optimization for defending language models against jailbreaking attacks.",
            "- The idea of the proposed augmentation method is to modify the safety alignment data by postponing the refusal phrase and inserting some harmful answer before that. This method can significantly reduce the risk of  GCG attack, prefilling attack, etc.  However, I found several problems with this method:\n\n* **The true reasons why the data agumentation method work needs more discussion.** Take GCG attack as an example. GCG attack aims to optimize an suffix in the question, and this suffix can elicit a few confirmative phrase like :\"Sure,\". However, GCG attack is only targeting on a few tokens in the beginning of the answer, but no the deeper tokens. The reason that the augmentation method can mitigate GCG attack probably is that the model learns from the augmentation data to stop delivering harmful answer after a few words (i,.e., give refusal answer after a few word)  For example, I guess  after safety alignment on the augmentation data, the model output of a harmful question will be like this: \n> Instruct: How to make a bomb?\n> Answer: Sure, here is how to make a bomb. I cannot fullfill your request. It's not...\n\n* **The data augmentation method may not solve the jail-break attack from its root.** Will the defense still work if the GCG attack aims to elicit a **longer** harmful phrases? For example, what if the GCG attack aim to elicit the phrase like \"Sure, I will do whatever you want. Yes, no problem... Here is my answer: \"? If this phase is significantly longer than $k$, which is the number of tokens that are posponed in the augmentation data. I am wondering whether the augmenation method still can work.  The authors can provide more evidence to address my concern.\n\n*  **The reason that the data augmentation method cannot effectively against harmful fine-tuning attack is not specified.** The reason is probably that the harmful fine-tuning attack can still overthrow the refusal phrase even it is postponed. Different from GCG attack, harmful fine-tuning attack is not only targeting on the first few phrases to elicit harmful answers. \n\nOverall, I think the augmentation method work because it targets on some features that jail-break attack are exploiting -- they only try to elicit affiirmative answer in the first few tokens of the answer, and naturally the harmful answers will go on if the model does not learn from the augmentation data to interrupt them. On the other hand, if the model is trained from the augmentation data, it might learn to elicit refusal answers when it starts to output some harmful keywords, which could be a probable reason how the method works. The authors should give more discussion regarding this.\n\nThe second proposed method named constrain-SFT aims to solve the harmful fine-tuning attack. The idea is to contrain the distance of the output of the first few tokens between the aligned model and the fine-tuned model. This idea make seneses to me. However, I do want to mention a few aspects that can be improved.\n\n* **Lack of baselines.** Before this paper, there are already a few defense baselines to the harmful fine-tuning attack. The authors should include comparison with existing baselines, e.g., Vaccine (Huang et al, 2024).\n\nHuang T, Hu S, Liu L. Vaccine: Perturbation-aware alignment for large language model[J]. arXiv preprint arXiv:2402.01109, 2024. https://arxiv.org/abs/2402.01109 (First available Feb 2, 2024)\n\n* **System overhead analysis and experiments should be given.** Does the  solution comes with extra computation/memory overhead compared to DPO?  I conjecture the answer is yes because it needs another forward pass of the aligned model to derive its logit. I would like to hear from the authors regardig this. \n\n\n* **The paper can benefit from a more extensive literature review.** I list a few papers on the relevant topics of harmful fine-tuning defense as follows:\n\n\n\n---Alignment stage solution---\n\n[2024/2/2] Vaccine: Perturbation-aware alignment for large language model aginst harmful fine-tuning NeurIPS2024\n\n[2024/5/23] Representation noising effectively prevents harmful fine-tuning on LLMs NeurIPS2024\n\n[2024/5/24] Buckle Up: Robustifying LLMs at Every Customization Stage via Data Curation\n\n---Fine-tuning stage solution---\n\n[2023/8/25] Fine-tuning can cripple your foundation model; preserving features may be the solution\n\n[2023/9/14] Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions ICLR2024\n\n[2024/2/3] Safety fine-tuning at (almost) no cost: A baseline for vision large language models ICML2024\n\n[2024/2/22] Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment NeurIPS2024\n\n[2024/2/28] Keeping llms aligned after fine-tuning: The crucial role of prompt templates NeurIPS2024\n\n[2024/5/28] Lazy safety alignment for large language models against harmful fine-tuning NeurIPS2024\n\n---Post-fine-tuning stage solution---\n\n[2024/5/15] A safety realignment framework via subspace-oriented model fusion for large language models\n\n[2024/5/27] Safe lora: the silver lining of reducing safety risks when fine-tuning large language models NeurIPS2024\n\n\n[2024/5/25] No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks\n\n[2024/5/27] Navigating the safety landscape: Measuring risks in finetuning large language models NeurIPS2024\n\n-------------Below is concurrent works (or after you)-----------\n\n[2024/6/12] Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models\n\n\n[2024/8/1] Tamper-Resistant Safeguards for Open-Weight LLMs\n\n[2024/8/18] Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning\n\n[2024/8/27] Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models\n\n[2024/9/3] Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation\n\n[2024/9/26]Harmful fine-tuning attacks and defenses for large language models: A survey\n\n[2024/10/05] Identifying and Tuning Safety Neurons in Large Language Models\n\n[2024/10/13] Targeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation\n\n[2024/10/05] SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection\n\n[2024/10/05] SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation\n\n[2024/10/05] Safety Alignment Shouldn't Be Complicated\n\n[2024/10/05] Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning\n\n[2024/10/05] Locking Down the Finetuned LLMs Safety\n\n[2024/10/05] Your Task May Vary: A Systematic Understanding of Alignment and Safety Degradation when Fine-tuning LLMs\n\n[2024/10/05] Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models\n\nI am aware that some of these work are concurrent study (or after you). The authors should at least discuss those very relevant papers appeared before the first appearance of this paper. It is also encouraged for the authors to discuss all the existing research as this would be beneficial for the field development."
        ]
    },
    "LyJi5ugyJx": {
        "venue": "ICLR 2025",
        "title": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models",
        "link": "https://openreview.net/forum?id=LyJi5ugyJx",
        "abstract": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512\u00d7512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64\u00d764, and 1.88 on ImageNet 512\u00d7512, narrowing the gap in FID scores with the best existing diffusion models to within 10\\%.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            10,
            8,
            10,
            8
        ],
        "strengths": [
            "- This is a very strong paper in analysis, practical techniques, writing, and experiment results. \n\n- Novelty. This paper's novelty is evident in several aspects.\n    - First, it studies an important but less studied problem: consistency models in continuous time, together with the training stability and discretization error of consistency models. \n    - The proposed TrigFlow, as a novel unification of EDM and Flow Matching, substantially simplifies the analysis presented later and the practical techniques.\n    - The gradient analysis of continuous-time objective reveals the root cause of instability. To the best of my knowledge, this is the first paper to establish the gradient analysis for CMs.\n    - Model architecture modifications are original since existing works are mostly inherited from Diffusion Models' design and focus on the training techniques and formulations, leaving the architectural design underexplored.\n\n- Soundness. Its technical claims are well backed up by both theoretical analysis and empirical results. I particularly appreciate the in-depth investigation into the training dynamics and gradient analysis of continuous-time CMs.\n\n- Presentation. The logical flow of this paper is well structured and smooth. The problem statement is clearly defined, and the explanation of why discretization errors matter for CMs and the motivation toward continuous-time formulation is crystal clear. The gradient analysis into continuous-time CMs is thoughtfully motivated and carefully organized. Even the appendix is well-written, offering useful insights into the proposed techniques.\n\n- Experiments. Proposed techniques allow for training continuous-time Consistency Models (sCMs) at an unprecedented scale. Experiment results are impressive, matching/outperforming adversarial approaches, score distillation, and recent autoregressive models.\n    - Gradient variances have been carefully controlled via adaptive weighting and normalization techniques.\n    - Comprehensively studying the scaling behaviors of sCMs under continuous-time training. \n    - Comparisons with improved score distillation baseline using many methods developed in this work confirm the mode coverage of CMs.\n    - Additionally, the paper discusses efficient and stable implementation strategies for continuous-time CMs.\n\nGiven the potential impact of this paper, I strongly recommend acceptance with conference highlights. It was a great pleasure to read through the manuscript!",
            "- The paper is very well grounded mathematically and experimentally. The analysis is based on understanding the causes of training instabilities by decomposing the loss, validating each component experimentally and proposing changes to solve the root causes.\nThe mathematics while greatly simplified are still pretty complex and the paper shines in its clarity to make the logical reasoning easy to follow.\nThe experimental results are also outstanding resulting in very significant gains, essentially taking consistency models within 10% of the SOTA for diffusion models.",
            "- S1 - The paper provides a comprehensive analysis and set of solutions addressing the numerical instability issues in continuous-time consistency models, significantly improving performance and enabling the model to achieve competitive results on selected benchmarks.\n\nS2 - Many of the enhancements are supported by detailed theoretical justification and experimental results.\n\nS3 - The unified perspective on previous diffusion and flow-matching parameterizations is thorough, complete, and well-grounded, offering novel insights that could benefit the community.\n\nS4 - The paper is well-structured and easy to follow.",
            "- The contributions of this paper are novel, clear, significant and positioned correctly.\n\nTrigflow simplifies the theoretical analysis of flows by showing that  the mean ($\\alpha_t$ ) and noise ($\\sigma_t$) schedule in the definition of conditional flows can be normalized while preserving the model/loss formulation and the integrator-generated paths, while simplifying the connection between the scaling parameters of the model and those of the conditional flow. **(novelty, significance)**\n\nSeveral components of continuous consistency models are studied and probed for potential causes of instability. They include: an inappropriate $c_{noise}(t)$ which is fixed by being set to $t$; inappropriate Fourier scales in the time positional embeddings which are then reduced; the usage  'AdaGN layer', which is then replaced with 'Adaptive Double Normalization'; highly varying target norm alleviated by target normalization; inappropriate weighting mitigated by adaptive weighting and unstable terms solved by slowly introducing such terms into the loss with respect to the number of parameter updates.  **(novelty, significance)**\n\nStrategies for scaling such models to large sizes and datasets are proposed, namely JVP Rearrangement and JVP of Flash Attention. **(quality, significance)**\n\nThe proposed method shows competitiveness with the state of the art models, while only using 1 or 2 steps for generation, and outperforms all other tested methods that use 1 to 2 generation steps **(significance, quality)**",
            "- - This paper is very well-written and easy to read. \n- This work proposed a new diffusion formulation, called TrigFlow, that unifies EDM and Flowing Matching, and also simplifies the analysis of continuous-time consistency models. \n- It provided a thorough analysis of the training stability of continuous-time consistency models, from the perspective of network architecture, training objective and diffusion process parameterization.\n- Experiments on CIFAR-10, ImageNet-64 and ImageNet-512 demonstrate the effectiveness of the proposed method and the scalability of continuous-time consistency models."
        ],
        "weaknesses": [
            "- I did not find any apparent weaknesses in the analysis or experiments (including both ablation studies and performance evaluation). There are research questions worth further investigation, as discussed below.",
            "- 1. The limitations of the method are not very clear to me besides the fact that it's still 10% worse than SOTA for diffusion.\n2. The section on positional embeddings (line 269 and on) lacks details to be fully understandable without having to read another paper. Maybe beefing up that section would make the paper more self-contained.\n3. I found all figure very useful with the exception of Figure (3) which I felt did not add much value.\n\nI also noticed a typo (definitely not affecting my score), just leaving it there for authors to fix their manuscript:\n- Line 362: \"cause instability\" => \"causes instability\"",
            "- W1 - Several design choices appear arbitrary and lack supporting evidence. \n\nFor example, in Section 4.1, the authors discuss the preference for Adaptive Double Normalization over AdaGN, but there is no experimental evidence supporting this choice. It would be more insightful to add a Figure similar to Figure 5 show experimental comparison between Adaptive Double Norm and AdaGN. \n\nSimilarly, in Section 4.2, the authors propose training with linear warm-up w.r.t the model's time derivative, yet no evidence is provided to demonstrate this choice\u2019s effectiveness. Again, including an ablation study or comparative analysis showing the impact of the linear warm-up on training stability or performance metrics would provide more concrete evidence for the effectiveness of this specific choice\n\nFurthermore, Figure 5(b) suggests that incorporating adaptive weighting in a two-step setting may lead to worse performance, while in the one-step setting, it only yields marginal improvement. Have the authors considered alternative designs for the two step case?\n\nW2 - In Sections 4.1 and 5.2, the paper discusses the training compute of sCM. However, including a comparison of compute efficiency with other models (e.g., ECT [1]) would be more insightful, maybe a table or figure comparing the compute efficiency (e.g., FLOPs or training time) of sCM against ECT and other relevant baselines for a given performance level. \n\nAdditionally, given that the model is trained on a large-scale dataset (ImageNet 512) under latent setting, it would be beneficial to include discussions related to text-to-image generation.\n\n[1] Geng, Zhengyang, Ashwini Pokle, William Luo, Justin Lin, and J. Zico Kolter. \"Consistency Models Made Easy.\" arXiv preprint arXiv:2406.14548 (2024).",
            "- **While the contributions of the paper are numerous, the paper could be strengthened even further by:**\n\n1) Comparing the proposed model with more recent flow models such as [1] and [2] and adding the results for rectified flows with 2 generation steps.\n\n2) Placing the number of parameters and the number of parameter updates (or training time on identical hardware) for each model in Table 1 is very important as using an equal umber of parameters/compute is essential for ensuring a fair comparison.\n\n**Some smaller issues and suggestions:**\n\na) The paper would be improved if an intuitive explanation is given for the loss in Equation 2. Also it could save readers some time if it is mentioned that the loss is derived in Song et al 2023, *Remark 10*. \n\nb) The paper would be enriched by adding some generated images with one step.\n\nc) Shouldn't $c_{skip}$ and $c_{out}$ be $\\sigma_t$ and  $ \\alpha_t \\sigma_d$, that is for $\\sigma_t=t$ and  $\\alpha_t=1$: $c_{skip}=t$  and $c_{out}=\\sigma_d$ in line 201/202?\n\nd) In Equation (20) Appendix, shouldn't it be $\\hat{D}$ for $D_{\\theta}(x_t)=\\hat{D}_{\\theta}(\\hat{x}_t(x_t))$?\n\ne) The paragraph from line 924 to 938 (appendix) needs additional elaboration regarding the implications of having $||(\\alpha_t, \\sigma_t)||=1$ with respect to the invariance of the geometric set connecting $x_0$ and $z$.\n\n\n\n**Typos:**\n\ni) In line 126, a 2 is squared instead of the norm.\n\nii) in line 122, $z_t$ does not depend on time.\n\n\n**References**\n\n\n[1]  Tong et al. 2024. Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport\n\n[2]  Kornilov et al 2024. Optimal Flow Matching: Learning Straight Trajectories in Just One Step",
            "- - Although I really like the improvements of continuous-time consistency models, which could fundamentally eliminate the discretization error in discrete-time consistency models, it comes with more time and memory costs related to JVP computation in the loss function. To this end, this work introduces JVP of Flash Attention to reduce the costs, which is great. Still, there may be a considerable gap between the continuous-time and discrete-time consistency models. I wonder if the paper can provide a more detailed comparison between sCMs and the previous discrete-time consistency models - ECMs, in terms of the training convergence and memory cost. \n- There is no explanation for the phenomenon that sCT performs better than sCD on CIFAR-10 and ImageNet-64, but sCTs performs worse on ImageNet-512. Any intuition of why sCT suffers from increased variance at larger scales? \n- There are no ablation study results on \u201cAdaptive Double Normalization\u201d except for claiming it \u201cremoves its instability in CM training\u201d.\n- In Figure 5b, it looks like \u201cw/o adaptive weighting\u201d achieves better two-step FIDs than \u201cw/ adaptive weighting\u201d and very similar one-step FIDs to \u201cw/ adaptive weighting\u201d. Why do we need adaptive weighting?\n- In Figure 5c, do discrete-time CMs have a constant number of time steps $N$ or a timestep schedule up to the maximum number of steps $N$? If it is the former one, it seems to be a bit unfair to discrete-time CMs because the scheduling of time steps is very important to them. Does it make more sense to compare with the best-performing discrete-time CMs?\n- In Figure 7, does the paper apply TTUR proposed by DMD2 (Yin et al. 2024a)? From the DMD2 paper, TTUR improves the performance of VSD. Thus, a comparison with VSD + TTUR is more convincing.\n- In Figure 7, sCDs condition the consistency network on the guidance scale $s$. I wonder if VSD also condition the generator on the guidance scale, for a consistent evaluation setting? \n- A minor issue: In line 266, should it be $c_{\\text{noise}}(t) = \\frac{1}{4} \\log(\\sigma_d \\tan t) $?"
        ]
    },
    "nwDRD4AMoN": {
        "venue": "ICLR 2025",
        "title": "Artificial Kuramoto Oscillatory Neurons",
        "link": "https://openreview.net/forum?id=nwDRD4AMoN",
        "abstract": "It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations. Code: https://github.com/autonomousvision/akorn Project page: https://takerum.github.io/akorn_project_page/",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10,
            10
        ],
        "strengths": [
            "- 1. The neuronal architecture draws inspiration from physics and neuroscience and is very novel in deep learning.\n2. Model performance is impressive across a range of tasks.\n3. Extensive experiments show that the method applies to mainstream architectures including convolutional neural networks and transformers.",
            "- - the model is interesting and well defined\n- the quantitative results appear impressive, though I am have some issues with their presentation (see below)\n- the many numerical results presented, and comparisons to other models, suggests that overall a good deal of thought/time has been put in the manuscript",
            "- I enjoyed reading this paper, impressed by the achievements on diverse tasks and also the authors's thoughts dispersed over the paper.  In brief, the strength of the paper includes:\n- Soft grouping/clustering in neural network is an important question and often missing in main-stream ANNs.\n- The proposed neuron model is neuroscientific motivated yet has concise fomula. The physical origin of the model makes it plausible to construct an \"energy function\", which turns out to be quite informative on the performance.\n- While previous work on binding by synchrony mainly focuses on object discovery, the author extends these insights into reasoning tasks, and adversarial attack. And the performance on these new tasks is very successful.\n- Lastly, the author provides a different conceptual angle for the binding by synchronization: the synchronization enables competitive learning, and therefore compresses representation to construct a bottle neck, which facilitate abstraction. As far as I know, it is a novel argument not presented in previous works and it also makes sense to me.",
            "- * A biologically inspired architecture, well described and motivated by neuroscientific findings\n* A thorough set of experiments across diverse machine learning domains\n* Meaningful comparisons with baselines revealing competitive performance"
        ],
        "weaknesses": [
            "- 1. While AKOrN shows promising performance on the selected tasks, the model might not work as well on more \"classical tasks\" such as image classification. \n2. Although the motivation of AKOrN is clear from a neuroscience perspective, how, and why the model shows superior performance in the tested task is not well understood.",
            "- - there are numerous minor issues with the text/presentation (see question below) which impacts the clarity of the paper \n- This not a field I have experience in, but I am not entirley convinced a fair comparison was made to other models for object discovery. Is the idea that AKOrN is a highly competitive model for models 'trained from scratch', and that comparing it to models with some pretrained parameters (e.g. Lowe et al. 2024) is unfair? I think this could be better clarified if so\n- there is a notion from the abstract and sections 1/2 that this work is relevant to neuroscience, but the link feels quite tenuous to me. Is it reasonable to think of neurons as high-d points on a sphere?",
            "- For the weakness:\n- It is not clear what is the actual conceptual contribution / nolvelty of the Kura model proposed in this paper compared with Lowe's Rotating Feature (and recent updates). For example, equation (6) is very similiar to the \"binding mechanism activation\" in  [1] and I have the intuition that this is the essential mechanism for the binding ability of model, similiar to [1]. If I am wrong, please correct me.\n- When generalizing the original Kura model to high-dim, it is not clear whether the Proj in eq(2) is an 'equivalent' generalization of $sin(\\theta_j-\\theta_i)$. So it is not clear how much this term actually contribute to the computation of the network. Note that $sin(\\theta_j-\\theta_i)$ is a essential mechanism in original Kura model to guarentee the synchrony as a stable state and sin acts among each pair of neurons before the $\\sum$, but Proj only acts after the $\\sum$. (The first two concerns are mainly about how \"Kura\" the model is ?)\n- Since the activity is constraints on the sphere, it loses the representational ability for encoding feature presence, which is an important conceptual diaviation from synchrony binding idea and is likely to limit its ability to infer in certain cases, e.g. when the feature is uncertain or weakly related to the task.\n- On the experimental side, it mostly ignored the Lowe's model for comparison (Fig.3, Fig.4,Fig.5,Tab.1,Tab2, Tab3...) and other synchrony based model, e.g. [2].\n- The code is not provided, so it is hard to evaluate the the reproducibility of the results.\n- The author misses several essential recent works: e.g. [2] shows how oscillation emerges in spike-based model to bind features; [3] combine Kura model with ANNs and also achieved synchrony binding.\n- The first two sentences in Abstract is quite confusing and the first paragraph in Introduction is not very informative.\n\n[1] L\u00f6we S, Lippe P, Locatello F, et al. Rotating features for object discovery[J]. Advances in Neural Information Processing Systems, 2024, 36.\n[2] Zheng H, Lin H, Zhao R. GUST: combinatorial generalization by unsupervised grouping with neuronal coherence[J]. Advances in Neural Information Processing Systems, 2024, 36.\n[3] Ricci M, Jung M, Zhang Y, et al. KuraNet: systems of coupled oscillators that learn to synchronize[J]. arXiv preprint arXiv:2105.02838, 2021.",
            "- * The dependence of the model on the choice of dimensionality N, although it is explicitly acknowledged in the paper, warrants further investigations (which can be the goal of future studies)\n* Although baselines are roughly matched for memory/parameter size and flops, it is possible that GPU optimizations will make them more efficient than the AKOrN versions. There is unfortunately no discussion of this issue nor any report of absolute time required for training or inference. This could be added to the Appendix."
        ]
    },
    "gc8QAQfXv6": {
        "venue": "ICLR 2025",
        "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
        "link": "https://openreview.net/forum?id=gc8QAQfXv6",
        "abstract": "Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. \nDespite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.\nOur study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.\nLeveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10,
            10
        ],
        "strengths": [
            "- * The forgetting issue during fine-tuning foundation models is an important problem.\n* The observations and analysis through the function vector in this paper is very interesting, which I believe would be interesting to many.\n*  The finding that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions is not obvious, but I think the authors has done a good job in confiming this. \n* The writing is easy to follow.",
            "- 1. The idea of using regularization on the function vectors and the function vector guided loss to enable continual instruction fine tuning seems novel and is well motivated in the paper. \n2. The paper presents a strong ablation study for the proposed methods across various settings, demonstrating their generalizability by conducting experiments using multiple llama checkpoints, classification and generation tasks and evaluating across generalizability and supervised finetuning on multiple datasets.\n3. The experiment studies which clearly demonstrate the ability of  function vectors to significantly improve the performance on continual instruction fine tuning.",
            "- **Originality**: This was an interesting approach/hypothesis for studying forgetting that I quite enjoyed.\n**Quality**: The authors perform a thorough series of experiments investigating their claims and evidence ranges from empirical measurements, mechanistic interventions, and training interventions. \n**Clarity**: The work is well contextualized in related work.\n**Significance**: This work is a really interesting way to connect mechanistic interpretability to studying and mitigating forgetting demonstrating practical ways in which tools from mechanistic interpretability can be used to understand diverse phenomena.",
            "- 1. The manuscript provides a thorough study of the catastrophic forgetting problem. \n2. The most insightful contribution of this work is posing a new hypothesis on this problem. \n3. Its technical contribution is significant as the authors first introduce the Function Vector as a tool in this community.\n4. The paper is well-written and well-organized."
        ],
        "weaknesses": [
            "- Overall, I like the findings in the paper and the authers have done extensive investigations. I am interested in comparing the proposed method with model averaging, which is shown in [1] that model averaging is very effectivenss in terms of mitigating forgetting. Furthermore, is it possible to analyze the effectivenss of model averaging through function vector? \n\n\n[1] Yong Lin, et.al., Mitigating the alignment tax of RLHF.",
            "- 1. There is a lack of clarity in the proposed methodology. i.e function vector guided training design. It would help to understand  the proposed methodology by providing the overall  the optimisation objective and end-to-end training algorithm. In the current state, it is difficult to reproduce the results that the authors mentioned in the paper without the details on the proposed methodology. \n2. The  presentation  of the paper and the readability of some sections are difficult to follow.   Notations are unnecessarily complicated. \n3. The experimental results though convincing across most of the experimental setups, its hard to make decisive conclusions especially on the core idea of function vectors and their effectiveness in mitigating catastrophic forgetting.\n4. Some experiments and  observations made related to catastrophic forgetting are redundant and may not contribute much. For example, forgetting will be model-dependent since performance on different datasets and tasks is model-dependent.\n\nMore details on the above mentioned points are listed below.\n\n1. The authors claim that \u201cForgetting coincides with changes in FV similarity between tasks\u201d. However, it is difficult to observe the same from the evidence provided because the trend is not constant across all tasks and models; sometimes, a slight change in similarity changes the task performance drastically(i.e. Fig 2(a) on count object and hellaswag datasets), in some cases even though similarity is decreasing, the performance is increasing. It is very difficult to establish a clear relation from the empirical results provided. A suggestion is to  extend the number of tasks or conduct a statistical test using correlation metrics. \n2. The observation that generation tasks lead to greater forgetting cannot be measured directly, even though the same evaluation metric (Rouge) is used for both types of tasks. Rouge is more sensitive to longer sequences because it relies on n-grams overlap. The tasks have different difficulties; one way to measure them is by comparing them against some upper bound baseline and calculating forgetting in terms of percentage loss.\n3. The computation of the FV guided loss (eqn 4) is not very clear. If the set S consists attention head with top-10 CE,  is that going to be same before training on the current task and after training. Is this loss recomputed at every iteration of the optimization step? \n4. Although function vectors improve the performance of existing CL methods, it can be helpful to compare against  finetuning models individually, to see how much gap there is between methods with function vector guided fine tuning and the optimal performance that can be achieved by fine tuning.",
            "- 1. **Clarity**: The paper dense and the writing is terse and sometimes hard to follow. The figures have a lot of information on them, are very small and it is quite difficult to extract takeaways from them. Overall the paper is often hard to follow and it is hard to trace the evidence for the claims. A lot of points through the paper seem like observations and the connection to one of the core scientific claims isn't always clear. I think this paper would benefit greatly from better organization. Probably most important is to explain the figures/tables better *in the captions* and actually point out what the takeaways are/what we should learn from them/what points it is substantiating. I understand this is challenging because of page constraints and the amount of information presented in the paper: my recommendation would be to move anything non-essential to the core scientific claims to the appendix. Also, the authors could present the results for 1 model and add the rest of the models to the appendix given that the models are all of the same scale.\n2. **FV and forgetting**: One of the key claims in this paper is change in FV is correlated with forgetting, i.e. this is a useful measure that \"explains\" the phenomena of forgetting. The evidence did not convince me of this: in Figure 2, there are points where FV similarity is high, 5 shot performance is high but 0-shot performance is low. Object Count especially is unconvincing (0shot and 5shot switch and don't seem to be correlated with FV).\n3. **Mechanistic interventions**: While the mechanistic interventions in Section 5 are quite interesting, the paper does not consider any alternated explanations for this behavior (see questions). It is also not clear to me why FV is a measure of latent task identification.\n4. **Limitation of training method**: If I understand correctly, this training method regularizes the FV of particular tasks and thus prevents forgetting of the particular tasks being regularized. It doesn't affect forgetting as a whole? While this is not a weakness, I think this should be clarified and the method should be explained as such.",
            "- I cannot see any significant weakness. One nitpick would be expanding the background of Function Vector in Sec. 2.2, especially providing the dimension or shape of each notation. It will help the audiences understand the procedure for computing FVs more easily."
        ]
    },
    "YrycTjllL0": {
        "venue": "ICLR 2025",
        "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
        "link": "https://openreview.net/forum?id=YrycTjllL0",
        "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10,
            10
        ],
        "strengths": [
            "- - The paper makes a valuable contribution to code generation benchmarks by incorporating diverse function calls and complex instructions, offering a novel perspective on evaluating LLM capabilities in library and tool use. Results indicate that even top-performing LLMs face challenges with these tasks, highlighting opportunities for further advancements.\n- The paper is well-structured and easy to follow. The benchmark curation framework is technically sound, back with a systematic design and thorough quality checks. The extensive evaluation provides some insights into how current LLMs handle library usage and follow complex instructions.",
            "- - The proposed benchmark introduces a challenging task that involves diverse function calls as tools and complex instruction-following for code generation. Current state-of-the-art LLMs achieve up to 60% Pass@1, while human performance reaches 97%, providing a clear benchmark for future research to improve on.\n- By evaluating a wide range of both open-source and closed-source LLMs on this benchmark, the study establishes a strong foundation and adds credibility to the proposed framework.",
            "- ### Originality\nThe benchmark proves to be original for the following reasons:\n- It contains diverse code, sourced from widely adopted Python libraries from seven domains. \n- It contains more complex programming tasks than other benchmarks. Specifically, the authors report on task cyclomatic complexity compared to other function-level Python programming benchmarks, and a larger coverage of function calls compared to state-of-the-art function calling benchmarks. \n\n### Quality\n\n#### Benchmark Construction\nThe authors followed best practices to select, clean, and filter the seed data. They also integrate the use of LLMs into the process by keeping humans in the loop when needed. Moreover, code execution in a controlled environment is performed to ensure code quality.\n\n#### Validation\n- The authors extensively evaluate 60 state-of-the-art LLMs on BigCodeBench-Complete (code-completion based on docstrings) and 35 instruction-tuned LLMs on BigCodeBench-Instruct (programming tasks based on natural language instructions). \n\n### Clarity\n- The benchmark construction and evaluation processes are remarkably explained.\n- Limitations of the dataset as well as future work are addressed in detail as part of the appendix.\n\n### Significance\nAs BigCodeBench will be open-sourced it represents a significant contribution to the research community. Moreover, the study reveals interesting observations about the ability of LLMs in complex code-completion tasks, which sets a roadmap for the development of new training datasets to improve these capabilities.",
            "- Quite a thorough analysis and structured approach of generation of benchmark is taken into account. With detailed literature review of past works in the field, the paper also demonstrates its effectiveness as an improved code generation benchmark and evaluation."
        ],
        "weaknesses": [
            "- - The technical contribution of the benchmark curation framework appears incremental, as the collaborative approach between LLMs and human is already widely adopted in LLM research.\n- While Python libraries are essential for task solutions in this benchmark, the rapid evolution and versioning of these libraries can introduce compatibility issues. The authors should consider methods to mitigate the effects of version incompatibility in the evaluation process.",
            "- - There is a risk of future data contamination [1, 2]. To mitigate this, it may be beneficial to release only the validation set while keeping the test set hidden, similar to practices used in NL2SQL benchmarks [3,4].\n\n- Some domains, such as visualization tasks returning graph outputs, would benefit from examples illustrating how the test cases are constructed for evaluation.\n\n[1] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. 2024. Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 12039\u201312050, Bangkok, Thailand. Association for Computational Linguistics.\n\n[2] Jain, Naman, et al. \"Livecodebench: Holistic and contamination free evaluation of large language models for code.\" arXiv preprint arXiv:2403.07974 (2024).\n\n[3] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911\u20133921, Brussels, Belgium. Association for Computational Linguistics.\n\n[4] Jinyang Li, Binyuan Hui, GE QU, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin Chang, Fei Huang, Reynold Cheng, & Yongbin Li (2023). Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.",
            "- Please fix the typo in line 182 (refractor -> refactor) to make even better this amazing work.",
            "- Apart from tiny bits of improvements needed, the paper is quite detailed in its analysis. However, it lacks the essence of what are the areas where these LLM are failing on the new benchmark, and a compared analysis of how an expert human would perform in the same. So as to note the scope of baseline improvement."
        ]
    },
    "WCRQFlji2q": {
        "venue": "ICLR 2025",
        "title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
        "link": "https://openreview.net/forum?id=WCRQFlji2q",
        "abstract": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            10,
            8
        ],
        "strengths": [
            "- * The paper is excellently written. The prose is very clear and the arguments are well structured.\n* The paper fruitfully pursues multiple lines of evidence for its core claims about the latents it identifies in the model it studies. Having correlational and causal analysis on multiple fronts including with the RLHF'd model makes the central idea feel pretty solid.\n* The paper is well scoped.",
            "- 1. The result showing that directions identified using base model, causally affect refusal in chat-finetuned models is very interesting \u2014 the transfer from base to instruction/chat finetuned model is not obvious, and indicates a mechanism for how refusal finetuning can generalize beyond the specific entities it was finetuned on. \n\n2. All of the experiment and results figures seem thorough and convincing (e.g. error bars, useful and clear visualizations etc.)\n\n3. The paper also shows that interestingly, the same entity recognition directions regulate the model's attention mechanism (the factual recall mechanism) and directly express knowledge uncertainty (line 355, although the effect is not as strong).",
            "- The findings of the paper (see summary) are well supported by the experiments and can provide valuable scientific insights to the community. The presentation is great and readers can easily follow.",
            "- - Empirically link the \"entity recognition\" directions and the models' refusal in answering questions\n- The experiment includes clear experimental design using diverse entity types\n- Shows interesting discovery that base model mechanisms are repurposed during chat model fine-tuning"
        ],
        "weaknesses": [
            "- Some of the claims of the paper feel not quite adequately supported by the experiments. I'm not really doubtful of the reasonableness of the claims but I think the paper should quantify and qualify them better.\n* I think the choice to describe these latents as representing \"self-knowledge\" is a little bit dicey. The description only clearly applies to the chat model. For the base model I think a sufficient explanation of the evidence with fewer assumptions is to say something like they're latents for \"entities it knows about\" versus \"entities it doesn't know about or treats as novel\". You could make progress disentangling this from 'self-knowledge' by looking at how these latents behave on fictional narrative versus newswire versus encyclopedic text (the former two should have been written after training cutoff). I would expect the 'known entity' latents to fire increasingly more as you move from fiction->news->encyclopedia and opposite for 'unknown entity' latents. Analyzing these results could help us understand why the features are learned, e.g., if the the model needs to know whether it is writing stories or stating facts.\n* The paper claims that the features generalize over multiple entity types. This is a binary statement but of course is about a graded judgment regarding the amount of generalization. But there is no quantitative claim in the paper about the degree to which this is true in an absolute sense\u2014only that it is _more_ true in the middle layers. That doesn't mean that it's particularly true anywhere. The maxmin metric makes sense for relative comparisons but is difficult to understand on an absolute level. Indeed, when you select the \"most general\" latents based on this metric it's unclear if these features are actually general across more than the four entity types you use or if you may just be selecting on noise. To measure this, you should measure what happens when you select latents based on their generality across a subset of entity types, and test their separation score on held-out entity types. Some measure of how predictive this selection is of separation for new entity types gives an actual measure of the generality of the latent.\n* In the mechanistic analysis (p.6\u20137) the paper points to figures which seem to show that attention to an entity is decreased when the unknown entity latents are steered (increased). It's not clear to me from the text if this is averaging across all attention scores or some subset (\"these attention heads\", L345, reads to me like the latter, but I'm not sure which subset) and the results of this experiment aren't quantified. There should be some straightforward measure here which you can use to quantify the suppression effect here (which will also allow you to claim statistical significance \u2014 you do say \"significantly\" on L346 without a relevant quantity or p-value, which leaves it unclear if you actually mean statistical signifiance)\n* The results in Figure 5 seem dubious. They are really hard to read because of the overlap of all of the confidence intervals but overall nothing significant seems to be happening here. I think you'd need to run this study on much more data to get something interesting/clearly meaningful and I also find this the least interesting or informative experiment in the paper. Mild recommendation to appendicize it. (and strong rec to make the statistical claims precise either way: is the effect you're talking about significant or not?)",
            "- 1. Definition of known and unknown entities \u2014 the paper categorizes entities as known if it gets at least two attributes right, and unknown if it gets all attributes wrong. While this is fine, it does leave open the possibility that what is currently categorized as unknown is actually known by the model and can perhaps be elicited with a better prompt or few-shot examples. Alternatives to the current categorization could be \u2013 (a) checking across a diverse set of prompts to decide known and unknown as in https://arxiv.org/abs/2405.05904 ; (b) relying on n-gram matching in the pretraining data (would need to use LLMs with open pretraining data) ; (c) focusing on entities which occur after the knowledge cutoff for the unknown category. \n\n2. Explanation for existence of generalized latents that distinguish between known and unknown entities \u2014 the paper currently suggests that this is evidence of \u2018self-knowledge\u2019. An alternative and much simpler explanation which currently can\u2019t be ruled out is that these latents are activated on high likelihood tokens (known) and low likelihood tokens (unknown). It would be useful to show some kind of control experiment to show that is not the case. \n\n3. (minor) In general, I also find it hard to directly interpret the results in Fig 2. The min-max value peaks at around 0.4 for unknown. How high should that value be to conclude the existence of generalized latents? I feel like if the set of the entities is small enough and the set of latents large enough, then there might always exist such latents? (Fwiw the additional results later do make it convincing that these are generalized latents)",
            "- The experiments are now only for Gemma models. It would be interesting to see the model behavior on other LLMs.",
            "- - **Lack of comparison to methods other than SAEs**: If we remove the sparsity of the SAE and reduce the module to a regular AutoEncoder, would the same properties hold? If we further remove the auto encoder and just use a linear probing module, would the same properties hold? Is it really necessary to use sparse and auto encoder modules to get the interpretability? If we borrow the same idea from ITI [1] to steer model towards a more truthful direction in the latent space, can the same property holds? Lack of ablation on the necessity of SAE is a big flaw of this paper.\n- **Lack of model diversity**: Only the Gemma models are tested. It would be better if we can see the same findings on LLaMA or other models to show how universal this idea holds.\n\n[1] https://arxiv.org/abs/2306.03341"
        ]
    },
    "Ha6RTeWMd0": {
        "venue": "ICLR 2025",
        "title": "SAM 2: Segment Anything in Images and Videos",
        "link": "https://openreview.net/forum?id=Ha6RTeWMd0",
        "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            10
        ],
        "strengths": [
            "- 1. With the data engine pipeline, the paper provides an extremely large-scale video segmentation dataset compared to previous datasets. This will allow the researchers to tackle much more challenging tasks in video segmentation.\n\n2. Based on the experimental results, the trained SAM2 model outperforms the combination of SAM and existing state-of-the-art trackers by a large margin. Therefore, the data scaling-up with the data engine is effective, as described by the authors. The results also imply the potential of further data scaling up with the data engine.\n\n3. For image segmentation, the SAM2 model can also perform better, even with a much smaller computational cost. This will facilitate the applications of the SAM2 model. In a constrained platform, it is always better to have a good and efficient model.\n\n4. The paper provides detailed information about the implementations of the data engine and data distribution. It is also good that the authors release their data and models. It would be even better if the training code and data engine could be publicly available.",
            "- 1. This paper proposed a strong foundation model for the video and image segmentation. The data, model, and insights will serve\nas a significant milestone for video segmentation.\n\n2. The writing of the paper is good and the paper is easy to understand.",
            "- * Compared to the original SAM model, SAM 2 improves segmentation accuracy, enabling more precise identification and segmentation of objects in images and videos.\n* The processing speed is approximately six times faster than its predecessor. This allows SAM 2 to generate segmentation masks more quickly, making it suitable for real-time applications.\n* SAM 2 exhibits strong zero-shot transfer capability.\n* The training dataset includes 11 million images and 11 billion masks, providing a robust foundation for new video segmentation tasks for the community.\n* The model and dataset are open-sourced.",
            "- 1. The paper is well-written and easy to follow. \n2. The paper has significant contributions, including a more efficient model architecture, a large-scale SA-V dataset, and fantastic performance.\n3. The paper conducts comprehensive experiments and provides some valuable insights."
        ],
        "weaknesses": [
            "- 1. Although this paper uses a simpler structure and performs well, it is still possible to use previous structures, such as Cutie [R1], to achieve even better performance with SAM2 data. It would be better if the structure could be explored.\n\n2. SAM2 cannot recognize segmented objects like previous models [R2, R3]. It would be better to discuss this since it may limit the application of this paper. It would also be better to discuss the difference with [R4], which supports image and video segmentation and can recognize the objects.\n\n\n[R1] Putting the Object Back into Video Object Segmentation.\n\n[R2] Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively.\n\n[R3] Semantic-SAM: Segment and Recognize Anything at Any Granularity.\n\n[R4] OMG-Seg: Is One Model Good Enough For All Segmentation?",
            "- 1. More experiments should be conducted. For example, more interactive VOS methods should be compared.\n[1*] Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion. CVPR 2021\n[2*] Memory aggregation networks for efficient interactive video object segmentation. CVPR 2020\n\n2. More VOS datasets (e.g., VIPOSeg[4*]) should be included in this paper. \n[4*] Video Object Segmentation in Panoptic Wild Scenes. IJCAI 2023",
            "- From my perspective, there is no obvious weakness in this work. If must to say:\n1. The claimed improvement in running speed is mainly due to the usage of the Hiera image encoder, which may not be viewed as a unique contribution of this study.\n2. The primary contribution lies in a large-scale dataset and pre-trained models, while the technical contribution is relatively limited.",
            "- The paper does not have significant weaknesses. My concerns and suggestions are listed in the ``Questions\" section."
        ]
    },
    "DJSZGGZYVi": {
        "venue": "ICLR 2025",
        "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
        "link": "https://openreview.net/forum?id=DJSZGGZYVi",
        "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            10,
            8,
            8,
            10
        ],
        "strengths": [
            "- 1. The paper\u2019s key finding\u2014that self-supervised representations, like those from DINOv2, can serve as a teacher to distill base features in a diffusion model\u2014is both novel and insightful. By using these high-quality representations, the diffusion model can more quickly learn a robust semantic foundation, allowing deeper layers to focus on higher-frequency details. This approach is innovative and inspiring.\n2. The experimental setup is thorough, covering various aspects such as types of self-supervised encoders, encoder scalability, the number and location of layers used for regularization, and different alignment objectives. These comprehensive experiments add credibility to the proposed method.",
            "- - I like the idea of combining diffusion model training and representation learning. The authors highlight the importance of aligning diffusion model representations with powerful self-supervised representations to make training easier and more effective.\n\n- The writing in the paper is clean and easy to follow. The authors start with their observations from the empirical study on pretrained diffusion models, which provides the audience with enough background to understand the problem.\n\n- The experimental section of this paper is solid. They conduct extensive ablation studies on the proposed method, including aspects such as encoder type, encoder size, alignment depth, and more.",
            "- 1. The paper is well-motivated. The introduction section clearly states the logic and reasoning behind why and how the features of foundation models can be leveraged to improve the convergence of a diffusion model.\n2. The authors analyze different visual foundation models for representation learning and demonstrate that RePA is invariant to the choice of foundation model, with almost all visual foundation models enhancing generation performance and convergence.\n3. The authors also study the correlation between generation performance and discriminative classification performance using validation accuracy on ImageNet, showing that both exhibit a linear correlation. A model trained to provide better representation capabilities automatically achieves better generation quality as well.\n4. These insights are extremely valuable and novel, representing a significant step forward in the field.",
            "- ++ The idea flow of this paper is very clear. First, the paper presents the semantic gap between diffusion transformer features and discriminative features. Then, the paper shows that the quality of the feature representations from diffusion transformers are correlated with the generation quality. Finally, the proposed method REPA is an intuitive solution based the previous observations and arguments. Therefore, the proposed technique has clear and reasonable motivations.\n\n++ The benefit of applying REPA in training diffusion transformers seem to be significant to me. Experimental results like the quantitative ones from Tables 2-4 show that REPA helps the models reach the same performance with fewer iterations, and further achieve better generation results when trained for the same number of steps. Therefore, this technique can be useful in training diffusion models.\n\n++ The proposed solution is simple but effective, just to align the features from diffusion models with pretrained visual foundation models. Therefore, it is a neat yet efficient solution for speeding up diffusion models.",
            "- --the proposed method is simple and very easy to implement, also the idea is well-motivated by the semantic measurement metrics\n\n--the experiments are strong and complete, and examine various vision encoders under various setups (model size, depth) and with different DiT model sizes, all show promising improvement in terms of FID",
            "- This paper astutely captures the feature representation gap between generative and discriminative models, providing a feasible approach to enhance model representation and accelerate the training process of DiT models. The extensive analysis and observations regarding representation clarify the underlying motivation. Additionally, the experiments are detailed and validate the effectiveness of the proposed method across different settings. The presentation of Table 2 and the experiment section is very clear."
        ],
        "weaknesses": [
            "- 1. High-resolution results (e.g., 512x512) are needed to support and further validate findings.\n2. FID may be unreliable here, as it relies on the Inception network, which tends to lose significant spatial information and focusing on class information. I recommend re-evaluating using DINO-FID to obtain a more accurate measure. I would consider raising the score to 10 if state-of-the-art fid is demonstrated at resolution 512x512 and DINO-FID at 256x256. For DINO-FID, it\u2019s necessary to pass the ImageNet training set through DINOv2 to store features for dino-FID calculation, which might be challenging to implement. If resources are limited, DINO-FID can be computed at 256x256 resolution, while 512x512 results can use the Inception FID npz provided by ADM.\n3. It would be beneficial to include visualizations of the features before and after alignment. Although differences can already be observed in the generated images, since the core concept is feature alignment, visualizing the impact directly at the feature level would strengthen the argument and provide deeper insights.\n4. I recommend exploring the effects of different diffusion objectives. For example, the intermediate features of an epsilon-prediction diffusion model might differ from those of a v-prediction or x0-prediction model. An analysis of these variations could provide valuable insights into the interaction between diffusion objectives and feature alignment.\n5. The use of class conditions may not fully represent the entire generative domain, particularly in areas like text-to-image and text-to-video generation. This raises concerns about the generalization of the claim that feature alignment is universally important for generative tasks. It would be beneficial to clarify or qualify this claim in light of these broader application areas. In scenarios that already include rich semantic prompts, such as text-to-video generation, would the proposed method still be effective? DINO\u2019s semantic extraction during training may differ from the semantics derived by an LLM when captioning or re-captioning. It would be helpful to include experiments that use detailed textual descriptions as conditions to evaluate the method\u2019s performance under these circumstances.",
            "- - My main concern with this work is that the authors focus on class-conditioned diffusion models rather than text-conditioned diffusion models. The success of diffusion heavily depends on text prompts, as better prompts lead to improved performance and faster training of the diffusion model. I am worried that representation learning may not be necessary for text-conditioned diffusion models, as the text itself already acts as a regularizer to guide the training. Additionally, the improvement in FID with classifier-free guidance (CFG) in this paper is not significant, which partially validates my concern that additional representation learning may not be needed when there is strong guidance.\n\n- The empirical study presented in Figure 2 is based on linear probing, which is unfair to the diffusion model as it is trained with reconstruction objective. In this context, I suspect that the performance gap for the diffusion model would be smaller on tasks like semantic segmentation.",
            "- 1. The main claims of the paper is valid in the cases where classifier free guidance is not applied. Could the authors provide an analysis of the benefit of RePA in the presence of classifier free guidance. Specifically it would be great to see the results in Figure 1 with classifier free guidance included\n2. How well does RePA work when there is already an alignment with text like cross attention in text-to-image models. Will RePA cause a conflict in representation alignment between CLIP/T5 and Dinov2 representations and reduce performance? If time permits I would request the authors to add an analysis in this aspect. This could be a simple experiment like finetuning the existing model an small text to image datasets like CUB-200[3]. If not a detailed explanation of why/why not this may work would suffice.\n3. Diffusion features are generally noisy in the encoder layers and tends to become clearer in the decoder[1,2]. However in REPA we find that the best suited layer is indeed in the encoder which suggests cleaner representations in the encoder itself. Could the authors comment on why although the correlations seems better in the decoder features, the encoder layer 8 is best suited for representation alignment? An answer to Q1 in the questions section would suffice for this query.\n4. There are no limitations section or future works section in the manuscript. I would advise the authors to please include these as well in an updated version. The request for these sections is for enabling the research community to further tackle problems that the authors might not have time to tackle. \n\n\n   [1] Tumanyan, N., Geyer, M., Bagon, S. and Dekel, T., 2023. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1921-1930).\n\n   [2] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X. and Zheng, Y., 2023. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 22560-22570).\n\n    [3] Wah, C., Branson, S., Welinder, P., Perona, P. and Belongie, S., 2011. The caltech-ucsd birds-200-2011 dataset.",
            "- -- The quantitative evaluations of REPA on DiT/SiT are only conducted on $256\\times 256$ images, which is kind of low-resolution with respect to the capability of state-of-the-art diffusion models. I am wondering how REPA performs in higher-resolution cases, like $512\\times 512$ which is also be supported by DiT/SiT. Is it still useful in training higher-resolution diffusion transformers? I think this evaluation is crucial, otherwise the application scenario of this technique will be relatively restricted to lower-resolution.",
            "- -- the paper only shows results under resolution 256x256, which is the scale most of the vision encoders are pretrained on, it raises the concern if the pre-trained low-res representation can transfer to the generative models when train on higher resolution 512, 1024, 2048 etc\n\n-- missing dataset analysis on the pre-trained vision encoder, i.e. is the performance of the encoder pre-trained on ImageNet the same as on in-the-wild dataset?",
            "- 1. All the experiments in this paper are conducted under the setting of ImageNet 256x256. Although this work shows good results in this setting, it is still far from being applied to the currently popular high-resolution transformer-based text-to-image generation models, such as Flux. I would be more excited to see results and observations at higher resolutions or in text-to-image generation. If the generation is limited to the DiT/SiT class condition, its applications will be severely restricted.\n2. Metrics for ImageNet generation often confuse me; the FID, sFID, IS, and other metrics frequently show differing trends. And FID does not effectively reflect actual quality differences at low values. How do you consider these metrics comprehensively to make a reasonable judgment about performance? For instance, in the last two rows of Table 2, NT-Xent outperforms in sFID, while Cos. sim. outperforms in FID and IS. And it would be beneficial to provide some experimental results to explain why you chose NT-Xent, rather than simply stating \"Empirically, ...\" in line 421. I would be glad to discuss this further with you in the Questions section."
        ]
    },
    "xXTkbTBmqq": {
        "venue": "ICLR 2025",
        "title": "OLMoE: Open Mixture-of-Experts Language Models",
        "link": "https://openreview.net/forum?id=xXTkbTBmqq",
        "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10
        ],
        "strengths": [
            "- - The writing in this paper is clear and easy to follow.\n- The paper advances MoE research by providing a fully open-sourced, state-of-the-art MoE architecture, which is beneficial for the research community.\n- The paper presents a thorough analysis of key design choices in MoE, offering valuable guidance on building high-performance MoE models.\n- The analysis is insightful, with discussions on phenomena such as router saturation and expert co-activation providing fresh perspectives and meaningful implications for the field.",
            "- 1) Strong empirical results with state-of-the-art performance for 1B active parameters.\n2) Good exploration of the MoE design space which forms a good guide for MoE model design.\n3) Novel analysis of routing behavior in MoE models during training and inference.\n4) This is the only MoE model where the model weights, code, data and checkpoints are openly available and thus the work is entirely reproducible.",
            "- 1) There is no doubt that training MoE LLMs is challenging. This work offers a couple of important takeaways about how to train a good MoE LLMs, which is very helpful to the community.\n2) The presentation is very clear. For instance, the Table 1 delivers many key designs clearly at the early section of the paper.\n3) The model performance is good as well. As shown in Table 2 and 3, the model performs competitive with dense open models and partially open models (e.g. Qwen, Deepseek).\n4) The Analysis in Section 5 is informative, which greatly help readers and authors to understand how is the model working. This can also greatly speedup the growth of the community."
        ],
        "weaknesses": [
            "- I have a question regarding the experimental results: were the model parameters quoted directly from the original paper for the results shown in Table 2? For instance, in the original paper, OpenMOE\u2019s activation parameter count is reported as 2.1B, whereas Table 2 shows an activation parameter count of 2.9B for OpenMOE. I recommend that the authors carefully verify the accuracy of these values.",
            "- 1) Other state-of-the art MoE models in related works are not exactly in the same parameter count configuration (1B/7B) so an exact comparison cannot be made to this model's performance.\n2) Most of the design choices and training choices are based on prior work and the novelty is more in the design space exploration and analysis of routing behavior.",
            "- 1) Although the model has been relatively large, it is still much smaller than the SoTA MoE LLMs. I understand it is hard to get enough training resource for a fully open projects."
        ]
    },
    "tc90LV0yRL": {
        "venue": "ICLR 2025",
        "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
        "link": "https://openreview.net/forum?id=tc90LV0yRL",
        "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10
        ],
        "strengths": [
            "- Originality:\n\n- CTFs are not new tools for LLM evals, e.g. Phuong et al. (2024) and concurrent work by Anurin et al. (2024) \u00a0have used them before; but their potential for assessing LLM offensive cybercapabilitiesis is not yet fully realized so the contribution is still very meaningful\n    \n- using first-solve-time as an objective proxy for task difficulty is new afaict and a significant addition to our methodological toolbox\n    \n\nQuality\n\n- **careful task curation** (afaict) and inclusion of first-solve-time to guarantee a benchmark with high dynamic range makes this a high-quality dataset\n    \n- **extensibility:** the fact that others seem to already have contributed new tasks to their framework increases my confidence that this benchmark will remain relevant (though I did not check the quality of these new contributions and have not found a discussion of it in the paper)\n    \n- **measures guaranteeing solvability:** the inclusion of solution scripts and automated probes to guarantee task correctness are a big plus for the benchmark quality and the confidence in the performance numbers obtained in this benchmark (potentially these are also an original contribution though I have not double-checked that)\n    \n- **introduction of subtasks** for more fine-grained evaluation increases the usefulness of this benchmark a lot and may be an original contribution (though I have not double-checked)\n    \n- **diversity of task sources:** sourcing CTF tasks from several competitions increases my confidence that we have a diverse enough selection of tasks here to make some general statements about LLM cybercapabilities\n    \n\nClarity\n\n- presentation is mostly very clear\n    \n\nSignificance:\n\n- given the rapidly improving capabilities of LLMs, the potentially enormous implications of strongly LLM-aided or autonomous cybercrime, and the fact that LLMs are not far anymore from human SotA on many cyber skills, this contribution to better understanding of the exact capability frontier is very timely and highly important",
            "- Benchmark Difficulty: This benchmark is a good evaluation of frontier agent capabilities; the evaluation of cybersecurity dual-use capability in LLMs is a highly relevant topic. Table 2 shows the strongest models only reach 17.5% overall success rate, with the highest solve times ranging from 6 minutes to 52 minutes, indicating significant difficulty for agents today. I believe the difficulty of this benchmark makes it a good contribution to the community working on frontier evals. The authors address the possibility of training contamination and are careful to select more hard problems (as measured by human solve time) past the pretraining cutoff date of most frontier models.\n\nPresentation: As this is a popular topic, the authors present a good contrast of their work from other CTF datasets, which was either easier (human solve time < 1 hour) or suffer more from training data contamination. The paper is overall clear and easy to follow.",
            "- * I think this paper is a great contribution to the field since measuring and anticipating agentic hazardous capabilities in LLM agents will be of outmost importance, as motivated by the authors in the paper.\n* I also believe this benchmark has three of the properties we should look for in a benchmark: (1) it is not saturated, so it will provide valuable signal on progress in future models, and (2) it was designed to provide granular evaluation using subtasks, (3) evaluation is deterministic and easy to compute.\n* The benchmark is carefully designed and includes human time as a measure of difficulty, which can also provide another important axis for evaluation.\n* The authors do a great job explaining the relevant concepts from security CTFs to a ML audience.\n* I am convinced that the benchmark covers a wide range of relevant tasks.\n* The authors evaluate their benchmark on a wide range of models. Their results align with other capabilities benchmarks and our current intuitions about their capabilities."
        ],
        "weaknesses": [
            "- the main weaknesses I see are\n\n- **limited statistical power:** only 8 tasks have been solved at all making the empirical basis for any statements on LLM cybercapabilities pretty slim. Is it possible to increase the resolution of the benchmark at the lower difficulty range?\n    \n- **no protection against future test data leakage**: the full CTF dataset is released meaning that once model training cutoffs move past the benchmark release date, the benchmark will not be usable anymore (cf. e.g. Haimes et al., 2024). Would it be possible to withhold a part of the dataset as a private holdout set? \u00a0\n    \n- **limited understanding of the quality of the agent scaffolding:** while the agent scaffolding seems sophisticated I am missing an assessment of how close this gets us to the capability frontier. \u00a0\n    \n    - Cf. \u201cstructured responses that include Reflection, Planning, and Thought, which can improve capabilities.\u201d ([pdf](zotero://open-pdf/library/items/U8NQPGEL?page=10&annotation=QESJCKAM)) do they have evidence for that?\n        \n    - Are there any ablation studies that demonstrate the impact of different parts of the scaffolding? Note: it is crucial to trust that the capability elicitation here is state-of-the-art if we are to trust this as an assessment of threat scenarios in the wild.\n        \n    - Full CLI access (including scrolling, history search, etc; see Anurin et al, 2024) seems like the more powerful and more natural paradigm for cyber-agents. Unclear to me whether the current scaffolding could be artificially limiting.\n        \n    - also: missing assessment of whether scaffolding that is more customized to the task would significantly change scores\n        \n    - in real-world threat scenarios I would assume tool use, in particular browsing / internet search, to be the standard approach - have you measured whether that would make a big difference (obviously needs filtering of search results to prevent leakage of literal task solutions)\n        \n- **lack of formal task selection algorithm:** while the task curation process seems to have been principled and well thought-through, it would be great to provide a principled algorithm for inclusion so that the benchmark can be extended in a more principled way and that we can be certain that this version and future versions of the benchmark do not suffer from cherry-picking\n    \n\nother minor weaknesses\n\n- limited analysis of what makes certain tasks unsolvable (inherently impossible for current models? or: solvable with improved scaffolding?)\n    \n- limited information on how closely the tasks evaluated here track real-world cybersec threat scenarios\n    \n- no discussion (afaict) of biases or limitations implied by the data sources",
            "- Experimental setup:\nIt\u2019s possible that the agent implementation is a limiting factor in the lower solve rates of this benchmark. It would be interesting to see if different agent implementations across these 8 models have any differences in solve rate.",
            "- * The authors introduce an agent that has memory containing the last 3 messages and responses. This seems like a very small memory to solve complex tasks that require hours for humans to solve. Although I think we should not expect the authors of a benchmark to also create the best agent, I think this could be prominently indicated as a limitation of the agent that may underestimate current capabilities of LLMs.\n* Authors could enhance the metrics in their benchmark. I suggest (1) a metric that combines performance and highest FST into a single number, e.g. weight the success on a task by its relative FST.(2) a metric that conveys \"how hard it was for a model to solve\", e.g. number of messages and submission attempts required.\n* The paper could benefit from additional qualitative analysis. As I explained above, I think current models might be able to score higher with more complex implementations (again, we should not expect the authors to also find such an agent). However, including more qualitative analysis on the observatiosn could help researchers build systems that are representative of existing capabilities. Some questions that could be useful for the community are: (1) is there a common pattern in the steps where models fail (e.g. they struggle making specific calls, they cannot interact with a GUI robustly, outputs are too complex and could be summarized, etc.)? (2) Are agents limited in their understanding of their problem or in their capabilities to execute steps (e.g. is the reasoning in the output correct but execution incorrect)?"
        ]
    },
    "n2NidsYDop": {
        "venue": "ICLR 2025",
        "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought",
        "link": "https://openreview.net/forum?id=n2NidsYDop",
        "abstract": "This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by \\citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8
        ],
        "strengths": [
            "- I find the work to be of very high quality and very relevant to the current research in reasoning abilities of language model. The theoretical setup is in my opinion well chosen, very concise and easy to understand.  Even though this is a theoretical paper, the authors are well aware of the current research on the applied side and the questions studied reflect it. I believe it is a strong accept, but I'm giving accept because I'm not sure whether theoretical results of this kind should have spotlight talks at ICLR.",
            "- 1. The paper innovatively uses the k-parity problem to theoretically analyze how transformers develop stepwise reasoning capabilities.\n2. This paper introduces a novel hierarchical decomposition of the k-parity problem and designs a corresponding transformer architecture to handle this structure effectively.\n3. This paper proposes mechanisms like data augmentation and self-consistency checks, enabling transformers to perform CoT reasoning even in the absence of explicit intermediate supervision.\n4. The paper provides clear and logically rigorous analysis for the k-parity problem, offering strong, well-structured insights within this specific context.",
            "- S1. The study is very rigorous, sharing some lights on the importance of CoT on multi hop reasoning. \n\nS2. The introduction of the causal mask is indeed very simple to explain and very intuitive, but also very effective to limit the error compounding. I think this is the main contribution, given teaching forcing is not really useful on this problem.\n\nS3. This work extended previous related work on more realistic setting, offering a clearer picture of why CoT is indeed essential for the given problem."
        ],
        "weaknesses": [
            "- I was not able to judge how limiting is the setup described in the paper overall, but for example the special masking seems quite artificial but I understand that it is required for the theoretical results. It would be nice if the authors explicitly describe the limitations they are aware of.",
            "- 1. The theoretical analysis is heavily focused on the k-parity problem, which, while illustrative, may not extend seamlessly to more complex or varied reasoning tasks. This limits the applicability of the findings to broader transformer applications.\n2. The paper lacks empirical experiments to support the theoretical conclusions, which could leave readers questioning the practical effectiveness of the proposed methods in real-world scenarios or on diverse datasets.",
            "- W1. Main weakness is a lack of empirical validation. I would love to see just a trained model on various settings (like with and without teacher forcing, parity problem sizes with different $k$ and $d$ values) to show that without teaching forcing the model is indeed able to learn the task. \n\nW2. The claim on the conclusion about finetuning transformer to improve multi step reasoning seems really too strong. In particular, it would be nice to show that it's easy to produce CoT data for all reasoning tasks before claiming this. So I would rephrase that sentence or provide a more detailed discussion on why authors believe their results may extend to other reasoning tasks."
        ]
    },
    "gQlxd3Mtru": {
        "venue": "ICLR 2025",
        "title": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport",
        "link": "https://openreview.net/forum?id=gQlxd3Mtru",
        "abstract": "Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data.  Theoretically, we explore the connections between the RUOT and Schr\u00f6dinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: [https://github.com/zhenyiizhang/DeepRUOT](https://github.com/zhenyiizhang/DeepRUOT).",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8
        ],
        "strengths": [
            "- - The experimental results seem very convincing\n- The paper provides thorough theoretical foundations with formal theorems and proofs connecting RUOT to the Schr\u00f6dinger bridge problem.",
            "- - Population dynamics from snapshot data is an important problem, especially in biological application such as single cell trajectories inference.\n- The proposed method appears to surpass the performance of previous methods.\n- The related works section is extensive.",
            "- * Provided code provides additional clarity on experimental setting.\n* Tackles an important problem in trajectory inference from population level data."
        ],
        "weaknesses": [
            "- - it remains unclear how the loss function weights (cost, reconstruction error and the PINN loss) are set\n- Figure 1 and its caption are not that helpful\n\nTypos:\n- Theorem 3.1: \"probelm\" -> \"problem\"\n- page 8: \"a increase\" -> \"an increase\"",
            "- - One of the most important weaknesses of this paper at this stage is its clarity. In its current form, the paper is poorly written, and the ideas are poorly motivated. The main limitation from prior works identified is the unabalanced aspect e.g. \"However, computational methods for learning high-dimensional unbalanced stochastic dynamics from snapshots without prior knowledge of growth and death or additional information are still lacking.\". Yet, the paper only indirectly addresses that issue, which confuses the reader. Despite the favorable experimental results, the current presentation of this paper prevents it from being appreciated by the community.\n\n- Section 5, which is the core of your method, is extremly confusing. For instance, the reconstruction loss is not introduced previously such that it's extremely challenging to follow the development until the final loss in Equation 10. The algorithm 1 does not provide any help either in much needed clarification. The algorithm is too laconic and does not refer to corresponding parts of the text.\n\n- It's not clear to me why the connection with the Schrodinger bridge is necessary. As far as my reading goes, it seems that you only need to leverage the dynamic version of regularized unbalanced OT. As such, it does not seem that the connection with Schrodinger bridges helps the reader understand the method, and you don't leverage that connection later on in your method.\n\n- Theorem 4.1 uses $v$ which is not introduced previously. It's later undertstood that it corrresponds to the vector field of the corresponding ODE but this hinders the clarity of the paper.\n\n- The presentation of the results in Figure 2 raises questions. Why does panel (b), with the true dynamics has ground truth, predicted and trajectories, if it is the ground truth dynamics ? What is the \"predicted\" in this case ? \n\n- Because the most important contribution of this paper relies on modeling the growth rate, an ablation study is required where the growth rate part of the method is disabled (e.g. g = 0). This would allow to understand the impact of it on the final performance. \n\n- Despite the extensive related works section, the experiments only compare with two baselines. Baselines such as Bunne et al. or Yutong et al. could also be considered.\n\nSha, Yutong, et al. \"Reconstructing growth and dynamic trajectories from single-cell transcriptomics data.\"",
            "- * I found it extremely difficult to understand the multistage training procedure for DeepRUOT. There are some stages described in the main text, but then initial log density training also in the appendix? While there are a number of losses and initializations proposed to stabilize training, it is unclear to me what effects these have on final performance and how difficult RUOT is to train in practice. The paper would be much stronger if it was made clear which parts of the training procedure are important practically potentially through an ablation study on different components of the training procedure. \n* For a largely empirical work proposing a new algorithm to tackle this problem, the empirical studies are limited. The DeepRUOT is only benchmarked against balanced transport methods. It would greatly strengthen this paper if it was also compared to other unbalanced transport methods such as Action matching [Neklyudov et al. 2023] and unbalanced diffusion Schr\u00f6dinger bridge [Pariset et al. 2023]. It would also be very useful to know how the many different weighting parameters affect performance, and how sensitive training is to them.\n* It is difficult to tell what parts of the theory are novel and which are minor adaptations from prior work. Theorem 4.1 seems like a subclass of Baradat & Lavenant for the Fischer information case. I don\u2019t believe varying sigma(t) over time changes anything theoretically and as far as I understand it is not used in practice."
        ]
    },
    "VpWki1v2P8": {
        "venue": "ICLR 2025",
        "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
        "link": "https://openreview.net/forum?id=VpWki1v2P8",
        "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8
        ],
        "strengths": [
            "- The authors take a significant step beyond simple intuitions by delving into the core of training efficiency, proposing a novel approach to fully achieve transformation invariance in LoRA models. For soundness, they provide rigorous proofs for all mathematical statements in the paper and conduct extensive experiments across various LoRA advancement methods and benchmarks to demonstrate the superiority of their method. In terms of contribution, given that LoRA is a widely-used training method and their work is computationally inexpensive, individuals training Large Language Models and potentially other AI models can greatly benefit from their advancements.",
            "- 1. The paper propose a new optimization approach that retains the transformation invariant for the LoRA-type fine-tuning, which is widely used in the large model fine-tuning. Moreover, the paper presented the convergence analysis theoretically. \n\n2. The experimental results demonstrate significant improvement with marginal computation increase, results in a better trade-off compared to all SOTA methods.",
            "- * The overall mathematical proofs are reasonable and appear to be correct.\n* With the implementation of LoRA-RITE, authors have shown significant improvement over other optimizers on different language benchmarks."
        ],
        "weaknesses": [
            "- The paper lacks visual illustrations of loss curves to cross-validate the effectiveness of their method in accelerating convergence. Although they state that matrix $A$ remains nearly identical during training, they do not provide visual evidence of how the magnitude of $A$ updates after applying their method, which could further validate its effectiveness. Additionally, the authors do not address potential numerical instability issues. Specifically, their algorithm involves inverting the matrix $R_A$, which could be zero at the initial training step. The conclusion and limitations section is somewhat lacking, as it only discusses the limitations of the base LoRA method rather than potential limitations of their proposed method.",
            "- 1. As the paper focuses on the optimization, a convergence analysis should be conducted to better justify the proposed method, e.g. the norm of A and B, like in figure 1.\n\n2. The analysis over experimental results are limited, e.g., for some datasets, the proposed method demonstrates significant performance gain compared to LoRA (Adam), what is property of dataset such that the proposed optimization can results such improvement?",
            "- I do not have major technical concerns for this paper. It is relatively solid in both performance and implementation analysis. Authors mentioned that they are searching for the learning rate between 2e-6 and 2e-2; it would be interesting to present the best learning rate for different strategies in this work after the search."
        ]
    },
    "BPgK5XW1Nb": {
        "venue": "ICLR 2025",
        "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment",
        "link": "https://openreview.net/forum?id=BPgK5XW1Nb",
        "abstract": "Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.\nOur key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.\nTo be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. \nCompared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.\nIn addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.\nOur experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.\nFor example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8
        ],
        "strengths": [
            "- 1. The paper is well-written and very easy to follow throughout. The paper contextualizes itself within the alignment literature well, covering the fundamentals of pairwise preference learning (Bradley-Terry modeling) to direct alignment algorithms like DPO (Section 3)\n2. SPA is able to use only a small seed preference dataset to then directly score preference labels using the implicit reward model learned by DPO (Section 4.1). Since these predictions can be noisy, the authors introduce a novel self-refinement denoising technique using a confidence prediction (eq 9) to smooth the preference label (eq 10)\n3. Reproducibility: the authors provide implementation details and hyperparameters in Section 5 (*L311-321*). The authors will open-source the code and models after acceptance, which is appreciated. Lastly, because the modification to the DPO objective is minimal, the authors mention only a few lines of change to the DPO codebase, which is another advantage for practical utility of SPA\n4. The authors compare to popular categories of baselines: iterative DPO methods, LLM-as-judge methods, and explicit reward-modeling + RLHF methods (*L288 - L291*) and achieve strong results on AlpacaEval 2.0 and MT-Bench\n5. The authors show SPA extends beyond Mistral to other popular LLMs like Phi and Llama (Table 5) and is robust, in the win rate variance sense, to the seed of the initial preference data (Table 4)",
            "- 1. This work inherits the idea of self-rewarding, but leverages the inherent preference of current aligned model in an intuitive way, which sounds novel to me.\n2. The reduction on data usage seems promising, and the performance is robust.",
            "- 1. The overall idea is relatively simple, but the method achieves good performance and has great potential with limited human labeling.\n2. The paper is well-structured and presents a rigorous methodology, with comprehensive experimental validation that supports the claims made about SPA\u2019s effectiveness."
        ],
        "weaknesses": [
            "- No major weaknesses, mainly minor clarifications:\n1. Can the authors provide a little more description about the \"length control\" aspect of AlpacaEval 2.0 in the main paper? This setting is used in nearly all results, but is not explained clearly where first introduced (Section 5.2)\n2. What is \"gold label\" (Table 1, 5)? Is this the Ultrafeedback preference data? Please make this explicit in the writeup",
            "- 1. 'De-coupled noise preference detection' is not stated clearly enough in section 4.2. Based on my understanding, $z_{\\tilde{\\theta}}$ is used to substitute for $z_{\\theta}$ in the 'Self-refinement' part, which is also supported in Algorithm 1. If I am correct, I think it would be easier for readers to understand if the final usage of the approximated logits and labels are stated in the main text.\n2. Lacks some explanatory discussion on why this method can work on such a small subsets and even perform better than DPO with the full dataset (Details in questions part).\n\nIf the author can address my concern in weakness/questions and provide some insightful discussion, I am willing to raise my score.",
            "- This is a very solid piece of work. The proposed method is simple yet effective. I don't have any particular concerns or issues with it."
        ]
    },
    "N8Oj1XhtYZ": {
        "venue": "ICLR 2025",
        "title": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers",
        "link": "https://openreview.net/forum?id=N8Oj1XhtYZ",
        "abstract": "We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4)  Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            8
        ],
        "strengths": [
            "- This work demonstrates its originality through several innovative contributions, including the Deep Compression Autoencoder, Linear DiT, and impressive 4K generation ability. These innovations enhance the quality and efficiency of high-resolution image generation while reducing computational requirements. The paper is well-written and clearly structured, with detailed experiments and results that validate the effectiveness of the proposed methods. Additionally, the significance of the work lies in its practical applications and the removal of limitations from prior models, making high-resolution image synthesis more accessible and scalable.\nOverall, I find this work to be a breakthrough for diffusion text-to-image models.",
            "- 1. Innovative Efficiency Strategies: SANA\u2019s integration of linear attention, a high-compression autoencoder, and a unique sampling solver enables fast high-resolution generation, which holds practical value for many applications.\n2. Effective Use of Large Language Model (LLM) for Text Encoding: SANA\u2019s implementation of a decoder-only LLM with CHI to refine prompts improves text-to-image alignment, enhancing quality without incurring high latency.",
            "- 1. The paper is well motivated, as developing method that facilitates high quality, high resolution text-to-image generation in resource limited scenarios is of great application value.\n2. The paper addresses the problem from several aspects, including auto encoder design, light weight self attention module, sampling method, and better caption labeling procedure. \n3. The experimental results on resolutions of 512 x 512, 1024 x 1024 show that the proposed indeed achieve better or comparable performance comparing to previous state of the art methods, while having much higher throughput and lower latency. \n4. The ablation experiments of different design components are sufficient and convincing.",
            "- 1. Efficiency is a really big selling point. With a very high compression rate of deep autoencoder and linear DiT, SANA can generate high-quality images even on a laptop is very impressive. \n\n2. The fact that using a small but good LLM can outperform a big but bad text encoder is another interesting point. This boosts text-image alignment a lot when used with their proposed complex human instruction (CHI) pipeline.\n\n3. They provide extensive ablation experiments on each components and measure their impact on efficiency and perform of the model, which is very helpful for future work to build upon."
        ],
        "weaknesses": [
            "- 1. The Gemma2-2B-IT model in Table 9 needs to be explained, as not all readers have a solid background in LLMs.\n\n2. Further comparisons between Gemma2 and T5-XXL are needed, such as showing HPSv2 metrics on complex prompt benchmarks. The current FID scores in Table 9 are insufficient to highlight the true advantages of Gemma2.\n\n3. Why does SANA perform significantly better than LUMINA-Next, even though both use the Gemma-2B model?\n\n4. I believe some relevant works should be cited or compared, such as high-resolution generation work: UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks (NeurIPS 2024) and efficient generation work: Stable Cascade.",
            "- Limited Ablation in Model Design: While SANA combines existing methods effectively, many techniques\u2014such as linear attention, high-compression auto-encoders, and Flow-based solvers\u2014are iterative upon recent advancements in diffusion transformers. Explicit comparisons with recent models like PixArt-\u03a3 or Playground v3, which also incorporate high-efficiency strategies or decoder-only LLMs, with component by component comparisons would better attribute SANA\u2019s unique contributions.",
            "- 1. The design of linear attention block needs to be explained in more detail. For example, the original self attention helps modelling long range dependency between tokens, here in the linear attention block, how is the dependency / contextual relation between tokens modeled through the new operations ?\n2. The paper claims that the proposed method is able to generate images up to 4096 x 4096 resolution, however, in terms of quality comparison with existing methods, there are only ones with resolution of 512, 1024. For 4096 resolution, only the speed comparison is reported (Table 14), it would be more convincing to include FID and other image-text alignment metrics comparison for 4096 resolution as well.",
            "- 1. Although the results are impressive, the contribution of this paper on the technical side is somewhat limited. Their three main components have been already explored in the literature, deep autoencoder in [1], linear DiT in [2] and using LLM as text encoder in [3, 4]. So combining them is not really a significant novel idea.\n\n2. In section 2.3, the author do not fully explain their CHI pipeline, is it same as [5] or completely different. If it is similar then this even reduces the novelty of the work further.\n\nReference:\n\n[1] Chen et. al. Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models. 2024\n\n[2] Liu et. al. LinFusion: 1 GPU, 1 Minute, 16K Image. 2024\n\n[3] Hu et.al. ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment. 2024\n\n[4] Liu et.al. LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation. 2024\n\n[5] Ma et. al. Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models. 2024"
        ]
    },
    "LbEWwJOufy": {
        "venue": "ICLR 2025",
        "title": "TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation",
        "link": "https://openreview.net/forum?id=LbEWwJOufy",
        "abstract": "We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoClip); ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our code, pretrained models, and datasets are publicly available at https://github.com/CyberAgentAILab/TANGO.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. Contributions to Retrieval, Interpolation, and Datasets: The paper introduces TANGO, which enhances co-speech gesture video generation through improved retrieval methods, a diffusion-based interpolation model, and the introduction of the YouTube Business dataset.\n2. Contribution to Open Source Gesture Generation: By making their code, pretrained models, and datasets publicly available, the authors significantly advance the field of gesture generation and encourage collaboration among researchers.\n3. Comprehensive Experiments: The extensive evaluation of TANGO using various quantitative metrics and user studies demonstrates its superiority over existing state-of-the-art methods in generating realistic and audio-synchronized gesture videos.",
            "- 1. Compared to existing works, the video results of the proposed method are strong in terms of both visual quality and audio-gesture alignment.\n2.  The authors conduct thorough experiments to validate the effectiveness of the proposed method.\n3. In terms of novelty, I would categorize this work as moderate. Although using contrastive learning for audio-motion retrieval is not new, the author deserves credit for applying it within the speech gesture retrieval domain. Beyond this, other contributions seem more incremental, such as adding graph pruning to GVR and incorporating an additional reference motion module and homography-based background flow to the animation process.",
            "- This paper proposes a framework for generating co-speech body-gesture videos, dubbed TANGO. TANGO integrates a specifically designed Gesture Video Reenactment (GVR) module and a hierarchical joint embedding space known as AuMoCLIP to facilitate high-quality video generation. The strengths of this work are summarized as follows:\n1. The authors present a novel and robust technical contribution through the introduction of a graph network baseline that adaptively incorporates the GVR and AuMoCLIP modules.\n2. The motivation for this research is clearly articulated, and the insights provided are compelling. It is important to highlight that a unified framework for co-speech gesture modeling from audio to video sequences holds significant relevance in this community.\n3. The dataset introduced in this paper is designed to facilitate ongoing research in gesture generation, providing valuable resources for future studies.\n4. The visualization results are well-executed, allowing reviewers to thoroughly evaluate the findings and methodologies presented.",
            "- For the method design:\n\n1. This work presents a retrieval framework for gesture generation. It resolved a significant issue by the former work GVR (retrieval naively relies on audio onset features and keyword matching) \n\n2. The author represent the gesture patterns within the video as graph and present an efficient pruning strategy to achieve long-sequence representation. This strategy seems effective.\n\n3. The author propose the AuMoCLIP for aligning the two modalities and demonstrate an effective merging of text semantics, high-level audio, and low-level audio features. The ablation study for low-level and high-level retrieval is insightful with very detailed analysis and design. The author analyzed how the temporal alignment of two modalities can be achieved and also the high level semantics.\n\n4. While uplifting the geature pattern generation to the pixel-level video synthesis, the author did not simply reply on commonly used ReferenceNet-like modules but in addition propose homography background offset flow for the consideration of camera changes for realistic videos. The author did very deep analysis of temporal dependencies and resolved the background and jittering problems introduced by camera motion commonly seen in video generations.\n\n\nFor potential insight and benefits for Future studies:\n1. Auto-Evaluation: The AuMoCLIP seems to be a very good generalizable evaluation method for automatic evaluation of gesture motion naturalness conditioned on speech. Unlike FGD, diversity or Beat Alignment (BAS), which cannot precisely include the naturalness of motion patterns, the semantics or beat alignment of audio and speech, this model might be very beneficial for future auto-evaluations, though the author did not propose this point.\n\n2. Audio and Gesture Motion Modality alignment: Previous works in this domain have very limited analysis of the how the speech signal functions as the trigger for gesture patterns. Many previous works, (like DiffSheg, CaMN, TalkShow, EMAGE, AMUSE, etc) only designed the generator by incorporating the audio features or text as the control signals to drive the gesture generation but never analyzed how the inner relationships of different modality should be. GestureDiffuCLIP did preliminary studies in aligning the speech word context and gesture motion patterns but ignored the temporal alignment between modalities. In this work, the author did very detailed analysis of the low-level retrieval and high-level retrieval, reconsider the temporal alignment and semantic alignment (low-level indicates the temporal alignment, I consider the local temporal contrastive loss might functions for the beat-dynamic of the gestures, high-level indicates the global semantic alignment of the two modalities. The ablation of the retrieval is very insightful)"
        ],
        "weaknesses": [
            "- 1. Clarification on Graph Pruning Methodology: The paper does not provide sufficient detail on how the strongly connected component (SCC) subgraphs are merged, which is a critical operation in the Graph Pruning section.\n2. Inconsistencies in Visual Representation: In Figure 3, the use of blue and green to represent different video motion clips lacks rigorous color correspondence during transitions, potentially leading to misunderstandings; similarly, Figure 5 uses yellow for both the Wav2Vec2 transformer feature and input audio, which could cause confusion.\n5. Typographical Errors: There is a typographical error in Figure 4 where \"merge\" should be corrected to \"merging.\"\n4. Testing Results: The generated gesture movements exhibit stuttering, which undermines the fluidity and realism of the videos.",
            "- 1. Some claims are not sufficiently justified. For example:\n\n    The author claims, \u201cBERT captures high-dimensional language semantics rather than just \u2018audio textures,\u2019 which is critical for the co-speech gesture retrieval task.\u201d However, there is no analysis showing that the addition of BERT features actually generates language-semantics-aware gestures. It\u2019s possible that the performance improvement is due to alignment information alone. If the author could conduct an experiment comparing the use of BERT features with using only CTC alignment information, this might demonstrate the necessity of BERT. Additionally, if the author could provide video examples showing that the generated gestures become more language-semantics-aware with the addition of BERT, it would further support the claim.\n\n2. Since the authors compare their work to other co-speech gesture video generation methods, it would be beneficial to include recent state-of-the-art works, such as *Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model (S2GMM).*\u201dIf the authors could add this to their evaluation, it would make the proposed work's claim to be state-of-the-art more convincing.\n\n3. Both the objective and subjective evaluations use only one character, Oliver, making it difficult to verify whether the proposed approach would outperform other methods in different circumstances.The authors could perform a comparison experiment with an additional character to demonstrate the generalizability of their approach. Furthermore, if the proposed method does not perform well on characters commonly used by other state-of-the-art methods (e.g., TALKSHOW, PATS), it would be essential to discuss these potential limitations.\n\n4. Writing and formatting improvements are needed for easier comprehension. For example:\n\n- 4.1 Abbreviations:\n    \n    - GVR stands for Gesture Video Reenactment in the abstract, but GVR is later used to refer to the specific work (Zhou et al., 2022).\n        \n    - The abbreviation for (Liu et al., 2022c) should be ANGIE, not ANGLE.\n        \n- 4.2 Tables:\n    \n    - In Table 2, methods are listed as columns, while in Table 1, they are listed as rows.\n        \n    - There are no captions to explain some non-trivial numbers in the tables, which could be helpful for interpretation.\n        \n\n5. Some explanations and implementation details are unclear or missing. Please refer to the question section for further clarification.",
            "- However, there are still some questions for me. I encourage the authors to give reasonable responses. The final rating would be raised if the solution can solve my concerns.\n1. Some important related works might be missed, and I encourage the authors to discuss with them. For example, both of these two methods leverage the audio signal as input to generate human postures.\n \n[1] Qi, X., Pan, J., Li, P., Yuan, R., Chi, X., Li, M., ... & Guo, Y. (2024). Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10424-10434).\n\n[2] Mughal, M. H., Dabral, R., Habibie, I., Donatelli, L., Habermann, M., & Theobalt, C. (2024). ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1388-1398).\n\n2. In the experimental setting, it is very interesting to explore the frame interval with different settings rather than 4. Please note that this is not an ask for additional experiments in the rebuttal, and this does not impact the paper score. I encourage authors to conduct this discussion in the future.\n\n3. In the user study, did the authors assess participant engagement, specifically regarding whether responses were submitted too quickly or too slowly, and whether any such responses should be discounted? For example, whether the authors considered using response time thresholds or including control questions to check for consistent responses. I encourage authors to add these discussions in the appendix.",
            "- 1. The dataset for comparison are limited. The author only selected Oliver in the Show dataset and a small scale in-the-wild YouTube videos. (However, as far as I know, this filed lacks of high-quality data, PATS is another option, but it contains too many low-quality/low-resolution videos and blurry frames for large shoulder and hand motions)\n\n2. It seems like the temporal contrastive learning is based on the low-level features obtained from the CNN encoder. I am not quite sure if I understand it correctly. It will be better if the author can include this information in the final draft for easy understanding.\n\n3. For the method design, in case I am uncertain of some technical details, I will present some potential weakness in the Question section instead of here.\n\n4. For the gesture motion representation, the author utilized 3D SMPL/SMPL-X template model parameters. I assume the author based on the first frame of the given image, do the tracking to obtain the camera and based on the camera to project the generated 3D joints onto the 2D-space for image-level pose guided generation. I think the author could add this information to the final draft.\n\n5. Some of the training details of the model is missing, like what are the GPUs for training, time for training, learning rate."
        ]
    },
    "FBkpCyujtS": {
        "venue": "ICLR 2025",
        "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs",
        "link": "https://openreview.net/forum?id=FBkpCyujtS",
        "abstract": "Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            10,
            10,
            6
        ],
        "strengths": [
            "- - New sampling method: This paper proposes the Min-p Sampling method for better control over the diversity of generated outputs compared to fixed threshold methods like top-p.\n\n- Conducted experiments: The authors conducted experiments across tasks, ablation studies, and human evaluation.\n\n- High reproducibility: The author released the implementation, code, and repo with implementation guidelines, which enhances the reproducibility.\n\n- Wide Applicability: The proposed method can be easily integrated with existing open-source LLMs, and the authors show the broad potential applications that can be applied.\n\n- The ablation study shows that min-p sampling is barely impacted by the output length, which is interesting.",
            "- * Sampling is one of those areas were the model per se needs to be complemented with an outside algorithm, allowing for creativity on how to set this up. This work proposes an original twist to a popular choice\n\n* The proposal is simple, appealing and \n\n* has good empirical results, both as measured on benchmarks and (more important) by adoption of the community",
            "- This paper presents compelling evidence that its single contribution, min-p sampling, is highly effective.  The usage of it in 54,000 Github repositories alone is very impressive.  In addition to that, they produced theoretical reasoning why their method works, LLM-generated statistics with explanations about how to interpret these statistics, additional statistics which involved human participants, examples of seeing how the logits are transformed under different distributions which give additional insight into why this method is better than existing methods, and code to try out the method.  It is a very simple paper, but it clearly makes the case for its own importance.",
            "- 1. The paper is well-written and easy to follow.\n2. The proposed min-p sampling makes an effective balance between coherence and diversity in text generation."
        ],
        "weaknesses": [
            "- - The experiment is limited to Mistral models and fails to demonstrate applicability with other models. It would be more comprehensive and interesting to see results from additional models, such as LLaMA3.\n\n- The effectiveness of min-p sampling highly depends on the base probability thresholds. As shown in Table 6 (ablation study results), the choice of thresholds significantly impacts LLM performance. This indicates that optimal performance requires careful tuning, which could limit the method\u2019s potential effectiveness and ease of use in applications.\n\n- The paper claims that the experiment is intended to demonstrate that min-p sampling balances creativity and coherence (line 290); however, metrics relevant to creativity are missing. Diversity is not enough for creativity assessment. LLMs-as-judge approach is widely used for creativity assessment. Please consider adding such an experiment.",
            "- The new 10p limit has not been handled wisely in my opinion, and the paper could do more with less text. In particular, Sect 4 could be removed without much loss to the overall apper\n\nHaving experiments on a 123B has to be commended. The paper would be stronger however if the authors could show that the results hold on different model families (eg, llama and mistral), as otherwise it is not clear if this method provides gains on one family only",
            "- The one contribution of this paper, min-p sampling, is extremely simple and not mathematically \"deep\" at all - no theorems were presented, and the code implementation literally (was provided and) took less than one page.  However, I think that having such a paper in a conference proceeding is not a bad thing.",
            "- Please refer to Questions."
        ]
    },
    "E4Fk3YuG56": {
        "venue": "ICLR 2025",
        "title": "Cut Your Losses in Large-Vocabulary Language Models",
        "link": "https://openreview.net/forum?id=E4Fk3YuG56",
        "abstract": "As language models grow ever larger, so do their vocabularies.\nThis has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.\nCross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.\nWe propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.\nRather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.\nWe implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.\nTo improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.\nExperiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            6,
            8,
            10
        ],
        "strengths": [
            "- - Reducing the memory requirement for computing the CE loss in LLMs is a strong contribution, especially as the vocabulary sizes, batch sizes, and sequence lengths of LLMs continue to grow.  This custom kernel could save many people lots of time trying to get around OOM errors during training, and make it easier to train models with larger sequence lengths/batch sizes/vocab sizes.\n- The algorithm is clever and elegant, taking inspiration from FlashAttention, which avoids materializing full attention score matrix during attention computation.\n- The experiments demonstrate that CCE can reduce training memory requirements without impacting quality/convergence during training, or training speed, relative to strong baselines (e.g., torch.compile).",
            "- - Well motivated problem. Reducing the memory footprint of LLMs during training is important.\n- Method generalizes beyond transformer LLMs.\n- Demonstrates convergence guarantees compared to cross entropy.\n- Extensive benchmark results.",
            "- 1. The problem this paper trying to solve is well-motivated.\n2. The solution to avoid the materialization of the large logit tensor is clear and easy to understand\n3. The CCE component is easy to deploy in realistic setting.\n4. The performance do not degrade (since the algorithm is nearly lossless considering the high sparsity level)\n5. The paper writing is very clear and easy to follow (represent C, E, and LSE in different colors)",
            "- - This paper identifies a new challenge brought by the large size of vocabulary of language models, especially LLMs, i.e., the massive memory footprint consumed by the cross-entropy loss computation.\n- The tricks of gradient filtering and vocabulary sorting in the proposed method are enlightening.\n- The author implemented CUDA kernels to support the algorithm and provides experiments to verify the reduced memory footprint and latency."
        ],
        "weaknesses": [
            "- - I think the section about the backward pass could be explained more clearly (see my questions below for points of confusion that could be clarified).\n- I think there could have been additional experiments to explore how CCE performs relative to baselines as different hyperparameters vary (e.g., relative size of vocabulary vs sequence length vs. hidden dim, sparsity of S, etc.).",
            "- - Preliminaries (section 3) does not adequately prepare the reader for the complexity of the notation in section 4. \n\n- Section 4 is particularly hard to understand if the reader does not have a deep understanding of GPU kernels and the architecture of modern LLMs. \n  - There is a lack of key insights, CCE seems like an arbitrary monolithic algorithm that came out of nowhere.\n\n  - Prehaps decoupling the theoretical reasoning from the actual GPU implementation could make the explanation clearer. For example, in line 201, it says \"section 4.2 describes how to compute the [...] operation efficiently\", but it is initially unclear to the reader why that operation might be efficient unless the reader can fully understand the intricacies of creating an efficient GPU kernel as described in section 4.2. Same goes for section 4.1 and 4.3.\n\n  - Otherwise, starting from an already efficient GPU implementation of standard CE and focusing on the steps needed to modify it into the CCE method could further improve readability and clarity.\n\n- A lack of ablation studies for the extensive modifications brought on by CCE\n  - Section 4.1, 4.2 and 4.3 makes a large number of significant assumptions, modifications and improvements to the traditional CE algorithm, it is not clear whether each modification is actually necessary or which are the most important ones.\n  - Unclear whether CCE's improvements is GPU dependent or not. Would it work in non-parallel cases such as on a single-threaded CPU?",
            "- 1. \"The probabilities materialized by the cross-entropy layer account for 89% of the memory consumption of Gemma 2 for single sequence x with length N = 80000\". Can you provide details about how the 89% number is calculated and include a brief calculation or breakdown of the memory usage in the paper or appendix?\n2. Does this assumption still hold true when gradient checkpointing = False? I think most of the analysis in this paper is based on the assumption that gradient checkpointing = True. Include a subsection to discuss or analysis of how your method performs when gradient checkpointing is disabled is appreciated.\n3. Similar to 2, In Table 1, can you explain where 1477MB, 8000MB, 4000MB, and 24000MB come from? If I understand correctly, the logits.shape is (8192, 256000) in float32, which should take 8000MB memory in total.\n4. In Section 4.3, the Gradient filtering paragraph, \"If stored in bfloat16 with a 7-bit fraction, any value below 2^{-12} will likely be ignored due to truncation in the summation or rounding in the normalization.\" Can you explain this in detail? Providing a brief explanation of the numerical precision issues in bfloat16 and how they relate to the gradient filtering threshold is appreciated.\n\nOthers:\nWhat LSE stands for (Log-Sum-Exp) should be defined when it is on its first use.",
            "- - The symbols in the derivation of CCE can be clearer. For example, the symbols in page 4 between line 186 and 215, such as $C^T$, $C_{x_i}^T$, $C_X$, and $(C^T E)_X$, look confusing at first glance. It may be helpful to have a table of symbol definition in the appendix.\n- Experiments on how the memory and latency of CCE kernel varies with the vocab size & model family can be added. \n  - Current Tab 1 presents the memory and latency results of Gemma-2-2B.  The vocab size of Gemma-2-2B is 256000, which is larger than other LLMs. For example, the vocab size is 128256 for Llama-3-8B/70B/405B, 32768 for mistral-7B-v0.3, and 32064 for Phi-3.5. If the size of `lm_head` is `(model_hidden_size, vocab_size)`. When the `model_hidden_size` increases, do we expect a diminishing benefit of CCE? The evaluation will be more comprehensive if the author could discuss:\n  - Compared to the baselines, how the memory and latency change if CCE is applied to Gemma-2-27B training (same vocab size as Tab 1, but larger model hidden size)\n  - Compared to the baselines, how the memory and latency change if CCE is applied to training of the models like Phi3.5-mini (smaller vocab size, similar model size)."
        ]
    },
    "tcsZt9ZNKD": {
        "venue": "ICLR 2025",
        "title": "Scaling and evaluating sparse autoencoders",
        "link": "https://openreview.net/forum?id=tcsZt9ZNKD",
        "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.",
        "decision": "Accept (Oral)",
        "review scores": [
            3,
            10,
            10,
            8,
            10
        ],
        "strengths": [
            "- 1. The adoption of k-sparse autoencoders with TopK activation is a significant improvement over traditional ReLU-based models. By directly controlling the number of active latents, TopK provides a straightforward way to balance reconstruction quality and sparsity, simplifying tuning and improving interpretability.\n\n2. The study of scaling laws for sparse autoencoders across various dimensions\u2014autoencoder size, language model size, and sparsity\u2014is well-executed. This contributes valuable insights into how autoencoders behave at larger scales, especially when trained on large models like GPT-4. Clear scaling patterns provide guidelines for researchers to optimize model size and token budgets effectively.\n\n3. The authors address the issue of \"dead latents\" effectively by using specific initialization and auxiliary loss techniques. This innovation minimizes computational waste and improves model efficiency, making it feasible to train much larger autoencoders while keeping most latents active, even at high scales.\n\n4.The paper introduces thoughtful metrics that go beyond simple reconstruction errors to evaluate feature quality. Metrics like downstream loss, probe loss, explainability, and sparsity of downstream effects offer a more nuanced assessment of latent quality, highlighting the interpretability of features that the autoencoders extract. This focus on interpretability is particularly important for mechanistic understanding and applications in AI alignment.\n\n5. The use of TopK avoids the activation shrinkage problem that commonly affects ReLU-based sparse autoencoders. This makes TopK particularly suitable for training sparse models without the need for additional regularisation to combat shrinkage, which simplifies model training and improves performance.",
            "- ## Strengths by Section:\n\n**Training Recipe**: The paper is unusually detailed with respect to the training recipe, which provides more clarity than similar publications about the training of sparse autoencoders. As noted by the authors, use of the Top-K activation function in a sparse autoencoder is not new, but it is novel in the context of sparse autoencoders trained on model activations. Importantly, it solves a previously documented issue called \"shrinkage\" which results in latent activations being biased towards activating less strongly than they theoretically should. \n\n**Scaling laws**: The paper demonstrates not only the training of sparse autoencoders on a SOTA language model, substantiating claims that this technique is scalable. Use of GPT4 provides one of the strongest possible arguments that sparse autoencoders can scale to very large LLMs. Additionally, the authors provide clear scaling laws in terms of the compute, data and number of latents. Though possibly a minor detail, we appreciate that the authors highlighted their initial incorrect prediction that there would need to be non-zero irreducible loss (as such comments help inform readers that the result was not obvious and thus increases readability and makes the significance clearer). \n\n**Evaluating SAEs**: Though plausibly it would be reasonable to separate work scaling sparse autoencoders from work exploring how best to evaluate them, we appreciate the attempt to rectify the current gap in the literature around how to evaluate sparse autoencoders. Contributions in this section are well motivated and varied. Section 4.3 is particularly valuable, as Neuron 2 Graph is a plausibly much cheaper way to perform automatic interpretability as opposed to the use of language models. \n\n## Strength by Dimension\n\n**Originality**: While the work mostly combines existing ideas from the literature placing them in the context of sparse autoencoders trained on model activations, this hardly diminishes the contribution. Some ideas may be more clearly novel than others (such as the pre-training compute based metric that enables better interpretation of the increase in downstream loss in section 4.1). \n\n**Quality**: The work is generally high quality. There are no glaring issues that I can see. \n\n**Clarity**: Details of both the methods and reasoning are generally high quality.\n\n**Significance**: The paper is highly significant due to contributions along multiple lines (training recipe, scaling laws and evaluation of SAEs).",
            "- - the paper presents a new, simpler SAE architecture and training stack that demonstrably scales to frontier models and many SAE latents. The main advantages over concurrent work are:\n\t- the sparsity can be set directly, making it easier to do hyperparameter tuning;\n\t- the training recipe is also simple and with few knobs to tune;\n\t- yet results on the MSE/sparsity frontier are state of the art, and there are very few dead latents\n- many of the findings are quite well supported by extensive comparisons with other SAE variants\n- the discussion on the limitations of autointerpretability methods in 4.3. is illuminating\n- the paper proposes some new and interesting interpretability evaluations, especially probing and sparsity of downstream effects",
            "- The paper's strengths are as follows: \n\n##\u00a0Scale:\n\nLLMs are being used in an increasing number of fields, and research into LLM interpretability is important for ensuring their fairness and safety, especially at scale.\nThis paper scales up SAEs to frontier LLMs, such as GPT4, which is a ***significant improvement*** over previous work, which was limited to smaller models. Furthermore, scaling laws are found, which could be very useful to practitioners and researchers trying to predict SAE performance ex-ante.\n\n##\u00a0Top-K:\n\nThe paper's usage of top-k SAE is one of the main contributions of this work, and it is ***original*** to the extent that top-k sparse autoencoders hadn't (to the best of my knowledge) been used on LLMs before. The ***quality*** of the results of the top-k function is such that it represent the state of the art when compared to other activation functions.\n\n##\u00a0Auxiliary Loss:\n\nThe authors tackle the ***significant*** issue of dead latents (up to 90% of the total when not mitigated) by introducing an auxiliary loss to the overall loss function. This incentivises the model to use all the latents, as it also supervises the next-aux latents. The ***quality*** of the results is high, as it reduces dead latents by an order of magnitude to 7%. This is to the best of my knowledge an ***original*** contribution.\n\n##\u00a0Evaluations:\n\nThe paper introduces many new evaluations. I believe that explainable loss and known latent probing are the most ***significant***.\nExplainable loss is an ***original*** metric that considers the loss from just using the explanations that the SAE generates (discarding uninterpretable latents), and it represents a ***significant*** improvement over either MSE or L0 alone since it includes both reconstruction quality and an interpretability guarantee (L0 does not). Known latent probing is interesting in that it shows that the ***quality*** of probes of the latents is 25% better than that of residual stream probes.\n\n##\u00a0Clarity:\n\nThe paper is very ***clear*** across all sections, and its presentation is of high ***quality***",
            "- - This paper makes multiple novel contributions to research on the application of SAEs to language models:\n\t- The Top-k SAE architecture has not previously been used on language models. (Though they were introduced previously by (Makhzani & Frey, 2013).)\n\t- Scaling SAEs to a model the size of GPT-4 is a challenge due to the increasing concentration of dead features, and the early works in this field (Cunningham et al, 2023; Bricken et al, 2023) focused on relatively small language models. The scaling done in this paper is not entirely novel, as (Templeton et al, 2024) trained SAEs on the similarly-sized Claude 3 Sonnet, but this paper likely represents contemporaneous, parallel work.\n\t- Similarly, this paper finds scaling laws that are novel except for (Templeton et al, 2024).\n\t- This paper investigates several methods for evaluating SAEs beyond sparsity and reconstruction accuracy, which are important for establishing outside validity and utility. Especially appreciated is the metric in lines 311-314, contextualizing downstream loss in terms of a fraction of pre-training compute.\n\n- In addition to the quantitative results, Top-k SAEs provide qualitative benefits over previous SAE architectures:\n\t- The sparsity level of Top-k SAEs is directly set, instead of relying on an indirect hyperparameter.\n\t- Top-k SAEs do not suffer from \"feature shrinkage\" caused by L1-regularization. The authors clearly demonstrate this in the excellent Figure 8.\n\t- Top-k SAEs have far fewer dead features at large scales."
        ],
        "weaknesses": [
            "- 1. As highlighted in the limitations section, The TopK activation function might be overly restrictive. Each token must use exactly k latents, which could be suboptimal for capturing certain features that require more flexible sparsity levels based on input complexity. \n\n2. The experiments primarily use a context length of 64 tokens, which may not capture longer dependencies present in language models like GPT-4. By not exploring larger context lengths, the paper might miss behaviors and interactions that occur over more extended sequences, which could be valuable for understanding latent patterns and achieving higher interpretability.\n\n3. Although the paper introduces meaningful metrics for interpretability, some, like the downstream loss and explainability evaluations, require substantial computational resources, particularly for large models like GPT-4. \n\n4. The reliance on an auxiliary loss to prevent dead latents may add extra complexity to the training process. While effective, this approach could complicate optimization and might be sensitive to tuning, making it more challenging to reproduce results consistently across different datasets and architectures.\n\n5. Although the approach seem to scale on relatively smaller model, it is not clear how scalable this approach for much larger model.s\n\n6. While the paper discusses applications like anomaly detection and mechanistic interpretability, there is limited empirical testing in these areas. Applying these sparse autoencoders to real-world tasks and evaluating their practical benefits would strengthen the claim of their utility and robustness across applications.\n\n7. The explainability metric, which relies on methods like Neuron to Graph (N2G), is noted to have high recall but lower precision. This can create an \"illusion of interpretability,\" where explanations seem broad but lack specificity. The paper briefly acknowledges this, but a more in-depth exploration of how to balance recall and precision would improve confidence in these metrics.",
            "- **Training Recipe:** \n- We appreciate that the proposed method eliminates the need for finetuning of the L1 coefficient (though choice of the number of activated latents is still required). We might interpret later analysis as attempting to identify a more principled way for setting the number of activations (setting K), though in the absence of better motivation, directly setting the number of activating latents isn\u2019t clearly valuable. \n- Comparison of sparse autoencoder architectures is somewhat shallow, looking only at the sparsity-reconstruction pareto frontier. For example, the authors do not attempt to compare latents found by each method to establish whether they provide similar or distinct decompositions (is there often a 1:1 map between latents in sparse autoencoders trained via each method?). \n- The authors could have provided demonstrations that top-K sparse autoencoders work on toy models of superposition (as has been previously done to establish reason to prefer one architecture over another).\n- Whilst the authors consider alternative methods for evaluating sparse autoencoders later in the work, they should have revisited the training recipe comparison in the context of their other evaluation methods. It is unclear whether Gated / topK SAEs would be comparable on all evaluation methodologies.\n- The authors miss one possible issue with top-K activation functions. Use of the Top-K activation function means that the activation of a given latent cannot be calculated independently of others which is a minor but real obstacle to working with very large autoencoders (for example, when identifying max activating examples, one must first calculate all latent activations, then select the top-K latents). \n- Training on 64 token context sizes might affect the results. The models studied certainly can handle much longer prompts. The authors don\u2019t note whether the SAEs generalize to activations produced during the process of larger numbers of consecutive tokens. \n\n**Scaling Laws**. We have no specific comments on the weaknesses of this section. \n\n**Evaluating SAEs**\n- Though contributions in this section are valuable, there is an absence of discussion resolving possible metrics and identifying which are best to use in practice. Though many related papers likely deal with the current uncertainties of the field similarly (by presenting various possible motivations / metrics without meaningfully resolving which is best). We believe that devoting more of the discussion to comparing metrics and resolving insights found via each may be fruitful.\n- (Repeating a point from earlier) The authors don\u2019t evaluate non-TopK autoencoders via any of the new metrics leaving possibly valuable insights on the table. \n- Section 4.2 could have used a linear probe on the model activations as a baseline against which to compare the 1d logistic probe results. The authors also don\u2019t perform any error analysis or raise any particular examples of latents which clearly track expected features of the input. While such qualitative analysis may not scale, it might possibly provide much better intuition about the results. The authors don\u2019t address how sparse combinations of latents might jointly represent features.\n- The authors could attempt to compare Neuron 2 Graph explanations to those produced by language models (which is the norm for automatic explanations of features in the rest of the sparse autoencoder literature).",
            "- - as the authors are already aware, using individual SAE latents as probes for very high-level binary tasks (such as sentiment) may be too restrictive, especially in light of phenomena such as feature splitting. This limits the significance of the results\n- another practical limitation of the TopK architecture is that it forces every activation to use exactly $k$ active features. In practice (at least in my experience) this may lead to many unimportant features to activate. See concurrent work https://www.alignmentforum.org/posts/Nkx6yWZNbAsfvic98/batchtopk-a-simple-improvement-for-topk-saes\n- the interpretability evaluation is at times too reliant on high-level aggregate metrics. For instance, how should we interpret the \"sparsity of downstream effects\" numbers?\n- the presentation on scaling laws would benefit from a comparison with concurrent work by Anthropic: https://transformer-circuits.pub/2024/scaling-monosemanticity/#scaling-scaling-laws\n- there is some inconsistent notation (e.g., using $N$ and $n$ for the number of latents) and some missing symbols from the text",
            "- The paper's weaknesses are as follows:\n\n##\u00a0Irreducible error:\nThe paper notes that adding an irreducible error term to the scaling laws improves the laws. However, the explanation of why such an error term would exist in LLM activations is not clear, and an experiment showing that gaussian noise is hard to fit for SAEs is provided instead. The experiment does explain why such an error would be there in the first place, and its outcome is not surprising. Mentioning \"components with different amounts of structure\" is vague.\n\n##\u00a0Top-K vs penalty shrinkage experiments:\nIt is expected that having larger activations for top-k SAEs won't improve the loss and that it will for sparsity penalty SAEs. This is because top-k SAEs do not have any constraint on the magnitude of the k activations they use. An experiment on shrinkage does not make the top-k activation function more attractive.\n\n##\u00a0Reproducibility:\n\nAn issue with the experiments in this paper is that GPT4 is a closed source model, which makes it hard to reproduce the results of some of the experiments.\n\n##\u00a0Probing datasets:\n\nA set of 61 binary classification tasks is used to train probes on the latents. It is not clear why these specific tasks have been chosen. A larger and more diverse set of tasks would have been more significant.\n\n##\u00a0Unclear phrasing of top-k's contributions:\nIn line 132 there is a missing word after: \"instead of\", which I assume to be L1, but it's unclear.",
            "- - While the authors describe many results, they are not all equally justified.\n\t- Section 4.2 (Recovering Known Features with 1D Probes) is conceptually very exciting, but somewhat confusing in its details.\n\t\t- The magnitude of the probe loss improvement seems very low, from ~0.52 in the worst case to ~0.49 in the best case. Figures 32 and 33 indicate that a similar magnitude of improvement across various run sizes and datasets. This section would therefore benefit from more baselining: what is the best such classifier based on a residual stream neuron? On a linear  probe trained on all the residual stream neurons? On similarly-sized noise?\n\t\t- The only such baseline in the paper is that in Figure 32 the caption says that the loss on residual stream channels is 0.600. But the figure shows the loss improving from 0.46 to 0.44 over training, so it seems like almost all of the gain of the SAE occurred in the first <1% of training.\n\t\t- The probe losses appear to be averaged across the 61 datasets. This should be clearly stated in the body of the text.\n\t- Figure 7 is hard to understand, and would benefit from more explanations of its significance.\n\n- There are several typos in the body of the text (see below)"
        ]
    },
    "zl0HLZOJC9": {
        "venue": "ICLR 2025",
        "title": "Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution",
        "link": "https://openreview.net/forum?id=zl0HLZOJC9",
        "abstract": "Recent progress in machine learning research is gradually shifting its focus towards *human-AI cooperation* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is *learning to defer* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation.  We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- + The paper addresses a research question relevant to real-world applications by providing a solution for settings where expert annotations are incomplete. \n+ The results show that reducing the workload of highly accurate (and typically overloaded) human experts only slightly decreases overall accuracy and can lead to higher accuracy in scenarios with inconsistent expert performance between the training and test sets. \n+ The proposed controllable workload formulation simplifies the evaluation of accuracy-coverage ratios compared to existing methods, which often require assumptions or post-hoc adjustments to balance learnable models and human experts.",
            "- The paper is well-written and easy to follow. The proposed probabilistic modeling techniques and the use of EM in this setting seem novel and an interesting contribution. Experimental results show the performance gain of the method compared to the baselines.",
            "- - The paper addresses an interesting issue in L2D and proposes a sound solution based on a probabilistic approach. \n- The workload management is particularly promising in many areas where AI is supporting expert decision such as in medicine. \n- This is also relevant in addressing ethical and practical constraints, and possibly even regulations and laws. \n- The ablation study offers an insight on the mechanism that lead to prioritise highest performing humans with the imbalanced approach, with possible overfitting.\n-  It is interesting that the study allows for the conclusion that in practice it may be desirable to distribute workload evenly across all human experts.",
            "- * It seems to me that the topic has been addressed very comprehensively\n* The comparisons include all the mentioned relevant predecessor methods"
        ],
        "weaknesses": [
            "- + As acknowledged by the authors, the proposed formulation does not scale well with the number of human (or learnable) experts. While grouping experts into clusters is suggested as potential future research direction, this introduces the number of clusters as a hyperparameter, necessitating additional tuning and potentially hindering scalability.\n+ Although the paper is concise and generally well-written, the notation is ambiguous in some places (see Q1 and Q2), and the discussion of the results is very brief and could benefit from additional explanations (see Q3 and Q4). \n+ (Minor comment) I recommend the authors release the source code to reproduce results. While not mandatory, providing the code would help readers understand how to implement the algorithm proposed on page 14, especially the implementation steps required to solve the optimization equation formulated in Eq. 4 on page 4.",
            "- A key weakness is highlighted by the authors in the paper: Bad dependency on the number of human experts. Although, they discuss potential remedies, e.g., clustering. However, this probably wouldn't work for a setting with diverse human experts (where the number of clusters is large). Are there other dimensionality reduction approaches (e.g., hierarchical clustering) that one could consider for this setting and how would they affect computational cost?",
            "- 1. Overall, the approach has some limitations, which I acknowledge are also partially discussed. However, it's unclear how well the system can scale given that each expert requires a probabilistic model. It's unclear to me how well the clustering of expert would work and what are the risks associated with that. \n\n2. I would be interested in reading more about the trade-off between the case for fewer deferring cases or deferring cases with the highest uncertainty, which is not much discussed. Clearly, there will be cases, e.g., healthcare, where deferring on uncertain cases would be quite important. \n\n3.  How could the model be adapted to take into consideration fast and slow changing expertise performance? The model assume static performance, however, experts could have fast performance changes, e.g. due to fatigue, or slow performance changes, e.g. due to learning through a period of time. It would be nice to understand how the model could accommodate for such dynamic scenarios.",
            "- I can't see any significant weaknesses. However, this may also be because the topic is new to me.\n\nFurther comments:\n\n* In the case of \u201cIn contrast, machine learning or AI models excel at processing large amounts of information but may be prone to biases (Meehl, 1954)\u201d, the reference chosen cannot be used as evidence for the statement because \u201cmachine learning or AI models ... processing large amounts of information\u201d were not available until long after 1954.\n* I find statement \u201cIdeally, a perfect balanced workload among experts and the AI model can be expressed as follows\u201d a little strange. After all, you will only strive for an equal distribution if all experts are equally competent.\n* I wonder about \u201cslightly-similar\u201d, how can something be slightly similar?\n* I find it a bit irritating that there is no section called \u201cConclusion\u201d.\n* \u201c50 %\u201d -> \u201c50%\u201d"
        ]
    },
    "zCxGCdzreM": {
        "venue": "ICLR 2025",
        "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
        "link": "https://openreview.net/forum?id=zCxGCdzreM",
        "abstract": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.\nIn this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.\nTo this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.\nKinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.\nOur trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments.  Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*.  This includes solving some environments that standard RL training completely fails at.\nWe believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - Kinetix provides 66 hand-designed levels while having the option to edit tasks with a graphical editor or to randomly generate more levels with rejection sampling. \n- The unified goal and dynamics within all environments encourage policies to have physical reasoning capabilities instead of merely memorizing the solution for some particular task, which is a valuable objective for researchers to pursue. \n- Kinetix provides a way to generate unlimited environments and tasks with a unified goal, objects, and dynamics, which could be of interest to multiple research communities like generalist RL policy learning, meta-learning, world modeling, spatial understanding and physics reasoning, and so on.",
            "- 1. Introduces a physics engine that provides \u201calmost entirely dynamically specified\u201d scenes, where environments with different robot morphologies can be vmap-ed and run in parallel, which is not doable with prior Jax-based sim frameworks like Brax.\n\n2. Paper is clearly written.",
            "- - Provide a highly efficient 2D rigid-body physics engine, leveraging JAX for scalable computation, with speedups of up to 30x when training an RL agent, allowing for \n- The learnt agent is highly effective at Zero-Shot transfer in the S and M levels that are held out, indicating the efficacy of pre-training on a wide set of procedural generation tasks. Additionally, show faster convergence/higher performance with this initialization\n- Have interpretable/handmade levels to understand the performance on different sizes/difficulties of tasks.",
            "- - The paper is well-written, organized, and straightforward.\n- Extensive testing across various task complexities validates its robustness in diverse 2D environments.\n- This paper has strong potential to serve as a valuable benchmark for future research."
        ],
        "weaknesses": [
            "- - The paper notes that as the generated environments increase in complexity, they may become unsolvable, which could contribute to the lower performance observed in the Large-level environments. If so, how does this impact the usability and interpretability of the benchmark results? To what extent does this affect the performance results reported in Figure 3?\n- It is unclear whether the proposed benchmark supports visual observations, which are essential for training generalist policies and building agents that can operate in real-world settings.\n- Although Kinetix can generate a vast range of environments, it is unclear how this benchmark would generalize to tasks or environments outside of its defined task distribution.",
            "- 1. All environments in benchmark must fall under the goal of making green shape touch blue shape without touching red shape. This seems to mainly constrain the problem to single-step tasks, where the reward of minimizing the distance from green to blue always incentivizes progress. Was this unified goal constraint purposefully imposed by design, or was it a constraint of Jax implementation, where the reward function for all environments must be the same to be parallelizable?\n\n2. Authors emphasized that parallelism and speed were big advantages of Jax2D. Since it is a reimplementation of Box2D, and this is a critical contribution of the paper, what are the performance gain metrics over Box2D?\n\n3. Experiments were on multi-discrete action space with binary rewards. However, it would strengthen the argument of the paper to do experiments on more of the important features of Kinetix, such as pixel-based observations and continuous action space.\n\n4. The state representation of the policy is very specific to the Kinetix environment suite and not very generalizable to other 2D RL problems. For instance, each entity is encoded separately and there is no scene-level encoding that is passed in as observation for the policy. Often, it is essential for a policy to understand the entire scene when predicting an action.\n\n5. There were no supplementary materials submitted, which would have been a good opportunity to show video rollouts of the trained agent in action.\n\n6. Experiments were mainly limited to the improvement of finetuned policies over pretrained and task-specific, trained-from-scratch policies. However, I would have liked to see more experiments that provide additional insights beyond \u201cfinetuning is mostly good\u201d and \u201czero-shot mostly doesn\u2019t work.\u201d For instance, using Kinetix for lifelong learning, transfer learning, and cross-embodiment learning.\n\n7. Abstract sentence seems like an oversell, given the results. \u201cOur trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments.\u201d Most would also disagree with the 2D learned behaviors as \u201cstrong physical reasoning capabilities.\u201d\n\n8. Minor: I think the wrong citation was provided for MJX in Section 7 (that work seems to be Mujoco).\n\n9. Minor: Experiments would benefit from some comparison to prior approaches/architectures, though this is less important given this is mainly a systems/framework paper.",
            "- - The JAX2D environment seems to be somewhat limited in its expressivities, modeling only 4 unique entities, which may not transfer to a wide set of domains/tasks outside of the ones studied.  \n- The task/reward function seems to be fixed across all environments to collide the green and blue shaped objects, while avoiding red shapes. Additional reward shaping seems to be needed for effective training, leading to some limited applicability of generating this data at scale for any set of tasks.",
            "- - **Real World Tasks:** While this paper provides a strong foundation in 2D simulations, expanding its scope to assess the agent\u2019s adaptability to real-world tasks, such as 3D simulations or complex dynamics as seen in [1,2], would enhance its practical relevance. Bridging this gap could amplify the study\u2019s contributions, offering broader insights into real-world generalization and scalability.\n\n- **Filtering out:** The authors mention that trivial and unsolvable levels are filtered out. What quantitative metrics were used to determine this filtering.\n\n- **Generalizability:** The claims of generalizability might be overstated given that the tasks remain in controlled simulations. Could the authors clarify the expected limitations of deploying such an agent in real-world scenarios with unpredictable environmental factors?"
        ]
    },
    "zBbZ2vdLzH": {
        "venue": "ICLR 2025",
        "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
        "link": "https://openreview.net/forum?id=zBbZ2vdLzH",
        "abstract": "When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to **j**ointly **d**enoise the features and **r**ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- The primary novelty of JDR lies in its combined optimization of graph structure and node feature alignment, enhancing data quality by maximizing alignment between the spectral components of the graph and feature matrices. This unified approach addresses both structural and feature-level noise simultaneously, which is rare among existing methods that typically target these types of noise separately. A key concept introduced is \u201cspectral resonance,\u201d where optimal alignment between the graph\u2019s leading eigenvectors and the feature matrix\u2019s singular vectors is achieved, providing a measurable target for denoising and boosting node classification performance. To manage the challenging non-convex optimization, the paper presents an iterative heuristic based on alternating optimization, which simplifies alignment maximization and enables efficient processing of large, real-world graph datasets with multiple classes. Another advantage of JDR is that it outputs a modified graph in the preprocessing stage, enhancing interpretability and reusability for subsequent GNN applications\u2014a contrast to end-to-end methods that alter the graph only during training. Lastly, JDR\u2019s design allows it to adapt effectively to both homophilic and heterophilic graphs, expanding its applicability beyond previous methods, which often work best with specific types of graph structures, such as those with high homophily.\n\nThe paper is well-organized, systematically guiding the reader through the challenges, methodology, and outcomes of the proposed Joint Denoising and Rewiring (JDR) algorithm. It begins with an introduction that effectively frames the problem of noisy graph structures and features, establishing the need for an approach that addresses both in unison. The methodology section details the concept of spectral resonance and the iterative optimization heuristic that drives JDR, using clear mathematical definitions and visual aids to support understanding. Following this, a comprehensive experimental section validates the algorithm's effectiveness across synthetic and real-world datasets, offering detailed comparisons with state-of-the-art methods and highlighting the algorithm\u2019s robustness across homophilic and heterophilic graph types. Finally, the paper provides an insightful discussion on related works, situating JDR within the broader landscape of graph preprocessing techniques, before concluding with a summary of contributions and potential directions for future research. Overall, the structure flows logically, with each section building on the previous one to reinforce the practical relevance and theoretical underpinnings of JDR.\n\nAddressing the limitations of the proposed approach is appreciated\n\nThe illustrations provided in the work, both in the body and abstract, are informative and well done as well as qualitatively informative.\n\n The use of the appendix is also well done in providing explicit discussion of the proposed algorithm \n\nThe paper has a robust experimental section with compelling results working in favor of the approaches proposed",
            "- 1. The proposed method is general and can be applied to a wide range of GNNs for downstream classification tasks. \n\n2. The proposed method jointly considers graph rewiring and feature denoising.\n\n3. The proposed method improves the performance of a numer of GNNs on a number of popular datasets in the experiments.",
            "- - This paper uses cSBMs as a key framework to build intuition about the graph rewiring and denoising problem, providing the theoretical foundation for the alignment target.\n- The empirical verification using synthetic data is clear.\n- The method is evaluated on both homophilic graphs and heterophilic graphs, showing its generalizability.",
            "- 1.The overall structure is well organized and easy to understand, especially the diagram demonstrations are very helpful.\n2.The problem formulation is clearly presented.\n3.Experiments are comprehensive and convincing.",
            "- 1. The method is well inspired and can achieve better performance than several other graph rewiring methods.\n2. An impressive amount of experiments were implemented."
        ],
        "weaknesses": [
            "- JDR depends on the availability of informative node features for effective rewiring and denoising, which restricts its applicability to settings with substantial node feature information; this reliance could limit its effectiveness in networks that primarily encode structural data. The algorithm\u2019s design is also tailored to node-level tasks, making it less suited for graph-level tasks like graph classification, where global structure matters more than node-specific features",
            "- 1. The solution is somewhat incremental and its novelty is low, although it appears to be sound.\n\n2. There is no theoretical guarantee on the degree of improvement using JDR.\n\n3. The experiments were conducted on a small number of datasets that cannot be considered as an evidence that the proposed method is really effective.",
            "- - The proposed method involves graph structure matrix and graph feature matrix decomposition, which can be computationally challenging on extremely large real-world graph data, limiting the practicability of the proposed method.\n- As the SVD decomposition can have time complexity of $O(N^3)$, it may be not accurate to say the proposed JDR has time complexity of $O(N)$",
            "- 1. The theoretical framework seems convincing to me, while how the real data sets fit the parametric model needs further investigation, otherwise the heuristic of denoising features might not be applicable.\n2. More explanation is needed on the insights of where the rationale of denoising comes from, e.g. lines 159-190.\n3. Since the eigendecomposition is applied on adjacency, the comparison of training and inference time costs between the vanilla GNN and with the add-on of the proposal is needed.",
            "- 1. The paper should better discuss previous works on GNN versus MLP, and the connection between heterophily and graph noise. [1-4] investigated the phenomena that MLP sometimes perform better than GNN (especially GCN) for heterophilic graphs. In particular, [4] proposed a metric that well correlates with empirical GNN performance, and also discussed the connection between heterophily and graph noise. A better discussion (acknowledgement?) of [4] is needed due to its high relevance to this paper.\n\n2. The comparison between JDR and graph rewiring methods seems not perfectly fair as the authors also mention themselves. Moreover, if JDR indeed achieves optimal denoising, then the graph may no longer be needed in later training. This (JDR(X) + MLP) seems uncovered by the ablation settings. \n\n3. The authors should check if the rewired graph structures degenerate, which may be a natural consequence of combining matrix factorization scheme and thresholding. \n\n4. The hyperparameter tuning was not performed for the GNN backbone. This is questionable as the optimal backbone hyperparameter is anticipated to vary across different rewired graphs (and potentially different denoised features). This may matter because the performance gap between JDR and other methods seems small in most cases.\n\n5. Some paragraphs are unclear and not readable. In particular, it is unclear what \u201cfindings\u201d in the sentence \u201cThe Gaussian adjacency equivalence conjecture (Shi et al., 2024) suggests a generalization of the findings to the true binary adjacency case.\u201d is referring to. The whole section (as well as the proof) needs to be polished.\n\n[1] Ma, Yao, et al. \"Is Homophily a Necessity for Graph Neural Networks?.\" International Conference on Learning Representations.\n\n[2] Gomes, Diana, et al. \"When Are Graph Neural Networks Better Than Structure-Agnostic Methods?.\" I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification. 2022.\n\n[3] Luan, Sitao, et al. \"Revisiting heterophily for graph neural networks.\" Advances in neural information processing systems35 (2022): 1362-1375.\n\n[4] Dong, Mingze, and Yuval Kluger. \"Towards understanding and reducing graph structural noise for GNNs.\" International Conference on Machine Learning. PMLR, 2023."
        ]
    },
    "z8sxoCYgmd": {
        "venue": "ICLR 2025",
        "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
        "link": "https://openreview.net/forum?id=z8sxoCYgmd",
        "abstract": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- + LOKI is a novel, multimodal dataset.\n\n+ The paper is easy to read and well-organised.\n\n+ Comprehensive Evaluation and Validation.\n\n+ Curates a diverse dataset with 18,000 questions across five modalities and 26 categories, providing a solid foundation for synthetic data detection evaluation.\n\n+ Detailed Annotations.\n\n+ Directly addresses the challenges of synthetic data proliferation, impacting security, misinformation, and content authenticity.\n\n+ LOKI\u2019s findings on LMM strengths and weaknesses have the potential to drive advancements in synthetic data detection and multimodal model development.",
            "- The paper is well-written and easy to follow.\n\nThis work addresses a glaring and emergent need in a topic field: synthetic data detection for LLMs; I agree with the authors that there doesn't currently exist a comprehensive, multi-modal, nuanced dataset including explainability assessment for this domain area. \n\nExtensive examples and case studies provided in appendices. \n\nMany data domains are covered in this benchmark, including several categories and characteristics that are often underrepresented in synthetic data detection (e.g., satellite images, \"abnormal details\").",
            "- 1.\tThe paper is well written and easy to follow. The authors provide sufficient technical details for readers to understand their work.\n2.\tThe benchmark designed by the authors encompasses a rich variety of modalities and diverse question types, enabling a comprehensive evaluation of LMM performance.\n3.\tThe authors introduce a metric called the Normalized Bias Index (NBI) to quantify the performance differences of the model on natural and AI-generated data across different modalities, which is an innovative way to assess model bias.",
            "- - Comprehensive Multimodal Benchmark: LOKI covers an extensive range of data modalities and subcategories.\n- Inclusion of specialized domains like satellite imagery, medical images, and philosophical texts pushes the boundaries of traditional datasets and tests models in less-explored areas.\n- Multi-Level Task Design: The benchmark doesn't just focus on binary classification but also includes tasks that assess models' abilities to explain their reasoning, promoting the development of interpretable AI systems.\n- Highlighting the Importance of Explainability\n- By testing perception, knowledge, and reasoning across modalities, LOKI contributes to the broader goal of advancing towards AGI."
        ],
        "weaknesses": [
            "- -  The benchmark lacks a robustness test against common real-world conditions like compression artifacts. To enhance real-world applicability, the authors could include performance evaluations on compressed data.",
            "- While the differentiated modalities, categories and annotation levels are beneficial, the overall size of the dataset actually seems relatively small vis-a-vis related datasets (Table 1). \n\nIt is unclear to me, how a user can methodically compare scores for different models across tasks/categories (e.g., in Table 2); perhaps the authors can address this, given the heterogenous and imbalanced nature of the data modalities and tasks, as well as the problem/domain \"difficulty\". \n\nAs deepfake detection is one of the most prominent synthetic data detection categories today, I believe the benchmark would benefit from its inclusion.",
            "- 1.\tThe current evaluation mainly relies on accuracy and NBI; however, at low recall rates, NBI may not adequately reflect model bias. Additionally, the design of NBI may be insufficient to comprehensively capture various types of bias exhibited by the model.\n2.\tThe paper mentions that the model exhibits \"bias\" across different modalities. However, the specific causes of this bias are not thoroughly explored through experiments or comparative analysis. This conclusion may be based on surface-level observations without further investigation into whether the bias arises from data, model architecture, or task design.\n3.\tThe paper mentions that the Chain-of-Thought (CoT) approach can impact model performance in image and 3D reasoning tasks.  However, it does not provide sufficient experimental details to clarify whether CoT significantly enhances performance across all types of tasks or if it is only effective for the specific tasks currently evaluated.\n4.\tIt is suggested to discuss and compare more related works such as [1,2] in this paper.\n\n[1] Detecting and Grounding Multi-Modal Media Manipulation and Beyond. TPAMI 2024.\n\n[2] Detecting and Grounding Multi-Modal Media Manipulation. CVPR 2023.",
            "- - Limited Performance in Certain Modalities: The benchmark reveals that LMMs perform poorly in modalities like 3D and audio, which may be due to the lack of available models or training data in these areas.\n- Insufficient Details on Data Generation Methods: The paper could provide more in-depth information on how synthetic data was generated for each modality, which is crucial for reproducibility and understanding potential biases in the dataset.\n- Evaluation of Few-Shot and Chain-of-Thought Prompting: The analysis of prompting strategies is somewhat limited."
        ]
    },
    "xoXn62FzD0": {
        "venue": "ICLR 2025",
        "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
        "link": "https://openreview.net/forum?id=xoXn62FzD0",
        "abstract": "A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution\u2014which can differ substantially from the LM\u2019s base distribution\u2014is generally intractable. In this work,\nwe develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis\u2014we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\\times$ larger, as well as closed-source, fine-tuned ones. \nIn support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. \n[Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- This work solves one important problem with some of the widely adopted constrained/structured generation such as Guidance, SGLang and Outlines: that is, these framework achieves control by masking out next-tokens that would violate the constraint, leading to biased sampling (compared to the ground-truth conditional distribution). By leveraging sequential Monte Carlo, the proposed technique is able to approximate unbiased sampling in a relatively practical/scalable way. Empirical evaluations demonstrate strong performance on challenging real-world problems.",
            "- The authors propose adapting SMC methods to novel semantic parsing tasks, resulting in notable performance improvements. \n\nThe author conduct a interesting analysis and shows that resampling improves the approximation of the global product-of-experts distribution and approximation quality are consistent with those observed in downstream accuracy evaluation.",
            "- Originality\n- To my knowledge, the extension of sequential Monte-Carlo to the task settings in the paper, and the specific generation receipe (re-weighting, resampling) are new. However, I am not closely familiar with [Lew et al 2023] or its subsequent papers (which are mentioned several times by the authors). Therefore, my evaluation of novelty may be slightly off. \n- Placing ideas such as token-masking, filtering out partial sequences, and selecting partial sequences to explore next in a probabilistic framework is a nice contribution (with the same caveats in the point above).\n\nQuality\n- The experimental evaluation presents a controlled study that ablates each component in the model.\n- Derivations and the divergence analysis seem to be of high quality.\n\nClarity\n- Once the reader becomes familiar with the terminology, the paper is written clearly and precisely. \n\nSignificance\n- The method could potentially be useful in settings where token-level and partial-sequence level constraint functions are available (e.g., those in the experiments). This has some generality (though could also be viewed as a limiting factor).\n- Placing more domains and settings into the probabilistic framing from [Lew et al 2023] helps to further the probabilistic perspective on sequence generation.",
            "- -Good motivation from analysis of weight formulations for importance sampling.\n\n-Benchmarks validate claims that the proposed algorithmic components improve downstream performance. Additionally, authors chose a sensible set of benchmarks.\n\n-Weight correction and resampling seem to be novel components."
        ],
        "weaknesses": [
            "- Some detailed analysis/case study on the sample complexity of SMC would provide more insights, especially how much better SMC is compared to naive importance sampling.",
            "- In terms of experiments: \n- The authors do not emphasize their unique algorithmic contributions within the experiments. The authors could also report the performance of LM with grammar constraint, weight correction and resampling as a regular SMC baseline to further show the effectiveness of semantic potential. Additionally, the authors lack a detailed comparison between their method and the highly relevant SMC method in https://arxiv.org/pdf/2306.03081, and should report it as a baseline, e.g., including without-replacement resampling. \n- For ablation studies, how the number of particles will affect the final performance should be analyzed.\n\nThere is no mention of the computation cost, it would be very useful if the authors could evaluate the efficiency of the proposed algorithm.",
            "- My primary concerns were on the experimental validation. The paper performs a self-contained, controlled experiment using one Llama 3 model on a set of tasks. As a result, it was unclear how the findings generalize to other models, or how they compare in terms of performance to other methods in the literature.\n\n1) For example, taking the example of DS-1000, the absolute numbers are quite low: the DS-1000 paper reports up to 39.2 performance (with Codex-002) versus 28.5% here (with Llama 3). These are *not* comparable since they use different models, but it would be nice to see how this method performs for models closer to the state of the art. Similarly, Lever [1] reports numbers on Spider from previous work that range from 67.0% to 81.9% [1]. The reason this is important is that the exact experiment setup can lead to different conclusions on the performance of methods, so it was concerning that the absolute numbers seemed low. However, the authors could potentially clarify this.\n\n2) It was also unclear why 10 particles was selected, since in these sampling methods the number of samples can impact performance, and we often want to understand how performance varies with the sampling budget. How does the method vary as the number of particles varies? Is there a sample-and-rerank approach that could outperform this method if it drew a large number of samples?\n\n[1] LEVER: Learning to Verify Language-to-Code Generation with Execution, Ni et al ICML 2023",
            "- -The method was not benchmarked against alternative methods. While the ablation study is useful, how does the method compare against other SMC-based techniques such as the ones cited in the related works section that are particularly relevant to this work? E.g. comparisons against the method in Lew et al. and Zhao et al. would be beneficial. There are other non-SMC-based methods that could also be benchmarked against.\n\n-Only Llama 3.1 8-B was evaluated. The manuscript would benefit from benchmarks on additional LLMs to see if results are consistent across similar sized LLMs. I would be curious to see if the benefits are as substantial on larger models, but I understand the authors may have limited computational resources for such analyses.\n\n-There is a lack of theoretical grounding as to the benefits of the components. E.g., a theorem rigorously showing the reduction in KL-Divergence shown in Figure 2 would strengthen the manuscript.\n\n-Notation can be difficult to follow at times. Exposition can be a bit drawn out in certain places, e.g. section 2. I appreciate the authors trying to point out the inefficiencies in each component of MC in order to justify their approach, but I think the exposition would benefit from a condensed explanation of, e.g., the computational burdens of IS."
        ]
    },
    "xDrFWUmCne": {
        "venue": "ICLR 2025",
        "title": "Learning to Discretize Denoising Diffusion ODEs",
        "link": "https://openreview.net/forum?id=xDrFWUmCne",
        "abstract": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at https://github.com/vinhsuhi/LD3.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- - The paper is well-written and easy to follow.  \n- It presents an easy solution to the sampling problem of diffusion models that only requires limited training time while obtaining. \n- The soft teacher loss is effective and simple to implement. \n- The evaluation is thorough and includes multiple models, multiple datasets, and multiple sampling strategies.\n\nIn general, I liked the paper and I lean toward acceptance. However, since this is not my area of expertise, I would wait for the discussion with the authors and other reviewers to increase the score to Accept and recommend borderline Accept for now.",
            "- - The LD3 algorithm is extremely lightweight, requiring only 100 samples and less than 1 hour on a single GPU to learn optimized sampling schedules.\n- The method is evaluated on a comprehensive set of pretrained models and compared against several baseline, showing improved quality in the majority of cases\n- A proper ablation study is done on the various choices/hyperparameters.",
            "- - The paper includes proofs of soundness of their proposed minimization objectives, going beyond purely empirical contribution.\n- The number of experiments is substantial across both datasets, baselines from prior work, and choice of pretrained models.\n- The experiments conducted include notoriously difficult datasets in the literature of diffusion step reduction like ImageNet, and shows improvement in more complex settings such as a text to image model.\n- The objective is cheap to train compared to prior work; the key being that a very low batch size of 2 is permissible to use.\n- Ablation studies demonstrate the importance of the proposed changes separately.\n- The samples presented qualitatively look very reasonable and show clear improvement over the usual hand-crafted timestep schedules, and they fix the random seed so the same samples can be comapred."
        ],
        "weaknesses": [
            "- Although I liked the paper, there are some concerns that, if addressed, would improve the paper. In the following paragraphs, I describe my concerns in detail:\n\n- In the table with the main results, sometimes it is not clear what the metrics are computed against. I suppose the metrics in table 2, 3, 4, and 5 are computed against random samples of the model using the accurate estimation of the ODE. However, if this metric is computed against the true distribution, the performance of the teacher with the accurate computation of the ODE should be shown (1000 steps). I think the evaluation protocol needs to be more clearly defined.\n\n- In a similar direction, Table 6 shows the performance of a teacher model using 8 steps. Why only 8 steps are used here? Would not the teacher use a higher number of samples?\n\n- The model used is quite simple being only composed of a single vector (or two in the decoupled version). From the results in Table 7, increasing the number of parameters leads to better results. Would increasing the complexity of the model lead to better results?\n\n- In the limitations section I found missing that the proposed method needs to be retrained for the target number of sampling steps. One model trained to generate images with 2 samples, would not be useful for 3 and a new model would need to be trained. This might be a problem since different images might necessitate a different number of steps to achieve good quality.",
            "- - There are several typos in the paper. See some examples below:\n    - Algorithm 1 Line 6: $x'_T \u2190 x'_T + ...$  must be $x'_T \u2190 x_T + ...$\n    - Line 251: $x'T \\rightarrow x'_T$\n    - Line 251: $\\Psi*(x_T) \\rightarrow \\Psi_*(x_T)$\n- Theorem 1 requires more explanation on its invertibility assumption. Specifically, if the NFE is small, functions $\\Psi_*, \\Psi_\\xi$ invertibility is a non-trivial fact which requires some justification on its assumption. \n- The method relies on a learned perceptual distance (LPIPS) to achieve optimal results, as shown by the significant quality drop in Table 7 when switching to a standard Euclidean loss. This raises questions about how well the method might generalize to other data types beyond images.",
            "- There are two main areas around which the paper could be much stronger. The first is in comparisons to distillation methods, which are among the strongest in the literature. The paper includes a comparison to progressive distillation and consistency distillation in Table 9, but it is really difficult to compare these methods apples-to-apples. There are details missing (please correct me if I missed these e.g. in the supplementary material) such as what models were compared and where are the baseline scores taken from; ideally the same model should be post-trained with the different techniques. The number of forward passes across methods also doesn't match, making it difficult to draw any conclusions. One conclusion that can be drawn, however, is that progressive distillation remains better than LD3 in FID score at NFE=8, albeit requiring much more compute to distill.\n\nThe other major weakness is the lack of careful qualitative comparisons to other step reduction methods. The vast majority of the qualitative samples are compared to hand-crafted schedules, which are the weakest baselines. This is really important, especially because prior work has shown that very low FID scores can be achieved somewhat adversarially, resulting in strange samples (e.g., consider the CIFAR10 samples in the GGDM paper), so quantitative results are insufficient to truly demonstrate that LD3 improves over all prior work. Careful side-by-side comparisons of different step reduction methods, derived from the same pre-trained model and using the same initial noise and matching NFE would be significantly more convincing.\n\nOverall, the work is strong, and the quantitative results already put this paper as a valuable contribution to the literature that should be accepted at the conference. I am opting for a weak accept, because the comparisons to distillation methods seem improper and incomplete, and the qualitative comparisons require more care. But even if so, due to the very low cost of the proposed technique and the achieved scores, the work already has intrinsic value. I strongly encourage the authors to address the concerns outlined above as it would make the work excellent."
        ]
    },
    "xByvdb3DCm": {
        "venue": "ICLR 2025",
        "title": "When Selection Meets Intervention: Additional Complexities in Causal Discovery",
        "link": "https://openreview.net/forum?id=xByvdb3DCm",
        "abstract": "We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - The authors have found an interesting flaw in previous methods in the presence of selection bias\n- The paper is well motivated with clear examples. Some examples can be made clearer (see below)",
            "- 1. The selection bias in interventional experiments is an under-explored but crucial issue in causal discovery, and the authors use two clear examples to illustrate why this problem matters and why the existing methods and simply augmenting DAG fail.\n\n2. The authors provide a solid theoretical foundation in this paper by (1) rigorously defining the interventional twin graph and characterizing its Markov properties and (2) proving the soundness of the proposed algorithm.\n\n3. Synthetic experiments show that the proposed method outperforms baselines in handling selection bias and remains robust as the number of variables increases. It also uncovers novel causal relationships in real-world applications.",
            "- 1. The problem of selection bias is important yet often overlooked in existing interventional causal discovery. The setting is general.\n2. This paper is technically sound, with clear formulation of the causal DAG and Markov properties. \n3. The illustrative examples enhance the paper's clarity, helping readers better understand the concepts.",
            "- The paper clearly lays out the problem of selection bias in causal discovery and why certain natural approaches to the problem are not sufficient. The paper also puts forward a very general solution to the problem and considers its consequences. Overall the paper is well-written, despite being notation-heavy.",
            "- - This paper studies a relevant and interesting problem - both mathematically and philosophically. It considers the question: \"What does selection bias actually mean\" and proposes a sound answer and the necessary mathematical framework to deal with such situations.\n- Based on the framework to treat selection bias, a sound and complete causal discovery algorithm is proposed.\n- The method is evaluated not only on synthetic data, but on real-world examples as well. This provides some confidence that it may be useful in practical applications."
        ],
        "weaknesses": [
            "- Some bits of the exposition are unclear (see below).",
            "- 1. While the introduction of the interventional twin graph and its Markov properties is rigorous, it may be challenging for readers to grasp at first glance due to its complexity. Providing a high-level explanation to offer an intuitive understanding would greatly benefit readers.\n\n2. The interventional twin graph is more complex than a simple DAG and involves additional nodes. It would be helpful if the authors discussed the computational cost of the proposed model compared to the simpler DAG, including an analysis of the algorithm's computational complexity under the new graphical model.\n\n3. The authors did not address the identifiability guarantees of the proposed method. It would be useful to know if the method can reliably identify the selection variables and under what conditions the true interventional twin graph can be identified.\n\n4. Minor typos:\n    * Line 48: \"We show that existing existing graphical representation paradigms\" --> \"We show that existing graphical representation paradigms\"\n    * At the end of line 169: \"models a completely different scenario\" needs to be revised",
            "- More comprehensive results and explanations of the empirical studies would be beneficial to support the effectiveness of the proposed algorithm. For example:\n1. For the simulation, can you report the proportion of true causal edges estimated as directed edges by the algorithm? Additionally, a comparison of the output graphs with the ground truth would illustrate how the new algorithm perform differently from other methods under selection bias. \n2. For the gene application, can you provide more comprehensive analysis of the result? \n3. For the education dataset, can you explain why the pre-intervention selection bias is a potential issue? Highlighting and interpreting key information of the resulting graphs would be helpful, as the current graph and variable names are difficult to follow.\n\nMinor comments about clarity:\n- The notation in this setting is dense and improvements of readability can be helpful for readers less familiar with the area. For example, \"CI\" in line 93 and different types of arrows in Example 7 can be clarified before their first appearance.\n- Typos: line 48 \"existing\", line 295 \"false\".",
            "- One thing that was unclear to me is how complete is the paper's characterization of Markov equivalence classes in the given model. I would think that the Markov equivalence class would encode all DAGs with the same conditional independence structure (i.e. the right-hand side of the implications in Theorem 1). However, the equivalence structure is defined with respect to the left-hand side of the implications in Theorem 1. This would seem to imply that the equivalency classes are not as fine-grained as they potentially could be.\n\nThe algorithm provided also suffers from this issue, in that the authors point out that it may not be complete. It is not clear to me how useful it is to have a causal discovery algorithm that is sound but not complete. The trivial algorithm that says there are no causal relationships is sound but not useful.\n\nThe simulation study not reporting any information on completeness is disheartening. While I understand that the paper does not contain any guarantees on completeness, in the simulations there is access to the ground truth. So it is hard to see how there is no way to evaluate the ability to discover some fraction of those relationships.\n\nAt a higher level, I'm not sure how much of the framing of the paper is specific to the selection problem. It seems like the approach of the paper is tackling the more general problem of causal discovery with unobserved latent variables. If that is not the case, then the paper should explain how their methods do not generalize to the latent variable setting.",
            "- The biggest weakness I see is the presentation of the paper. The first two sections are dense, but give a good introduction and motivation to the problem, based on good illustrations in Examples 1 and 2.\n\nHowever, Section 3 is the most painful piece of text I have read in a while. It relies mostly on mathematical notation to bring across the main points and lacks the contextualization in prose. I appreciate that examples are given in Section 3, but even those are a bit cryptic and fail to provide an accessible intuitive understanding. I suppose there are three main reasons for this: (1) Writing about complex SCMs is inherently difficult and a certain level of formalism is necessary - not much you can do here. (2) The amount of content in the main paper, given the page limit might a bit too much. Some of the more technical parts could be relegated to the appendix and exchanged for more contextualization. (3) The text could consider the reader's state of mind more. Some examples:\n\n- L211f: introducing the functions $f^*$ uses the mathematical symbols for the corresponding variables in the counterfactual basal world to introduce them, but does not use the word \"counterfactual\". That means as a reader, I either have it in working memory, or I have to go back to the definition and jump back again to the sentence to parse it.\n- As far as I can tell, abbreviations like \"CI\" and \"MAG\" aren't defined, or used before they are defined, e.g. \"PAG\".\n\nSuch presentation choices add unnecessary mental effort for understanding, and I would think twice if I'd go back to this paper and build on it for future work - not because it's wrong, but because of the mental effort to access the information."
        ]
    },
    "wg1PCg3CUP": {
        "venue": "ICLR 2025",
        "title": "Scaling Laws for Precision",
        "link": "https://openreview.net/forum?id=wg1PCg3CUP",
        "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- Strengths:\n\n1. The paper tackles an important issue with the introduction of a bit precision scaling law. While this topic has been explored before, the theoretical scaling law presented in this work offers valuable guidance for the efficient deployment of models in real-world applications. The implications of this work could be transformative for the field.\n\n2. The authors have provided a wealth of experimental results that not only validate the existing scaling laws across different model sizes but also demonstrate the generalizability of previously unseen scenarios. This thorough experimental section strengthens the paper's contributions and is persuasive.\n\n3. The manuscript is particularly strong in its methodological rigor, with a clear articulation of the scaling laws and their implications for precision in deep learning models.",
            "- (1) The paper studies a meaningful topic, the scaling laws of precision, which is a new topic following the scaling law of data and parameters.\n\n(2) The paper gives a good presentation. I especially appreciate the introduction to quantization. I'm not familiar with how quantization works in detail, so it helps a lot.\n\n(3) The paper shows interesting findings in Sec. 3.1 Fig. 2: more pretraining tokens result in lower performance for post-train quantization with a high quantization rate.\n\n(4) The paper shows interesting findings in Sec. 4.1 Fig. 3: KV cache is more sensitive to the change of precision when precision is low, but when precision is high, KV cache is more robust to the change of precision compared with weights and activations.\n\n(5) The paper shows interesting findings in Sec. 4.3 Fig. 6: there would be cases where training in low precision leads to better evaluation loss.\n\n(6) The paper generally shows that the proposed scaling law works well in the experimental setting of the paper.",
            "- - The paper introduces a new dimension to the well-established scaling laws by incorporating precision as a critical factor. This is an important contribution because most prior work focused on model size and dataset size without considering precision, which is increasingly relevant due to hardware advancements supporting lower-precision computations. By doing so, the authors offer a more comprehensive framework for understanding and optimizing model performance under different training and inference conditions.\n\n- The authors fit on over 465 pretraining runs across different precisions (3-bit to 16-bit) and sizes (up to 1.7 billion parameters), providing a robust dataset to validate their proposed scaling laws. The empirical results are consistent with the theoretical predictions, achieving high R^2 values (e.g., R^2 = 0.97 for post-training quantization degradation). \n\n- The paper offers actionable insights into how low-precision training can be compute-optimal, particularly in scenarios where hardware constraints or cost considerations are paramount. For example, it shows that training larger models at lower precision can sometimes be more efficient than using higher precision, which is a valuable insight for practitioners looking to optimize both performance and computational costs.",
            "- 1.\tThe proposed scaling law unify the post train quantization and quantized training into a single functional form.\n2.\tThe finding in the section 4.3 is inspired and the conclusions are consistent with usual experience and give a theoretical explanation.\n3.\tThe experiment is adequate and reasonable and the paper is well written."
        ],
        "weaknesses": [
            "- no clear weakness.",
            "- (1) The paper uses the dataset Dolma for experiments. Though it's hard, it would be interesting to see how pretraining data affects this law.\n\n(2) The paper uses the OLMo-style models for experiments. It would be great to give a general introduction to OLMo-style. Are they transformer-based model? While the abstract states the scaling law for language models, there would be other types of language models other than OLMo-style models, such as SSM.",
            "- - While the paper focuses extensively on integer-type precisions (e.g., 3-bit, 8-bit), it does not explore floating-point types like FP8 or BF16 in as much depth. Given that floating-point formats are widely used in modern hardwares, this omission limits the generalizability of the findings to real-world applications where floating-point precision is common. This could limit the applicability of the scaling laws in environments where floating-point precision dominates, potentially requiring further research to adapt these findings.\n\n- The experiments are conducted on specific hardware setups that support low-precision computations, such as GPUs optimized for integer-type operations. The fitted constants and trends may not generalize well across different hardware architectures or future technologies that handle precision differently. This may reduce the long-term relevance of the paper\u2019s findings as hardware evolves.\n\n- Maybe I'm missing this, but the paper suggests that compute-optimal precision is around 8 bits but does not deeply explore scenarios where precision drops below 4 bits (e.g., binary or ternary quantization). Given that future hardware may support even lower precisions, this limits the scope of the findings.\n\n- While pretraining cost optimization is thoroughly explored, inference-time costs -- especially in real-time or latency-sensitive applications -- are not given as much attention. In many practical deployments, inference-time efficiency is more critical than pretraining cost savings. This imbalance might limit the practical applicability of some of the findings in scenarios where inference-time efficiency is more important than pretraining considerations.",
            "- 1.\tThe paper use the $N(1-e^{P_{w}/\\gamma_{w}})$ to fit the left in the figure 3. But I think the power law is the most commonly used in all kinds of scaling law form. I suggest the author could compare the exponential with power law like $N(1- A*P_{w}^{\\alpha})$."
        ]
    },
    "weM4YBicIP": {
        "venue": "ICLR 2025",
        "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "link": "https://openreview.net/forum?id=weM4YBicIP",
        "abstract": "With the introduction of video diffusion model, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals such as movement regions to stabilize movements, which compromise the naturalness and freedom of motion. To address this issue, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed two key modules: an inter- and intra-clip temporal module and an audio-to-latents module. These enable the model to better utilize long-term motion dependencies and establish a stronger audio-portrait movement correlation. Consequently, the model can generate more natural and stable portrait videos with subtle facial expressions, without the need for manually setting movement constraints. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios. Video samples are available at https://loopyavataranony.github.io/",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The proposed method is solid, with enough technical contributions to address the long-term dependency between motions and audio conditions.\n\n2. The experiment results are strong enough compared to prior works and baselines, in particular on FVD metrics and DExp metrics.\n\n3. Both qualitative results and the demos shown in the supplementary webpage are appealing and convincing enough, where the long-term dependencies and correlations between audio and portrait motions are consistently maintained.\n\n4. Overall, the paper is well-written and easy to follow, albeit having many technical details.\n\n5. The human study results clearly show that the proposed method perceptually outperforms other baselines and prior arts.",
            "- 1. The paper introduces an end-to-end audio-only conditioned video diffusion model, which moves beyond traditional methods that rely on spatial constraints for motion stabilization.\n\n2. The proposed novel modules like inter- and intra-clip temporal modules and audio-to-latents module are well-designed, resulting in more natural and consistent portrait movements and leading to better synchronization and more expressive facial movements.\n\n3. The paper includes extensive experiments that demonstrate Loopy\u2019s superiority over other audio-driven portrait diffusion models both quantitatively and qualitatively, with evidence of more lifelike and stable video outputs in the supplemental website.\n\n4. The paper is well-written, the proposed components and architecture are described clearly.",
            "- 1. The motivation is clear. The authors focus on the weak correlation between audio and portrait motion in end-to-end audio-driven methods.\n2. Overall, this paper is easy to follow. The proposed TSM module is technically sound in its design, and the experimental validation is effective.\n3. Many synchronized and vivid portrait videos are generated.",
            "- 1. The results are good. \n2. The introduction of two modules (Temporal and Audio) is reasonable and interesting. Ablation study supports the benefits of these modules."
        ],
        "weaknesses": [
            "- 1. For audio-to-latent module, why replacing it with cross-attention module leads to largest performance drop as seen in Table 3. What are missing from cross-attention that makes it fail to perform as good.\n\n2. During inference, audio ratio and ref ratio are manually set for classifier guidance, an ablation study is suggested to their impact on the final quality of generated video to have some insights about this weighting scheme.\n\n3. Could the proposed method be further optimized and adapted to real-time settings, where the audio is being played and video follows interactively?\n\n4. What are limitations of the proposed method and what could be improved? Are there failure cases where the generated motions cannot follow the audio closely?",
            "- 1. While the audio-to-latents module improves the audio-motion correlation, there is no mention of how different audio characteristics (e.g., background noise, varying loudness) might impact the model\u2019s performance, which could be critical for real-world applications.\n\n2. The paper lacks a detailed analysis of potential failure modes or scenarios where Loopy may struggle. Highlighting these cases would provide a more balanced view of the model's robustness and limitations.",
            "- 1. In the A2L module, the effects of Movement and Expression on the method have not been thoroughly validated. The audio inputs shown in Fig. 4 are somewhat confusing. I assume they refer to audio features from wav2vec. \n2. Human expressions are closely related to many facial details, but the implementation in the paper is rather trivial. \n    1) the detected landmarks are too sparse and not accurate enough (DWPose), which makes it difficult to capture a person's expression accurately. \n    2) using the variance of keypoints to calculate head movement and expression changes presents several practical issues, \nsuch as the entanglement of head movement and camera movement. Why not use FLAME coefficients or results from other emotion estimation methods? \n3. The TSM module needs a deeper discussion on its impact on overall computational efficiency.\n4. In Tables 1 and 2, the methods perform worse than others on some metrics, especially those related to Glo and Exp. The authors do not provide detailed analysis or discussion on this.\n5. The paper has several writing issues. Some symbols and abbreviations are introduced without explanation, such as TSM in Fig. 2. Additionally, some text in the figures is too small to read, such as \"other computational layers\" in Fig. 3. The main paper does not reference Table 2. There are also some typos, such as in Line 302, where there is an error with punctuation.\n6. The paper does not include a discussion of the limitations of the proposed method.",
            "- 1. Lack of ablation of stand-alone intra- / inter-temporal model. Is both of them necessary or only the inter-clip temporal layer is enough?\n2.  The functionality of the Temporal Segment Model is unclear. Is it for capturing the appearance of the character under different expressions? If so, why (L478) longer motion frames lead to worse results?\n3. Similar to the above issue. I watched the video samples of the ablated model. Seems to me the ablation of either part leads to similar degradations \u2014 lack of head pose variance and subtle expression. This makes me unclear about the different roles of the two proposed modules."
        ]
    },
    "vzItLaEoDa": {
        "venue": "ICLR 2025",
        "title": "Open-World Reinforcement Learning over Long Short-Term Imagination",
        "link": "https://openreview.net/forum?id=vzItLaEoDa",
        "abstract": "Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be \u201cshort-sighted\u201d, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The jumpy prediction technique within the long-term imagination framework is innovative as it departs from the fixed interval approach prevalent in previous work, offering increased flexibility in jumpy prediction\n2. The paper is well-organized and clearly written.",
            "- - The paper is mostly well written. \n- The method proposed is novel and the results are promising comparing to the baselines.",
            "- - **Significance**\n  - Long-horizon world modeling and reinforcement learning in open-world environments are important problems.\n  - The proposed approach is insightful and successfully addresses these problems.\n- **Originality**\n  - The proposed approach involves the combination of multiple novel and inisightful components.\n- **Quality**\n  - Overall the quality of the paper is relatively high, with the method reasonably clearly explained and analyzed.",
            "- 1. It introduces a method for generating target states in MineDojo (or possibly in other 3D-RPG games).\n2. It demonstrates the feasibility of training a world model with jumping transitions and optimizing policies over such transitions.\n3. The illustration is clear and the writing organization is good."
        ],
        "weaknesses": [
            "- 1. The proposed method employs a hierarchical structure, yet the baseline comparisons are made with flat learning methods. Including comparisons with hierarchical MBRL methods like Director[1] could greatly strengthen the paper.\n2. Equation 9 appears to have an inconsistency in the time indexing; should the bootstrapping term $R^\\lambda_{t+1}$ be $R^\\lambda_{t+\\hat{\\Delta}_{t+1}+1}$ ? \n3. The use of\u00a0 $\\lambda$ -return in evaluating the policy might introduce bias since it should be evaluated with on-policy data,, but the predicted jumpy state, $\\hat{z}_{t+1}$, might not aligned with the learning policy.\n4. The paper focuses on Harvest tasks. Including results from other complex, long-horizon tasks, such as the Tech Tree task group from MineDojo, would better demonstrate the framework\u2019s effectiveness.\n\n[1]: Hafner, Danijar, et al. \"Deep hierarchical planning from pixels.\" Advances in Neural Information Processing Systems 35 (2022): 26091-26104.",
            "- - Although the high-level idea is straight-forward, the implementation is overcomplicated. \n- The method feels very ad-hoc to the Minecraft tasks studied in this paper. It doesn't come into my mind about any other relevant tasks other than Minecraft where the proposed method can be applied.",
            "- - **Clarity**\n  - Some aspects of the paper are not particularly clear. The main one is the use of the word 'jumpy' throughout the paper. The meaning of this word is assumed, but is not defined in the paper or standard usage as far as I'm aware, and is relatively unscientific, so I feel it is not the right word to use. 'Multi-step' state transitions seems more appropriate. If the authors were attempting to highlight that the number of steps can vary, then 'variable-step' transitions would be better. At the very least, 'jumpy' should be properly defined at the beginning of the paper.\n  - Similarly affordance maps may not be familiar to all readers and the exact meaning of this term can vary. For example, a short clarification early in the paper such as ''...affordance maps, that elucidate which parts of an observation may be associated with reward, ...\" would be helpful.\n  - Some other unclear aspects/minor mistakes include:\n    - L326: \"employs *an* actor-critic algorithm to learn behavior *from* the latent state sequences...\"\n    - L351: Grammar is slightly wrong and confusing, should be: \"Notably, since long-term imagination does *not* involve actions, we do not optimize the the actor when long-term imagnation is adopted.\" \n    - L354: Also worth highlighting the difference with the DreamerV3 loss. \"The loss of the actor is therefore equivalent to DreamerV3, with an additional factor of $(1-\\hat j_t)$ to ignore updates from long-term imagination:\"\n    - L361: \"on the chellenging...\"\n    - L500: Doesn't make sense. Maybe \"Our work is also related to affordance maps for robotics.\" is sufficient?\n    - \"*Learning to Move with Affordance Maps*\" [1] should likely also be compared and cited here.\n\n- **Limitations**\n\n  - This approach has important limitations that are not mentioned. In particular, the approach is limited to embodied agents navigating a 3D environment in which there are objects associated for which reward is obtained by approaching them. Therefore the approach assumes, for example:\n    - Observations are of a 3D environment\n    - Actions are navigation actions of an embodied agent\n    - Rewards are assoiated with identifying or moving towards objects\n    - A reward model is available to identify high reward regions of observations\n  - The experiments are limited to Minecraft for which these assumptions hold. This approach would likely not work as well even in Crafter [2] for example, which provides a 2D 'open-world' analogue of Minecraft, since objects do not become larger as you move towards them.\n  - The approach also relies on both the long-term and short-term models being used, given only the short-term model is able to update the actor. While the thresholding of $P_{jump}$ can partially be used to address this, this is not particularly robust, and still requires some close-up objects in initial exploration for the standard one-step world model to be used, so the approach may not work as well in very sparse environments.\n  - There is still significant value of the approach despite these limitations, and the paper is reasonably scoped, but they should be included in the limitations at the end of the paper, which are currently overly brief and narrow.\n\n  **References:**\n\n  [1] \"*Learning to Move with Affordance Maps*\", Qi et al., 2020\n\n  [2] \"*Benchmarking the Spectrum of Agent Capabilities*\", Hafner, 2021",
            "- 1. Since MineCLIP is an important tool for this work, I suggest the author include a brief introduction of what MineCLIP can do in section 3.2/3.2.1 or an appropriate position. This would help readers who are not familiar with research on MineDojo to understand this paper.\n\n2. In the abstract, \"We argue that the primary obstacle in open-world decision-making is improving the efficiency of off-policy exploration across an extensive state space.\" This seems not closely connected to the main contribution of this paper. I suggest paraphrasing it to highlight \"across a long horizon\" or something that is more related to the topic.\n\n3. Though the method sounds promising for solving MineDojo tasks, it may not be a general method for all kinds of open-world games. Such as in 2D games or fixed camera control tasks.\n\n    (a)  Before seeing the target for the first time in the training process, there won't be a reasonable goal-directed reward or jumping option, the exploration still requires extensive enumerates.\n\n    (b) The crop and resize operation (or assumption) is only useful for 3D visual navigation tasks.\n\n    (c) When the target is occluded, the world model still needs to perform step-by-step imagination.\n\nIf these are true, I suggest the authors include a sentence or so in the limitation section to clarify it."
        ]
    },
    "vf5aUZT0Fz": {
        "venue": "ICLR 2025",
        "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
        "link": "https://openreview.net/forum?id=vf5aUZT0Fz",
        "abstract": "Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The authors tackle an important problem. The usage of data mixtures during pre-training is not well understood but is an essential part of modern foundation models.\n2. While the idea of using model averaging after an inner loop of training on dedicated subsets of data is not particularly novel, it might have a big impact on pre-training, given the encouraging results.",
            "- - The setup proposed in this paper looks very satisfying, and it seems to solve several problems both in the industry and in research labs.\n- The value proposition seems clear to me.\n- The deployed methodology appears novel.\n- The literature research looks satisfactory to me, given the scope of the paper.",
            "- - The paper is well-written and easy to follow.\n\n- The idea of decoupling embedding matrix and transformer block in pre-training within the federated learning framework is novel.\n\n- The authors answer the raised research questions with meaningful and extensive experiments.\n\n- The results generally confirm that DEPT can improve the generalization and plasticity of the models."
        ],
        "weaknesses": [
            "- 1. Writing can be improved or misses important information. For example, for the experimental setup, I struggle to understand L205-311, and information on software/hardware, such as how many FLOPS  or hours training took, is missing.\n2. Some claims are overstated: M-T outperforms DEPT in 5/11 datasets in Table 2. I am not convinced that Trim and Glob perform identically (L377).\n3. An important additional baseline would be models trained on individual data sets. This would give insights into the advantages/disadvantages of model averaging.",
            "- - [addressed] The paper's form is well below the required writing standards. To address this, I'd suggest specific improvements, such as:\n  - Standardizing method names throughout the paper and tables (SPED vs SPEC, GlOB vs GLOB vs Glob, ...)\n  - Clearly defining the performance metrics used and specifying explicitly whether lower or higher values are better\n  - Adding a reference to Table 1 in the main text\n  - Improving table readability by adding summary statistics (averages...), using bold or color highlighting, or splitting into multiple tables (moving some languages to an Appendix).\n- [addressed] Not enough arguments are brought forward to justify the issue with diverging models during training. I have myself never experienced this phenomenon in similar setups. As such, it's difficult to rule out that it might be the result of bugs in the training code or poor hyperparameter choices, rather a general phenomenon. \n  - A better description of the exact training methodology and the hyperparameter search would help alleviate concerns, here.\n  - An ablation study or an explanatory paragraph isolating factors that contribute to divergence would also help.\n- [addressed] The lack of comparisons with baselines not trained by the authors is worrying. \n  - I would prefer for external baselines to be added, even if some added context is necessary to explain away unfair comparisons (could be an appendix).\n- [addressed] Without devising a clear methodology to perform inference on SPEC-type models, the paper feels a bit incomplete. \n  - I'd suggest to the authors to briefly outline a proposed inference methodology for SPEC models, and to discuss the challenges and potential approaches for inference with these models in more detail.",
            "- - The data sources are not always clear given a dataset. The proposed pipeline only works if the domains are known. Otherwise, some manual or automatic clustering has to be used to create different sets of data.\n\n- The multi-domain data is almost only in English. But for the multilingual data, the data of each language should also contain various domains. Therefore there are confounding variables. A natural question would be whether the model can generalize to the same domains across different languages.\n\n- No downstream tasks in natural language understanding or generation are evaluated on the resulting models. But such further evaluation is important."
        ]
    },
    "v593OaNePQ": {
        "venue": "ICLR 2025",
        "title": "Learning to Search from Demonstration Sequences",
        "link": "https://openreview.net/forum?id=v593OaNePQ",
        "abstract": "Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. The code is available at https://github.com/dixantmittal/differentiable-tree-search-network.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            6
        ],
        "strengths": [
            "- 1. Novel Architecture: The paper proposes a novel neural network architecture, D-TSN, which embeds the inductive bias of a best-first search algorithm, allowing for the end-to-end learning of planning components from demonstration sequences. \n\n2. Joint Learning of Planning Components: D-TSN jointly learns the encoder, value function, and world model. This is advantageous when the world model is not given but is needed to be learned from data. \n\n3. Variance Reduction Technique: Authors use an effective variance reduction technique using a telescoping sum in the REINFORCE algorithm to addresses the high variance associated with policy gradient methods. \n\n4. Comprehensive Experiments: The method is applied to a wide variety of tasks, such as reasoning problems, navigation, and game environments, supporting the claim that it is versatile and effective across domains. \n\n5. Improved Performance: The authors show that D-TSN outperforms baselines, showing its problem solving performance in challenging tasks with limited supervision, especially in jointly learned world model settings.",
            "- - Builds on TreeQN and improves a significant limitation of the prior work, i.e., having a fixed tree structure. In reality, search algorithms should attempt to filter large action spaces and focus computation on promising variations in the tree. The proposed work gets around this limitation by sampling from the action space, and using REINFORCE to differentiate through the discontinuity of sampling.\n- Strong empirical evidence that the proposed method improves on TreeQN, and having the modules trained separately.\n- Strong ablation results showing the effectiveness of the proposed method.",
            "- + This paper introduces a differentiable treee seach and planning network to alleviate the suboptimal search results that arise when using only action and observation sequences to learn to plan in many modern data-driven plannning problems. The tree search basically synergizes submodules in the form of an encoder, value, reward and transition functions from a network by inducing the bias of a best first search into the neural architecture.\n\n+ I love the motivation stated for constructing an search tree expansion policy that is stochastic in nature. But the justification for why it ensures the continuity of the loss function when the search tree is not fixed is missing. I am referring to lines 55-56.\n\n+ I love the conceptualization and the synergy of REINFORCE, mathematical mechanisms to reduce variance in the REINFORCE Loss owing to possibly biased estimates, the continuity proof (though I have questions hanging over the proof of Lemma B.3 to be fully satisfied with this poposition) .\n\n+ I love that the conclusion section meticulously summarizes the problem, contributions, and shortcomings. Kudos to the authors.",
            "- i) This work introduces the integration of the algorithmic inductive bias from the best-first search algorithm into a differentiable network, enabling automatic guidance of the tree search process through gradient backpropagation.\n\nii) This work underscores the significance of maintaining continuity of both the parameter space and the loss function which are dependent on the tree structure. To address this, the authors advocate for the adoption of a stochastic expansion policy to fulfill these prerequisites. \n\niii) The experiment results are compelling. And it is particularly noteworthy to see the success achieved in tasks involving LLM."
        ],
        "weaknesses": [
            "- 1. Limited to Deterministic Environments: The current implementation is restricted to deterministic decision-making problems with discrete action spaces.\n\n2. Computational Complexity: The computational complexity for the approach  might be high, because it consists of constructing search trees and performing REINFORCE updates. This can be a problem especially when applying for deeper trees or larger action spaces.\n\n3. Scalability: scalability is not thoroughly analyzed for longe-horizon tasks or higher dimensional state space.",
            "- - Seeing how the approach handles large action spaces remains an empirical question, since currently, there is no policy that directly outputs a distribution over actions, instead the method requires the application of the transition network and the reward network for every action, which is not scalable to settings like, say, Go.\n- The proposed method is mostly applicable to discrete action spaces with deterministic environments. Improving on this remains a future empirical question.",
            "- While I do love the mathematical contributions of the paper, I think there are essentials that are missing in the logic, organization, and flow of arguments that require a thorough review before this paper makes it into an acceptance. A principal one is the following (mentioned in the summary box but repeated here. The authors would do well to alleviate my concerns): Arguing that for a slight perturbation in network parameters, the implementation of the loss function in equation (2) could generate a tree structure that causes the loss function to become noisy, the authors equated this to a lack of continuity in the loss function space. I think they mistook stochasticity in gradient propagation with discontinuity. The whole premise of the contribution of the paper is based on this assumption that is barely proven to be true or false before the authors dived into a host of mathematical analysis that resulted in the loss function on Line 272 (please number all your equations to allow for easy referencing in the future). \n\n+ The claim in the last paragraph of Theorem 3.1 that slight changes in the network parameters could cause discontinuity in the tree structure seems anecdotal and not backed up by a solid proof. I would love to see a concrete reasoning (analytical proof or abundant empirical  proofs) behind this claim that warrants section 3.5 and Appendix C.\n\n+ Grammatical errors fly out of the page hither and yon throughout the paper; the uthors would do well to carefully organize their arguments, present their logic convincingly throughout the paper, and punctuate and label every equation appropriately!\n\n+ The logic in the paper could use a more thoughful presentation. Here is an example critique:\n     - In the \"introduction\", it is stated in the first paragraph that constructing a search tree from expert data is infeasible due to lack of practicality or scarcity. The authors make an assumption that a search tree is a principal prerequisite for information retrieval (IR) without any justification as to why it may be better than alternative IR methods. Then in the second paragraph, they mentioned how search and planning could be better executed in the presence of a simulator or world mode. While I find this premise alluring, I find it disingenious that the authors claimed that the search could be incomplete because the search process may visit regions unexplored during training. I think the reasoning here is incomplete and should be revisited by the authors.",
            "- i) Previous methods have introduced diverse differential network architectures to integrate different search algorithms into networks, as mentioned in the related works. It is unsurprising that integrating the best-first algorithm into the network has also yielded success. Thus, it would be beneficial to compare the architectural variances between this method and previous methodologies.\n\n\nii) This work trains the overall network using an offline dataset. However, as extensively deliberated in preceding offline RL studies, this paradigm may get stuck when facing out-of-distribution states or actions. Thus, a comparative analysis between online training and offline training for the newly proposed network architecture could provide valuable insights."
        ]
    },
    "uHLgDEgiS5": {
        "venue": "ICLR 2025",
        "title": "Capturing the Temporal Dependence of Training Data Influence",
        "link": "https://openreview.net/forum?id=uHLgDEgiS5",
        "abstract": "Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms\u2014especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula\u2014are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- The method introduces a novel concept by capturing data influence in a trajectory-specific manner rather than assuming permutation invariance, which is a common limitation in conventional influence estimation methods. It outlines assumptions and derives an approximation error bound, lending theoretical credibility to the approach. The approach is designed with computational efficiency, including several techniques to reduce the memory and computational cost. It enables identification of high-value data points at different stages of training, allowing practitioners to curate datasets more effectively.",
            "- Quantifying data influence is an important task and is crucial active sample selection. However, existing method, the influence function, does not care the order of samples' arrival, making it unsuitable for vast majority of stochastic algorithms. This paper address an important research gap. \n\nThis paper is mostly well-written. The mathematical ideas and their intuitions are clearly presented and is easy to understand. \n\nThe discovery of stages of sample influence is potentially a significant contribution to the wider machine learning community. It reveals how SGD utilize each individual sample at different stages of the training and highlights the importance of selecting data at the right time.",
            "- 1.\tThe motivation of this paper is clear: modern training paradigms\u2014especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula\u2014are sensitive to data ordering.\n2.\tIt introduces a computationally efficient embedding method, making it feasible to apply influence estimation to large-scale models without retraining.\n3.\tEmpirical results demonstrate high fidelity in data influence estimation and reveal nuanced phases in training that inform efficient data selection strategies.\n4.\tIt provides insights on the training dynamics of foundation models: a very brief high-influence region at the start, a much longer low-influence basin, and a region in the later training stage with gradually increasing influence, resuming to a high level.",
            "- Many machine learning algorithms especially those for supervised learning typically assume the training data points are permutation invariant. Using such an assumption, one can divide the training data for both training and validation using strategies such as k-fold CV, leave-one-out CV etc. As the authors mentioned, many modern machine learning training paradigms especially for foundation models violate this assumption. The goal of this paper is to capture the dependence of data influence on the optimization trajectory during training. The authors proposed the trajectory-specific leave-one-out (LOO) error. Efficient computation was developed as well. Overall, the paper addresses an interesting and timely problem."
        ],
        "weaknesses": [
            "- The method is explicitly tailored for SGD and is not readily applicable to other popular optimizers like Adam. Although using SGD as a proxy is discussed, this limitation restricts the method's applicability to a broader range of models.\nThe evaluation with ground truth focuses on specific datasets and model types (e.g., MNIST, MLP) due to the computational cost, which may limit the generalizability of the findings. \nSeveral assumptions are made in this paper, such as model layer independency, learning rate scheduling, which might not be satisfied and lead to reliability issue in such circumstances.",
            "- Probably due to the lack of a dedicated related work section, it is not clear where does the authors' contribution begin. For example, has TSLOO been studied before or is it a novel concept proposed by  the authors? In Section 2, the first paragraph seems to suggest that this is authors' proposal. However, a later sentence says, \"while the technique of unrolled differentiation Hara et al., 2019 explicitly aims to approximate TSLOO ...\" It seems the idea of TSLOO already exists in earlier works. If this is true, authors should cite existing works when introducing TSLOO at the beginning of Section 2. \n\nSimilarly, the approximation in Section 3.1 was also used by Hara et al., 2019 and is \"well-established\" in the literature. This makes me wonder how is the proposed work positioned among existing literature. For example, comparing to Hara et al., 2019, what is the **methodological innovation**? If everything before Data Value Embedding (DVE) is a part of literature review, the authors should make it clear. \n\nDVEmb in Theorem 2 is an interesting idea. However, the interpretation of this quantity is unclear to me. For example, \non line 247, \"this expression suggests that similar training points encountered in later iterations may have a stronger impact on the data influence score of earlier training points. \" I am not sure I understand this statement.  z* are hand picked by the user so why do users care about influence score of earlier training points? Please provide a clearer explanation of this statement and why a practitioner should care about this interpretation. \n\non line 252, if z' is identical or highly similar to z*, their gradients will be closely aligned, leading to a significant change in DVE... I don't understand the meaning of \"change\" here. I guess the authors are talking about **tracking the influence of samples over the training iterations**. However, this setting isn't made clear. I suggest the authors state the context and \"setting the scene\" before explaining the interpretation of Theorem 2.",
            "- 1.\tLimited Real-World Validation: While the paper\u2019s experiments demonstrate high fidelity on small datasets like MNIST and reduced subsets of larger datasets (e.g., 1% of the Pile), the method may not have been fully validated on more challenging, real-world datasets. This leaves questions about its robustness and scalability when applied to diverse, large-scale data used in production.\n2.\tPotential Overhead in Implementation: While the method reduces some computational costs, it still requires considerable storage and processing resources, particularly for storing per-sample gradient information and data value embeddings. For truly large models, such as those with billions of parameters, this may limit its practical utility without further optimizations.",
            "- The problem considered in the paper is certainly interesting. However, it is important to make it clear regarding the type of CV considered in the paper. In supervised learning, CV such as LOO CV was mainly used to estimate the model performance such as prediction errors. The type considered in this paper instead is to quantify the loss change with versus without a particular data point. In this sense, it is perhaps more sensible to call it as influence of the point instead of error. The authors may reconsider the terminology to avoid confusion. If the term of LOO error remains, it is important to clarify the difference of this type and the traditional CV error.\n\nRelated to the previous point, in the traditional sense of CV error or LOO error, we would look for small CV error for model selection etc. In the sense of LOO error in the paper, the error is quantified as model\u2019s loss change on a validation data when the training data point is removed from the training set. However, large change in the loss does not necessarily imply big differences in terms of the model performance between with versus without that particular training point. But the goal was to evaluate the model performance of that point on model trained on data without that training point. More discussion and justification are needed. \nTo address this issue, this review would like to suggest the authors to:\n\n1. Explicitly compare and contrast their LOO error definition with traditional CV error\n2. Discuss the implications of using loss change rather than model performance metrics\n3. Provide examples or experiments showing how their LOO error relates to changes in model performance"
        ]
    },
    "uAFHCZRmXk": {
        "venue": "ICLR 2025",
        "title": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models",
        "link": "https://openreview.net/forum?id=uAFHCZRmXk",
        "abstract": "Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and to a bias towards objects over other factors, such as attributes. In this analysis paper, we investigate both phenomena thoroughly. We evaluated off-the-shelf VLMs and while the gap's influence on performance is typically overshadowed by other factors, we find indications that closing the gap indeed leads to improvements. Moreover, we find that, contrary to intuition, only few embedding dimensions drive the gap and that the embedding spaces are differently organized. To allow for a clean study of object bias, we introduce a definition and a corresponding measure of it. Equipped with this tool, we find that object bias does not lead to worse performance on other concepts, such as attributes per se. However, why do both phenomena, modality gap and object bias, emerge in the first place? To answer this fundamental question and uncover some of the inner workings of contrastive VLMs, we conducted experiments that allowed us to control the amount of shared information between the modalities. These experiments revealed that the driving factor behind both the modality gap and the object bias, is an information imbalance between images and captions, and unveiled an intriguing connection between the modality gap and entropy of the logits.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- I've reviewed this paper and noticed many improvements from the previous version:\n- Takeaway 1 is now much clearer. The section effectively demonstrates how common confounding factors affect results, and shows that when they control for these factors, a smaller modality gap indeed leads to better performance in downstream tasks.\n- The authors have introduced a new metric called RMG, which successfully addresses the limitations of L2M.\n- Takeaway 2 is interesting, and it leads nicely into Takeaway 3, which shows that we can actually reduce the modality gap using post-hoc methods.\n- Overall, this is a well-written study that thoroughly examines both the modality gap and object bias in contrastive vision-language models.",
            "- **[S1]** This paper introduced a new perspective on the modality gap, providing rich insight to the researchers and practitioners.\n\n**[S2]** The paper is well-written and extensive experimental analysis is sufficient to validate their hypothesis and argument.\n\n**[S3]** Proposed metrics to measure the modality gap (i.e., RMG) and object/attribute biases (i.e., MOAD) are well designed to intuitively compare the corresponding components.\n\n**[S4]** The provided appendix is \u200b\u200bsufficient to follow the details and experimental setting of the paper, enabling a better understanding.",
            "- - Clear and concise introduction of the problem, motivation, and takeaways.\n- MOAD metric provides quantifiable measure of object bias.\n- Well-designed experiments show compelling evidence for information imbalance hypothesis. Good coverage of 98 VLMs (e.g., CLIP and SigLIP).\n- The proposed connection between the modality gap and entropy is intriguing",
            "- 1. The paper provides in-depth investigation of the relationship between the inferior performance of contrastive VLMs on downstream attribute recognition and two potential causes (modality gap and object bias). They show that the relation is not substantial (though they claim that with better control the relation on modality gap exists, but I am not personally convinced given their evidence). This is informative for people studying these two factors. Importantly, they demonstrate that information imbalance might be a more important (causal) factor to investigate.\n2. They give evidence for reducing the information imbalance leads to better attribute recognition performance, as a solution to this problem on contrastive VLMs. They also provide a way to do this by using trainable temperature.\n3. The experiments presented in the paper are in general well-controlled."
        ],
        "weaknesses": [
            "- - I think most of weaknesses that have been seen in the previous submission was addressed.",
            "- **[W1] Mismatch in the level of information imbalance between synthetic data and real data settings**\n- The authors provide experimental validations on fully-controlled synthetic data and real data.\nWith synthetic data, information imbalance is defined based on the number of attributes, making it reasonable and sufficient to validate their hypothesis.\nHowever, half/quarter captions of real data are derived by randomly dropping the part of captions.\nI wonder if this strategy can hold the same hypothesis as in synthetic data. Please see Q2 below.\n\n**[W2] The lack of evidence on actual pretrained VLMs**\n- This paper suggests that the modality gap or the object bias can be reduced by data filtering or caption enrichment, and the performance can also be improved. However, there is a lack of analysis on real pretrained VLMs (e.g. pretrained CLIP and SigLIP) with large modality gaps. See Q3 below.",
            "- - There could be more analysis on real-world datasets to help solidify the claims.\n- [Minor] There could be more in-depth discussion on the limitations of the findings.\n- [Minor] There could be more explanation of mitigating the information imbalance.\n- [Nitpick] Typo: \"Extend details for Section 5\" -> \"Extended details for Section 5\")",
            "- 1. To me, the major claim in the paper is that information imbalance causes both modality gap and object bias, and that even though these two are not substantially correlated with attribute recognition performance, decreasing information imbalance mitigates them and seemingly increase the overall attribute recognition performance. However, the paper spends most of the content to demonstrate details on modality gap and object bias, many of which are overly detailed. For example: in section 4.2, the authors talk about many attempts to naively close the modality gap, all of which fails to keep the model performance. I think contents like this can be briefly summarised and the details can be put into supplementary materials, since these details might diverge into information that doesn't help readers grasp the core of the main claim.\n2. The major claim is interesting and informative. However, the evidence of it is mainly done on the simple synthesised dataset MAD. The authors give evidence on real-world datasets in Figure7 (c), but the evidence on RMG is much weaker (if significant at all) compared to that with MAD. Since this is evidence for the major claim, the author might consider adding more experiments (more model architectures, more datasets, and presumably a better way to control the number of attributes) for this.\n3. The authors claim that \u201cwhen we control for these factors, we observe the expected negative correlation, i.e., a smaller gap seems to correlate with better performance\u201d (L245). However, the data they provided cannot support this claim, even when treating the significance values loosely. Some training datasets (e.g., DataComp-1B, OpenAI) clearly give positive correlation. The authors should consider removing this claim or providing more substantial evidence.\n4. The overall presentation of the paper is not ideal, in that many \u201cquestions\u201d (e.g., \u201cIs object bias explained by the global word frequencies of the dataset?\u201d) can be turned into statements. The question-answering patterns in the writing highlight the motivations. However, in this paper there are so many of them, and the text devoted for completing this pattern probably does not worth it. The author might consider using more statements instead."
        ]
    },
    "tyEyYT267x": {
        "venue": "ICLR 2025",
        "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
        "link": "https://openreview.net/forum?id=tyEyYT267x",
        "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - Discrete diffusion is an interesting research direction with potential interesting applications. This work here presents a new way to improve this, allowing to interpolate between auto-regressive and diffusion models by changing the block size.\n\n- Good experiments, good studies.",
            "- 1. The idea of a semi-autoregressive model, while not new (there is already a rich literature of models combining autoregression with diffusion, while focusing more on continuous diffusion like SSD-LM (see Weakness point 2), in this paper is developed via discrete diffusion. This is a way of proceeding that is very natural, and shares many parallels with novel methods in the literature focusing on block-wise parallel decoding such as [1] (https://aclanthology.org/2023.acl-long.689). It seems clear that a combination of sequential and parallel inference is a good solution, especially for speeding up the models, but this aspect is not much explored in this work (see Weakness point 4). Also the fact that one can generate sentences of arbitrary length is a big improvement over previous models in terms of usability, paired with the ability to estimate likelihoods (which SSD-LM cannot do).\n2. Showing the tightness of the NELBO (Appendix C) is an interesting theoretical argument that relates the standard autoregressive objective to discrete diffusion objectives.\n3. Good experimental section and good results over previous discrete diffusion approaches. Still one should have done experiments without first pre-training auto-regressively (see Weakness point 3)\n4. The paper is written in a good way, having a good structure, not containing errors and being easy to follow. Still there are two sections (Section 4 and 5) in which some parts are not clear (see Weakness point 1), but apart from those the execution is solid.\n\nReferences:\n\n- [1] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023. Accelerating Transformer Inference for Translation via Parallel Decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336\u201312355, Toronto, Canada. Association for Computational Linguistics.",
            "- The key contribution of this paper is to demonstrate that it is possible to bridge part of the perplexity gap between diffusion-based and autoregressive language models by making a diffusion model partly autoregressive.\n\nThe analysis of gradient variance in section 4 is highly appreciated, and will influence the way in which other investigators conduct research.",
            "- The paper is easy to follow. The paper proposes a semi autoregressive discrete denoising diffusion model and provides an efficient training and sampling algorithm compared to the continuous semi-autoregressive DPM (SSD-LM). The method is incrementally novel. As a semi-autoregressive diffusion model, the paper claimed that the method has advantages in generating full-length sequences that are longer than the length of the output context chosen at training time compared to existing diffusion language models. The paper provides a comprehensive ablation study and well-designed experiments to verify the SAD3-LMs."
        ],
        "weaknesses": [
            "- - Some details are unclear (but I think these can be solved quite easily).\n\n- Some discussion on the motivation of such models would be nice.\n\n- No speed measurements (for the different cases: Training, sampling, evaluating log probs for given seqs). Also for the vectorized training.",
            "- 1. Some aspects of the work, especially Sections 4 and 5, are poorly defined and demonstrated. For example it is not very clear why the variance of the gradient estimator has the formula in Eq. 9, and also not very\u2028clear why extreme masking leads to poor variance. Authors should try to re-formulate these parts in a more sound way.\n2. The authors are missing a good number of citations:\n    1. First in [1] (https://aclanthology.org/2023.acl-long.689), the parallel GS-Jacobi decoding algorithm is proposed for text autoregressive models and has a similar structure to the approach proposed by the authors: move on blocks from left to right and perform parallel decoding on each block. The difference between the two approaches is that in [1], parallel decoding is done via Jacobi iterations while here instead one uses discrete diffusion. Even more related, a follow-up [5] (https://arxiv.org/abs/2403.00835), trains a consistency-like model [6] (which is a type of diffusion model) to speed-up the Jacobi iterations on each block. I see many parallels between the presented method and these two methods, especially the second. The authors should cite those papers and especially discuss in how they differ with [5].\n    2. Additionally, apart from other prominent approaches for discrete autoregressive diffusion like [3] (https://openreview.net/forum?id=Lm8T39vLDTE), there is a rich literature on combining autoregression with continuous diffusion, e.g. [2, 4] (http://proceedings.mlr.press/v139/rasul21a.html, https://openreview.net/forum?id=0EG6qUQ4xE), which the authors barely touch (citing SSD-LM and Diffusion-lm). Please integrate some more references from this literature.\n3. Since SAD3-LM is first pre-trained with standard autoregression for 850K steps is it fair to posit it as a semi-autoregressive discrete diffusion model? It can be that it learns representations especially on the pre-training part and the fine-tuning part is more accessory. I think experiments should have been performed also without pre-training as well in order to see how standard autoregressive the pre-training impacts the final model.\n4. One advantage of combining sequential autoregression with parallel decoding, as in [1, 5], is to improve inference speed, given that parallel decoding could be done in less steps than $L\u2019$ steps (the number of steps required for autoregressive decoding on such block). I understand that the goal of this work is not much on improving efficiency with respect to autoregressive models since one does 10K steps instead of 1024 steps (Table 7), and I appreciate the reduced inference burden with respect to SSD-LM, but what is the point on doing discrete diffusion if there isn\u2019t any gain on standard autoregression? I agree it is good to have an alternative to standard autoregression, hence all the efforts in the discrete diffusion area, and maybe improved perplexity will be reached in future, but still I think improved efficiency over standard autoregressive should be the main goal to be sought. I do not think authors need to compare with [1, 5] (which do achieve a speed-up, especially in [5]) but at least point and discuss as future work how to target these goals starting from their proposal and maybe improving via the faster models that are researched in diffusion distillation literature (e.g. the already cited consistency models [6]). \n\nReferences:\n- [1] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023. Accelerating Transformer Inference for Translation via Parallel Decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336\u201312355, Toronto, Canada. Association for Computational Linguistics.\u2028\n- [2] Rasul, K., Seward, C., Schuster, I., & Vollgraf, R. (2021, July). Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning (pp. 8857-8868). PMLR.\n- [3] Hoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B., van den Berg, R., & Salimans, T. Autoregressive Diffusion Models. In International Conference on Learning Representations.\n- [4] Wu, T., Fan, Z., Liu, X., Zheng, H. T., Gong, Y., Shen, Y., ... & Chen, W. (2023, December). AR-DIFFUSION: auto-regressive diffusion model for text generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems (pp. 39957-39974).\n- [5] Kou, S., Hu, L., He, Z., Deng, Z., & Zhang, H. CLLMs: Consistency Large Language Models. In Forty-first International Conference on Machine Learning.\n- [6] Song, Y., Dhariwal, P., Chen, M., & Sutskever, I. (2023, July). Consistency Models. In International Conference on Machine Learning (pp. 32211-32252). PMLR.",
            "- The statistical significance of performance differences is not specified.  The differences are large enough to appear statistically significant, but it would be nice to have confirmation.",
            "- My main concern is the motivation of integrating a discrete diffusion processing in AR. As when $L'=1$, the method performs the same as AR, however, significant higher NFEs, extra tuning required on noise schedules, and complex training and sampling algorithms are involved. The paper can be further improved by showcasing its advantages in real-world applications compared to AR methods.\n\nMoreover, please see Questions for the details."
        ]
    },
    "tTPHgb0EtV": {
        "venue": "ICLR 2025",
        "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
        "link": "https://openreview.net/forum?id=tTPHgb0EtV",
        "abstract": "Harmful fine-tuning attack poses serious safety concerns for large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \\textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss,  we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk.     Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at https://github.com/git-disl/Booster",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. The research topic on tackling harmful fine-tuning is important and timely because of the urgent need to ensure the trained LLM can resist alignment attacks.\n\n2. The proposed approach is intuitive and clear, based on the clear definition of harmful perturbation. The experimental results improve significantly over the baselines, demonstrating the effectiveness of the proposed approach.\n\n3. The writing is very clear and easy to follow. The formulas and pseudo-code clearly describe the algorithm and Figure 3 demonstrates how the proposed method works.",
            "- 1. The paper presents a novel approach, Booster, that effectively minimizes harmful perturbation during the alignment stage, thereby improving the safety and reliability of fine-tuned language models. The method is simple yet effective.\n\n2. Its computational efficiency, requiring only three forward/backward passes per optimization step, makes it suitable for practical applications with frequent fine-tuning requests.",
            "- - This paper proposes a new method of defending harmful fine-tuning attacks at the SFT stage\n- The proposed method is shown to be effective on the datasets evaluated in the paper\n- Thorough analyses are conducted to understand how Booster works under different alignment and task-specific fine-tuning scenarios.",
            "- * This paper is well-written, with appropriate tables and figures to demonstrate their idea and motivation.\n* Alignment-time harmfulness prevention is quite interesting to me. This once-for-all method for harmful content prevention sounds promising and efficient.\n* Boost demonstrates decent performance in various benchmarks and experiment settings."
        ],
        "weaknesses": [
            "- 1. In line 375, Booster initially has a relatively low harmful training loss. What is the reason for this? Does it mean that the model sees the harmful data in advance and trains it a little bit before the testing stage?\n\n2. Adding more samples of the datasets can make it more clear how the model works and beat the baselines.\n\nOverall, I think this paper is clear and technically sound.",
            "- The addition of a regularizer introduces trade-offs in terms of the balance between aligning the model and minimizing harmful loss. Finding the right balance can be challenging and might lead to varying results depending on the specific application or dataset.",
            "- - Section 3.2, which seems to be the motivation part of the paper, is not easy to follow for the following two reasons\n    - The term *harmful score* is not properly defined\n    - How the **Derived Insights** are derived from the observations is highly unclear. The causal relation between the first part of the sentence (*Because harmful fine-tuning data is considered to be inseparable from the benign data*) and the rest of the sentence (harmful perturbation is indeed inevitable in the user fine-tuning stage) cannot be justified by Figure 2. It is also unclear whether the experiment shown here relates to the proposed method.\n- The experiment settings and results are weak. The paper uses SST2, AGNews, GSM8K, and AlpacaEval for the experiments. However, **all the experiments, except the experiments in Table 3, only reports the results of SST2**. Considering that SST2 is a very simple task for LLMs nowadays, only reporting the results for SST2 is a major weakness. Moreover, considering that GSM8K and AlpacaEval are more widely adopted for evaluating current LLMs, the results of **Booster in Table 3 are not convincing: Booster has a very high harmful score on AlpacaEval while Booster\u2019s harmful score is not better than two baselines on GSM8K**. This makes me doubt the effectiveness of the proposed method on more challenging tasks.\n- Presentation can be improved. In the first two paragraphs, the paper does not mention what is the dataset reported here. This makes it hard for me to evaluate the experiments at first. The notations with tilde are not defined in the paper.",
            "- * Limited metrics. This work only reports harmful scores and fine-tuning accuracy. However, one intuitive limitation of alignment-time harmfulness prevention methods is they could hurt aligned LLMs' performance. The author should consider adding this experiment and testing the aligned LLMs with and without Boost directly.\n* Section 3.2 is not convincing enough. The authors try to validate the concept of harmful perturbation in Section 3.2. However, the Figure 2 they used to demonstrate this is something too simple and not convincing enough."
        ]
    },
    "tPNHOoZFl9": {
        "venue": "ICLR 2025",
        "title": "Learning Dynamics of LLM Finetuning",
        "link": "https://openreview.net/forum?id=tPNHOoZFl9",
        "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, \ngives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
        "decision": "Accept (Oral)",
        "review scores": [
            10,
            8,
            8,
            6
        ],
        "strengths": [
            "- 1. They motivate and ground the study of training dynamics for LLMs well and highlight key difficulties in applying standard influence function style analyses to LLM finetuning and preference optimization, a highlight of the preliminary material.\n2. The formalism and analysis in Section 3 is generally clear and well written. Figures 2 and the bullet point enumeration in 3.3 are concise presentations of the core claims of the analysis.\n3. The experimental design of the empirical verification section is well done and cleanly visualized, particularly the use of completions $y^{+}$ gpts, gptf, test, hum.\n4. Proposal for a mitigation to the \"squeezing effect\" (while not the main contribution of the paper) is well motivated by the analysis and simple to implement.",
            "- 1) The paper has strong theoretical and experimental backing for their analysis.\n2) First paper to propose a framework extending learning dynamics to LLMs.",
            "- The paper is overall well written, clearly explains the motivation and the approach. The use of simple \"pedagogical\" examples also helps in communicating the main message of the paper.\n\nThe main contribution seems to be in the application of the theory to the different loss functions used in LLM finetuning, making explicit use of the \"decomposition\" of the effective learning dynamics into 3 different terms, and identify which one directly depends on the loss function being used. This is demonstrated to be productive as it can provide (at least a preliminary) explanation for different observed behaviors.\n\nOverall, this is an interesting paper and the results are likely to be of interest to the community.",
            "- 1. Analyzed the theoretical process behind DPO gradient updates. Explained deeply how the effect of gradient updates with respect to the training examples affect the model's performance on unrelated examples. Provided a theoretical framework for (potentially) understanding the origin of hallucinations.\n2. Explained a common problem in DPO that was not illustrated well in prior work. For example, Rafael et.al. 2024 only vaguely claimed that \"DPO decreases the likelihood of all data in favor of extrapolated responses\", but this paper clearly indicated that the decreased likelihood all (or mostly) added to the greedy decoding sample (y^*).\n3. The paper is well written and easy to follow."
        ],
        "weaknesses": [
            "- 1. The clarity of Sec 3.1 when discussing the causal masking and teacher-forced production of the full next-token logits set could be improved, though this is admittedly tricky.  It is possible that for some readers, a diagram of the matrix structure here could be helpful since most papers don't tackle the more complex formulation of the influence problem and so readers may not be clear on it. (The reviewer imagines a teacher-forced causally masked model forward on input and label sequences X,Y as a forward on an augmented view of X,Y according to causal mask M, such that if the length of X+Y=N, the input has X' now has dim NxN (lower triangular consisting of all prefixes of the sequence X+Y) and we consider the output to be labels Y_i for each row of X' corresponding to the next token following the tokens in row X'_i.)\n\n2. Additionally, while the analysis relies on the kernel values between the feature maps of $\\mathcal{X_0}$ and $\\mathcal{\\tilde{X_u}}$ to implement the pressure transfer between various samples during learning, these intermediate values are not actually experimentally estimated. To concretely tie the analysis to the empirical observations, it would be useful to compute examples of the decomposition values for the terms in Eq 5 and Eq 7. If this intermediary relationship can be shown as clearly as the loss trends described in Figures 3 and 4, a stronger assertion of causality between the theoretical model described by Eq 5 and Eq 7 and the empirical learning dynamics can be made, strengthening the impact of the work.",
            "- 1) Did not find any specific weaknesses.",
            "- There are two major limitations here that, while are (somewhat) acknowledged by the authors, can benefit from a more careful discussion and/or analysis.\n\n1. The first is the obvious limitation that the entire analysis is being done under the assumption that the \"feature map\" is held fixed, and only the classification layer is changing during learning, but the extent to which this assumption holds in practice is not being evaluated at all. Without such an evaluation (for example, either by quantifying/tracking the change in $\\mathcal{K}^t$, or by repeating some experiments when the weights are *actually* frozen) it is hard to say whether the theoretical explanation about the squeezing effect is in fact an important part of the observed phenomena (or whether it is largely / to some extent driven by more complicated dynamics that involves changes in the feature map as well). I think this should, at the very least, be acknowledged more explicitly in the text.\n   \n2. The second limitation is more nuanced, and has to do with the relying of some intuitive understanding of \"similarity\" (between prompts/examples), that may or may not be valid. Again no evaluation is offered -- despite the fact that the authors explicitly specify how this similarity should be understood (i.e., it is measured by the eNTK, $\\mathcal{K}$ ). It is far from being clear a-priori that the learned features agree with some intuitive judgement of similarity. Moreover, what \"dissimilar\" means, in the extremely vast space of strings, is also non-trivial. For example (and as pointed out by the authors), it might be that all \"well-formed\"/grammatical sentences are somewhat similar, and the model is able to push probability mass to entirely non-grammatical/nonsensical strings. This leads to some arguments being made only on the basis of some intuition. For example, \"the model\u2019s confidence on y+u keeps increasing and the update energy... gradually decreases\"  (Line 350) -- this seems to be inconsistent with the fact that we see no slow-down of the updates for $\\mathbf{y}^+_{u}$ itself.\n\nFinally I would also like to point out that peakiness (/\"squeezing\") effect in text generating models followed by RL tuning is a phenomena that has, in fact, been observed and to some reason explained before, including the observation that the on/off policy choice might play an important role in dealing with the \"exposure bias\". The paper will benefit from referring to some of these previous works, for example:\n- Caccia et al., Language GANs Falling Short, (arxiv 2018 / ICLR 2020)\n- Choshen et al., On the Weaknesses of Reinforcement Learning for Neural Machine Translation (arxiv 2019 / ICLR 2020)\n- Kiegeland and Kreutzer, Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation (naacl 2021)",
            "- Disclaimer: Only checked the main text and the Appendix A. Did not check Appendices B,C,D.\n\n1. Need a \"related work\" section. The paper in its current form does not explain the relations between \"learning dynamics\" with related literature, for example, preventing negative transfers during transfer learning, neural tangent kernels, or loss landscape that enabled adversarial attacks. Offering a \"related work\" section whereas reducing the theoretical focus a little bit would help ICLR readers better position your work.\n\n2. Need to indicate assumptions in your prop 1 and section 3.2 clearer. As far as I could see, there are at least 2 assumptions: \n- Higher order terms has little to no impact to learning dynamics\n- K changes slowly during the initial training stages and thus has a limited impact to training dynamics\n\nPlease list out these assumptions (potentially what I did not listed out) so that users will follow and understand the limitations of your analysis.\n\n3. Should design more experiments that \n\n- (a) During off-policy DPO, show that no other sentence's log probability significantly increased other than y^*. Although your theoretical analysis seems to have proved this, we still need to show this point in experiments due to your analysis assumptions. The sampling should be much more extensive to show the non-existence of such examples (and statistically significant)\n- (b) For SFT, quantitatively show the causal effect of increased hallucinations due to the \"pull up\" effect of similar but negative examples (scaling up more than MNIST)\n- (c) For both experiments, show that neuron tangent kernels (your K-term) did not change much, and thus the learning dynamics can be fully attributed to your G-term\n- (d) Qualitatively demonstrate the repetitiveness of off-policy DPO towards the end of fine-tuning\n- (e) Quantitatively measures how your proposed approach reduced hallucinations / answer repetitiveness via more rigorous benchmarks, other than just a measurement of the win rate"
        ]
    },
    "t8FG4cJuL3": {
        "venue": "ICLR 2025",
        "title": "Classic but Everlasting: Traditional Gradient-Based Algorithms Converge Fast Even in Time-Varying Multi-Player Games",
        "link": "https://openreview.net/forum?id=t8FG4cJuL3",
        "abstract": "Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games.\nHowever, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs.\nBesides, the limited known attempts on the games with time-varying payoffs are in two-player bilinear time-varying zero-sum games and strictly monotone games. By contrast, in other time-varying games, the last-iterate behaviours of two classic algorithms, i.e., extra gradient (EG) and optimistic gradient (OG) algorithms,  still lack research, especially the convergence rates in multi-player games.\nIn this paper, we investigate the last-iterate behaviours of EG and OG algorithms for convergent perturbed games, which extend upon the usual model of time-invariant games and incorporate external factors, such as vanishing noises.\nUsing the recently proposed notion of the tangent residual (or its modifications) as the potential function of games and the measure of proximity to the Nash equilibrium, we prove that the last-iterate convergence rates of EG and OG algorithms for perturbed games on bounded convex closed sets are $O({1}/{\\sqrt{T}})$ if such games converge to monotone games at rates fast enough and that such a result holds true for certain unconstrained perturbed games. With this result, we address an open question\nasking for the last-iterate convergence rate of EG and OG algorithms in constrained and time-varying settings. The above convergence rates are similar to known tight results on corresponding time-invariant games.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- Although I am not familiar with the majority of the literature, I believe the class of time-varying games studied in this work is rich and interesting. Therefore, the novelty of this work should be appreciated.",
            "- - The paper is well-motivated, with the introduction and related work sections effectively contextualizing recent advances in time-varying games. The authors clearly articulate how this work improves upon existing results in the field. \n- The paper is well-organized: it begins with foundational concepts, including definitions of the game, learning algorithms, the measure of proximity to Nash Equilibrium (tangent residual) and its properties, as well as assumptions about the games. The authors then proceed with separate analyses of the convergence of EG and OG methods, systematically outlining the proof techniques and presenting each step in detail.\n- While I did not examine every detail of the proofs, the mathematical reasoning appears clear and correct.",
            "- While previous convergence results for EG and OG are mainly provided for time-invariant games and time-varying games that are restricted to be two-player, bilinear, zero-sum, this paper pushes the boundary and shows fast convergence results of EG and OG algorithms for multi-player, time-varying games that eventually converge to smooth monotone games and with convex constraint set for actions. In general, I think it is a strong submission and the results are quite interesting, espcially that the author(s) use tangent residual to measure proximity to Nash to overcome challenges could appear when using gap functions."
        ],
        "weaknesses": [
            "- 1. Could the authors improve the theorem statements? Although I appreciate the accuracy of the mathematical formalism, the text has not been refined adequately to improve the clarity of the statements. I suggest either expressing some of the quantities involved in Theorems 1 through 4 in English or/and predefining some of the maximization functions before the actual statement. \n2. There exist multiple typos in the manuscript. A careful read is advised. I mention here the ones I find the most hurtful for clarity:\n* In lines 85 and 88 z^{*} is used before it is defined. \n* In line 163 \\lim_{t \\to \\infty} G is not defined in Definition 2.",
            "- - My primary concern is that the contribution of this paper may be perceived as an incremental extension of the method from [1] to time-varying games.\nIn specific, for proofs of last-iterate convergence of both EG and OG, the author seems to follow the same proof procedure as [1] does but adding additional analysis for the time-varying $G_t$'s which results in additional terms in the bounds. \nFor example, this paper's Lemma 4 argues best-iterate convergence with rate $\\mathcal{O}(1/ \\sqrt{T})$ of tangent residual, and this corresponds exactly to Lemma 5 of [1], while Lemma 13 and 15 corresponds to Lemma 3 and 4 of [1] respectively. Moreover, Theorem 1 which establishes the monotonicity of the tangent residual, corresponds to Theorem 2 of [1].\n\nI acknowledge the authors' careful analysis of the perturbation terms. However, could you highlight and summarize the main proof techniques that are novel and distinct from those used in [1]? This additional clarification would be greatly appreciated, and discovering additional novel technical contributions would very likely lead me to improve my evaluation.\n\n\n\n\n[1] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning in multi-player games. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 33904\u201333919. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\ndb2d2001f63e83214b08948b459f69f0-Paper-Conference.pdf.",
            "- 1. (Starting line 432) The numerical experiments section\n- The author(s) claim that they provide results for general multi-player games beyond two-player zero-sum games. I assume by this claim they mean multi-player general-sum games. However, the experiments only show results for zero-sum cost functions. I think the author(s) should also show results for general non-zero-sum games involving multiple players in order to make their point. Otherwise, I would like to hear more elaboration on that end.\n- The perturbation added to the two players seems to be identical; is that something required for the results to hold? If not, I would encourage the author(s) to remove the restriction in their experimental results.\n- The initial z0 picked in the experiments seems to be very non-arbitrary---both players start from exactly the same point. Could the author(s) elaborate on this? Ideally, the empirical results can be much strengthened by a Monte-Carlo study with many different initial conditions and report of statistical results.\n\n2. Other minor aspects: on a high level, quite some notations and concepts are used in the paper without giving definitions. I would appreciate a clearer presentation.\n- Line 131: variables in the game tuple undefined; game itself undefined; constrained set Z undefined\n- Line 141: variational equality \u2192 variational inequality\n- Line 171: existence of at least one Nash equilibrium better be stated explicitly as an  assumption\n- Eq (1):  Euclidean projection operator undefined\n- Line 226: Dr(z) undefined"
        ]
    },
    "t7P5BUKcYv": {
        "venue": "ICLR 2025",
        "title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts",
        "link": "https://openreview.net/forum?id=t7P5BUKcYv",
        "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) **Low Computing Overhead**: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) **High Performance**: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) **Deployment Friendly**: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1$\\sim$2.1$\\times$ expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- With the proper hyper-parameters, MoE++ introduces zero-computation experts who can reduce the computational load by bypassing or simplifying processing for certain tokens, leading to efficient resource use.\nThese heterogeneous experts with suitable routing designs can improve the model's efficiency and effectiveness.",
            "- The authors provide a fairly simple and effective addition to the standard MoE architecture which effectively allows more heterogeneous computing in different layers with minimal overhead. The paper is written clearly and a thorough set of ablations are performed.",
            "- - This work improves the existing MoE framework in terms of both efficiency (throughput) and effectiveness (performance), making it impactful for real-world applications.\n- This paper is well-written and insightful, with clear motivation and illustrations.\n- Figures and tables are clear and easy to read."
        ],
        "weaknesses": [
            "- 1. Parameters such as $\\tau$, which regulate token allocation between zero-computation experts and original, may complicate model tuning, as the performance and burden distribution are sensitive to them.  \n2. The pathway-aware routing with gating residuals adds complexity to the expert selection process, which may require careful tuning for optimal results.\n3. The dynamic routing mechanism can still result in load imbalances, especially under diverse data distributions, which may lead to underutilized or overloaded experts that affect efficiency.",
            "- The empirical gains don't seem to be too significant and can only start to be seen at large scale ~7B parameters.",
            "- - While Table 1 shows the \u201ccomplexity between the proposed MoE++ and MoE\u201d and zero-computation experts enjoy a complexity of 0, they still likely lead to some extra computation overhead. Therefore, this work lacks real-world wall-clock time demonstrations of these zero-computation expert operators, especially regarding a batch of tokens.\n- This work introduces three types of zero-computation experts (i.e. zero, copy, and constant) but provides minimal justification for this specific set. From Table 5, we can see they only conducted basic combinatorial ablations."
        ]
    },
    "stUKwWBuBm": {
        "venue": "ICLR 2025",
        "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
        "link": "https://openreview.net/forum?id=stUKwWBuBm",
        "abstract": "A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \\emph{all} $n$-player matrix and finite-horizon Markov games.  In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality.  To validate the expressivity of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model. We validate our findings on a simple multi-agent reinforcement learning benchmark. Our results open the doors for to the principled development of new decentralized multi-agent reinforcement learning algorithms.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- - Writing is clear overall (I have some concerns for some aspects though, see next section) and the paper is well-organized. The proofs are well exposed in the appendix. \n- Investigating the tractability of games combining bounded rationality and risk aversion is interesting. In particular I find the idea of achieving tractability independently from the payoffs interesting. \n- The paper covers matrix games and their extension to finite horizon Markov games. More importantly the results hold for general sum games.  \n- Modelling the behavior of decision making agents and their strategic interaction beyond expected utility theory is interesting on its own.",
            "- * Novel contribution while bringing a theoretically grounded view on risk aversion and bounded rationality to Markov games which can eventually be applied to more complex MARL scenarios. Adjusting the games so that RQE is achieved tractably by no-regret learning and having the tractability not depend on the underlying game structure is useful.\n* Claims are supported extensively by proofs.\n* The experiment on Cliff Walk demonstrates the risk averseness well.\n* Paper is well written in general.",
            "- - The authors perform a rigorous analysis and provide convincing arguments for a well-defined system.\n- The work could have real economic applications.\n- There is good novelty in applying the dual formulation of [1] to this multi-agent setting and constructing the loss functions in Eq. (6) and (7).",
            "- - The authors introduces games with risk-averse agents and bounded rationality, allowing for the computation of risk-averse quantal response equilibrium (RQE), which is realistic and computationally feasible.\n- RQE can be computed in polynomial time in all $n$-player matrix games under specific risk aversion and bounded rationality conditions, irrespective of the underlying payoffs.\n- The authors present a sampling-based algorithm with finite-sample guarantees for learning approximate RQE in unknown environments."
        ],
        "weaknesses": [
            "- - My main concern about this paper is the following question: What exactly makes the introduced games tractable? The paper argues that it is the combination of bounded rationality and risk aversion. I find the focus on tractability rather confusing. \nRisk aversion has its own motivation but it is unclear if it is required here for tractability. To be precise, I think the contribution in this paper is that you can achieve tractability (for computing another solution concept  than NE/CCE relating to a modified game) independently of the knowledge of payoff functions (their bounds I guess). More precisely, I think bounded rationality is already enough to allow for tractability but it should require some assumptions on the payoffs which this paper avoids. Please correct me if I have missed something here. The paper does not exactly present the contribution this way. In particular, I think the following statements would gain in clarity: \n\n-- l. 111-112: \u2018RQE not only incorporates features of human decision-making but are also more amenable to computation than QRE \u2026\u2019 Why? There is no extended discussion about this whereas I believe it is essential to clarify the contributions. \n\n-- l. 235-238: While risk aversion might not be enough, I think bounded rationality should be enough (e.g. QRE as demonstrated in a number of works) under relevant assumptions on the structure of the game (mainly conditions on the bounded rationality level compared to the magnitude of the utilities/rewards). I find the presentation order confusing here. You start by introducing risk aversion and show its shortcomings as for the tractability of computing equilibria in such risk averse games to motivate the need of something else but equilibria should become tractable only with bounded rationality. \n\n-- From the title and the abstract (and throughout the paper), the emphasis is on tractability, as if both are actually needed to \u2018achieve\u2019 tractability. If this is not the case as bounded rationality is the main enabler, I would maybe highlight less this computational aspect and clarify that the combination brings you the possibility to achieve tractability under a condition on both levels of bounded rationality and risk aversion which is independent of the rewards. How does this condition compare to the one for say QREs? What\u2019s the price to pay and advantages (if any) for RQNEs compared to QREs (beyond the advantage of incorporating risk aversion)? \n\n- Equilibrium collapse is briefly mentioned in l. 294. I would like to see a more in-depth description of how the results of this paper connect to the prior work regarding this phenomenon. It seems that the extended game you introduce seems to have some zero sum structure under a perhaps more stringent assumption than what you set (\\epsilon_1 = \\xi_{2,1} and \\epsilon_2 = \\xi_{1,2}, a condition satisfying your condition), under which J_1 + J_2 + \\bar{J}_1 + \\bar{J}_2 = 0. This can be seen from setting \\lambda = 1/2 in your proof of Theorem 7 p. 21, l. 1133-1146. Is this an observation that connects to prior work about zero sum games and equilibrium collapse? It is also worth mentioning that setting \\lambda = 1/2 leads to a condition that is consistent with your extension to the n-player setting (l. 1226-1232). \n\n- I think the introduction and motivation of (6) and (7) needs some improvement. Could you give more motivation and intuition? \n\n- The notations $\\xi_1^*, \\xi_2^*$ are quite confusing, especially that you also use the notations $\\tau_1, \\tau_2$ in Fig. 1 without defining them in the main part. What do you mean precisely by $\\tau_1, \\tau_2$ satisfy the conditions of Theorem 2 (l. 328)? How do the \\tau s and \\xi s relate?I believe one issue in the presentation is that you chose to hide \\tau in the definition of the regularizer D without putting it in front of D (as it appears from the appendix). Maybe a solution is just to add the examples to the main part to clarify this and further comment on the relationship between the \\xi s and \\tau s. \n\n- Comments about experiments: While the paper focuses on RQREs, experiments do not seem to illustrate the concept. What are the RQREs in this game and why? Are they different from NEs? Why would they be more reasonable? \n\n- The introduction (and some other parts of the work e.g. l. 772-780), highlights the limitations of CCEs. Then Theorem 2 assumes access to a CCE of the 4-player game (which can be computed with a no-regret algorithm). While this CCE is for the modified (regularized) game and not the original one, why would this alleviate all the limitations that are inherent to CCEs in general and that you highlight in the paper? More specifically, the facts that the set of CCEs can be large, may have support on strictly dominated strategies and that stationary CCEs are PPAD-hard to compute in dynamic general sum games (l. 47-51). For the last point, does it mean that your output is also a nonstationary CCE since you also rely on no-regret algorithms (l. 8 of algorithm 1).   \n \n- Dependence on the action space size: a host of theoretical results in the MARL literature have devoted efforts to \u2018break\u2019 the \u2018curse of multi-agency\u2019 and go from the product of action spaces to the sum. Your sample complexity is exponential in the number of players. While I understand that space might have been constraining, at least a comment is needed here on the theorem. The very last point of the conclusion mentions scalability but this is not clearly delineating the limitations of the result and why. \n\n- I have several other questions that I would like the authors to address for clarifying their contributions and the technical novelty.\n\nMinor:\n- l. 47-48: \u2018the set of CCE can be large (introducing an additional problem of equilibrium selection)\u2019: This equilibrium solution problem is already largely relevant for Nash equilibria, I find the argument confusing here. This is even more confusing since you also rely on CCE computation (for a modified game though) in the paper. \n- l. 305-307: H notations do not seem to be used in the main part. \n- l. 377, eq. (9): \\tilde P V: the notation is a bit confusing here, I guess it is the usual dynamic programming notation but \\tilde{P} \\in \\Delta_S and V \\in R^S is a bit unclear, \\tilde{P} is usually a matrix. \n- (24) l. 1045: for the argmax to be unique I guess you need D_i to be strictly convex w.r.t. p_i here. \n- Proposition 1 p. 20: bounded rationality does not seem to play any role in obtaining this result, this is probably worth mentioning.",
            "- * There is only one example game for Markov game results and it is very toy.\n* Bounded rationality is less obvious than risk averseness on Cliff Walk experiments. Interpreting bounded rationality from the experiments is difficult.\n* Paper has some minor typos (example line 160).",
            "- - The notation could be a bit confusing at times; the difference between $\\pi$ and $p_i$ is not clearly defined and can be hard for the reader to follow.\n- Typo in line 160: \"we make u a general class...\"\n- The experiments lack breadth in demonstrating the algorithm\u2019s efficacy across diverse problem settings. Only one cliff walk example is demonstrated, and very standard simple benchmarks are used for the repeated game.",
            "- - While the paper applies duality and risk aversion concepts, these techniques have been previously explored in the context of game theory and multi-agent reinforcement learning, raising questions about the technical novelty of the contributions.\n- Extending the findings to finite-horizon Markov Decision Processes (MDPs) may be somewhat straightforward, as the process involves relatively conventional methods. While this extension adds scope, it may not significantly deepen the theoretical insights or complexity of the approach.\n- The proposed algorithm requires access to a simulator, which is a strong assumption and may limit the practical applicability of the method, especially in real-world scenarios where such simulators are unavailable or costly to obtain."
        ]
    },
    "sbG8qhMjkZ": {
        "venue": "ICLR 2025",
        "title": "Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent",
        "link": "https://openreview.net/forum?id=sbG8qhMjkZ",
        "abstract": "We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\\KSD$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant 'negative part' proportional to $N$ times the expected $\\KSD^2$ and a smaller 'positive part'. This observation leads to $\\KSD$ rates of order $1/\\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Mat\\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- This paper establishes quantitative convergence guarantees for finite-particle (and discrete-time) SVGD, with a much better dependency on problem parameters than the only previous known analysis. This is thus arguably the first satisfactory convergence bound, for an algorithm that has attracted considerable attention from theoreticians. So this paper's achievement is highly significant.\n\nThe main novel insight used in this paper is remarkably clean. It is also quite satisfying, in that it allows for an intuitive proof strategy. \n\nThe presentation of the paper is clear, although a bit technical from section 4 onwards.",
            "- The SVGD algorithm was well studied in practice for several years already. However, its theoretical understanding was rather limited. As well discussed by the authors, past work only tackled particular (simpler) cases discretization mechanisms of the continuous SVGD, leaving the most interesting case open. That is the finite-particle discrete-time setting. In this setting they solve this problem and obtain polynomial convergence guarantees for different types of metrics, under certain conditions.  \nThe paper is generally well-written, although certain mathematical details are omitted.",
            "- * In KSD, this paper significantly improves the particle dependence over Shi & Mackey (2024) (which is known to be the previous best result) by considering the evolution of the joint distribution of N particles.\n* In Wassertein-2 distance, this is the first convergence rate, albeit showing curse of dimension.\n* The discussion on related works is extensive and informative.",
            "- The paper is extremely well written, and while I have not checked all the proofs in detail, the results appear to be formally correct. The authors also do a good job explaining the literature.\n\nThe authors bypass the challenges faced by previous attempts at deriving convergence rates for SVGD in the KSD metric by working with the joint density of particle locations and the N-fold product target measure, exploiting a simple and elegant relationship related to the evolution of their KL divergence."
        ],
        "weaknesses": [
            "- Section 5 on propagation of chaos (POC) could benefit from a little bit more motivation: it is not clear why POC would be a desirable property for SVGD.\n\nPossible typos:\n- In Assumption 1(b) and Lemma 1, p_0^N needs to be C2 (not C1) for p^N(t,.) to be C2\n- in Assumption 2(d), maybe = is meant to be <=\n- typos on line 430",
            "- ### General comments\n\n- Some of the conditions in the main results are not easy to verify. See the Questions section. \n- The paper is generally well-written, although certain mathematical details are omitted. \n\n### Mathematical comments \n\n- The derivations starting from *line 281* are not well-explained. Why does the first term on the right-hand side vanish on second line? How is the third line derived?\n- In order to obtain (17), the term $(\\sum_j V(x_j)/N)^{2\\alpha}$ is upper bounded  by $\\sum_j V(x_j)/N$. This does not seem to be correct, even with the assumption $2\\alpha < 1$.\n- *line 927*. \"Routine computations give...\". More details on this part would ease the reading. \n\n### Notation\n- *Theorem 2*  $P(x_1(t) \u2208 dx)$ is slightly vague from a mathematical point of view. Perhaps explicitly stating what is meant by this notation, would make the theorem clearer. \n-  $p^N (t,\\underline{z})$ is defined in Lemma 1, but later in the paper the notation is changed to $p(t,\\underline{z})$. \n- *notation in the proof of theorem 1* To keep notation self-explanatory I would suggest to use the number of particles in the notation for $H(0)$ in the proof of Theorem 2. That is to use $H^N(0)$, as later in the proof of the last claim you use $\\sup_L H^L(0)/L$.\n\n### typos\n\n* *line 271* equation equation -> equation.\n* *line 291* missing a period at the end of the equation.\n* *line 674* Extra square bracket in the equation. \n* *line 747,749* There is a $\\sum$ sign missing in the first term of the right side and in the definition of $f(n)$. \n* *page 8* literature literature -> literature",
            "- * Some notations should be further clarified, and there is not a subsection collecting all the notations which makes certain parts not that readable.\n* This paper is short of some discussion on assumptions.\n* Comparison with previous analysis including assumptions and rates is not that clear.\n* The discussion on the convergence in Wasserstein-2 distance is weak.\n\n\nFor more details, see Questions below.",
            "- A key element in the development is the N-fold product target measure $\\pi^{\\bigotimes N}$. Although it's definition can arguably be derived from its name, I think it would be nice to formally define it too; pressumably, $\\pi^{\\bigotimes N}(x_1,\\cdots,x_N) := \\pi(x_1)\\times\\cdots\\times\\pi(x_N)$."
        ]
    },
    "rfdblE10qm": {
        "venue": "ICLR 2025",
        "title": "Rethinking Reward Modeling in Preference-based Large Language Model Alignment",
        "link": "https://openreview.net/forum?id=rfdblE10qm",
        "abstract": "The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear *why* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. \nIn this paper, we first establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization, this is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. \nWe highlight the critical concept of *order consistency* in reward modeling and demonstrate that the BT model possesses this property.\nMoreover, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. \nTo offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using $6$ base LLMs, $2$ datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- As BT model has been widely used as a  default paradigm in reward modeling, there are few work disscuss the validity for this pattern. This work gives a detailed theoretical analysis of using BT model in reward modeling and offers other alternative methods for reward modeling and  conducts rich ablation studies on them. The theoretical analysis reveals some very basic logics of reward modeling and BT model, which offers a deep explanation of how reward modeling and BT model works.",
            "- 1. Provides a thorough analysis of the BT model's application in LLM alignment, identifying unique challenges and offering theoretical justification, including the introduction of asymptotic theory for BT regression in preference-based reward modeling.  \n2. Proposes `order consistency` as a core principle in reward modeling, leading to an alternative to the BT model. Additionally, the use of cross-prompt comparisons enhances training by expanding available comparisons, which further contributing to performance improvements.  \n3. The paper is presented in a clear and coherent manner. The authors introduce the purpose behind each step before detailing the results, providing a helpful summary of expected outcomes. Despite the dense content, the writing remains accessible and easy to follow.\n4. The experiment is thoughtfully designed to validate the proposed methods and offer valuable insights to readers.",
            "- - The paper provides a theoretical framework to justify the use of order consistency. \n- The authors conduct extensive experiments with diverse setups, support their claims about the classification-based model\u2019s advantages over BT.\n- The paper is well written."
        ],
        "weaknesses": [
            "- 1. As illustrated in paper, the label of classifation is simply to  assign (1, 0) to the prefered answer. It is seemingly to be a very rough way for building classifation labels since there are pairs may both be, we say like, 'helpful'. One is labeled as 'chosen' in raw dataset just becase it is more helpful than the rejected one. But it does not mean the rejected one should be  classified in to 'not helpful'.  For example, chosen answer may get a 10/10 score in helpful, and rejected answer may get 9/10.  Can you disscuss this limitation and its potential impact on their results? And have you ever considered alternative labeling schemes that might better capture nuanced differences between responses?\n2. The  experiment model size only ranges from 2B to 8B. It remains unknown weather CLF method still more effective when applied to larger models. Do you have any hypotheses about how these results would be if scale to larger models? Can you discuss this limitation and propose how future work could address it?",
            "- I have no major concerns. I have only one minor suggestion: Figures 2 and 3 should include dataset names in each row for added clarity.",
            "- - The paper evaluates reward model performance primarily through BoN sampling improvements. While BoN results provide insights into relative ranking, they do not directly reflect the reward model\u2019s prediction accuracy or error, which is central to the theoretical claims in Theorem 2.2. Including accuracy or error metrics in the evaluation could provide a more comprehensive validation of the theoretical bounds on prediction error, demonstrating that the model\u2019s predictions align with the theoretical guarantees.\n- In RL training (e.g., using PPO), the RM\u2019s calibration is crucial: a well-calibrated RM should assign progressively higher rewards to better trajectories. Theorem 2.2 focuses on minimizing prediction error, which may not ensure adequate calibration for RL. To fully evaluate the proposed approach, an analysis of the reward model\u2019s calibration or additional experiments using PPO (or other algorithms) would help determine if the model can effectively support RL tasks where calibration quality directly impacts learning stability and performance."
        ]
    },
    "pqOjj90Vwp": {
        "venue": "ICLR 2025",
        "title": "Towards a Complete Logical Framework for GNN Expressiveness",
        "link": "https://openreview.net/forum?id=pqOjj90Vwp",
        "abstract": "Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical perspective and present a general method for determining the homomorphism expressivity of arbitrary GNN models, as well as addressing several open problems.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- **Solid Theory**: The paper constructs a solid theoretical foundation through extensive mathematical proofs and derivations.\n\n**Unified Framework**: By establishing a connection between GNNs and first-order logic, the paper proposes a theoretical framework that can unify different GNN models. This framework allows for a systematic comparison of the expressiveness of different GNN models.\n\n**Novel Perspective**: The paper analyzes and explains the expressiveness of GNN models from the perspective of first-order logic, providing a new perspective for understanding these models.",
            "- - A new theoretical framework for assessing the expressive power of GNNs by relating it to fragments of FO\n- Classification of existing MPNNs, subgraph GNNs\n- Ability to assess the expressive power wrt to the ability to capture certain substructures (motifs) in graphs\n- Affirms conjecture from prior work",
            "- 1. The contribution is significant and will be of interest to the ICLR community. It unifies existing work on logical expressivity of GNNs, as well as showing the consequences that this has for two other important methods of analysing GNN expressivity.\n2. The paper makes good use of space, sticking to the recommended 9 pages and rarely making redundant statements.\n3. The quality of the work appears to be good, although I haven't been able to properly verify the soundness of the proofs.\n4. The authors have made it clear where their work fits into the existing literature, and what their contribution is."
        ],
        "weaknesses": [
            "- **Limited Comparison and Analysis**: The paper does not specifically analyze what each operation in first-order logic implies about what the model has learned, nor does it directly compare with other existing expressiveness assessment frameworks, such as the work mentioned by Zhang et al.\n\n**Practical Guidance for Model Development**: Although the paper provides an assessment framework, it lacks specific guidance on how to use this framework to guide the development of high-expressiveness GNN models.\n\n**Insufficient Experimental Validation (Not necessarily a con)**: The paper lacks an experimental section to validate the effectiveness, rationality, and correctness of its framework.",
            "- - The presentation could be improved in several places. For instance, there could be more illustrations and examples throughout the paper. \n- Nitpick: I would refrain from assigning very positive attributes, \"simple,\" \"elegant,\" etc., to your own results and proofs.",
            "- Major:\n1. The main section of the paper lacks intuitions as to why the theorems are true, as well as proof sketches. These would greatly benefit the reader, particularly for a central result such as Theorem 3.\n2. Due to a mixture of typos and imprecision, it is sometimes unclear what the authors are intending to stay from a technical perspective. This is particularly problematic in a paper that consists solely of theory. For example: on L262, the \"logic sets at the bottom\" have so far only been presented to be able to be atp(u), so what else could this refer to? And on L171, what does 'u' refer to? One might reasonably assume a node, but further down it also appears to refer to node tuples.\n3. I have many other smaller concerns with the paper, which altogether come together into a major concern.\n\nMedium:\n1. L187 the example is far too simple and doesn't do enough to explain the idea of having a sequence of operations within each layer. It is not obvious how other real GNNs can be made from these building blocks. Using a more complex example like Local 2-GNN would be more beneficial.\n2. In Theorem 3, equivalent logic sets are being used for each x_i, but they have so far only been defined for GNNs, not these building blocks. A more general definition of equivalent logic sets is needed.\n3. L327 \"auxiliary logic set\". One might assume that this relates to global readouts, given that these greek letters were just used above. However, that interpretation does not seem to fit with the proposition. It is not clear, and should be stated explicitly what this logic set is for, as well as using different notation to separate it from global readouts if that is not what it intends to represent.\n4. L331 the jump from the GNN models to their expressivity is not clear. It would be helpful to see an example of the whole pipeline in effect: GNN model -> GACNN -> equivalent logic -> simplification of the logic\n5. L406 the construction is presented in a somewhat clunky manner, and would benefit from being both more precise and concise. E.g. the enumeration (a-c) is just the definition of a graph that can be done inline\n\nMinor:\n1. L35 \"many studies\" needs a citation\n2. L38 it is unclear why k-WL lacks interpretability\n3. L49 \"although provides\"\n4. L55 MPNNs are already a generalisation of several GNNs models, so it is misleading to say they analyse different GNN models separately.\n5. L76 \"is denoted\"\n6. L97 \"leave\"\n7. L105 \"predicate\", \"corresponds\"\n8. In section 3.1, GNNs are referred to without having been defined yet.\n9. L140 \"district\"\n10. L152 \"the elements\" -> \"each element\"\n11. L166 no need for \"etc\", since you already said \"including\"\n12. L172 should be clearer that it's a sequence of two operations, not just two operations\n13. L182 \"variants\", but only one is described\n14. L187 the appendix should be referenced here for the construction of the other GNN models within this framework\n15. L254 \"existed\"\n16. L255 \"edges\" -> \"neighbourhoods\", since we're generalising to node tuples\n17. L270 \"function\", \"Similar as previous\"\n18. L281 \"is\" = \"are\"\n19. L298 \"its layers\" -> \"a layer\", since all layers have been assumed to have the same structure\n20. L309 \"provides\"\n21. L323 \"prediction\"\n22. L365 \"although computes\"\n23. L376 \"tasks\"\n24. L392 should this not be \"homomorphism expressivity\"? Homogenous has other connotations. Also \"beautiful\" is emotive language and should be avoided\n25. L404 full stop should not be there\n26. L464 - 466 is a bit verbose, and restating the obvious"
        ]
    },
    "pQqeQpMkE7": {
        "venue": "ICLR 2025",
        "title": "On Scaling Up 3D Gaussian Splatting Training",
        "link": "https://openreview.net/forum?id=pQqeQpMkE7",
        "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- (1) This work enables the distributed training of 3DGS, which can be very useful to accelerate the training of 3DGS.\n\n(2) The paper is well-written and easy to understand.\n\n(2) This work did exhaustive experiments on very large-scale datasets (e.g., the MatrixCity dataset)",
            "- **Motivation**\n* This paper has a strong motivation of expanding the training of 3DGS into multiple GPUs/machines. The vanilla 3DGS has a limited number of primitives supported due to the GPU memory limit, but the exceptional performance of 3DGS indicates its great usefulness in large-scale reconstruction. This paper is monumental in providing support in terms of parallel training.\n\n**Method**\n* This paper correctly identifies the per-Gaussian and per-pixel stages of the training pipeline of 3DGS. Since it split the set of Gaussians onto each GPU, the first per-Gaussian stage is relatively simple. For the per-pixel stage, the paper provides intuitive and empirical support for the locality property of 3DGS for each pixel. The paper cleverly leverages this property to design a sparse communication to transport the 3DGS across GPU at a low speed and bandwidth sacrifice. This enables the distributed training of 3DGS.\n* This paper rigorously analyzed the scaling property of the 3DGS training process, and carefully designed a hyper-parameter setting paradigm based on the independent gradient assumption and empirical support. \n* To further improve the utilization of all GPUs, the load balancing algorithm is introduced to ensure each GPU can adaptively adjust their workload during training, because the number of Gaussians required for each region of the image cannot be predetermined.\n\n**Experiments**\n* This paper conducts extensive experiments across various scales of scenes under different experiment settings. The experiment result are well presented to demonstrate the improvement in primitive count, training speed, and rendering quality.",
            "- 1. Proposal of parallelization with respect to different stages of the rendering pipeline of 3DGS is interesting.\n\n2. With the proposed method, training 3DGS on high-resolution images is possible.\n\n3. Batch processing can significantly increase the training speed.",
            "- 1. Originality.  A reasonable parallel training strategy with novel modification for 3DGS based on valuable insights.\n2. Quality. Sufficient experimental validation on effectiveness of the method.\n3. Significance. Pioneering and well-optimized parallel training implementation for 3DGS, resolving critical challenges in synchronized parallel 3DGS training. On the one hand, GrendelGS avoids tedious model pertaining or data and primitive partitioning in prior works like VastGaussian or CityGaussian. On the other hand, this work enables efficient training when multiple GPUs are available, especially under instant requirements or large-scale scenes."
        ],
        "weaknesses": [
            "- (1) The related works are put into the last section of the paper, which is weird to me. It lacks discussion of some existing distributed methods, such as **DOGS** (Chen and Lee, NeurIPS 2024), and RetinaGS(Li, et, al, arXiv 2024).\n\n(2) In Sec.6 Related Works, The discussion of VastGaussian, CityGaussian, and Hierarchical Gaussian is wrong. They do not merge the resulting images. Instead, they merge the sub-models. And the authors also claimed that \"None of these systems can consider a full large-scale scene directly and achieve the same quality as ours\". This is ***over-claimed and does not respect existing works since they did not compare to any of these methods*** but only tried different configurations of their Grendel methods.\n\n(3) Baselines are missing in the current version. It is a good system work to me. However, over-claiming something in the paper won't make it a better work. I would like to see a fair comparison of the method to more baselines (original 3DGS, VastGaussian, CityGaussian, DOGS, etc.) and raise my score if they do so.",
            "- **Method**\n* The paper follows the vanilla 3DGS design and does not consider the Level of Detail support. Since this paper focuses on the large-scale reconstruction, LOD is essential for downstream applications. It is not clear how the proposed method can support LOD.\n\n**Experiment**\n* The quality improvement is not as significant as the number of primitives or the training speed. However, I would like to argue that this is because of the lack of an appropriate dataset to evaluate a very large-scale reconstruction, which limit the performance of this paper. \n* Although the training speed improvement is very significant with this parallel training, large scenes require very time-consuming pose estimation to be done first. The overall training time including the pose estimation might not be improving as much as the reported improvement. This is not the problem of this paper, but does weaken the impact of this paper.",
            "- 1. This paper lacks a conclusion and limitations, making it difficult to understand its overall contribution contribution and appear incomplete.\n\n2. Similar to the proposed learning rate scaling, RAdam [1] also suggests a method for variance rectification. It also rectifies the learning rate using variance to enable adaptive learning and, I believe, takes batch size into account. However, the advantage of the proposed method remains unclear.\n\n3. The rendering speed is expected to be slow due to the time required for Gaussian transfers between GPUs, however, the paper does not provide details regarding rendering speed.\n\n4. Based on Fig. 14, it appears that increasing batch size does not decrease the maximum number of Gaussians significantly.\n\n5. As shown in Fig. 11, for datasets that are not high-resolution like Rubble, the performance gain becomes marginal even if the number of Gaussians exceeds what can be handled by a single GPU.\n\n[1] Liu et al., *On the Variance of the Adaptive Learning Rate and Beyond*, ICLR 2020",
            "- 1. While the superior efficiency over the original 3DGS is clearly and sufficiently illustrated through experiments, it would be beneficial to show if this synchronized parallel training mechanism shows higher efficiency over existing open-sourced asynchronized paradigms, such as HierarchyGS, OctreeGS, or CityGS. Note that for a fair comparison, the batch size and overall iterations should be aligned. Considering the limited time for rebuttal, a comparison with one existing method on 2-3 representative large-scale scenes is enough.\n2. The Independent Gradients Hypothesis is introduced to support the hyperparameter scaling law. It's reasonable that the hypothesis is true on Rubble or even other large-scale scenes, since the views are extremely sparse and approximately independent. However, for condensed views over small scenes or single objects, the hypothesis may not well suit the reality, especially when the batch size grows.  It seems more reasonable to limit the hypothesis to cases with significant view sparsity, or check if the scaling law brings worse results on small scenes."
        ]
    },
    "pISLZG7ktL": {
        "venue": "ICLR 2025",
        "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
        "link": "https://openreview.net/forum?id=pISLZG7ktL",
        "abstract": "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy\u2019s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\\% success rates in novel environments with unseen objects.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- The authors present a compelling paper with a hypothesis that until very recently were not widely known in the field of robot learning. To summarize the strengths of the paper:\n\n1. The focus of this paper, behavior cloning models that generalize to new objects and environments zero shot, has only very recently been shown to work, so analyzing the underlying principles that determine the success of this process is very important.\n2. The authors approach this problem in a principled way, collecting a lot of data, training a number of models, and evaluating on those model in the real world over different environments is a hard but important problem.\n3. The scaling law proposed by the authors seem to hold up in further examinations with two different tasks trained post-facto.\n4. The visual presentation of the objects and scenes where data is collected is done wonderfully.\n5. The extra tips in the appendix for practitioners for training more general policies seem to be also very helpful for further research in this direction.",
            "- The paper is well written and easy to follow. The experiments are clear and the results are presented well. Insights into which data is most useful and how many demonstrations are required to solve a task are certainly very helpful. I particularly like that all experiments are executed on a real robot arm in reasonably cluttered environments.",
            "- - This study is follows a rigorous protocol to examine scaling laws across diverse environments and objects, with a fair evaluation method to minimize evaluation bias.\n- The paper presents efficient data collection strategies, which are crucial for the robot learning field, where data collection is costly and limited.\n- This work plans to open-source the code, data, and model, which can be useful for future research on robot learning generalization.",
            "- - The authors effectively contribute to the discussion of scaling laws for robot learning through extensive experiments in generalisation across unseen objects and environments. \n\n- The authors provide practical validation of their findings through using their insights to establish optimal data collection strategies for training policies, which they demonstrate on another set of tasks. \n\n- The authors provide a useful discussion around evaluation criteria for task success and provide an example of defining a scoring rubric for evaluating task success on specific tasks. \n\n- The authors provide ample information on their experimental setup and ensure that their experiments are reproducible.\n\n- The overall quality of the experiments and discussion is good, while there are some limitations to this work the authors address many of these limitations in their discussions and provide solid experimental evidence for their claims. This is a valuable contribution to scaling laws for robot learning, while more work needs to be done overall I find this paper to be valuable."
        ],
        "weaknesses": [
            "- While the paper is advancing robot learning in a positive directions, there are possible improvements that can be made. For example:\n\n1. The primary issue with the paper is that it does not mention the initial conditions for the robot and the environment \u2013 and how they were varied. For example for the pouring task, it is unclear whether the cup and the red dot is located at the same relative position to the bottle, and if so, if it's sufficient for the robot to open-loop follow a training trajectory afterwards to complete the task. To understand what the authors mean by \"90% success\", we must have a good sense of what the task and environment variations look like.\n\n2. Similarly, the authors don't create much of a variation in the task difficulty \u2013\u00a0as a result it is hard to tell if the scaling laws derived in the paper scales in a similar way with harder or easier tasks.\n\n3. The \"new tasks\" that the authors collect data for and evaluate in (towel folding, unplug charger) does not seem to be of a similar level of difficulty as the primary tasks talked about in the paper.",
            "- The analysis focuses mainly on two relatively simple tasks (pouring water and rearranging a computer mouse). While the paper would benefit from investigating more tasks and also including more challenging tasks, I am aware that it is quite an effort to set up additional experiments with real robots and collect large training sets.\n\nThe objects considered in the tasks are always objects of the same category (e.g., water bottles) and, thus, very similar. I believe that this is the reason why increasing the number of objects per environment in Figure 6 does not improve the task performance much. Essentially, the objects are always quite similar, while the environments differ quite significantly, and therefore, it is more important to collect data from more environments rather than with different objects in the same environment.",
            "- - **Details on data**\n  + Considering the multiple other factors that can affect learning performance, the authors should provide data details for each environment, object, and environment-object variation. This could include specifying the number of demonstrators [1] (with their IDs), listing the data collection protocol [2], and showing relevant statistics (e.g., the number of failed demos, action variance [3], and task horizon for each demonstrator [3]). Such information would clarify that any observed variation is due solely to the environment, object, or environment-object pair rather than bias in the data itself.\n  + One concern about particular data collection hardware (i.e., UMI) is that it may introduce added bias. For instance, if a specific teleoperator is highly skilled, they might gather higher-quality data by using optimal speed movements or demonstrating behaviors at the precise height where the robot is positioned. The authors could clarify this in relation to the above point.\n- **Limited algorithmic variations**\n  + The experiments use only a single algorithm with a single seed, which may restrict to make a comprehensive conclusion.\n \n### Referneces\n- [1] Mandlekar et al., \"What Matters in Learning from Offline Human Demonstrations for Robot Manipulation\"\n- [2] Zhao et al., \"ALOHA Unleashed: A Simple Recipe for Robot Dexterity\"\n- [3] Belkhale et al., \"Data Quality in Imitation Learning\"",
            "- - Generalisation performance is only discussed for policy learning in the single-task setting, the paper doesn't consider the multi-task setting which remains very relevant in modern robot learning applications. \n\n- The scale of the demonstration datasets is limited, more work is required to ensure that the scaling laws hold true at larger scales. \n\n- Generalisation performance is only discussed with respect to two criteria, the authors also argue that these criteria encapsulates all factors a policy may encounter in real-world deployment. In general, I would say that this argument is incorrect. \n\n\"We categorize generalization into two dimensions: environment generalization and object\ngeneralization, which essentially encompass all factors a policy may encounter during real-world\ndeployment.\"\n\n- The authors argue that they focus on out-of-domain generalisation, however they do not quantify out-of-domain in their experiments. This is more generally a challenge for evaluating generalisation performance of machine learning experiments. It remains a weakness of the current approach as we aren't quantifying out-of-domain which makes it challenging to comment on generalisation performance. With this being said the appendix provides visuals of the objects and environments which provides sufficient evidence for data diversity to support the authors claims. \n\n- The scoring rubric for tasks has the potential to introduce subjective bias. In spite of this potential weakness, the authors effectively argue the need for this rubric versus more rudimentary metrics such as MSE. The results of this work hinge on a readers confidence in the scoring rubric for experiments, in general I feel confident in the quality of this work, however, it is important to mention this point."
        ]
    },
    "o2Igqm95SJ": {
        "venue": "ICLR 2025",
        "title": "CAX: Cellular Automata Accelerated in JAX",
        "link": "https://openreview.net/forum?id=o2Igqm95SJ",
        "abstract": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- The issue the library tackles is impactful. CAs play a huge role in various areas of research and having efficient implementations is, of course, key to experimenters. Dramatic improvements in performance have been known to enable a way broader research community in other fields.\n\nThe library also unifies various scenarios in which CAs are used that have been fragmented over multiple platforms.\n\nThe foundation in JAX also seems to allow for comparatively easy integration with other approaches in the field of AI. (A discussion of that point might be helpful.)\n\nThe libraries capabilities are demonstrated on a well-selected range of applications.\n\nVarious interesting novel applications are defined and well presented in the paper.",
            "- The article is clearly written and makes a compelling argument for cellular automata. It demonstrates their utility in image analysis and even reasoning tasks. The use of GPU acceleration through JAX is timely and relevant as a new exploration of GPU acceleration capabilities. As the main contribution is a software library, it provides some experimental comparisons with other open-source libraries.\n\nThe secondary contribution of the article are three novel applications of NCA. The most concrete of these is the application to the 1D ARC dataset. This demonstrates that NCA can follow patterns from a small number of examples, allowing them to rival GPT4's in-context learning on visual reasoning tasks. To my knowledge, this is the first study of NCA for reasoning; the demonstration that NCA can do low-data pattern matching, whether or not that is \"reasoning\", is very interesting.",
            "- The paper is very well written and easy to read. The authors have made presentation a clear priority in the paper and should be commended. \n\nThe software itself is also a great idea. Reducing the time it takes for people to start working on these kinds of studies and to do so in an accelerated, modern way is very useful and this work presents a framework doing exactly that.",
            "- Originality: While this paper does not necessarliy re-invent new methods, they have created an original and unified tool to be used by practitioners of cellular automata in a highly efficient way for a broad set of possible implementations. This is a tool that will surely accelerate the study of emergence and self-organization in cellular automata.\n\nQuality/Clarity: The paper is clearly written with sufficient background context to motivate the necessity of this tool as well as citing the relevant literature that has sparked recent interest in deeper neural cellular automata architectures.\n\nSignificance: Related to the originality of this tool, it is significant for practitioners of this field and lowers the barrier of entry for researchers to produce highly efficient code and run larger scale simulations and experiments. As the field of cellular automata is deeply intertwined with the study of emergence, scaling such simulations is a crucial requirement to push the field to new grounds. With such a tool, such experiments become much more feasible and cheaper for researchers."
        ],
        "weaknesses": [
            "- The paper stresses some standard library features (like documentation or examples) too much compared to more research-relevant parts.\n\nThe performance comparison should be broader. Various libraries exist. Why were only two tested in only very specific cases?\n\nThe performance difference to TensorFlow is left unexplained. TF should use hardware acceleration as well, so what is happening there?\n\nThe figures are not properly referenced in the text.\n\nSome formal problems exist:\n- superfluous \")\" in \"Section 2)\" on page 4\n- ill-formatted citation of \"Mordvintsev et al.\" on page 6\n- many wrong quotation marks on pages 8-10\n- \"doesn't\" on page 8\n- \"a NCA\" on page 8\n- superfluous comma \"training set,\" page 10\n- ill-formatted reference to Appendix A on page 10",
            "- A question I had in reviewing this was its relevancy to ICLR. It isn't the standard ICLR paper, so I consulted ICLR's official CFP, which includes:\n\n+ generative models\n+ causal reasoning\n+ infrastructure, software libraries, hardware, etc.\n\nI believe the article fits into these categories, however the match to ICLR and the general machine learning literature could be strengthened. For example, these works use NCA in more traditional machine learning settings:\n\nGrattarola, Daniele, Lorenzo Livi, and Cesare Alippi. \"Learning graph cellular automata.\" Advances in Neural Information Processing Systems 34 (2021): 20983-20994.\n\nTesfaldet, Mattie, Derek Nowrouzezahrai, and Chris Pal. \"Attention-based neural cellular automata.\" Advances in Neural Information Processing Systems 35 (2022): 8174-8186.\n\nPajouheshgar, Ehsan, et al. \"Dynca: Real-time dynamic texture synthesis using neural cellular automata.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\nNone of these are cited, despite being high-profile studies of NCA in machine learning (NeurIPS, CVPR). As the main contribution of this article is a software library, it would be very valuable to know if these studies or similar ones could be reproduced using CAX. For submission to ICLR, this article could do more to relate the contribution to machine learning.\n\nThe second major weakness is the limited experimental study. The studies proposed in section 5 function more as demonstrations of the CAX library than new experiments due to their lack of rigor. Only the 1D-ARC experiment presents quantified results and a comparison with another method. The novel diffusion method presented in 5.1 is only demonstrated on a single example, and the self-encoding MNIST digits experiment in 5.2 is only validated through visual comparison of 1 image per digit. As such, claims like \" As shown in Figure 7, the NCA successfully reconstructs MNIST digits on the red face, demonstrating its ability to encode, transmit, and decode complex visual information\nthrough a minimal channel\" are not fully supported. Experimental details and full reporting of experimental results are necessary to support such claims. As demonstrations of the CAX library, these examples are interesting, but as experiments, they fall short.",
            "- While the paper presents a valuable tool, it does so in a very surface-level way. Most software discussions are in the API demonstration, which shows users how to put it into practice. This is important, but in an academic paper presenting the framework, the framework should take centre stage. Further, benchmarks against common test systems are very welcome in a software paper, but the comparison with GPT-4, while certainly interesting, is more of a research result in and of itself and not directly related to the software. I am sure there is more to the code that replacing numpy with JAX, but that should be laid out and explained in the manuscript. \n\nMy biggest concern is that ICLR isn't quite the correct platform for such a paper. I understand the desire for it to go there as it is related to ML but it is, at the end of the day, software. If the paper were to present a novel approach to structuring CA algorithms and packaging that up, it would be a different story. However, this decision is likely up to conference organizers. I can see the benefit of using ICLR as a platform to spread this software and perhaps help a lot of scientists. But from a newness perspective, it is not a breakthrough.",
            "- - the performance increases plotted in figure 3 I feel are not sufficiently demonstrating the utility of such a library. I say this as a user of JAX and PyTorch and know that there are many performance increases when using JAX, and so there needs to be a much more comprehensive case being made here that JAX is actually introducing large speed ups. This is also increasingly important as there doesn't seem to be a massive speed up in the \"Growing cellular automata\" experiment, so perhaps the authors should compute performance changes for a larger set of experiments. (For example for the [self-organizing textures](https://distill.pub/selforg/2021/textures/) experiment, or [cellular automata controlling cartpole](https://arxiv.org/abs/2106.15240) experiment. \n\n- as someone that has had to write JAX code for my own custom CA implementations, it would be great if the appendix included some annotated code that explains how efficient JAX implementations can be written. I know that curious readers can always read the code on github, and there are some examples shown such as on line 281-289 as well as the example notebook in the appendix (but it has minimal comments), but it would be extremely helpful for new programmers to understand the thought process for writing such a library. This is doubly important because when trying to implement this library for custom experiments, a certain degree of programming in JAX will be required. This is possibly slightly outside the scope or responsibility of the authors, but I think it would go a long way for allowing researchers to adopt this library if there were tutorials or detailed annotations of code that explain this process (I know for myself this would have been very helpful a few years ago).\n\n- I very quickly tried to run one of the colab notebooks for the 1D-ARC tasks and had problems with importing. I did not test with all the notebooks but this should probably be double-checked across the board. I recieved two errors:\n   - (on the second cell: `%pip install \"cax[examples] @ git+https://github.com/879f4cf7/cax.git\"`): after installing most packages, I recieved the following message: `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.`\n    - (on the third cell: `from cax.core.update.residual_update import ResidualUpdate`): \n~~~\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-3-7bc79b5c20c3> in <cell line: 11>()\n      9 from cax.core.perceive.depthwise_conv_perceive import DepthwiseConvPerceive\n     10 from cax.core.perceive.kernels import grad_kernel, identity_kernel\n---> 11 from cax.core.update.residual_update import ResidualUpdate\n     12 from flax import nnx\n     13 from tqdm.auto import tqdm\n\n1 frames\n/usr/local/lib/python3.10/dist-packages/cax/core/update/mlp_update.py in <module>\n      5 import jax.numpy as jnp\n      6 from flax import nnx\n----> 7 from flax.nnx.nnx.nn import initializers\n      8 from flax.nnx.nnx.nn.linear import default_kernel_init\n      9 \n\nModuleNotFoundError: No module named 'flax.nnx.nnx'\n~~~"
        ]
    },
    "nGiGXLnKhl": {
        "venue": "ICLR 2025",
        "title": "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures",
        "link": "https://openreview.net/forum?id=nGiGXLnKhl",
        "abstract": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks. Similar to the Vision Transformer (ViT), our model demonstrates robust global processing capabilities, efficiently handles sparse inputs like masked images, and can scale up to accommodate both large-scale parameters and extensive datasets. Its distinctive advantage is its reduced spatial aggregation complexity, enabling seamless processing of high-resolution images without the need for window operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code and models are available at~\\url{https://github.com/OpenGVLab/Vision-RWKV}.",
        "decision": "Accept (Spotlight)",
        "review scores": [
            8,
            8,
            8
        ],
        "strengths": [
            "- 1.The paper is well-written and easy to follow;\n\n2.Extensive empirical evidence supports the effectiveness of the model, indicating its practical application potential;\n\n3.The adaptation of RWKV for visual tasks goes beyond mere transfer, incorporating modifications that enhance its suitability for image processing.",
            "- - Comprehensive results across three tasks.\n- Interesting direction to replace ViTs with other efficient techniques that provide on-par or better performance.",
            "- - Novel contribution for quad-directional token shifting called Q-shift. This essentially increases the models range of semantic understanding\n- Authors expanded causal RWKV to a bidirectional global RWKV. They modified the exponent in the RWKV attention that leads to transforming the absolution positional bias to relative bias.\n- Well written paper with through experimentation including MAE pretraining"
        ],
        "weaknesses": [
            "- Major\uff1aAlthough the paper discusses RWKV and mamba as prominent RNN-based linear attention models transitioning from NLP to computer vision, it lacks a direct comparison with the mamba (vision) model. Such a comparison would be valuable for assessing the respective strengths and weaknesses of each model in the field of computer vision.\n\nMinor\uff1aFrom an innovation perspective, the approach of adapting other domains' mature architectures to computer vision, similar to what was done post-transformer, appears somewhat incremental.",
            "- - The gains in efficiency seem to be relatively minor, i.e., it is not an order of magnitude which still brings the question whether it is worth exploring these type of models to replace ViTs to begin with or not.\n\n For example, in Table 2 ViT-L vs. VRWKV-L 191.1 vs 189.5G FLOPS and parameters at 309.5M vs. 334.9M respectively. I think the reduction in FLOPS when scaling to Large variant seems to be around 3G. Similarly in Table 4, FLOPS 446.8 vs. 421.9G with the expense of an increase in the number of parameters. I am not expert in such type of methods focused on improving efficiency but I do not see the results are impressive enough to show the benefit from the V-RWKV design, especially that it is increasing the parameters.",
            "- - The authors implemented a bidirectional shift operation that removed the vertical shift in Q-shift, thereby enabling for MAE pretraining. IMO this is a source of complexity. As a result, MAE finetuning needs to be done in Q-shift manner"
        ]
    },
    "mtSSFiqW6y": {
        "venue": "ICLR 2025",
        "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
        "link": "https://openreview.net/forum?id=mtSSFiqW6y",
        "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            10,
            6
        ],
        "strengths": [
            "- * Clear presentation, covering both technical details and high-level ideas\n* Comprehensive experiments that emphasise the strengths of the proposed method\n* Reported speedups seem significant, and seem like they could be very useful for practical serving of LLMs",
            "- The paper is well-written. The argument is clear and reasonable. The proposed method shows effectiveness.",
            "- - Clever idea that was well-executed. The paper is well-written and the experiments make sense. The results are impactful in practical terms (people should use this) and for spawning future research (there are many possible variations on how to train the linear classifier).\n\n- I liked that the authors decided to go with the simple linear classifier since it worked best, rather than overcomplicating things\n\n- I also liked the systematic investigation with illustrative examples for how judge decoding differs from speculative decoding\n\n- Achieves state-of-the-art performance (141 tokens/s for 8B/70B-Judge, 129 tokens/s for 8B/405B) on standard hardware (2-8 H100 GPUs).",
            "- - The authors clearly present and motivate their ideas around standard SD verification potentially being overly stringent and distinguishing between semantic correctness and target distribution \u201calignedness\u201d. The motivation is straightforward to understand with the examples provided and it\u2019s clear how standard verification can be arbitrarily strict depending on the target model resulting in less than desired inference efficiency using standard SD at times. \n- The reported speedups over standard SD are significant, largely in part due to the increase in the number of accepted tokens using this new judge decoding verification process compared to standard verification. However, the authors correctly point out also that this is not necessarily an apples to apples comparison given the loss of output distribution guarantees compared to standard SD.\n- The investigations into token embeddings signal errors was particularly interesting in that \u201cincorrect\u201d statements in text could be pinpointed based on the target model\u2019s latent embedding because of the models tendency to try to immediately rectify its response following an inaccuracy and the resulting dataset curated based on this could prove useful to others if released"
        ],
        "weaknesses": [
            "- * Some of the results (e.g., lack of agreement between different models / human drafts and models) seem to be obvious and do not help to make the argument\n* The use of a disjunctive criterion (standard SD accepts or JD accepts) is a bit surprising, and left me wondering in what cases (and how often) JD rejects when SD accepts. The paper would be strengthened by investigating this more closely.\n* Sect. 5.3 (OOD performance) is very important for deployment of the proposed method, and so it would be useful to expand on this. In particular, Sect. 4.1/\"Dataset Curation\" does not go into much detail into what distribution the training data follows, and which tasks are covered well. It would be helpful to understand whether there is a strong alignment between the TokenCourt training data and the considered evals.",
            "- I do not see significant weakness.\n\nBut the sentence \"To speed up inference in such a setting, Chen et al. (2023); Leviathan et al. (2023) concurrently propose speculative decoding (SD)\" in the introduction section is incorrect.\nAt least, there is a much earlier one in 2018 that proposed speculative decoding: \"Blockwise Parallel Decoding for Deep Autoregressive Models\" by Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. The author skipped a large amount of literature on speculative decoding before 2023.\n\nAuthors are required to address this issue.",
            "- I would have liked if the authors addressed the possible impact of their method on less binary (correct vs incorrect) metrics like reward model scores or chatbot ELO scores. I could imagine to get a more noticeable regression on those, since the writing style of models is more important in that setting. Perhaps something for the discussion?\n\nThe Burns et al 2022 paper (https://arxiv.org/abs/2212.03827) does some nice work on eliciting concepts with linear probes which could be relevant here.",
            "- - Given the fact that verification is being changed so there is no longer a guarantee of output distribution match, the speedup comparisons are no longer apples to apples and have to also be paired with quality comparisons to ensure quality is maintained. Additionally, given the way the judge model is trained on \u201ccorrect\u201d and \u201cincorrect\u201d inputs, it seems that for novel tasks, a new tuning dataset must be hand curated for each task to train the judgment model to recognize \u201ccorrectness\u201d making the overhead of applying a technique like Judge Decoding significant compared to other SD modifications.\n- The notion of \u201ccorrectness\u201d is easiest to understand in the context of text generation tasks that are Question&Answer based. However, it\u2019s unclear how this idea would work for more open-ended creative generation tasks where \u201ccorrectness\u201d may not be as easily defined and matching the target model output distribution may be ideal.\n- An additional condition to applying Judge Decoding is for the draft model to be of high-quality that can produce longer valid generations otherwise the approach may not provide that much of a speedup. \n- The reported experimental results are somewhat limited such as the out of distribution performance results being restricted to only a few lines and the full results not being reported in the appendix at least in comparison to the baselines etc. Additionally, it\u2019s unclear what task is being reported on for Table 1 whereas Figure 6 shows accuracy across a variety of tasks. It would be helpful to also see the relative speedups across each of these variety of tasks as well.\n- For the most part, the authors are forthcoming about weaknesses and limitations to their judge decoding approach and many of these points will align. However, the amount of pre-requisites needed to apply the technique as well as required overhead for verification of maintained quality due to the loss of output distribution alignment are substantial."
        ]
    },
    "mMPMHWOdOy": {
        "venue": "ICLR 2025",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
        "link": "https://openreview.net/forum?id=mMPMHWOdOy",
        "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- **1. Strong Results** -- I put a lot of premium on this strength and use this to justify my overall rating. Many of the gains from training on Math Evol-Instruct are more than 10 points. More importantly, it is quite impressive to design something that outperforms strong proprietary models, so if this method is as strong as the paper claims, then this is something that the community will definitely quickly pick up on.\n\n**2. Thorough Experiments and Baseline Comparisons** -- Various scales ranging from 100M to 1B to 70B, with different base models. Has comparisons with several math-specific models (e.g. Mammoth, Math-Shepherd, etc.) I also liked the analysis and ablations in Section 4.4\n\n**3. Scalable Method** -- The whole process is fully automated, which makes it scalable. I imagine this can also be adapted to other process-intensive reasoning domains outside math (e.g. coding).",
            "- This paper is well-written and rich in detail. The introduction of the Instruction Reward Model (IRM) is novel and useful. The experiments are comprehensive, and the analysis is thorough, providing deep insights into the framework's effectiveness. I believe the idea of integrating IRM with PRM could be useful for any math LLMs (only undergoing SFT).",
            "- The paper is well-written, with comprehensive analysis and ablation experiments. \n\nIt represents a valuable exploration of process supervision and IRM in math reasoning, achieving impressive performance.\n\nAt the same time, their method is data efficient.",
            "- 1. This paper provides a scalable approach to enhance LLM reasoning capabilities in math without external computational tools.\n2. Notable improvements in metrics across diverse benchmarks.\n3. The introduction of dual reward models (IRM and PRM) effectively improves model reliability and accuracy. \n4. The synthetic data and process supervision paradigm have had a great impact on the community."
        ],
        "weaknesses": [
            "- **1. PRM labels from GPT-4** -- Not really sure what to think of this. On one hand, I feel such direct distillation like this would limit the effectiveness of a method at larger data scales. On the other hand, the results seem to be good (and also this is one key part that makes the process fully AI-automated.)\n\n**2. Unclear presentation** -- The paper assumes that readers are already previously familiar with Evol-Instruct, as it devotes very little time to talking about it in the intro or related work. The narrative is messy -- there are certain concepts (e.g. \"grade school\" and \"high school\" questions) that were introduced once out of nowhere then never mentioned again. There are a number of rows on Table 1 that were never discussed in the main text. Figure 1 is difficult to understand. There are also several typos and poorly-worded sentences.\n\n**3. Somewhat marginal contribution** -- Evol-Instruct previously existed. PRM previously existed. This paper basically took Evol-Instruct and PRM and used them to train a model. To nitpick a bit, I think a more comprehensive paper would cover more domains such as code.",
            "- The primary concern with this paper is the unfair comparison of baseline models in the results. While the authors claim that both supervised fine-tuning (SFT) with Math Evol-Instruct and reinforcement learning (RL) with the Instruction Reward Model (IRM) and Process Reward Model (PRM) are beneficial for enhancing mathematical reasoning, these approaches\u2014SFT with synthesized data and the use of various reward models for RL\u2014represent parallel research lines.\n\n- In Table 1, the authors compare their model, which has undergone both SFT and RL, with models that have only undergone SFT. This comparison is unfair because these SFT models could also be further enhanced with RL techniques to improve mathematical reasoning (e.g., using ORM for RL on DartMath). It would be more appropriate to isolate the effects of SFT and RL for a fair comparison. In Table 1, the authors should compare the performance of models that have undergone SFT with Math Evol-Instruct against existing baselines such as MetaMath and DartMath. Additionally, comparisons with baselines like MetaMath and DartMath on the LLaMA-3.2 backbone would be valuable, as their training data is publicly available.\n\n- In analyzing the impact of training data size, the authors should compare their approach with the best method for SFT using synthesized data, specifically DartMath. MetaMath, which was developed around a year ago, uses GPT-3.5-turbo for data augmentation, making it an outdated and potentially unfair baseline.\n\n- It appears that SFT with Math Evol-Instruct yields inferior results compared to other SFT methods. From Table 4, the LLaMA2-7B: WizardMath-SFT scores 35.6 on MATH, which lags behind models like XwinMath and Skywork. Likely, it would also lag behind LLaMA2-7B fine-tuned on the DartMath training data. This suggests that the main contribution of the paper is in the RL component. Therefore, the primary focus should be on the results obtained with different reward models, as presented in Table 4, utilizing various SFT backbones.\n\n- Table 7 lacks adequate baselines; at least, the authors should include LLaMA-2-7B trained on the DartMath training set. This table also suffers from the same fairness issues as Table 1.\n\nI recommend that the authors reorganize the paper better to emphasize their contributions to the \"RL part.\"",
            "- NA",
            "- 1. The training data for RLEIF is derived partly from a designated training set and synthetic from existing models, such as GPT-4. How might the reliance on specific LLMs for generating synthetic data (like GPT-4) affect the scalability of this approach for researchers without access to such models? Have the authors explored alternative methods for generating diverse mathematical problems that don't depend on proprietary models?\n2. While RLEIF has undergone extensive evaluation in the context of mathematical reasoning, it may also hold potential for applications in other reasoning domains. Have the authors considered applying RLEIF to other reasoning tasks like logical deduction or causal inference? It would be valuable to see if the benefits observed in mathematical reasoning transfer to these domains, and what modifications might be necessary."
        ]
    },
    "m2nmp8P5in": {
        "venue": "ICLR 2025",
        "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
        "link": "https://openreview.net/forum?id=m2nmp8P5in",
        "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines.",
        "decision": "Accept (Oral)",
        "review scores": [
            8,
            8,
            8,
            8
        ],
        "strengths": [
            "- 1. Innovative approach: It is innovative to use LLMs for scientific equation discovery and symbolic regression. LLM-SR integrates LLMs' prior scientific knowledge and and code generation ability to achieve good performance.\n\n2. Robust evaluation: The authors have carefully designed benchmark problems that align well with the real-world applications.\n\n3. Comprehensive analysis: The qualitative analysis effectively evaluates the model's OOD generalizability  and the ablation study provides insight into the contribution of each of model's component.\n\n4. Well written: The paper is well written with clear data presentation.",
            "- 1. **Innovative Application of LLMs for Scientific Equation Discovery**:\n   - The authors introduce an innovative approach by leveraging the scientific knowledge and code generation capabilities of large language models (LLMs) for equation discovery. Representing equations as code and integrating data-driven feedback and optimization provides a novel perspective for scientific modeling.\n\n2. **Experience Buffer and Iterative Optimization**:\n   - The use of an experience buffer to maintain high-quality equation examples and facilitate iterative optimization is a unique feature. This mechanism helps the model learn and recall effective generation patterns, improving the quality and efficiency of equation exploration.\n\n3. **Comprehensive Multi-Domain Benchmarking**:\n   - The paper demonstrates the method\u2019s applicability across multiple scientific domains, including physics, biology, and materials science. The newly designed benchmarks, which aim to prevent memorization of known equations, reflect the authors' commitment to scientific rigor.\n\n4. **Promising Experimental Results**:\n   - Despite certain challenges and areas for improvement, LLM-SR shows promising performance compared to existing symbolic regression methods, particularly in out-of-domain tests. This indicates potential for tackling complex scientific problems and suggests a valuable direction for future research.",
            "- 1. To the best of my knowledge, framing SR as equation _template_ discovery seems novel and a useful contribution.\n2. The analyses presented to demonstrate LLM memorization of the Feynman dataset are quite convincing.\n3. The new benchmark problems are well-motivated and look like sound contributions.\n4. The paper is, overall, clearly written and easy to follow, and visualizations are used well in various places.",
            "- 1. The approach of using LLMs to generate symbolic equation structures combines programming and scientific prior knowledge, setting it apart from traditional symbolic regression methods.\n2. LLM-SR shows strong performance not only in-domain but also in out-of-domain scenarios, which is often challenging for symbolic regression techniques.\n3. By leveraging scientific prior knowledge and a structured experience buffer, LLM-SR reduces the search space, achieving good results with fewer iterations compared to traditional methods."
        ],
        "weaknesses": [
            "- The paper does not provide detailed insight into how LLM-SR handles edge cases, such as noisy data, or highly complex functions. This could be important for real-world applications where data may be imperfect or equations are inherently intricate.",
            "- **Complexity and Necessity of the Method**:\nThe authors introduce complex mechanisms such as sampling strategies and an island model, showcasing innovative efforts. However, clearer justification is needed to explain the necessity of these elements and their specific contributions to enhancing the method\u2019s performance. Simplifying and elucidating these mechanisms' actual benefits would improve the clarity and persuasiveness of the method.\n\n**Utilization and Verification of Prior Knowledge**:\nAlthough the authors assume that the LLM leverages its scientific prior knowledge to generate reasonable equations, the paper lacks detailed analysis and experimental validation of how this mechanism functions and its tangible effects. Further exploration and evidence would more convincingly support this claim and make the method\u2019s core advantage clearer to readers.\n\n**Breadth and Realism of Experimental Design**:\nThe current experiments are based on data generated from known equations, providing a basic evaluation. However, data in real-world scientific research often contains noise and outliers, and the method\u2019s performance under such conditions is a critical indicator of its practical applicability. Including datasets with measurement errors and outliers would help demonstrate the method\u2019s robustness and generalizability in complex, realistic scenarios.\n\n**Impact of Programming Capabilities on Experimental Results**:\nThe paper emphasizes the LLM's ability to generate programming code but lacks analysis of whether these programming capabilities interfere with the experimental results. The boundary between the LLM\u2019s programming skills and scientific reasoning remains unclear, which might mean that experimental outcomes reflect coding proficiency rather than pure scientific inference. Clarifying the role of these aspects in equation generation would aid in better understanding the method\u2019s true strengths and limitations.",
            "- My main concerns with the work are twofold:\n1. The paper reads in a manner that moderately overclaims novelty. The main idea appears to be the combination of numerical optimizers with LLM-based optimization over templates, besides the contribution of new SR problems. However, the motivation, currently, is primarily devoted to comparisons with non-LLM methods, which seems unfair. Since the framing of SR problems within the context of LLM-based optimization is not new (Table 1 from [1] is a useful reference), it would be more appropriate to motivate the work by describing existing LLM-based SR methods, clearly stating their problems, and then showing how the new method addresses them.  \n2. My second concern is regarding the choice of baselines. While I appreciate the inclusion of several standard SR methods, I think the comparisons do not precisely show the benefits of the paper's contributions.  \n  (a) Given prior works such as [1, 2, 3] in LLM-based SR/optimization, it would have provided utility to see the performance of the proposed method in comparison to its closest counterparts. Alternatively, if the argument is that some of the ablations subsume these methods, it would then be useful to see those ablations presented as the main baselines on all the datasets instead of only on a partial set.  \n  (b) The ablation results from Figure 6 show that a substantial drop in performance occurs when coefficient optimization is removed. Based on this, it would have been useful to see whether simply equipping the non-LLM baselines with a similar coefficient optimizer would close the gaps that we currently see in Table 1. I am currently not convinced that the LLM is needed to facilitate the evolutionary search of equation templates.  \n\nMinor:\n1. The choice of describing the contents in Section 2.4 as \"Experience Management\" is mildly concerning. Given a large body of prior work in evolutionary computation, it would be appropriate to avoid fragmenting the literature for the benefit of differentiation and instead re-use existing terminology. If there is indeed an aspect that warrants a new term, please do correct me.  \n2. It was mildly odd to read that mathematical equations have been \"unreasonably\" effective in describing complex phenomena. Unclear why we think it is unreasonable.  \n\n[1] Meyerson et al., 2023. Language model crossover: Variation through few-shot prompting.  \n[2] Romera-Paredes et al., 2023. Mathematical discoveries from program search with large language models.  \n[3] Yang et al., 2024. Large Language Models as Optimizers.",
            "- 1. The framework relies heavily on the embedded scientific knowledge of LLMs, which may be biased, incomplete, or inaccurate for certain scientific domains, potentially limiting its robustness.\n2. Although the paper addresses different domains, it would benefit from more diverse and complex real-world datasets to further validate the method\u2019s general applicability."
        ]
    }
}