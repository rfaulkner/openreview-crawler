{
    "bEgDEyy2Yk": {
        "venue": "ICLR 2025",
        "title": "An efficient implementation for solving the all pairs minimax path problem in an undirected dense graph",
        "link": "https://openreview.net/forum?id=bEgDEyy2Yk",
        "abstract": "We provide an efficient $ O(n^2) $ implementation for solving the all pairs minimax path problem or  widest path problem in an undirected dense graph. It is a code implementation of the Algorithm 4 (MMJ distance by Calculation and Copy) in a previous paper. The distance matrix is also called the all points path distance (APPD). We conducted experiments to test the implementation and algorithm, compared it with several other algorithms for solving the APPD matrix.  Result shows Algorithm 4 works good for solving the widest path or minimax path APPD matrix.  It can drastically improve the efficiency for computing the APPD matrix.  There are several theoretical outcomes which claim the APPD matrix can be solved accurately in $ O(n^2) $ . However, they are impractical because there is no code implementation of these algorithms. It seems Algorithm 4 is the first algorithm that has an actual code implementation for solving the APPD matrix of minimax path or widest path problem in $ O(n^2) $, in an undirected dense graph.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            1,
            1
        ],
        "strengths": [
            "- The paper does not contain any significant contribution.",
            "- The problem of finding all pairs of minimax path is interesting and has applications within ML",
            "- the problem is well motivated in general.",
            "- Judging from the manuscript, this is the first implementation for $O(n^2)$ time algorithm for the all pairs minimax path problem. Because this algorithm works for any graph including dense graph (where $m=\\Theta(n^2)$), this is actually an optimal algorithm."
        ],
        "weaknesses": [
            "- - the paper is in area of exparimental algorithmis with no siginificant contribution to machine learning, \n- there is no siginificant contribution to algorithms, i.e., the paper just implents in a straight-forward way the algorithms and tests it, there is not technincal challange in this implementation, nor the tests introduce any nowel methodology,\n- the paper makes false statement that previous quadratic time algorithms have not been impmemented - this is not true as even the orginal paper by Sibson from 1973 contains the Fortran implementation of the algorithm.",
            "- * A straight-forward implementation of an existing algorithm cannot be considered publishable research\n* Writing quality and presentation is extremely poor\n* Theoretical claims about complexity is not formally established.",
            "- while the problem is well motivated, finding an implementation for the algorithm is not very well motivated. It seems that no implementation existed before because the algorithms were not very efficient, and the algorithm of Liu is quite simple and efficient. The mere fact of coding that algorithm up is a simple classroom exercise it seems. If there are techniques used for going from the pseucode to the implementation, the authors have not described them, and it seems from the code that there are no such techniques. \nBesides the above fundamental point, the write up could be polished, there are several grammatical mistakes. \nabstract: \"it is a code implementation of...\" -> \" this paper is a code implementation of the minimax path algorithm in Liu\".\nline 18:: works good -> works well.\nline 106:: the sense -> the sign\nin the related work it is good to note if there are any other papers like your paper where they implement an algorithm from a previous paper.",
            "- The most prominent weakness of this work is that it only provides an implementation of the existing method of the previous paper of Liu (arXiv 2023). I think the implementation itself can be considered as a significant contribution when the original algorithm has some difficulties in implementing and they are tackled by some novel or dedicated techniques. However, the implemented algorithm (Figure 1(a)) is much simpler that seems to be easily implemented, and actually it is implemented by short Python codes (Figure 1(b)). We do not find any implementation techniques in Section 3; Section 3 consists of just an explanation for the existing method of Liu. This means that obtaining simple algorithm that is easy for implement has already been done in the previous work. Thus, I think this is not a significant contribution.\n\nAnother weakness of this work is that it is never compared empirically with the existing (and traditional) O(n^2) algorithm. In the conclusion, the authors claim that the SLINK algorithm of Sibson (1973) is difficult to implement by quoting the conversation with other people. However, we easily find some SLINK implementations such as https://github.com/battuzz/slink and https://github.com/jackyust/SLINK_CLINK . Moreover, the paper of Sibson also has a FORTRAN implementation of his algorithm. Thus, it is questionable that the SLINK algorithm is hard to implement. At least, the authors should try to implement SLINK and compare it with the proposed implementation.\n\nMinor comments:\n- The citations should be enclosed with brackets. Maybe you use \\citet{...} instead of \\citep{...}, don't you?\n- Tables 1 and 2 are significantly wider than the paper width; please fit these tables into the paper width."
        ]
    },
    "P49gSPmrvN": {
        "venue": "ICLR 2025",
        "title": "Time-dependent Development of Scientific Discourse: A Novel Approach Using UMAP and Word Embeddings",
        "link": "https://openreview.net/forum?id=P49gSPmrvN",
        "abstract": "This study presents a method for visualizing the time-dependent development of a scientific discipline using UMAP (Uniform Manifold Approximation and Projection) and text embeddings. This study demonstrates how the evolution of research interests and topics in a specific field can be mapped over time by encoding the abstracts of scholarly articles into a high-dimensional space and then projecting them into a 3D space. This computational approach converts the history of discourse into a point-cloud that can be further studied as a manifold and as a time series, which leads to new insights into the dynamics of scholarly discourse and the emergence and disappearance of research themes.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            1
        ],
        "strengths": [
            "- -",
            "- The text embedding-based approach to study scientific discourse and evolution is a promising direction.",
            "- * The use of word embeddings and UMAP is valid, especially for large-scale textual data visualization.\n* The approach could potentially serve as a useful tool for meta-scientific analysis or historiography."
        ],
        "weaknesses": [
            "- -",
            "- Unfortunately, I believe that the paper is not a good fit for the ICLR conference. The paper does not make a significant contribution to the field of representation learning or machine learning. The paper presents a qualitative exploratory analysis of the embeddings of paper abstracts over time, which does not represent a significant advancement from the common practice of analyzing embeddings that are performed routinely in various contexts. \n\nFor instance, conferences like NeurIPS have been providing embedding-based visualization tools for multiple years. Companies like nomic.ai have been providing the temporal analysis of scientific literature over time at the scale of millions of papers (https://www.biorxiv.org/content/10.1101/2023.04.10.536208v1). Research groups like AllenAI, Northwestern University (e.g., Dashun Wang), University of Chicago (James Evans), Indiana University (YY Ahn), Northeastern University (Albert-L\u00e1szl\u00f3 Barab\u00e1si), Stanford University (Dan Jurafsky), and many others have been extensively employing paper embeddings and other related methods to study scientific discourse and evolution. Given this long list of existing work (with much more sophisticated analyses) that I can't list all here (and not cited by the paper), the manuscript would need to provide a lot more results and analysis to make meaningful contribution. I'm sorry to say that the paper misses the mark quite substantially in this regard.\n\nAn important issue that the author should also pay attention to is that we shouldn't simply use the UMAP axes as meaningful coordinates. This has been extensively critiqued by Lior Pachter and other researchers (e.g., https://www.biorxiv.org/content/10.1101/2021.08.25.457696v1).",
            "- * The choice of embedding model is not adequately justified, nor are comparisons made with other models to strengthen the findings.\n* The visualizations lack clarity, making it difficult to extract meaningful information from them.\n* The paper does not engage deeply with related works, missing out on critical discussions about existing methods for analyzing research trends.\n* The results fail to provide actionable insights and the paper misses contributions."
        ]
    },
    "8QTpYC4smR": {
        "venue": "ICLR 2025",
        "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
        "link": "https://openreview.net/forum?id=8QTpYC4smR",
        "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            1,
            1
        ],
        "strengths": [
            "- The paper provides an overview of the technologies employed in the development of Language Models (LMs).",
            "- None, as can be seen in the manuscript, Section 3.11 (Comparison with Recent Reviews):\n>To provide a more comprehensive overview, we compare our findings with recent reviews on LLMs. Notably, Bommasani et al. (2021) and Zhao et al. (2023) offer extensive analyses of the latest advancements and applications of LLMs, including ethical considerations and deployment challenges. These reviews highlight the importance of continuous benchmarking and evaluation to ensure that\nLLMs are developed and used responsibly. **By integrating insights from recent benchmarks and reviews, this section provides a broader perspective on the current state of LLM research, highlighting both the progress made and the challenges that remain.**\n\nThat is, this paper does not offer any contribution other than those mentioned in Bommasani et al. (2021) and Zhao et al. (2023).",
            "- It is important to provide a review on LLMs in the era of the vast development of LLMs.",
            "- 1. Provides a summary of LLM architecture, application areas, limitations, practical usage, and future directions. Which could be useful to get a quick idea for new LLM researchers to get the basic idea. However, it is more effective as a blog post and not at all a research survey paper."
        ],
        "weaknesses": [
            "- The paper\u2019s objectives are not clearly defined. While it purports to review Large Language Models (LLMs), the models it examines are not currently regarded as large by contemporary standards (such as open models like LLaMa-2, OLMo, etc.) [1]. Furthermore, despite claiming to explore future directions for LLMs, the paper fails to address this topic adequately. For instance, specific future applications of LLMs in new fields or emerging challenges associated with the expansion of LLM could have been explored [2].\n\n[1] Bommasani, Rishi, et al. \"On the opportunities and risks of foundation models.\" arXiv preprint arXiv:2108.07258 (2021).\n\n[2] Li, Sha, et al. \"Defining a new NLP playground.\" arXiv preprint arXiv:2310.20633 (2023).",
            "- * The paper does not present any novel or meaningful contribution.\n* The content is outdated.\n* The organization, particularly in Section 3 is very poor.\n* There are missing or wrongly cited references.\n* **The paper feels empty:** Most of the subsections in Section 3 contain a single paragraph with (in the best case) a single reference.",
            "- I believe this paper is not appropriate for ICLR. This is not a \"systematic\" review. The content is superficial and outdated. The insights are not valid.",
            "- i. This survey does not provide any new novel insights in comparison to what is known already about LLMs (e.g., [1], [2] etc.). It is more of a straightforward summary of the architectures, applications, and limitations of LLMs, lacking in-depth critical review.\n\nii. Moreover, although the paper title claims the paper as a systematic survey, the discussion on different topics in this review is also superficial. \n\niii. Even though the paper mentions LLMs, the discussion is more around typical transformer-based language models like BERT, GPT, and T5 without offering new insights. \n\niv. The paper relies heavily on vague citations. Moreover, some of the citations have \"?\" marks. This demonstrates that the paper lacks attention to detail. Potentially, this paper was written without any comprehensive research.\n\n1. Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X. and Gao, J., 2024. Large language models: A survey. arXiv preprint arXiv:2402.06196.\n\n2. Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z. and Du, Y., 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.\n\nv. The discussed limitations and proposed future directions also do not offer anything new. \n\nvi. Figure 2 also looks pretty bad. The text size in the caption is also very large in comparison to the paper text."
        ]
    },
    "5kMwiMnUip": {
        "venue": "ICLR 2025",
        "title": "NEMESIS \\\\  Jailbreaking LLMs with Chain of Thoughts Approach",
        "link": "https://openreview.net/forum?id=5kMwiMnUip",
        "abstract": "Large Language Models (LLMs) are increasingly being deployed across various\napplications, making the need for robust security measures crucial. This paper\nexplores multiple methods for jailbreaking these models, bypassing their secu-\nrity protocols. By examining five distinct approaches\u2014Multishot Jailbreaking,\nthe Mirror Dimension Approach, the Cipher Method, the \u201dYou are Answering the\nWrong Question\u201d Method, and the Textbook Jailbreaking Method\u2014we highlight\nthe vulnerabilities in current LLMs and emphasize the importance of fine-tuning\nand secure guardrails. Our study primarily employs chain-of-thought reasoning,\nwhich can be further enhanced through reinforcement learning techniques. Fur-\nthermore, we propose that our findings can serve as a benchmark against emerging\nsecurity measures such as LlamaGuard, providing a comprehensive evaluation of\nLLM defenses. Our findings demonstrate the effectiveness of these methods and\nsuggest directions for future work in enhancing LLM security. This research un-\nderscores the ongoing challenges in balancing LLM capabilities with robust safe-\nguards against potential misuse or manipulation.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            1,
            1
        ],
        "strengths": [
            "- No.",
            "- 1. The paper addresses a highly important and timely topic, namely LLM red-teaming.\n\n2. The innovative approach of leveraging chain-of-thought prompting to further improve the success rate of LLM jailbreaking is interesting.",
            "- This paper explores multiple methods for jailbreaking LLMs",
            "- Each of the five proposed jailbreaking is empirically effective, within some setups that are not described.",
            "- I regret to say that I can't identify any strengths."
        ],
        "weaknesses": [
            "- This paper only runs existing attacks. No novelty. No new findings.",
            "- 1. While the five methods discussed in the paper are well-documented in previous research, the novelty of this study could be more clearly highlighted.\n\n2. The evaluation section would benefit from baseline comparisons to help readers better understand the strengths of the proposed techniques.\n\n3. The presentation could be improved for greater clarity. For example, in Figures 2 and 3, what question set you are using for evaluation? What does \"effectiveness\" mean in Figure 2? Why is \"effectiveness\" a merely binary number? Which LLM is being evaluated in Figure 3, and is it the same model as in Figure 2? Why do the ASR values differ for each jailbreaking method before the attack?",
            "- 1. The paper appears disorganized and reads like an unfinished draft. Basic issues in formatting persist, such as incorrect reference formatting with placeholders and question marks. Additionally, the structural flow does not align with typical research paper conventions. For instance, the paper should clearly outline its contributions at the end of the introduction. Furthermore, there should be a dedicated evaluation section to assess the proposed approach comprehensively. In Section 3, subsections are over-divided, with new subsections every 4-5 lines, which hinders readability. Figure 1 also needs improvement in both clarity and presentation quality.\n\n2. The proposed method, as it stands, is a straightforward combination of multiple manually devised strategies, many of which are already established in the literature [1][2][3][4].\n\n[1] Zeng, Yi, et al. \"How Johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs.\" arXiv preprint arXiv:2401.06373 (2024). \n\n[2] Jin, Xiaolong, Zhuo Zhang, and Xiangyu Zhang. \"MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds.\" arXiv preprint arXiv:2402.01706 (2024). \n\n[3] Deng, Gelei, et al. \"Pandora: Jailbreak GPTs by retrieval-augmented generation poisoning.\" arXiv preprint arXiv:2402.08416 (2024). \n\n[4] Yang, Xikang, et al. \"Chain of Attack: A Semantic-Driven Contextual Multi-Turn Attacker for LLM.\" arXiv preprint arXiv:2405.05610 (2024).\n\n3. Moreover, the paper lacks evaluation against existing defenses and does not compare its performance with relevant baselines, which limits the credibility and impact of the proposed attack strategy.",
            "- 1. The layout of the manuscript is not in a standard academic style. The setup and results sections are missing. \n\n2. related to (1), as there is no setup section, it is hard to interpret the results. For example, the metrics (e.g. definition of success in Fig 3) and benchmarks (e.g. tasks used for Fig 3) remain unknown. \n\n3. related to (1), in each subsubsection of section 3 (i.e., section 3.1.1, 3.2.1, 3.3.1, 3.4.1, and 3.5.1),  the manuscript makes claims about the effectiveness or limitations of each jailbreaking approach respect to some setups, yet these claims are not grounded by empirical evidence. Similarly, claims made in Section 5 are neither well grounded. \n\n4. While there is an evaluation of each of the five methods in the framework, there is no evaluation of the whole framework. \n\n5. The contribution of this paper is not clearly defined. For example, is the reference approach a contribution to this paper?\n\n6. The current contribution of the current draft is low. Many existing works have proposed multi-turn jailbreaking approaches and multi-approach security benchmarks. Additionally, the current selection of jailbreaking approaches in the framework is unjustified.",
            "- The paper has several significant weaknesses, and I will highlight a few:\n\n- The overall contribution of the paper is unclear. What specific question are the authors trying to answer? It reads more like a brief survey summarizing known attacks, but even this is done in a general and superficial way.\n\n- The references are not in a standard format, and there is a \"?\" in the introduction failing to point to the correct paper.\n\n- Several named papers or methods, such as PathSeeker, are not cited.\n\n- The format of the background section is unusual, with important works missing, such as: \nAnil C, Durmus E, Sharma M, Benton J, Kundu S, Batson J, Rimsky N, Tong M, Mu J, Ford D, Mosconi F. Many-shot jailbreaking. Anthropic, April 2024.\n\n- There is no results section, except for a very brief subsection in the methods section.\n\n- The main figure is overly simplistic and poorly constructed, with unnecessary gridlines and little useful information.\n\n- Claims made in the introduction, such as attributing jailbreak attacks to LLM architectural features, are never addressed or followed up on throughout the paper."
        ]
    },
    "SI6zocV2SS": {
        "venue": "ICLR 2025",
        "title": "CAN - CONTINUOUSLY ADAPTING NETWORKS",
        "link": "https://openreview.net/forum?id=SI6zocV2SS",
        "abstract": "Catastrophic forgetting is a fundamental challenge in neural networks that prevents continuous learning, which is one of the properties essential for achieving true general artificial intelligence. When trained sequentially on multiple tasks, conventional neural networks overwrite previously learned knowledge, hindering their ability to retain and apply past experiences. However, people and other animals can learn new things continuously without forgetting them. To overcome this problem, we devised an architecture that preserves significant task-specific connections by combining selective neuron freezing with Hebbian learning principles. Hebbian learning enables the network to adaptively strengthen synaptic connections depending on parameter activation. It is inspired by the synaptic plasticity seen in brains. By preserving the most important neurons using selective neuron freezing, new tasks can be trained without changing them. Experiments conducted on standard datasets show that our model significantly reduces the risk of catastrophic forgetting, allowing the network to learn continually.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            1,
            3
        ],
        "strengths": [
            "- The idea of utilizing Hebbian learning to help the model continually learn is interesting, as Hebbian learning is a well-known synaptic plasticity rule in animals. Experiments might provide insights into why these mechanisms can help animals continually learn.",
            "- The paper connects the Hebbian learning with neuron isolation, selectively freezing neuron weights based on importance calculation, which provides some valuable insights.",
            "- The use of Hebbian rule when evaluating the neurons' importance is interesting.",
            "- If there is a good conceptual reasoning behind the proposed use of Hebbian learning (see my comment on the matter under \"weaknesses\"), the direction explored in the paper may be promising."
        ],
        "weaknesses": [
            "- 1. Overall the motivation behind the method is not well explained. The motivation behind Hebbian learning is to take inspiration from animals, but it is not clear to me why we want to use a separate network evolved with Hebbian learning to compute the importance score. It's unclear how this dual-network design is linked to biological continual learning.\n2. More importantly, the experiments are way too lacking. The method is only tested on MNIST with 0-4/5-9 as two separate tasks and only compared to the vanilla network. The paper mentioned quite a lot of previous work but they are not compared as baselines. It's understandable that these bio-inspired methods might not surpass SOTA methods in the field but at least the method should be tested on a range of scenarios against some reasonable baseline.",
            "- The writing of this paper is poor, lacking a summary of related work in the field. There are many existing methods that combine Hebbian learning with weight updates, but the paper fails to mention the limitations of current methods or the advantages of the proposed approach. Additionally, the figures and tables are quite rudimentary, making it difficult to understand the authors' intent. The experimental results lack detail, including mathematical descriptions and specific training and testing procedures. More importantly, there is no extensive and comprehensive comparison, and the limited experimental results are insufficient to demonstrate the effectiveness of the model.",
            "- 1. The authors did not provide specific details that are crucial for their study. Specifically, the second equation does not include any indices, which makes its interpretation difficult, and the architecture of a neural network used in this study is not specified. \n\n2. The authors split MNIST into two disjoint tasks. The experimental setup is too simple to evaluate the algorithm properly. \n\n3. Although multiple earlier studies used MNIST to evaluate continual learning algorithms, the authors did not provide any comparison to other studies.",
            "- 1) While experiments demonstrate that Hebbian learning mechanism prevents catastrophic forgetting, it is not clear why (conceptually) this would be the case. In other words, why does association by activity select the nodes that are relevant for one task while excluding those that belonged to past ones? It may be my missing backgeound in the application of Hebbian learning, but I recommend authors to include a more detailed explanation of this part of the conceotual background for people who are lacking this background.\n\n2) All experiments are basically the same measured with different metrics. One of the metrics would suffice.\n\n3) Task identification and new task recognition is not a trivial task. Without this, it is questionable whether any method can be called to realize continual learning or not. (The authors mention that this is not a challenge they are tackling, but without this ffull continual learning setup the relevance of a destructive adaptation problem is questionable.)"
        ]
    },
    "OXIIFZqiiN": {
        "venue": "ICLR 2025",
        "title": "A Dual-Modal Framework Utilizing Visual Prompts for Enhanced Patch Analysis",
        "link": "https://openreview.net/forum?id=OXIIFZqiiN",
        "abstract": "Patch representation learning has emerged as a crucial innovation in software development, leveraging machine learning techniques to advance software generation workflows. This approach has led to significant enhancements across various applications involving code alterations. However, existing methods often exhibit a tendency towards specialization, excelling predominantly in either predictive tasks such as security patch classification or in generative tasks like the automated creation of patch descriptions. This paper presents a groundbreaking approach to patch representation learning through the Image-Guided Code Patch Framework (IGCP), a novel architecture that bridges the gap between code analysis and image processing domains. We introduce a rigorous mathematical foundation for IGCP, leveraging measure theory, functional analysis, and information geometry to formalize the domain adaptation process in patch representation learning. The optimization dynamics of IGCP are rigorously analyzed through the lens of Stochastic Gradient Langevin Dynamics, providing convergence guarantees in both convex and non-convex loss landscapes. Empirical evaluations demonstrate that IGCP not only achieves state-of-the-art performance in patch description generation but also exhibits remarkable domain generalization capabilities.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            1
        ],
        "strengths": [
            "- N/A.",
            "- I have limited experience in this specific domain, particularly with patch analysis in the context of code. As a result, I find it challenging to fully assess the contributions and strengths of this paper.",
            "- Patch analysis is an important topic to study and a field where many ML techniques could potentially be explored. The authors also attempt to propose their method from a very theoretical point of view.",
            "- N/A"
        ],
        "weaknesses": [
            "- See above.",
            "- In my view, the paper has some formatting issues, as all text following Section 3.5 appears to be in italics. Additionally, the description of the experimental setup is somewhat lacking, particularly in terms of metric explanations, which are difficult to follow.",
            "- I highly suspect that the whole paper is generated with some LLMs. See the following comments.\n\n1. The presentation of the methodology is unclear and difficult to follow. After careful reading, the proposed methodology remains ambiguous and lacks coherence. The entirety of Section 3 feels like a collection of complex mathematical definitions that seem disconnected from one another, without a clear explanation of how they contribute to the overall methodology.\n2. The theoretical analysis is very weak and sometimes lacks clear definition. For example, in theorem 3.1 the final claim is that \"If $k$ is characteristic, then the mapping $Z \\to \\mu_Z$ is injective. However, commonly this claim itself serves as the definition of a kernel $k$ being characteristic. Similar situations happen at multiple other places, where the assumption is too strong to reduce the claim to some very obvious facts.\n3. The paper is not clearly finished at various places. For example, in line 351, 355, 365, 368, the equations or texts after the colon is missing. This makes it very hard to comprehend what the authors wanted to express.\n4. The experiment setting is very unclear, and I don't understand why all the experiment sections are italicized, making it very difficult to read. In line 428, the author states 'to evaluate the performance of IPPMF'. However, there is no mentioning of what 'IPPMF' is anywhere in the paper.",
            "- N/A"
        ]
    },
    "MGceYYNvXp": {
        "venue": "ICLR 2025",
        "title": "Project MPG: towards a generalized performance quotient for LLM intelligence",
        "link": "https://openreview.net/forum?id=MGceYYNvXp",
        "abstract": "There exists an extremely wide array of LLM benchmarking tasks, whereas oftentimes a single number is the most actionable for decision making, especially by non-experts. No such aggregation schema exists that is not Elo based, which could be costly or time consuming. Here we propose a method to aggregate performance across a general space of benchmarks, nicknamed Project \u201cMPG\u201d, here dubbed Model Performance and Goodness, in addition referencing a metric widely understood to be an important yet inaccurate and crude measure of car performance. Here, we create two numbers: an ``Goodness'' number (answer accuracy), and a \u201cFastness\u201d number (cost or QPS). We compare models against each other and present a ranking according to our general metric as well as subdomains. We find significant agreement between the raw pearson correlation of our scores and thosee of LMSys, even improving on the correlation of the MMLU leaderboard to LMSys.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            1,
            1
        ],
        "strengths": [
            "- - The paper is well-written and easy to follow\n- They get good correlations with LMSys (a difficult benchmark to re-produce due to online ranking LLM answers) for their benchmark evalution method, improving upon MMLU (however, see weaknesses as well).",
            "- Clear Motivation: The paper effectively highlights the gap in the benchmarking field, specifically the lack of an accessible, single aggregated metric that balances accuracy and computational efficiency. The intention of this benchmark is generally a good one.\nPractical Application: The target audience is well-defined, and the authors justify the metric's utility for smaller organizations and rapid internal testing. If done right, this could be a helpful resource.",
            "- The paper has a number of strengths. First, the paper tackles an important lacuna in LLM research - accuracy is being prioritised over efficiency. The community must move towards measurement frameworks that balance performance against the efficiency of models. A preference for performance incentivises developers to constantly scale up their models and continue to use a vast number of resources. Benchmarks and schema like MPG that also incentivise efficiency can help to kickstart progress towards efficient architectures. Second, the authors cover a number of state-of-the-art LLMs, including closed- and open-source models, which is commendable. Third, they situate their work within the context of a number of other approaches and include comparisons between them and Project MPG.",
            "- This paper's topic is of interest since the community is suffering from the lack of a golden standard to measure an LLM's true performance."
        ],
        "weaknesses": [
            "- - If I'm not mistaken, I believe the starting point for the paper and the solution are not consistent. Especially the part where, who will need such a unified score to make a selection for which LLM to use, and why they cannot simply use other benchmarks. If a developer wants to compare different in-house models (that are not available online), can't they simply use offline benchmarks that are strictly related to the metrics they care about? Why would they need a unified score that may include unrelated metrics (such as social sensitivity)?",
            "- There are actually many studies showing that MPG is not a good measure of gas consumption. See, for example, here:\nhttps://www.science.org/doi/full/10.1126/science.1154983\nhttps://journals.sagepub.com/doi/abs/10.1177/237946151500100109\n\nThe introduction already misses out on some related papers, such as:\ntinybenchmarks: https://arxiv.org/abs/2402.14992 -> one of the first papers attempting to reduce benchmark size\nmetabench: https://arxiv.org/abs/2407.12844 -> essentially a very similar effort, measuring abilities via one number but over many more LLMs and --from what I can tell-- more principled?\n\n\nFor the score aggregation, the tree is just assumed (not estimated or derived). It's like imposing structure from the beginning, while this structure could have also been estimated. One can also see later that the factuality and problem solving benchmarks formed a cluster.\n\"We measured an average Queries-per-Second (QPS) by simply timing the response rate of every\nprompt that was sent to the external servers for our specific benchmark questions.\" -> I think this might cuse all sorts of problems where time might not only reflect how long the query took. \n\nAll of page 6 is a heatmap and a table, it really feels like there just isn't enough here for an ICLR paper. I think this would be better as a workshop paper. \n\nIn the conclusion section, the project is --all of a sudden-- called IQ. I think this really seems to show that this has been a rushed submission. The first paragraph seems to be from an earlier version of the paper, whereas the second paragraph seems to be an exact repetition of what had been said before.",
            "- The paper contains many weaknesses. \n\nFirst, the presentation of the paper is poor. There are numerous spelling errors, syntactic disfluencies, and semantic ambiguities throughout. This makes it very difficult to follow the paper and discern the authors' methodology and the novel contributions of the paper. \n\nSecond, I find the presentation of the 'Fastness' number as problematic. It is defined as the average time taken to respond to every prompt sent to external servers for benchmark questions. This seems incredibly noisy as a measure because it is contingent on the connection speeds of the authors, the traffic on the various servers being used, and rate limits on the proprietary APIs. I am not sure how reproducible these numbers would be and whether the QPS values taken from RunPod instances of open-source models are commensurable with the QPS values from the different APIs (e.g., Anthropic, OpenAI, Google). The authors are right to remark that the numbers would be different and give different rankings depending on the benchmarks and the questions, but there are so many other variables to consider here too. In any case, I would have liked to see the QPS values being normalised by the number of tokens in the query. I think that would go some way to improving comparability between benchmarks and between models. It is also not clear which kind of averaging is used to compute QPS, but I would recommend the geometric rather than the arithmetic mean since these are rates.\n\nThird, the authors present their latent capability analysis as being in some sense novel. However, there is an extensive literature that proposes methods such as theirs, with added sophistication. In the cognitive sciences, the field of psychometrics has produced a number of latent capability estimation procedures, most notably, Item Response Theory. The proposal in this paper is also closely related to (Bayesian) Structural Equation Models which are used to infer latent abilities and cognitive capacities in social and developmental psychology and computational psychiatry. In recent years, these methodologies have been used and extended in the field of AI Evaluation, including for LLM capability evaluations. The authors should look at the papers listed at the end of this section. In particular, Burden et al. (2023), Kipnis et al. (2024), Polo et al. (2024) and Wang et al. (2023).\n\nFourth, the capability hierarchy in Figure 2 seems arbitrary. Finding names for the latent constructs is a common problem in latent factor estimation, but usually such hierarchies are accompanied by independent evidence for why certain test sets are deemed to probe certain capabilities and not others. Understandably, the MPG project and methodology extends beyond this exact capability hierarchy, but since it is in the paper, it needs some motivation.\n\nFifth, the methodology for latent capability estimation is difficult to follow. Is this a hierarchical Bayesian network? Is it a structural equation model with priors? What does it mean when the authors say 'we propose a form of a Monte-Carlo Markov chain (MCMC) to simulate latent questions from the aggregate beta distributions.' It is unclear what latent questions are.\n\nFinally, the paper seems to end by ignoring the QPS measure and focus mostly on the latent capability measure and how well it recovers LMSys scores, etc. Much more time ought to be spent on the idea of a Pareto frontier between the two components of MPG and determining how we might create a balanced metric between these two facets. This is all left unsaid. \n\nReferences\n\nBurden, J., Voudouris, K., Burnell, R., Rutar, D., Cheke, L., & Hern\u00e1ndez-Orallo, J. (2023). Inferring Capabilities from Task Performance with Bayesian Triangulation. arXiv preprint arXiv:2309.11975.\nWang, X., Jiang, L., Hernandez-Orallo, J., Stillwell, D., Sun, L., Luo, F., & Xie, X. (2023). Evaluating general-purpose ai with psychometrics. arXiv preprint arXiv:2310.16379.\nHern\u00e1ndez-Orallo, J. (2017). Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement. Artificial Intelligence Review, 48, 397-447.\nHern\u00e1ndez-Orallo, J. (2017). The measure of all minds: evaluating natural and artificial intelligence. Cambridge University Press.\nKipnis, A., Voudouris, K., Buschoff, L. M. S., & Schulz, E. (2024). metabench--A Sparse Benchmark to Measure General Ability in Large Language Models. arXiv preprint arXiv:2407.12844.\nPolo, F. M., Weber, L., Choshen, L., Sun, Y., Xu, G., & Yurochkin, M. (2024). tinyBenchmarks: evaluating LLMs with fewer examples. arXiv preprint arXiv:2402.14992.\nPellert, M., Lechner, C. M., Wagner, C., Rammstedt, B., & Strohmaier, M. (2023). AI Psychometrics: Using psychometric inventories to obtain psychological profiles of large language models. OSF preprint.\nYe, H., Xie, Y., Ren, Y., Fang, H., Zhang, X., & Song, G. (2024). Measuring Human and AI Values based on Generative Psychometrics with Large Language Models. arXiv preprint arXiv:2409.12106.",
            "- This paper appears to lack a solid foundation in terms of both theoretical motivation and empirical rigor. The idea seems to be introduced without substantial grounding, and the experimental methodology appears somewhat ad-hoc, merely applying the approach to several well-known LLMs without providing accompanying code, open-source datasets, or a thorough explanation of the measurement's validity. Moreover, the vague correlation presented in Figure 3, which is the main result, leaves the effectiveness of the measurement uncertain. Based on these limitations, I do not recommend accepting this paper."
        ]
    },
    "LFn7s8yRUF": {
        "venue": "ICLR 2025",
        "title": "EXPLORING THE IMPACT OF DATA AUGMENTATION ON LOCALIZED PERSONALIZED AI TRAINING WITH LLAMA3 AND LORA",
        "link": "https://openreview.net/forum?id=LFn7s8yRUF",
        "abstract": "With the development of personalized AI models, particularly those emulating characters from novels, games, anime, and films, a significant challenge is the scarcity of suitable dialogue data. These works often feature distinctive styles and character dialogues that may not generalize well to everyday conversations. Data augmentation is crucial for enriching these limited datasets, ensuring sufficient data for learning the target character\u2019s tone and linguistic habits. This paper investigates the impact of various data augmentation techniques on personalized AI models in NLP, specifically focusing on models trained using LLaMA3 through Low-Rank Adaptation (LoRA). We employ different data augmentation strategies, including random deletion, synonym replacement, swapping, random insertion, back translation, and paraphrasing. To provide a comprehensive analysis, we apply these techniques across three distinct datasets, each representing different dialogue styles and contexts. By systematically comparing these methods, we demonstrate their influence on model performance and robustness. This study provides valuable insights into the effectiveness of different data augmentation strategies for enhancing the versatility and robustness of personalized AI systems trained with LLaMA3 using LoRA.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            1,
            1
        ],
        "strengths": [
            "- # Originality\n\nThe proposed approach consists of low-rank fine-tuning applied on a training dialogue dataset with data augmentation. The approach in itself is not particularly novel, and its presentation would benefit from a more thorough literature review.\n\nThe introduced dataset is novel, and its elaboration would benefit from a more detailed presentation.\n\n# Quality\n\nOnly one set of experiments has been run, using one available open-weight model and the dataset that the paper introduces. This is insufficient to get to sound conclusions as to which data augmentation techniques are better in general for the task of dialogue modeling.\n\n# Clarity\n\nThe paper is easy to follow, but misses key details in how the dataset has been constructed: the paper should mention where the data comes from, how it was processed, what are the key characteristics (# of episodes, of tokens, of characters...), how the training/validation split was done...\n\n# Significance\n\nThe understanding of which data augmentation techniques are most relevant in order to adapt LLMs to small datasets is of prime importance for the field. However, it seems that the proposed method and the results do not give very strong indications of winning strategies.",
            "- 1. **Clear and Relevant Problem Statement**: The scarcity of personalized dialogue data is an issue of practical importance for character-based AI models. The study addresses this with a focus on data augmentation, an area less explored in NLP.\n2. **Multiple Metrics Used**: The paper evaluates on both training and validation loss, as well as BLEU and ROUGE scores, providing valuable insights for optimizing dialogue data augmentation.\n3. **Resource-Conscious Approach**: The authors' choice of LoRA for fine-tuning and Unsloth for training efficiency is thoughtful and appropriate for low-resource environments, demonstrating an understanding of practical constraints in the field.",
            "- None",
            "- - At present, the paper does not offer any clear value to the community. The problem of data gathering and evaluating LLMs ability to roleplay is an important challenge, but the present work fails to engage with the broader literature both on these particular models and more specifically in this research space nor to offer carefully controlled experiments which answer questions from the wider field."
        ],
        "weaknesses": [
            "- Regarding the proposed dataset:\n* It is not clear where the training data comes from and how it was curated\n* It is not clear how the proposed dataset relates to existing work, and which specific problems it aims at solving\n* There is no measurement of training data leakage. Given their popularity, it seems likely that these TV shows are quite prevalent in LLM training data\n\nRegarding the fine-tuning results:\n* There is no comparison with a baseline that does not use data augmentation\n* There is no comparison to any few-shot prompting scenario\n* It is not clear what the fine-tuning for a specific character consists of: how is the autoregressive loss computed when multiple characters are talking?\n\nRegarding the paper presentation:\n* The paper spends several pages presenting topics that are not directly relevant for the subject at hand. While understanding low-rank fine-tuning is a topic of importance, it is not particularly relevant to spend 1 page on it, at the expense of experimental details and results.\n* The abstract mentions 3 different datasets, but the results cover only two of them.",
            "- 1. **Incremental Contributions**: While the topic of data augmentation in NLP is valuable, the paper\u2019s contributions appear largely incremental. Many of the techniques explored (e.g., synonym replacement, backtranslation) are well-known and widely used. The paper could benefit from exploring more novel augmentation techniques tailored to character-based dialogue data's nuances. Additionally, *about half of the paper is describing past techniques* like EDA, Paraphrasing, LoRA, etc (specifically, Section 2, and 3), and there is nearly not enough content for this paper to be considered an impactful contribution (figure 3 has too little information to be taking a whole page and Table 1 font seems overtly big as if just to fill some space). \n2. **Limited Dataset Scope**: Only two datasets are used, both of which are highly specific in context (e.g., Paimon from *Genshin Impact*). This narrow focus raises questions about the generalizability of the findings to other personalized dialogue systems or broader NLP applications. Including a wider variety of datasets or character types would add robustness to the conclusions.\n3. **Paraphrasing Overfitting Concerns**: The paper notes overfitting issues with the paraphrasing technique but does not fully explore ways to mitigate this. Although the authors acknowledge the limitations of paraphrasing with large models in domain-specific contexts, a more thorough investigation into alternative paraphrasing techniques or domain adaptation methods would strengthen the study.\n4. **Issues With the References**: a) There are some invalid entries in the references, such as, \"Unknown Author\" and b) The reference list is too short. I suggest the authors to conduct a more thorough review of the current literature.\n\n### Suggestions\n\n1. **Broaden Dataset Variety**: I think the authors are already aware of this problem, but including datasets that represent a wider range of conversational styles or character personalities would improve the generalizability of the results. Alternatively, conducting a detailed analysis of *why* certain augmentation techniques are particularly suited or unsuited to the specific characteristics of these datasets would be more interesting.\n2. **Investigate Advanced Augmentation Techniques**: Consider exploring novel or advanced data augmentation techniques that could better align with the semantic and contextual requirements of character dialogue, such as sentiment-aware or context-sensitive paraphrasing. Additionally, using domain-adaptive techniques for paraphrasing may mitigate overfitting while still generating diverse examples. There is also a huge body of work on using off-the-shelf LLMs for data augmentation that the authors should explore.\n3. **Refine Analysis of Computational Constraints**: Given the choice to work with the smaller LLaMA3-8B model, it would be beneficial to address the specific trade-offs involved more explicitly. This discussion could include any adjustments made to balance memory efficiency with model performance and how this may limit model generalization capabilities.\n4. **Improve Paper Organization**: We don't need to describe past methods like EDA, LoRA, etc. in so much detail. Instead, I would suggest focusing on including more strong baselines (like LLM-based data augmentation).",
            "- - Overall lack of clarity. It is unclear to me what is the major claim of the paper. The paper mention several techniques on data augmentation methods, but none of the it's explained in details nor formally. The results are also mysterious, leaving out most of the details. \n- The results on the paper are non-sensical, the plots are meaningless, showing no particular trends. \n- The paper contains screenshots from other papers (llama3), without a clear citations.",
            "- - The methods studied don't address the core challenges the authors themselves present. While the authors highlight issues that most training data used for this task occurs in a particular thematic framework as a limitation, the data augmentations they use, which are all methods which have been carefully studied in existing work but are not attributed, only perform local word and structure level permutations on data.\n- The paper, as written now, spends much more time explaining (somewhat tangential) background than seems necessary. The concrete experimental setup doesn't begin until almost halfway through the work.\n\\\n\\\nFor example, the section on Unsloth in section 2.4 and the inclusion of external model evaluations on benchmarks that are unrelated to personalization in *both* Figure and Table 1 have little to do with the core research questions. These sections distract from the narrative of the paper and should be moved to the appendix to make room for more substantive experiments on the primary topic.\n- The work primarily presents a study on just two characters from role-playing that the paper both introduces itself and compares minimally to any external baselines. It is poorly contextualized in the field without evaluations on datasets from the broader community or comparisons to even simple baselines like prompting the model to roleplay.\n- There are many signs that the work was minimally refined before submission. Despite having only 11 citations , in these minimal citations there are obvious errors such as listing \"Journal Name\" as the publication venue and \"Unknown Author et al\" as the authors (the work in reference has both publicly known authors and a venue of publication: https://openreview.net/forum?id=rc2h1h89aDi). The figures have mislabeled legends (in figure 3 `BT Loss` is repeated twice) and there are typos where content is duplicated at the start of paragraphs (Line 79: \"Wei and Zou (2019) Wei & Zou (2019) introduced\")"
        ]
    },
    "L3tW9nbcEM": {
        "venue": "ICLR 2025",
        "title": "Schrodinger's Memory: Large Language Models",
        "link": "https://openreview.net/forum?id=L3tW9nbcEM",
        "abstract": "Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current practice, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research lacked a deep exploration of LLMs' memory capabilities and the underlying theory. In this paper, we use the Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on the memory ability. We argue that LLM memory operates like Schr\u00f6dinger's memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            1,
            3
        ],
        "strengths": [
            "- The paper addresses an important topic. It is crucial that we study in detail the similarities and differences in memory processes between humans and LLMs, as well as devote plenty of effort do understand how memory manifests in transformer architectures.\n\nThe paper is relatively clearly written.\n\nThe work has both a theoretical and a practical component. It is always commendable when practical experiments are informed by theory.",
            "- The general idea of connecting deep theory about functions to concrete Transformer-based LLMs (for example, via the Universal Approximation Theorem) is interesting and promising.",
            "- The paper investigates memory in LLMs, which is timely given the current explosion of research on LMs, and is thematically aligned with ICLR.\n\nThe decision to test memorization on poetry is a step in the right direction.",
            "- The paper tries to address an important and timely question about the memory mechanisms in LLMs;  \nThe choice of structured datasets (particularly poetry) is suited for testing memory recall in LLMs and evaluating the relationship between model architecture and memory capacity;  \nThe attempt to connect UAT with LLMs memory is interesting and provides a mathematical basis for discussing dynamic response mechanisms in LLMs;  \nThe comparison between human and LLM memory provides insights that could bridge cognitive science and AI."
        ],
        "weaknesses": [
            "- ** Novelty **\nUnfortunately, the paper is not sufficiently novel to pass the high standards of the ICLR conference. The main empirical result in the paper is a rediscovery of a well-known phenomenon of overfitting. As is well-known, LLMs do indeed possess a remarkable ability for verbatim memorization of items in the training set.\n\n**Quality**\nUnfortunately, the quality of the literature review or the theoretical justification of the proposed work is not sufficient to pass the high standards of the ICLR conference. The authors pose a number of rhetorical questions, such as \"So, is this sentence stored within a single neuron?\" entirely ignoring decades of memory research in Psychology, Cognitive Science, and Neuroscience. It is also staggering to see them claim 'in summary, the term \u201dmemory\u201d was traditionally used to refer specifically to human memory before the emergence of LLMs'. This entirely ignores the research on memory in animals and even plants, not to mention the related concepts of the memory of materials, cultural memory, and a plethora of other related concepts.\n\nI find it commendable that the authors ask ambitious, fundamental questions, such as \"moreover, if this memory is stored in a fixed set of neurons, then every time the question is raised, the response should be identical, since the retrieval would be from the same static content.\" Unfortunately, however, such questions have been extensively studied not even for centuries, but for millennia. If these questions are truly of interest, I suggest starting with the works of Pavlov, Skinner, and Tolman to gain a historical perspective on how this and related questions have been approached by the scientific community.",
            "- Unfortunately, the article suffers from several shortcomings that I will point out section-wise:\n\n\nIntroduction: \n\nOverall, the main topic of the article should be clarified. There needs to be a proper section on related work to outline and delineate current research on this topic so that readers know what the state-of-the-art is and why this paper's contributions are contributions in the first place. This would also help readers unfamiliar with the topic to navigate the article more efficiently. Also, a large portion of the article is concerned with the Transformer architecture, but the original paper is not cited.\n\n\n\nUAT and LLMs:\n\nOverall, the formulation of the theoretical background severely lacks precision and formalism. A significant portion of the notation has not been defined (for example, $C(I_n)$) before using it. Furthermore, the authors write (starting in line 095/096):\n \n\u201c[...] then a finite sum of the following form: [...] is dense in $C(I_n)$\u201d\n\nHowever, \"a finite sum\" can not be dense in the set of continuous functions on the hypercube (which is meant by $C(I_n)$). The authors then continue to reference the article by Wang & Li (2024b), stating (line 131):\n\n\"[...] parameters in the multi-head attention mechanism are modified dynamically in response to the input.\"\n\nIt is unclear whether this refers to the forward or backward process. Overall, the entire section is very confusing, with statements like (starting in line 137):\n\n\"[...] the UAT's parameters are fixed once training is completed, [...]\"\n\n\"UAT\" stands for Universal Approximation Theorem - what is meant by parameters?\n\nA large portion of section 2.1 is very similar to the beginning of Section 2 of the paper by Wang & Li (2024b) (who also do not cite the original transformer paper). As an example, the error in line 102 is the same ($\\theta \\in \\mathbb{R}$ needs to be $\\theta_j \\in \\mathbb{R}$).  \n\nFinally, the reference \"(Cybenko, 2007)\" refers to the article published in 1989 (the same year Hornik et al. published their paper) - is this reference incorrect?\n\n\n\nThe Memory of LLMs:\n\nThis section discusses memory for humans and LLMs and introduces the authors' definition of memory. They criticise other works for the lack of a fundamental theoretical framework and vague definitions of memory (see line 059 and following), but the definition presented in this work seems no different in these regards. The authors should tie their definition to the introduced theory in the previous section and make the formulation more rigorous.\n\nOverall, there is no cited work (apart from the Wikipedia definition) in Section 3.1, which seems more like a blog post than an academic article. This leaves statements like (line 168)\n\n\"The brain does not have a structure analogous to a database for storing information.\"\n\nunfounded. \n\nThe datasets are attributed to \"Unknown\" - although the Huggingface user accounts are available via the provided links.\n\nThe authors calculate the mean average accuracy to evaluate the models' ability to recall poems. However, it is unclear when a poem counts as predicted correctly. Did the authors employ plain string matching? If so, how are newline characters and translations by spaces handled? \n\nFurthermore, most training details are missing. For example, what were the learning rate and batch size? Based on the provided information, no experiments are reproducible (no code is available).\n\nRegarding Table 1: What do the hyphens stand for?\n\nThe authors also claim in line 253/254 that\u2014based on the results in Table 1\u2014\" LLMs possess memory capabilities, which align precisely with the definition of memory we established.\"\n\nIn my view, the authors seem to reduce memory to a capability every overfitted LLM can develop: reproducing tokens in order. Here, I use the term \"overfitted\" as training for 100 epochs on such a comparatively small corpus of text (2000 poems) seems excessive.\n\nFinally, given the same input, if the LLM could reproduce the poems in reversed order, starting with the last token (or character) and ending with the first token (or character) - would this count as memory according to the provided definition and would the metric in Equation (4) reflect this?\n\n\n\nA Comparison Between Human Brains and LLMs:\n\nSimilar to Section 3.1, the discussion about similarities and differences between human memory and memory in LLMs at the beginning of the fourth section lacks academic references and empirical evidence to support claims such as (line 373/374)\n\n\u201cThese poems are not stored in specific areas within the model; they are dynamically generated based on input.\u201d\n\nand (line 395)\n\n\u201cAlthough the predictions in Figure 3 are incorrect, they still align with linguistic conventions and somewhat correspond to the titles of the poems. This can be seen as creativity.\u201d\n\nSome related articles discuss the case of Henry Molaison, which led to the authors hypothesising about similar dynamics for LLMs, but these are likewise unfounded. In particular, I do not see sufficient evidence for the third contribution mentioned in the introduction.  \n\nThe authors likewise state in line 082 that they\n\n\u201c[...] conduct a comprehensive analysis of human and LLM abilities, with a focus on memory ability.\u201d, \n\nwhich I also do not see provided.\n\n\n\nOverall, the article severely lacks formalism, experimental details and references/experiments to support the author's claims. The introduced definition of memory needs to be improved, that is, sharpened, theoretically motivated and empirically justified. \n\n\n\n\n\n\n\nMinor: \n\nGrammar and spelling mistakes need to be corrected, for example:\n\nLine 137: \u201cThis ability enables the Transformer to adaptively fit based on the input [...]\u201d\n\nLine 219: \u201cWe select the poems from datasets and the requirement is the combined length of the input and output to a maximum of 256 characters.\u201d\n\nLine 241/242: \u201c[...] are the prediction and ground true of the i-th exmple.\u201d\n\nLine 351/352: \u201cAfter fine-tuning the model for 100 epochs on CN Poems, the results are shown in Table 2.\u201d\n\nLine 406/407: \u201cThe larger and higher the quality of dataset, the stronger [...]\u201d",
            "- While the experimental work is a step in the right direction, I do not believe the paper in its current form would be a good fit for ICLR.\n\n\n1. The authors aim to provide a precise definition of memory, yet the definition they provide is informal, and the only other definition they compare to is one from Wikipedia (ignoring the vast literature on, e.g., dense associate memory, episodic vs working memory etc).\n\t1. While the goal of Section 3.1 is to define memory, the definition provided is imprecise. For instance, is memory a function from inputs to outputs? Is it a function from (inputs, outputs) to a truth value? The authors go on to use next-token prediction accuracy as their metric for memory in Section 4. I suggest the authors to formally define memory in the next version of the manuscript. \n\n2. The work presented is quite speculative, and the tone of writing imo would be better aligned to a non-ML conference.\n\t1. Section 3, especially 3.1, reads to be long-winded and imprecise-- while it tries to define memory, it instead provides many examples without a formal definition of memory.\n\t2. Section 4, which compares brains to LMs, does not cite the relevant literature on neuroscience (except for one case study in l427). \n\t3. l424 \"Each update may be right or wrong, but with a vast number of humans exploring the world, we gradually inch closer to the truth, ultimately leading to innovation.\" This line (and similar ones) should ideally be toned down or omitted for a machine learning conference.\n\n3. The work does not engage with the vast literature on memory in neuroscience, cognitive science and machine learning. Instead, it overinterprets and over-draws parallels between neural networks and human cognition. I'm listing several examples here:\n\t1. l269 The connection to human cognition requires a citation.\n\t2. l421: It could be viewed as the weight parameters in our brains are randomly initialized... \n\t3. Table l207-212: Answer 2,3,4 -> Answer 1, 2, 3. I disagree that one would conclude \"minor distortions, severe distortions, and memory loss\" from these examples. If the answer is \"I do not know\", then it could be that the person never learned Newton's first law.\n\n4. Methodology\n\t1. The selected poems are common ones likely found in training data, especially for the Chinese language models. Therefore, it is not guaranteed that each poem is trained on the same number of times (t=100 epochs). How do you disentangle the effects from pre-training on these poems?\n\t2. The memorization metric based on accuracy is not sufficiently different from Carlini et al 2022, which defines memorization as $k$-extractability, $k$ being the number of tokens in the input prompt. \n\t3. The connection between experiments and UAT is never made.\n\n5. Several unsubstantiated claims about linguistics\n\t1. l271 \"Chinese is a more complex language\" is vague-- I would remove this sentence altogether or specify exactly which components of Chinese are more complex than English with citations from the linguistic typology literature.\n\t2. l214-215 is rather abrupt and would go better in the introduction of the section. The second sentence \"Now, we believe that LLMs also exhibit memory\"-- needs citation\n\n6. Lack of engagement with the UAT / LLMs literature [1-3], which show that Transformers, in the general case, are not universal approximators-- only under certain conditions.\n\n[1] Alberti et al. 2023. Sumformer: Universal Approximation for Efficient Transformers\nhttps://proceedings.mlr.press/v221/alberti23a.html\n\n[2] Kratsios et al. 2022. Universal Approximation Under Constraints is Possible with Transformers\u00a0\nhttps://openreview.net/forum?id=JGO8CvG5S9\n\n[3] Luo et al. 2022. Your Transformer May Not be as Powerful as You Expect\n(RPE based transformers aren't Universal Approximators)",
            "- 1.The paper claims to use UAT to explain LLMs' memory abilities, but the connection between Eq. (3) and the memory mechanism is not rigorously established. The authors need to prove how the dynamic fitting capability directly relates to memory retention. This connection might be strengthened by providing a step-by-step derivation linking Eq. (3) to specific memory processes, or designing an experiment that directly tests the relationship between dynamic fitting and memory retention.  \n2.While the paper develops an extensive theoretical framework using UAT in Section 2, the experiments in Section 3 do not directly validate or connect to this theory. This gap might be addressed by quantitatively relate the parameters in Eq. (3) to the observed memory performance in the experiments.  \n3.Section 3.3 proposes using memory ability as an objective measure of LLM capabilities but does not relate this metric to standard benchmarks. The authors should demonstrate how their memory assessment correlates with or complements existing evaluation methods.  \n4.The output length effect analysis in Section 3.4 only tests up to 512 characters. The authors should investigate how memory performance degrades with sequence length and compare this with theoretical predictions from their UAT framework.   \n5.The paper fails to examine an essential aspect of memory systems - their ability to recognize unknown information. Specifically, there are no experiments testing whether models can reliably indicate when they encounter previously unseen poems, nor is there a comparison between \"memorized\" vs. \"hallucinated\" outputs' characteristics. The paper could benefit from adding specific experimental designs to test the models' ability to distinguish between known and unknown information. This might be achieved by including a set of previously unseen poems in the test set, or evaluating how the models distinguish between known and unknown information.  \n6.The experiments only measure binary success (correct/incorrect recitation) without examining the deviations as metioned in Section 3.1.  \n7.The comparison between human and LLM memory is largely speculative and lacks scientific rigor. Many claims about human memory mechanisms are made without proper citations or empirical support. This could be improved by designing experiments to more rigorously compare human and LLM memory, perhaps drawing on established methods from cognitive psychology."
        ]
    },
    "gbJNFxcicC": {
        "venue": "ICLR 2025",
        "title": "Mask R-CNN for Automated Multi-Species Malaria Parasite Detection",
        "link": "https://openreview.net/forum?id=gbJNFxcicC",
        "abstract": "This study investigates the automatic detection and segmentation of malaria parasites across various Plasmodium species using Mask R-CNN, an advanced deep-learning architecture. Expanding on earlier studies in digital malaria diagnosis, we apply pixel-level segmentation to overcome the drawbacks of previous approaches. 971 microscopic pictures of four Plasmodium species\u2014P. falciparum, P. malariae, P. ovale, and P. vivax\u2014taken from Rwanda's healthcare facilities make up our dataset. This dataset was used to train the Mask R-CNN model, which produced excellent mean average precision (mAP) scores for all species, with P. vivax and P. malariae showing the most excellent performance with mAP 0.9575 and mAP 0.9459, respectively. Compared to earlier techniques, this method shows notable advances in parasite localization and delineation, suggesting the possibility of more precise and effective malaria diagnosis in clinical settings.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1
        ],
        "strengths": [
            "- Easy to read paper, tackling a relevant societal problem.",
            "- The dataset gathered and annotated is quite large for a medical dataset, which is excellent work and sure to be useful to those who can make use of it in the future. Although I assume it will not be possible to make the dataset open due to privacy concerns. \nFurthermore, detection results appear to be good, i.e. the mAP score is decent.",
            "- The application of Mask RCNN may be new."
        ],
        "weaknesses": [
            "- The paper lacks sufficient novelty. It is a straightforward application of mask R-CNN to malaria parasite detection without any adaptation of the methodology.\n\nAddtionally, and although the authors state \"... this method shows notable advances...\", there is no empirical comparison with SOTA or baseline methods, no ablation  studies.\n\nThe dataset is also not properly described, not being clear about the difficulty of the task (for instance, class distribution, distribution of the number of parasites per image, etc.).",
            "- There is too little novelty to be had. The authors are essentially applying an existing model to a new dataset and obtaining good results. Applying existing models to their own problems is what good engineers do all around the world, all the time. While the paper's results may be valuable for people in the medical domain interested in the state-of-the-art for malaria parasite detection, I do not believe it is of interest to the machine learning community at large. \nFurthermore, it's even difficult to place the results as state-of-the-art, because no comparisons are made with previous models. Some are touched upon in the literature review section, but in terms of quantifiable results, we only have test and validation mAP for Mask-RCNN. How can we know that Mask-RCNN performed better than the other approaches with only this information? The verbal claims need to be substantiated with further evidence. \nThere is also no interesting domain-specific analysis which could give us insights into why the dataset is interesting, or why other models struggled while Mask-RCNN was good. Or even where Mask-RCNN seemed to be failing. We only find nuggets of common knowledge which suggest that the model performs \"very well\" due to reasons such as it being \"end-to-end\", and among possible improvements are gathering a larger dataset and training with a more powerful GPU.",
            "- The is just like a project report, not ready for conferences like ICLR.\n\nMask R-CNN is 7 years old, there is no novelty of the work in terms of methodology.\n\nThe introduction is very simple, CNN and R_CNN etc are known by default and there is no need to , it seems they don't have the knowledge of recent advances of AI,  e.g. YOLOv5 is outdated.\n\nThere are no comparisons with other detection/segmentation method, and no ablation studies."
        ]
    },
    "cya3eEczAx": {
        "venue": "ICLR 2025",
        "title": "Adaptive Proximal Gradient Optimizer: Addressing Gradient Inexactness in Predict+Optimize Framework",
        "link": "https://openreview.net/forum?id=cya3eEczAx",
        "abstract": "To achieve end-to-end optimization in the Predict+Optimize (P+O) framework, efforts have been focused on constructing surrogate loss functions to replace the non-differentiable decision regret. \nWhile these surrogate functions are effective in forwarding training, the backpropagation of the gradient introduces a significant but unexplored problem: the inexactness of the surrogate gradient, which often destabilizes the training process. To address this challenge, we propose the Adaptive Proximal Gradient Optimizer (AProx), the first gradient descent optimizer designed to handle the inexactness of surrogate gradient backpropagation within the P+O framework. \nInstead of explicitly solving proximal operations, AProx uses subgradients to approximate the proximal operator, simplifying the computational complexity and making proximal gradient descent feasible within the P+O framework. We prove that the surrogate gradients of three major types of surrogate functions are subgradients, allowing efficient application of AProx to end-to-end optimization.\nAdditionally, AProx introduces momentum and novel strategies for adaptive weight decay and parameter smoothing, which together enhance both training stability and convergence speed.\nThrough experiments on several classical combinatorial optimization benchmarks using different surrogate functions, AProx demonstrates superior performance in stabilizing the training process and reducing the optimality gap under predicted parameters.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1
        ],
        "strengths": [
            "- - The topic of the paper is interesting.\n- The paper provides a review of related works.",
            "- 1) The proposed work is interesting and aims to tackle a well-known issue arising in the non-differentiability of the loss function in Predict & Optimize.\n2) The numerical experiments show promising results for the proposed approach. \n3) The introduction and related works are well-written.",
            "- The paper tries to tackle an important problem in the popular P+O framework. Proving convergence is not straightforward, even in a simple setting with exact gradients."
        ],
        "weaknesses": [
            "- There is a major error in the proof of theorem 1: on line 772 a lower bound is incorrectly combined with an upper bound. Since this is the basis for the main convergence result, this error compromises the theoretical results presented.  \n\nI believe there is another error in the proof of corollary 2 (883-886). The sequence $||d_k|| $ need not converge to zero. For example, consider, $||d_k|| = (\\delta\\eta)/c$ which satisfies (44) but clearly does not converge to zero.\n\nI also believe that there are major flaws in the specification of Algorithm 1. For example on line 3 the $ \\hat{c_k} = \\hat{c}(\\theta_k) $, while in line 10 $ {\\hat{c}}_{k+1} $ is computed as an update sequence based on the gradient. On line 12 a set of smoothed $ \\tilde{\\hat{c}}_k $ are computed but not utilised (so far as I can tell). \n\nAnother concern I have with is the smoothing function selected by the authors. To compute the gradient of $f(\\hat{c}) = \\frac{1}{2} || \\hat{c} - c||^2$ with respect to $\\hat{c}$ requires knowledge of the \"true cost parameters\", which precludes practical implementation. \n\nAdditionally, paper is significantly hindered by the quality of the writing with many awkward and confusing sentences, confusing notation and typos. The paper is difficult to follow due to theses issues. For example, in section 2.1 equation (3) is stated with no relation to the previous paragraph and (4) is stated with no discussion. There are issues like this in almost every section of the paper.",
            "- Major Comments:\n\n1) The proof of the main theorem is incorrect. The inequality in Line 772 does not necessarily hold as a result of Line 765. The authors should revisit the proof and correct these details. \n2) The paper claims that this paper uses a proximal update. However, the update is given by $\\hat{c}_{k+1} = \\hat{c}_k - \\eta (\\nabla f(\\hat{c}) - g(\\hat{c}))$, where  $g$ is an inexact gradient estimate of the non-smooth loss/regret term. The authors claim this is \"implicitly a proximal update\" in line 188, but this resembles a subgradient descent instead. It would benefit the paper if the authors could further elaborate on how this update relates to or a proximal update, or revise their claims if they cannot justify this connection.\n\n3) The main theorem relies on $R(\\hat{c}) = c^\\top(z^\\star(\\hat{c}) - z^\\star(c))$ being convex in $\\hat{c}$. However, it is not obvious that the regret is convex. This paper would benefit if it either provides a proof of convexity for the regret function, or discuss the implications if this assumption does not hold and how it might affect the validity of the results.\n\nMinor Comments:\n\n1) The authors should update their references. For example, \"Differentiation of Blackbox Combinatorial Solvers\" is not cited properly, as it is already a published article. \n2) The paper uses $\\nabla R$ and $g$ interchangeably. However, $\\nabla R$ is the true gradient of the regret (and assumes $R$ is differentiable), whereas $g$ is an approximation. The authors should update this in, e.g., Line 5 of the pseudocode and in line 259.  \n3) Line 052: \"non-differentiate\" -> \"non-differentiable\"\n4) Line 068: \"gradient inexact\" -> \"inexact gradients\"\n5) $R(\\hat{c})$ is never explicitly written. It would make the paper more readable if the authors explicitly defined it in Section 2\n6) Line 161: \"introduces\" -> \"introduced\"\n7) Line 233 \"approachfocuses\" -> \"approach focuses\"",
            "- The paper actually does not contain what promises. It seems that it actually only adds a squared loss on costs (prediction parameters) to an existing surrogate (or, equivalently, adds a gradient of squared loss to an existing surrogate gradient) in an obfuscated way and then basically uses a custom version of Adam optimizer and weight decay.\n\nSpecifically, It recalls the proximal gradient descent (Eq 8) but does not use it (\"In practice, computing the proximal operator $prox_{\\eta R}$ can be impractical in P+O problems. Instead, we utilize the existing surrogate gradient...\"). Equation 9 then reveals that this surrogate with the gradient of $\\ell^2$ norm is used.\n\nNext, Theorem 1 is not proven correctly (and probably does not hold in this form):\n- Equation 13 in the statement contains constant $\\delta$, which is not quantified and is mentioned only in Eq. 5 in section 2.2. Here it requires that the bound is uniform in $\\hat c$. It is mentioned that it is non-negligible. Indeed, the true gradient is always zero or undefined (since it is the gradient of the solution of an LP, i.e., of a piecewise constant function). Therefore $\\delta=\\sup_{\\hat c}\\|g(\\hat c)\\|$, where $g(\\hat c)$ is the surrogate gradient.\n- It is not clear, what $d_k$ is, since $\\nabla R(\\hat c_k)$ is not uniquely defined.\n- l.834: The inequality does not hold (for instance, take $\\eta$ close to $1/L$, then LHS is close to zero, but RHS is negative (close to $-\\delta/L\\|d_k\\|$))\n- L836: 'Higher order terms' $L\\eta^2\\delta^2$ cannot just 'be neglected' as there is no limit taken. Also, here it is incorrectly assumed that $\\delta$ 'is small.'\n\nThe proof of Corollary 2 is wrong.\n- Equation 44 does not imply that the sum converges. Consider for instance the sums $\\sum_{k=1}^N 1/k -\\sum_{k=1}^N 1/\\sqrt k\\le 0$, they both diverge.\n(I skipped reading the proof of the next corollary)\n\nThe paper is full of incorrect or inexact statements:\n- l.67 \"The problem of gradient inexact caused by the agent function for P+O under the end-to-end framework has not been emphasized, and research is lacking.\" The above-mentioned papers are devoted to exactly this.\n- l.36 \"end-to-end approaches are also an emerging topic in the decision-making process.\"\n2. Inexact Gradient Challenge in P+O Framework\n- \u201cThe existence of errorbound can mislead the direction of descent, which will eventually lead to the problem of unstable or non-convergence of the training process.\u201d The true gradient is always zero (or nonexistent); hence any informative descent direction will actually increase this delta. Therefore, there is no connection between this delta and some unstability or non-convergence (or maybe it is completely opposite, that for convergence, it is required delta to be large.)\n3.  Adaptive Proximal Gradient Optimizer (AProx)\n- l.165 \"$R(\\hat c)$ is not trivial\". What does it mean?\n- l.172 \"The number 1/2 as coefficients of $f(\\hat c)=\\tfrac12|\\hat c-c|^2$ is to avoid its excessive influence on the  gradient of the composite function.\" No, it is to avoid unnecessary constant 2 in the proximal gradient step.\n- l.188 \"This approach effectively integrates the proximal operator implicitly and allows us to proceed without its explicit computation.\" No. it just ignores the proximal map completely.\n4. Theoretical Convergence Analysis\n- l.240 \"It is worth noting that Lemma 2 rests on the fact that R(\u00b7) is a convex function. In the solution approaches of P+O, most of the constructed surrogate functions can satisfy convexity.\" It is not true that \"CMAP...involve convex functions\" does not imply it is convex. \"DBB uses linear interpolations...\" (this is correct) but does not ensure that the result is convex (which is not, in general), similarly for IMLE.\n\nIt uses nonstandard or misleading terminology:\n- l.59 \"agent function\" and \"agent gradient\" for surrogate loss/gradient.\n- l.64 \"discovergence\"\n- It often uses the term (and notation) 'gradient' for objects that are not, in fact, real gradients but surrogate ascent direction.\n- \"training rounds\" (l.352 ), \"step size\" (l.367) and \"calendar hours\" (l.506) are used instead of epochs\n- l. 80 \"We ... give an inference on the rate of descent.\"\n- l.74 \"We propose the inexact surrogate gradient problem\"\n- l.77 \"the optimizer, which is improved on the proximal gradient.\"\n- l.156 \"to address the inexact gradient challenge in Predict+Optimize (P+O) challenge\"\n- l.170 \"we used the l2 paradigm term for the prediction error\"\n\nExperiments\n- I am not able to understand the setting of the benchmarks in Tables 1, 3 and 5. It is claimed that \"Table 1 shows the step size and training time per epoch required for convergence when training with several different regrets.\" I guess that \"regrets\" means \"surrogates\" and refers to one of the methods of IMLE, NID, CMAP, SPO, or DBB. However, it is not described how the statistics were calculated. Also, the std in the tables is so large that no conclusions can be drawn from it.\n- I do not understand the experiment setup. The metric used (relative optimal gap) measures the performance of the trained model and not the optimizers. However, the training is not mentioned. Next, it is not clear how significant the results are. No statistical testing or even a std was reported.\n\nOverall\n- The paper proposes an enhancement to optimization within the P+O framework but lacks clarity and rigor in both theoretical claims and experimental evaluation.\n- The main contribution\u2014adding a squared loss to an existing surrogate with a custom optimizer\u2014is presented obfuscated, does not bring any novel insights in the field, or does not help to understand the existing methods better.\n- Theoretical issues arise, especially in Theorem 1 and Corollary 2, where the proofs contain major flaws.\n- Misleading terminology and insufficient setup description and statistical analysis in experiments limit the work\u2019s impact."
        ]
    },
    "qU1GtrDDst": {
        "venue": "ICLR 2025",
        "title": "Representation learning for financial time series forecasting",
        "link": "https://openreview.net/forum?id=qU1GtrDDst",
        "abstract": "The accurate forecasting of financial time series remains a significant challenge due to the stochastic nature of the underlying data. To improve prediction accuracy, feature engineering has become a vital aspect of forecasting financial assets. However, engineering features manually often requires domain expertise. We propose to utilise an automated feature generation architecture, Contrastive Predictive Coding (CPC), to generate embeddings as input to improve the performance of downstream financial time series forecasting models. To benchmark the effectiveness of our approach, we evaluate forecasting models on predicting the next day's log return on various foreign exchange markets with and without embeddings. Finally, we assess our CPC architecture by employing the same trained encoder on different currency pairs and calculating the Sharpe ratio of our strategies.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            1,
            1,
            3
        ],
        "strengths": [
            "- The description of model architecture is clear.",
            "- 1. While CPC itself is not new, this paper attempts to apply it in a challenging, stochastic domain\u2014financial time series\u2014which traditionally poses difficulties for predictive models due to noise and volatility. This application may contribute to the field by assessing CPC\u2019s generalizability to non-deterministic data, even if further methodological enhancements are needed.\n2. The paper evaluates CPC embeddings not just on a single currency but across multiple currency pairs (USDJPY, USDSGD, EURGBP), demonstrating an effort to test its adaptability. This cross-domain testing shows that the embeddings have the potential to capture broader, generalizable patterns, although more rigorous experimentation would solidify this claim.",
            "- The study tackles a pertinent research issue with possible academic or practical implications, serving as a solid foundation for further investigation. The endeavor to implement automated feature generation architecture to generate embeddings as input to improve the performance of downstream financial time series forecasting models. demonstrates an engagement with the subject, albeit certain aspects want improvement.",
            "- The authors considered using contrastive representation learning to automate the feature engineering process.",
            "- - Developing time series methods that are robust to inherent strong stochasticity in the data is significant.\n\n- The application of CPC to the financial domain seems novel.\n\n- Some of the presented results are promising."
        ],
        "weaknesses": [
            "- Literature\nThe work has limited literature research. I give some examples on related works: \n\n1. Deep timeseries architectures\n[1]: Zhou, Haoyi, et al. \"Informer: Beyond efficient transformer for long sequence time-series forecasting.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 12. 2021.\n[2]: He, Yangdong, and Jiabao Zhao. \"Temporal convolutional networks for anomaly detection in time series.\" Journal of Physics: Conference Series. Vol. 1213. No. 4. IOP Publishing, 2019.\n[3]: Wu, Haixu, et al. \"Timesnet: Temporal 2d-variation modeling for general time series analysis.\" arXiv preprint arXiv:2210.02186 (2022).\n[4]: Goswami, Mononito, et al. \"Moment: A family of open time-series foundation models.\" arXiv preprint arXiv:2402.03885 (2024).\n\n2. Timeseries representation learning\n[1]: Eldele, Emadeldeen, et al. \"Time-series representation learning via temporal and contextual contrasting.\" arXiv preprint arXiv:2106.14112 (2021).\n[2]: Yue, Zhihan, et al. \"Ts2vec: Towards universal representation of time series.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.\n[3]: Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" arXiv preprint arXiv:2211.14730 (2022).\n\nIt is important to compare the proposed methodology with recently proposed representation learning technique to prove the effectiveness. \n\nExperiment \n1. Baseline comparison against existing representation learning method: In order to claim that the proposed method is effective, the author should also consider comparing the proposed method with some existing methods for timeseries representation learning, see corresponding section in literature above. \n\n2. Ablation: The author should conduct ablation study on the pipeline to investigate the critical components that influence the performance. Some examples can be 1) varying model architecture for contrastive learning 2) experiment with different methods to create negative & positive samples for the proposed contrasitive learning method\n\n3. Evaluation metric: \nMarket friction assumption: Can author provide the market friction assumption used during backtesting (such as transaction cost, assumed deviation from position entry, etc.) In addition, could you clarify whether your calculation of return for sharpe involves transaction cost? I ask this because in this type of trading strategy, despite the win-ratio (% profitable trade) might be higher than chance due to certain gain in predictive power, transaction cost could be very high and potentially eat up all the profit. This should be taken care of because the baseline, buy-and-hold, completes in two transactions. \n\nSome other financial metrics to consider: max drawdown, sortino ratio. This gives the reader another perspective of the strategy from risk perspective.\n\nAnother note is that it seems like a position can be short only. If this is true, It would be helpful to provide margin needed at anytime just to be practical. \n\n4. Size of dataset: Since the test conducted was only 20% of three currency timeseries. It would be helpful to backtest on more currency timeseries or other asset timeseries. This is to make sure the arrived results are statistically significant. \n\nAdditional Comment\n1. Figure 6: It is not convincing that this plot shows the effectiveness of produced embedding. Essentially the unsupervised classes are plotted. Then, it is expected that the boundary from k-mean should be clear. The author should conduct some analysis on each cluster to see if there are unique properties for each of them. \n\n2. lowest error rate with zero and mean model: It would be helpful for the author to provide more insights here. For example, does this suggest the forecasting method are worse than trivial baseline? If justified using mean-reverting property, can you show us that a forecasting model with mean-reverting behavior-forecast work well?",
            "- 1. The paper offers minimal novelty, as it relies on Contrastive Predictive Coding (CPC), a technique introduced in 2018, without significant modification. The sole methodological addition is the custom negative sampling based on the Black-Scholes model, which does not sufficiently advance. More substantial methodological innovation would be required for a contribution of significance.\n2. The experimental scope is limited and omits comparisons with current state-of-the-art models for time series forecasting, such as iTransformer, PatchTST, and DLinear. Additionally, the paper does not benchmark CPC against recent self-supervised learning models tailored for time series data, like TimeDRL, TS2Vec, or TS-TCC. This lack of comparative analysis weakens the results\u2019 credibility and makes it difficult to assess the true efficacy of the proposed approach.\n3. The paper suffers from organizational and formatting issues, which detract from clarity and professionalism. Notably, the references do not adhere to the ICLR format, and sections are structured in an unconventional manner. For instance, the Related Work section should be distinct, and the Methodology section includes experimental details that would be better placed in a separate Experiments section. Improved structure and adherence to standard formatting would enhance the paper's readability and alignment with conference expectations.",
            "- This paper has significant flaws in its writing, such as the absence of a proper literature review and problem formulation. Structurally, it resembles a technical report rather than a rigorous academic paper, lacking detailed descriptions of the methodology, adequate experiments to validate the model's effectiveness, and comprehensive information about the data used.",
            "- - citation reference format is wrong\n- there is no comparison with SOTA models or even vanilla transformers. there are only 11 references, authors should take a detailed look at the current landscape of literatures on financial time series forecasting\n- the metric used are not standard time series forecasting metric such as MSE and MAE\n- the experimental result is weak (only table 1)\n- the presentation is poor, CPC is not explained in detail in related work. there are missing references in the paper. For example \"In the plots below\", there is no reference to which plot the authors are referring. there are many typos in the paper: normalise -> normalize, maximise -> maximize, and so on. \n- there is a strong lack of novelty, CPC is already proposed and this work seems to be a simple application of CPC on financial time series. \n- min-max data normalization may be problematic in financial time series because extreme values significantly affects the normalized data. Extreme values are highly prevalent in financial time series, but one extreme value could lead to 99% of the data compressed into a tiny range after min-max normalization.",
            "- Major:\n- I did not find the paper\u2019s motivations convincing. The first sections only mention that CPC was never evaluated in the financial domain. I think this is more of a necessary condition rather than sufficient. The paper would be much stronger if convincing arguments are provided on why CPC is a good choice for tackling stochasticity and why is preferable w.r.t. other unsupervised representation learning methods.\n\n- The contextualization of this paper within the current literature is very poor. Only 3 references are provided in the literature review section: CPC, an application to anomaly detection, and an LSTM-based architecture. Overall, only 11 references are provided. I think the paper would benefit from a better presentation, up to recent works, of time series unsupervised representation learning [1] and forecasting [2], preferably discussing relevance to the financial domain.\n\n- The presentation and organization of the paper\u2019s content is very poor. A considerable portion of the paper (~3 pages in total) is dedicated to presenting naive baselines and details about the standard ML pipeline (preprocessing, normalization, windowing, train/test split), while all the important figures, describing the architecture and presenting the main results, are piled in the Appendix with minimal text. I think the authors should consider moving to the main text all tables/plots that they believe are relevant to support their claims.\n\n-  The learned embeddings are only evaluated in conjunction with linear regression, which severely limits the impact of the work, especially given the targeted venue.\n\n- The paper claims LSTMs struggling to learn from the considered dataset and being outperformed by linear regression and naive baselines (Sec. 3.2-3.3-3.4). However, Figure 7 (in Appendix) is the only result supporting this. Furthermore, the experimental pipeline does not consider a validation set for early stopping and model selection, which is arguably fundamental for deep architectures. Given the sensitivity of the paper's claim, I think it should be supported with more empirical evidence, e.g., learning curve, hyperparameter optimization.\n\nMinor\n- There are some inconsistencies in the notation. For example, $x_t$ is found on both sides of Eq.2;  notation is inconsistent between Eqs.1-2 and between Eqs. 3-4-5-6. \n\n- The definition of the CPC-LR acronym is missing.\n\n- Figure presentation could be improved, e.g., all plots in Fig. 8 are missing axis labels; captions are very concise; text in Fig.3-5-7 is small. \n\n- The paper might benefit from a more gentle introduction to some domain-specific terminology, e.g., allocations, buy-and-hold, return plots.\n\n\n[1] Meng, Qianwen, et al. \"Unsupervised representation learning for time series: A review.\"\u00a0arXiv preprint arXiv:2308.01578\u00a0(2023).\n\n[2] Benidis, Konstantinos, et al. \"Deep learning for time series forecasting: Tutorial and literature survey.\"\u00a0ACM Computing Surveys\u00a055.6 (2022)."
        ]
    },
    "zuuhtmK1Ub": {
        "venue": "ICLR 2025",
        "title": "Differentiable Implicit Solver on Graph Neural Networks for Forward and Inverse Problems",
        "link": "https://openreview.net/forum?id=zuuhtmK1Ub",
        "abstract": "Partial differential equations (PDEs) on unstructured grids can be solved using message passing on a graph neural network (GNN). Implicit time-stepping schemes are often favored, especially for parabolic PDEs, due to their stability properties. In this work, we develop a fully differentiable implicit solver for unstructured grids. We evaluate its performance across four key tasks: a) forward modeling of stiff evolutionary and static problems; b) the inverse problem of estimating equation coefficients; c) the inverse problem of estimating the right-hand side; and d) graph coarsening to accelerate forward modeling. The increased stability and differentiability of our solver enable excellent results in reducing the complexity of forward modeling and efficiently solving related inverse problems. This makes it a promising tool for geoscience and other physics-based applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- By employing an implicit scheme with optimized gradient computation, the proposed method reduces the required number of time steps. \nThey present a differentiable framework for both forward and inverse methods, enabling a learnable numerical approach based on discrete time steps. \nAdditionally, the paper explores applications in inverse problems, often employing irregular unstructured grids as used in practical scenarios.",
            "- * The work tries to build a framework that works with mesh coarsening, forward, and inverse problems.",
            "- Figure 1 effectively illustrates the overall pipeline, demonstrating experimental results that apply the combination of GNN and FVM to both forward and inverse problems.",
            "- The question is interesting and combining GNN with finite element method seems natural."
        ],
        "weaknesses": [
            "- While the underlying idea is promising, the paper would benefit from stronger experimental or theoretical justification for the proposed methodology. Additional clarity and motivation for the approach would enhance the paper\u2019s impact.",
            "- * The novelty of the work is limited. Incorporating FVM into GNN is not new and considered in, e.g., [Jessica et al. ICML 2024 https://arxiv.org/abs/2311.14464 ] and [Horie et al. ICML 2024 https://arxiv.org/abs/2405.16183v1 ]. The construction of gradients presented in Section 2.3 seems strongly related to the adjoint method, which is a standard way to deal with inverse problems. The implicit method for GNN is considered in the area of implicit GNNs, e.g., [Gu et al. NeurIPS 2020 https://arxiv.org/abs/2009.06211 ]. The authors state that these are their novelty, but there is existing work for each. The authors should cite these works and clarify the added novelty from the authors.\n* The evaluation is weak. There is only one baseline for the experiment in Section 3.2 and nothing for the ones in Section 3.3 and 3.4. With the current form, the reviewer cannot asses the effectiveness and superiority of the model.\n* The presentation is not clear. The figure may miss the labels (a), (b), and so on for Figures 2, 3, and 4. It is not clear what is \"data 1\", \"fitting 1\", \"data 2\", and \"fitting 2\" in Figures 2 and 3.",
            "- First and foremost, the paper feels incomplete. The biggest concern is the lack of discussion about other approaches that use GNNs or integrate FVM with deep learning to solve PDEs. A \u201cRelated Work\u201d section should be added to explain how the proposed model differs from recent studies and highlight its novelty. Although Section 2 on theory explains the problem setup to some extent, more detailed steps and methods for training the proposed approach should be included. Section 3, the experimental part, merely lists the results for forward and inverse problems without discussing how this method compares to existing GNN- and FVM-based approaches. For instance, the study \"Learning to Solve PDE-constrained Inverse Problems with Graph Networks\" solves inverse problems using GNNs\u2014how does the proposed method differ from this approach, and what advantages does it offer? Experimentally, does it outperform in solving inverse problems?",
            "- 1. The writing is subpar. There are many typos and grammatical errors. For example, \"Compute $\\nabla_bL$  with (12), whats is equivalent so the solution of a single linear system.\" should be \"Compute $\\nabla_bL$ with (12), which is equivalent to solving a single linear system.\"\n2. One main focus of this paper is the incorporation of implicit solver. However, using an iterative solver and in a deep learning setting is well-studied in the Deep Equilibrium Models (DEQ) literature. The authors should compare their method with DEQ.\n3. The experiments are not very convincing. The results in Section 3.4 is very poor and in no experiments the authors compare their method with other methods."
        ]
    },
    "zPxlHOLxmh": {
        "venue": "ICLR 2025",
        "title": "From Counseling Transcript to Mind Map: Leveraging LLMs for Effective Summarization in Mental Health Counseling",
        "link": "https://openreview.net/forum?id=zPxlHOLxmh",
        "abstract": "The increasing number of patients with mental health illness has heightened the cognitive load on therapists, making it challenging for them to provide personalized care that each patient requires. Summarizing counseling sessions can aid mental health practitioners in recalling key details. However, most existing research on summarization focuses primarily on text-based summaries which often require significant cognitive effort to read and interpret. Visual-based summary such as mind maps is proven to help enhance cognitive understanding by giving a quick overview of topics and content. Nevertheless, due to the complex nature of counseling which involves substantial qualitative data, generating visual-based summaries using traditional AI models can be challenging. With the recent advancements in Large Language Models (LLMs), these models have demonstrated the capability to perform tasks based on instructions and generate outputs in various formats. In this study, we develop a web-based summarization tool that serves as a pipeline in performing summarization of counseling transcripts into visual-based mind map summaries using LLMs. We conducted a human evaluation to validate the effectiveness of the generated visual-based summary based on criteria of accuracy, completeness, conciseness and coherence. Our findings show that our web-based summarization tool can effectively extract key points from counseling transcripts and present them in visual-based mind maps, demonstrating its potential in enhancing insights for therapists, ultimately simplifying the process of documenting counseling sessions.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            1,
            3
        ],
        "strengths": [
            "- The integration of LLMs, specifically GPT-4o Mini, with PlantUML for generating mind maps from qualitative counseling data is a creative and original contribution. This approach moves beyond traditional text-based summaries by providing a visual representation of complex counseling sessions, which is underexplored in the mental health domain.",
            "- - Interesting problem of visual summaries is explored. Mind maps are an effective and quicker way of information communication, so using such things to help therapist is a good idea.\n- It is good that the authors have developed a web based tool which can help people.\n- The paper is easy to follow.",
            "- - The authors provide a web based tool that can be used by mental health professionals for building mind-map visual diagrams of past notes\n- They show that their tool can build reasonably good mind maps evaluated using human experts.",
            "- Originality:\nThe paper introduces the use of mind maps for summarizing counseling transcripts, which is an interesting shift from typical text summaries. Applying language models to generate these visuals in a mental health context is a practical adaptation, though it builds on existing summarization methods.\nQuality:\nThe method is clearly explained but lacks depth in certain areas. While the pipeline from prompt design to mind map visualization is understandable, the approach could benefit from a stronger methodological foundation or more comprehensive testing. Human evaluations are included, but the evaluation setup could be more rigorous to fully support the tool\u2019s effectiveness.\nClarity:\nThe paper explains the problem and solution in a straightforward way, with enough detail to understand the main approach. Visual examples help clarify how the mind maps work, although some sections could be streamlined for readability.\nSignificance:\nAddressing the cognitive load for therapists is a relevant issue, and this tool could be helpful in simplifying their workflow. While the impact is practical, the approach is incremental and may inspire further exploration of visual summarization but is unlikely to be transformative on its own."
        ],
        "weaknesses": [
            "- 1. Limited Sample Size and Representativeness\n\nThe study utilizes only 20 randomly selected samples from the MEMO dataset for the evaluation. This small sample size is not sufficiently representative of the diverse range of counseling conversations that occur in real-world settings. As a result, the findings may not generalize well to broader applications. To strengthen the validity of the results, it would be beneficial to include a larger and more varied dataset. This expansion would help in capturing a wider array of counseling scenarios and linguistic nuances, thereby enhancing the robustness of the tool's performance and its applicability to different contexts.\n\n2. Reliance Solely on Human Evaluation Lacking Reproducibility\n\nThe evaluation of the generated visual summaries is based entirely on human assessments from a small group of participants who are researchers in the field of Information Technology, not mental health professionals. This approach raises concerns about the reproducibility and objectivity of the results. Human evaluations can be subjective and may vary significantly between different evaluators. Incorporating quantitative evaluation metrics, such as adapted versions of ROUGE or BLEU scores for summarization tasks, could provide more objective measures of the tool's performance. Additionally, involving mental health professionals in the evaluation process would offer insights that are more aligned with practical therapeutic needs, thereby increasing the reliability and validity of the findings.",
            "- - No substantial contribution is there in this paper for it to be a part of main research track, maybe a better fit for demo track for some conference.\n- Only 20 mind maps evaluated by only 3 people with using just 1 LLM as the summariser might not be enough to be considered as a thorough investigation. I would suggest the authors to conduct a more detailed analyses by using more LLMs in the place of GPT-4o Mini to summarise the transcripts and then perform a comparison of different LLMs here.\n- In a nutshell, the paper does not propose a new problem, does not have a new dataset, does not propose any novel method, and does not contain any interesting observations or analysis. It is a very engineering pov paper, which is not a good fit for this venue.",
            "- - The paper lacks novelty - as it is merely an application of using LLMs for a very specific use case. The prompting itself is also not smart or novel in any way as it just has a structured output that is parsed into a mind-map. I dont believe this would be of interest to many people.\n- There are no comparisons to text based summaries for this use case and it is not clear how these visual summaries could be more useful to a mental health professional. Several ablations are missing.\n- The contributions is really just the prompt and the web based tool.\n- There is also no comparison between the use of different kind of models / LLMs use",
            "- 1. Limited Sample Size in Evaluation\nAlthough the authors mention that they selected 20 transcripts as a \u201cpreliminary\u201d sample, the decision to limit the entire study to these 20 samples is problematic given that the original MEMO dataset contains 212 transcripts. Relying solely on such a small subset, when a much larger set is available, raises concerns about the representativeness of the findings. A larger, more diverse sample could provide a stronger basis for evaluating the tool\u2019s reliability across different counseling scenarios and patient-therapist interactions. Expanding the sample size is essential to enhancing the credibility of the results and giving a fuller picture of the tool's performance.\n2. Insufficient Evaluation Criteria and Sample Size of Evaluators\nThe evaluation approach has two issues: (1) the limited expertise of the evaluators and (2) the small number of evaluators. Referring to the evaluators as \u201cparticipants\u201d implies they were not specially trained or highly qualified to evaluate counseling summaries, which could compromise the quality of feedback. A robust evaluation for this type of tool typically requires input from domain experts \u2014 such as mental health professionals \u2014 who can reliably assess criteria like accuracy, relevance, and therapeutic usefulness based on their experience.\nAdditionally, the study relies on only three evaluators, which is insufficient for a reliable, statistically meaningful assessment. This small evaluator pool, combined with limited experience in mental health, limits the confidence one can have in the evaluation results.\n3. Lack of Comparisons with Other Summarization Methods and Models\nThe paper does not compare mind maps with alternative summarization formats, such as text-based summaries or visual formats like concept maps. This absence weakens the justification for mind maps as the preferred format, as their claimed cognitive benefits remain untested. Additionally, the study only uses GPT-4o Mini, without comparing it to other language models. This limits the scope since other models might perform better or capture nuances differently.\n\nBy addressing these issues \u2014 expanding the sample size, involving a more qualified evaluator pool, and incorporating comparisons with other methods \u2014 the study would achieve a higher standard of rigor and offer stronger evidence for the proposed tool\u2019s effectiveness."
        ]
    },
    "y2ch7iQSJu": {
        "venue": "ICLR 2025",
        "title": "Budget-constrained Active Learning to De-censor Survival Data",
        "link": "https://openreview.net/forum?id=y2ch7iQSJu",
        "abstract": "Standard supervised learners attempt to learn a model from a labeled dataset.  Given a small set of labeled instances, and a pool of unlabeled instances, a budgeted learner can use its given budget to pay to acquire the labels of some unlabeled instances, which it can then use to produce a model. Here, we explore budgeted learning in the context of  survival datasets, which include (right) censored instances, where we know only a lower bound c_i on that instance\u2019s time-to-event t_i.  Here, that learner can pay to (partially) label a censored instance \u2013 eg, to acquire the actual time t_i for an instance [eg, go from (3yr, censor) to (7.2yr, uncensored)], or other variants [eg, learn about 1 more year, so go from (3yr, censor) to either (3.2yr, uncensored) or (4yr, censor)].  This serves as a model of real world data collection, where followup with censored patients does not always lead to complete uncensoring, and how much information is given to the learner model during data collection is a function of the budget and the nature of the data itself. Many fields, such as medicine, finance, and engineering contain survival datasets with a large number of censored instance, and also operate under budget constraints with respect to the learning process, thus making it important to be able to apply this budgeted learning approach. Despite this importance; to our knowledge no other work has looked into doing this. We provide both experimental and theoretical results for how to apply state-of-the-art budgeted learning algorithms to survival data and the respective limitations that exist in doing so. Our approach provides bounds and time complexity  theoretically equivalent to standard active learning methods. Moreover, empirical analysis on several survival tasks show that our model performs better than other potential approaches that might be considered on several benchmarks.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- - The selected problem is very relevant and interesting for real life data applications. The paper well states the problem and aspire to provide a solution.\n- Authors propose three alternative methods to demonstrate the strength of their approach.",
            "- The use of active learning in survival analysis is an interesting idea.",
            "- - This paper is clear, domains are well introduced (whereas the paper is at the intersection of survival analysis and active learning)\n- The results are convincing, and the authors have made the effort to compare themselves with \u201cnaive\u201d methods (sanity checks part), even though there is no work dealing with this case.",
            "- 1) The **problem addressed in this paper is relevant** and the existing research on the topic of AL for survival data is indeed limited. \n2) The proposed approach is **an original combination of existing ideas**, namely budgeted learning and BatchBALD for AL, with the latter being \"tweaked\" to work with survival data."
        ],
        "weaknesses": [
            "- 1) Paper contains multiple typos, see e.g line 210. \n2) Terms are not clearly defined, e.g. Algorithm 1, line 275, what is a_BB_surv ? \n4) The paper is not very clearly written, there is missing description of  the learner, there is missing debate on the distributional properties of the BALD model.  \n5) The results in the main section seems cherry-picked, e.g. it would be good to include more points in the budget constraints rather than 0, 1, 5, 20.\n6) It would be good to provide study on the synthetic dataset to demonstrate how the proposed methods perform in the controlled setting.",
            "- The use of de-censoring as the label in survival analysis does not have much practical usage. It could be used in \"reliability\" in industry, though. The written English is not clear. The use of words and punctuation like  \"and\" and \",\" make the sentences hard to understand. For example, see  lines 269 and 282.",
            "- - I have some minor comments, it's more a question of presentation than content.",
            "- 1) The **contributions are not very clear**. The authors should clearly state which contributions they are claiming. Is the extension of BatchBALD to a budget-constrained setting the central contribution? If so, why is the theoretical analysis included in the appendix rather than in the main text? Is the adaptation of BatchBALD to survival data the central contribution? Both? The narration is ambiguous with respect to this, thus unnecessarily increasing the difficulty of assessing novelty.\n2) The **novelty is unfortunately low**. It is not clear how the proposed approach for accounting for the budget constraint is different from the existing approaches for budgeted learning from the literature. The adaptation of BatchBALD proposed for survival data is also very incremental and not adequately grounded. The authors also mention that \"so far no method has been developed looking at\nbudgeted learning with survival data\" - however, there are other works in the literature looking at applying BALD for right-censored data (see e.g., https://arxiv.org/pdf/2402.11973). How does the proposed approach relate to these works?  \n3) There are **several concerns regarding the soundness** of the proposed approach. The extension of BatchBALD to a budget-constrained setting is not clearly described and is not adequately grounded. The proposed approach is an adaptation of the algorithm proposed in (Khuller et al., 1999) \"by omitting the first two lines\". However, it is not clear what lines the authors refer to and, most importantly, what are the theoretical grounds for doing so. Similarly, the adaptions of BatchBALD for survival data are essentially presented as \"algorithmic tweaks\" (e.g., setting probabilities to zero and renormalizing) without a proper theoretical justification. Lastly, it is not clear how the authors handle the fact that the queried instances can still be censored - are the censored times assumed to be known a priori before querying? If so, how limiting is that assumption in terms of the applicability of the proposed approach? \n4) The **limited application scope** of the proposed approach is also a topic of concern. Although I agree with the authors that AL for survival data is a topic worth investigating, and the literature is indeed lacking on this, the application scope is, in my opinion, broader than the medical domain application presented in the paper. This limits the target audience of the paper quite a lot. I would encourage the authors to consider revising their presentation to broaden the scope of application to other domains as well. For example, the authors mention applications in finance and engineering in the abstract - it would be interesting to discuss those as well. Lastly, the authors consider only approaches where time is discretized into bins (they use Multi-Task Logistic Regression as the underlying survival model), and the proposed adaptation of BatchBALD to handle survival data in Section 5 relies on this assumption, which can be quite limiting. This should be discussed in the paper. \n5) The presented **claims are not adequately supported by empirical evidence**. Looking at the results in Table 1, the improvements over standard BatchBALD are very marginal. However, the text claims that \"BBsurv outperforms other algorithms when budget is equal to 20 across all 3 real world datasets\", which does not seem to always be the case. In fact, even \"Entropy\" seems to perform quite comparably - did the authors perform a statistical significance test? Confusingly, the Table 1 caption states that \"Budget = 10\", which is inconsistent with the text. Moreover, the results in Table 1 seem to be inconsistent with Figures 1 and 2. If one considered a vertical line at Budget = 10 (or 20, whichever it is), shouldn't the values and relative order of the approaches be consistent with Table 1?\n6) The **presentation can be significantly improved**. There are several typos, poorly constructed sentences, incorrect grammar usage, sentences that are too informal for scientific paper, etc. I strongly encourage the authors to carefully revise the writing and overall presentation of the paper. Other aspects of the presentation, such as mathematical derivations, can be significantly improved. For example, the authors should include numbers for the equations. Similarly, it is unclear how the authors transition from the eq. of the mutual information at the beginning of Section 4.1 to the computation of its right-most term 2 equations below. The right term is an expected entropy, while 2 equations below only the entropy is considered. Can you clarify?"
        ]
    },
    "xTrAA3UKPa": {
        "venue": "ICLR 2025",
        "title": "SWGA: A Distributed Hyperparameter Search Method for Time Series Prediction Models",
        "link": "https://openreview.net/forum?id=xTrAA3UKPa",
        "abstract": "We propose a distributed hyperparameter search method for time series prediction models named SWGA (Sliding Window Genetic Algorithm). Compared to current genetic algorithms for hyperparameter search, our method has three major advantages: (i) It adopts a configurable sliding window mechanism to effectively combat overfitting from distribution shifts inherent in time series data. (ii) It introduces a warm-up stage using Bayesian optimization-based methods to generate a good initial population. (iii) It supports distributed hyperparameter search across multi-node computing clusters, enhancing both scalability and efficiency. To demonstrate SWGA's efficacy, we conduct hyperparameter search experiments on time series datasets from various domains. The experiment results show that our method consistently finds a hyperparameter configuration that achieves better performance on out-of-sample time series data compared to the traditional genetic algorithm. On average, it reduces the out-of-sample loss by about 56.1%.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- 1. SWGA introduces a combination of genetic algorithms with a sliding window approach tailored specifically for time series forecasting, addressing the distribution shift and non-stationarity challenges unique to this domain.\n\n2. The authors have provided detailed computational procedures by providing multiple algorithm boxes and demonstrated their impact on real-world applications.",
            "- * The algorithm is parallelizable, significantly decreasing the computation time required to find the optimal set of hyperparameters.\n* Evaluation is conducted on a sufficient number of widely known real-life datasets.",
            "- * It\u2019s a relevant problem for time series forecasting.\n* It shows some potential for the sliding window strategy.",
            "- - the paper is clear and concise, though it presents some nits reported below\n - the method is applied on multiple datasets and on a variety of different models."
        ],
        "weaknesses": [
            "- 1. The paper writing has too much redundancy that can be simplified or moved to the appendix. It is also not clear why the proposed method is distinguished from other hyper-parameter optimization approaches.\n\n2. In the experiment section, there is also a lack of comparison with other hyperparameter optimization methods specifically designed for time series data.\n\n3. The SWGA algorithm\u2019s performance might be sensitive to parameters like window size, population size, and mutation rates. However, the paper lacks an exploration of how these parameters impact outcomes.",
            "- * In the experiments, the proposed algorithm is only compared with the traditional genetic algorithm. It would be beneficial to evaluate other hyperparameter search techniques, such as the classic sliding window search and k-fold validation technique.\n* The models evaluated in the experiments lack diversity. There are 3 tree-based models (CatBoost, LightGBM, XGBoost), 1 recurrent model (LSTM), and 1 attention-based model (Transformer).\n* The authors mention high computational costs and inefficient exploration of large hyperparameter search spaces as disadvantages of commonly used techniques. However, the experiments only involve accuracy comparisons. Wouldn\u2019t it be beneficial to demonstrate that the proposed hyperparameter search technique converges more quickly?\n* In Appendix A.1, search spaces are provided for the DLinear and PatchTST models, which are not included in the experiments.\n* The proposed algorithm combines variations of three widely used search techniques from the literature with minimal modifications. The novelty does not seem satisfying.",
            "- - The contribution is minor. I can\u2019t agree that using TPE instead of Random Search is an actual contribution. To fill the initial population, doing a small HPO with TPE should have better results than random configurations. It\u2019s too obvious. Also, to show the sliding window is a better HPO strategy, it needs to first deliver better quality and second is cost effective. While the authors\u2019 experiments (Table 1 & 2) show SWGA has better quality than GA, I am not sure the baseline of GA is properly implemented due to lacking of detail. Also, there are tons of HPO methods that can be combined with the sliding window and they can be used to show sliding window is indeed helpful.\n\n- The technical correctness is hard to access given the current state. Many important details are missing. For example: \n  - How do the authors ensure that the only baseline GA is comparable with the SWGA? For me, to see if the sliding window helps, taking Figure 1 as an example, it should be a single GA on the average performance of 12 splits. Assume population size is M, then SWGA takes trains M*12 times and the single GA baseline also trains M*12 times.\n   - Line 333: \u201cwe use seven historical timesteps to predict one timestep ahead\u201d Does this mean the authors sample segments of size 8 from training and validation set? How many are sampled?\n\n- The experiments, especially Table 3, does not support the contribution.Table 3 only shows HPO helps, not why sliding window or warm start is a good strategy. The part of scalability does not fit into the current story. The contribution of the paper, as claimed by the authors, are the sliding window and warm up. The focus is not distributed training or scheduling etc.\n\n- The terminology and notations are not precise and conventional. For example:\n  - Line 35 \u201dthe model can achieve better performance on out-of-sample data with a matching distribution\u201d What does out-of-sample data with a matching distribution mean? It\u2019s also strange that the authors call the prediction window as out-of-sample data. It may or may not be out-of-sample.\n  - Line 141, please check notation, many misusages.",
            "- Important weaknesses:\n - the contributions are strictly practical, thus it is of fundamental importance that the method is accessible to researchers and reproducible. Particularly, ICRL strongly encourage to add a \u201cReproducibility Statement\u201d (https://iclr.cc/Conferences/2025/AuthorGuide), and the paper is missing it. Furthermore, no code is provided as supplementary material, making the results hard to reproduce and inaccessible\n - table 1 shows that it\u2019s the TPE addition that outperforms the other method, but no experiment shows that the proposed method is better than just TPE.\n - results are not reported with confidence intervals. Thus, there is no way to check if the results are just due to variance.\n - No information is given on the training procedure of the GA counterpart. Furthermore, multiple GA algorithms are present in the literature, but it\u2019s never reported which is the one that is applied for the results.\nMinor weaknesses\n - line 178, TPE is criticized for its ability to scale badly with dimensionality, though the paper uses it to initialize the population, inheriting its downside\n - the 4th contribution point is pointless, given that no code is provided as supplementary material\n - the 2nd contribution point states that the proposed approach should address the distributions shift induced by the nature of time-series. However, nowhere in the paper this is investigated, and specifically, nowhere is addressed what are the other options apart from SWGA and why they should fall short in time series predictions\n - in the \u201cconclusion\u201d section, it is stated \u201cAdditionally, we also demonstrate the good scalability of SWGA\u201d. However, no reference to other algorithms/approaches is given. Thus, with a lack of baselines, it\u2019s completely irrelevant\n\nMy recommendation is to reject the paper.\nThe main reasons behind this opinion are:\n - the contributions are minors, and they can be summarized in a smart initialization of the GA using TPE, and a sliding window training approach.\n - the lack of reproducibility of the results\n - the narrow comparison with other approaches.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nWriting concerns:\n - citations should be between parenthesis if not part of the main text (e.g. line 39-40 and then the rest of the paper, respectively use \\citep for parenthesis and cite for normal citation)\n - line 68, TPE was never defined (will be defined later, though it should be defined on the first usage)\n - \u201cBut, their full\u201d line 107\n - \u201cK-fold cross-validation effectively reduces the risk of overfitting\u201d: how? It\u2019s a validation method, not a regularization\n - The equation on line 140 is not numbered, and it contains 2 \u201ci\u201d indexes. Please use different letters to avoid confusion\n - line 161 \u201cwhile domain adaptation is to\u201d\n - line 258 263, the pseudocode contains a line break that is irrelevant\nPersonal opinions:\n - contribution 2 and 3 are not contributions, but positive aspects of the method, maybe better to write them outside of the bullet points\n - the first two paragraphs in 4.1 are repetitive, and the explanation on how the GA works should be part of the background section"
        ]
    },
    "wJVZkUOUjh": {
        "venue": "ICLR 2025",
        "title": "EXAGREE: Towards Explanation Agreement in Explainable Machine Learning",
        "link": "https://openreview.net/forum?id=wJVZkUOUjh",
        "abstract": "Explanations in machine learning are critical for trust, transparency, and fairness. Yet, complex disagreements among these explanations limit the reliability and applicability of machine learning models, especially in high-stakes environments. We formalize four fundamental ranking-based explanation disagreement problems and introduce a novel framework, EXplanation AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine learning, particularly from stakeholder-centric perspectives. Our approach leverages a Rashomon set for attribution predictions and then optimizes within this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that minimize disagreement with diverse stakeholder needs while maintaining predictive performance.  Rigorous empirical analysis on synthetic and real-world datasets demonstrates that EXAGREE reduces explanation disagreement and improves fairness across subgroups in various domains. EXAGREE not only provides researchers with a new direction for studying explanation disagreement problems but also offers data scientists a tool for making better-informed decisions in practical applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- - The paper addresses a critical challenge faced by researchers and practitioners: how to proceed when even explainable AI tools disagree on feature importance. Additionally, it incorporates model and stakeholder rankings, making the approach quite comprehensive.\n- The paper tackles its proposed problem by integrating methodologies from several different areas, including the XAI literature as well as general AI methods for optimization challenges.\n- The insight to make the process end-to-end differentiable is both creative and practically useful.\nIn the appendix, the authors demonstrate the impact of the choice of $\\epsilon$ on the Rashomon set, which serves as a valuable methodological sensitivity analysis.\n- The empirical results test their methods across a variety of settings: 6 OpenXAI datasets, both synthetic and empirical; and 2 pre-trained models (logistic regression and artificial neural networks).",
            "- The technical framework of end-to-end optimization problem which involves constructing the Rashomon set, DMAN, sorting networks and multi-heads architecture is interesting.",
            "- The scientific contribution presented in the paper is valid and interesting. The proposed method has the means to create machine learning models that can produce explanations that match an arbitrary ideal. The paper presents a new, model-agnostic method to accomplish this, which would be a meaningful and important contribution to the field.",
            "- The problem that the paper focuses on is interesting and the paper does a good job at disentangling the four scenarios of explanation disagreement. \n\nOriginality and Significance (of the problem): High\nQuality and Clarity: Poor"
        ],
        "weaknesses": [
            "- The paper integrates several methods and techniques to address the explainability disagreement issue; however, it can feel somewhat dry and lacks the depth and technical details that would enable readers to fully appreciate the contributions and identify strengths and weaknesses. Technical jargon used to describe the methodology needs precise definitions, mathematical arguments should be clearly defined and explained, and additional background information would offer useful entry points for readers. Most of the following points align with this suggestion. This lack of precise definitions also contributes to my limited confidence in the recommendations, as it made it challenging to fully assess the work's potential impact.\n- The loss functions for $L_{sparsity}$ and $L_{diversity}$ are not defined clearly in the paper or the appendix.\n- Precise mathematical definitions of what a mask is and how it is derived are essential for readers to understand the methodology in depth, as this concept is central to the approach.\n- Consider adding a sentence or footnote to define core terms in your algorithm, as these abstract concepts can vary in meaning:\n    - \"attribution set\", \"model representations\", \"model characterizations\", \"end-to-end optimization\".\n    - For instance, in the sentence \"Training a Differentiable Mask-based Model to Attribution Network (DMAN) that maps feature attributions from model characterizations for use in the next stage,\" it would be helpful to clarify precisely what \u201cmodel characterizations\u201d entail.\n    - Also, in Equation 4, where $f_{DMAN}^*$ is defined as the optimal surrogate model in the Rashomon set that describes feature attributions, it appears to be the loss between $f_{DMAN}$ and a set comprising ${\\text{masks}, \\text{attributions}}$. Minimizing the output to such abstractly defined elements would benefit from more clarity.\n- It would be helpful to include insights or references for the result in row 196. Why is faithfulness proportional to agreement? Is this a theoretical result, an empirical finding from the paper, or something else?\n- Figure 1 provides few entry points for readers and doesn\u2019t seem to aid in understanding at its current placement. Consider either removing it or adding more descriptive captions to clarify each step (similar to Figure 2, which includes more context). Suggestions include captions for the rankings, lightbulb, etc. Additionally, why are stakeholders grouped together in the first half but not in the second half?\n- Figure 2 is clear, but it seems to appear too early in the paper. Moving it to the end of Section 3 might make it more helpful, as readers would have more context to interpret it.\n- Is the fairness improvement an explicit objective of the EXAGREE model? If so, please explain the rationale and mechanism. If it\u2019s an outcome of the empirical analysis, please clarify this in the paper, as empirical results may not generalize across all applications.\n- In row 276, in the explanation of the \"Ranking Supervision and Correlation Metric,\" it would be beneficial to provide more context and motivation for this metric and how it fits in the big picture of your methodology before defining it.",
            "- However, this paper should be rejected because:\n(1) it is built on weak understanding of explainability\n(2) there is a weak connection between \"explanation disagreement\" and the solution\n(3) has questionable experiment design and metrics without sufficient justification\n(4) it is unrefined\n\nMy biggest concern is that the paper uses local explanatory models (i.e. LIME, Integrated Gradients) to generate global (model-level) explanations. In Section 2.3, the authors mention that they have adapted feature attribution methods for local explanations \"by averaging feature attributions across all instances to obtain global attributions.\" These methods were not designed to be used this way. Although SHAP does have functionality to provide model-level feature attribution, it takes an average of the **absolute** attribution across instances. \n\nMy impression of the paper's proposed solution, EXAGREE, is that it attempts to address \"explanation disagreement\" by finding a model that aligns with stakeholder expectations (i.e., based on domain knowledge) through examining its post-hoc explanation. There is one critical assumption here: the post-hoc explanation is faithful to model behavior --- something we cannot take for granted (see e.g. [Adebayo et al. (2019)](https://arxiv.org/abs/1810.03292)). Besides, I don't think the solution addresses the problem of \"explanation disagreement\", but is rather a model-selection tool using post-hoc explanations. I see that there are two cases of \"explanation disagreement\" (both of which is mentioned in the paper):\n1. Models with similar performance give different explanations (explanation method fixed)\n2. Explanation methods provide different explanations for one model (model fixed)\nEXAGREE addresses 1 to an extent but not 2 --- the paper does not make this clear. The authors seem to suggest that the \"stakeholder centric\" approach can address complex disagreements (Section 2.2). But I don't see how it addresses case 2.\n\nMoreover, I am not convinced that \"higher agreement between rankings implies greater faithfulness\". Bad actors might want explanations that hide the discriminatory nature of their models, hence want features to be ranked a certain way. In fact, several works have highlighted that explainability methods are prone to manipulation:  [Slack et al. (2020)](https://arxiv.org/abs/1911.02508), [Aivodji et al. (2019)](https://proceedings.mlr.press/v97/aivodji19a.html), [Goethals et al. (2023)](https://arxiv.org/abs/2306.13885). Explainability methods are tools to gain insight into a model (to potentially build trust) not project our desired belief upon the model.\n\nAs a result, the metrics, which are based on an unsubstantiated assumption that agreement $\\implies$ faithfulness, and the empirical results fall short of achieving the goals outlined in the introduction: to identify models \"that provide fair faithful and trustworthy explanations.\"\n\nThe experimental design uses the \"ground truth\" explanation, the coefficients of the LR model, as \"stakeholder needs.\" It is inappropriate to compare this to the explanations of the ANN model. I do not understand why we would want ANN model explanations to agree with LR explanations. Note that this is quite different from what [Agarwal et al. (2022)](https://arxiv.org/abs/2206.11104) did in their experiments. The experiment setup in general is quite confusing.\n\nIn line 463, the discussion regards explanation methods as \"stakeholders\". I question whether it is appropriate to frame it this way as it is difficult to imagine a stakeholder wanting rankings \"like LIME\".\n\nFurthermore, the paper in its current state does not seem refined. The authors introduce the problem of explanatory multiplicity but do not make an effort to elaborate on how and why it hinders trust in the model (what about the explanation method?). Also, figures 1 and 2 are not helpful in improving the readers' understanding of the EXAGREE process. Figure 1 is especially confusing regarding what it is meant to portray.",
            "- The paper\u2019s formalization of four types of disagreements lacks sufficient motivation and clarity, especially in comparison to the foundational work it draws from. For example, the classification of \"model disagreement\" as a type of \"explanation disagreement\" is unclear - different models that produce the same predictions can indeed have different internal mechanisms of doing so - in this case explanations should disagree and illuminate this fact rather than obscure it. This notion aligns with the concept of Rashomon sets, where multiple models with similar predictive performance can have significantly different decision boundaries. I encourage the authors to clarify their rationale for categorizing model disagreement within the explanation disagreement framework and to elaborate on how their approach handles cases where differing explanations for similar predictions might reveal essential model behaviors rather than obscure them. There is no \"model disagreement\" problem.\n\nThe paper's contributions then are better studied and understood in the context of a related line of inquiry about adversarial attacks and explanation fairwashing, such as Slack et al (https://doi.org/10.1145/3375627.3375830). The primary problem addressed appears to involve modifying the explanations produced by a Rashomon set of models to align with a predefined set of explanations from an external oracle. This connection is effectively illustrated, though not explicitly addressed, in Table 1, where metrics such as FA, RA, SRA, and others are reported for the ANN model. Notably, the original OpenXAI benchmark (Agarwal et al.: https://dl.acm.org/doi/10.5555/3600270.3601418) does not provide ground truth for ANNs. What this paper does (I think) is use the LR coefficients as ground-truths to measure metrics such as FA, RA, SRA, etc against a **different** model - an ANN! I recommend the authors clarify their novel approach to calculating these metrics for the ANN model and discuss the ethical implications of aligning ANN explanations with LR model coefficients.\n\nFinally, the paper\u2019s entire framing around \"explanation agreement\" (motivated in Section 2) could be made clearer. Rather than resolving \"model disagreement,\" the proposed SAEM approach seems to modify model explanations without altering predictive accuracy, which could be viewed as a form of adversarial attack on explanations. I encourage the authors to address how this approach contrasts with adversarial manipulations of explanations (if at all), discuss potential connections to explanation fairwashing, and consider any ethical implications that arise from intentionally adjusting explanations while maintaining predictive outputs. What is presented in this paper as an SAEM to resolve the apparent \"model disagreement\" class of explanation disagreement problems is essentially a means to make the FIS score for the ANN model to match the coefficients from the LR model trained on the same data - this is an adversarial attack.",
            "- The paper has a lot of weakness in my opinion and I delineate them as follows. You do not need to address all of them in the rebuttal, I have marked points that are major weakness, you can focus on them. \n\n1. Starting with Figure 1 on Page 1 -- totally unclear. How does the right side of the figure indicate agreement while the left does not? (this figure is not crucial to understand the paper, but I am just making a point of why the paper is so unclearly written and you do not need to address this point in the rebuttal, there are many more important problems. )\n\n2. [Major] In Section 2 (Preliminaries), there are several mistakes in the formulation:\n\n**a)** M* is not defined in Eq. 1. I assume what you mean is the optimal model by M* (the model with the best performance on the data). \n\n**b)** a_1, a_2,...,a_p are not defined as features.  \n\n**c)** In Equation 1, I think the sign should be >= instead of <= (the loss of the other models will be higher than M* (assuming M* refers to optimal model)\n\n**d)** Why is $\\epsilon$ multiplied by L(M*), usually similar performing models are supposed to be an absolute threshold, not relative to the loss of the original model. \n\n3. [Most Major] In Section 2.1, you mention 4 axes of explanation disagreement. I am with you on the model and method disagreement, but what does stakeholder and ground truth disagreement mean? \n\n**a)** In Stakeholder disagreement you mention \"Different stakeholders in S might prefer different rankings\". Why would that ever happen? Do these stakeholders want something that is not real and want fake explanations that aligns with their mental expectations of what a model should care about? What is an example of such stakeholders wanting something different from reality? Your current example of a data scientist wanting statistical significance and domain expert valuing different features **does not** answer this. Does wanting statistical significance and valuing different features amount to wanting different ranking of features (this also assumes that these rankings won't be same in the first place)? And even if they want different rankings, should a technique be optimized to suit their demands and provide fake explanations? \n\n**b)** In ground truth disagreement you mention \"ground truth interpretations (i think you mean explanations) from interpretable models can conflict with post-hoc explanations\" -- yeah this is obvious, but does it matter? If I have an interpretable model, why do I care what a post-hoc technique says, I will never ever use that for such a model? Why would one do that? \n\n**c)** In the primary objective of the paper you write: \"identify a well-performing model that minimizes disagreement (or maximizes agreement) between model explanations and stakeholder expectations. \" -- where stakeholder expectation is either something we should not give them because it is fake or something I do not understand. Your rebuttal will help me understand if it is the latter (a solid convincing example is required). \n\n5. Section 2.3, I think the title of that subsection should be \"Evaluation Metrics\" and not \"Evaluation Matrices\". Anyway, in that section you mention two metrics: faithfulness assessment and fairness assessment. In faithful assessment you give a further breakdown in several metrics -- however, most of these metrics seems highly correlated, for e.g., the attribution values directly affect the ranking, so FA, RA, RC, SA, SRA, PRA should have extreme correlation (when you consider the attribution values with its sign). Can you measure this please? If they are highly correlated, then that is effectively one metric and not six. If they are not correlated, can you explain why that is not the case (except the case where you consider the absolute values, its obvious then). \n\n6. You have used the word \"mask\" in line 256, without defining, what does it mean? Also in line 259 what does the phrase \"bridging the gap between models in Rashomon set and their feature attributions mean\" mean? each model in the Rashomon set (or any model for that matter) has a feature attribution (produced by any technique), what is the gap between them -- they don't even lie in the same space, one is in the weight space and other is in the explanation space (they have different dimensionality). \n\n7. [Major] You have this complicated (and not at all well explained) pipeline of computing SAEM which is basically the models that have the highest agreement with stakeholder requirements. Please tell me why do you need this pipeline? You have the models that are trained on that dataset, you can use that to compute the set of the models that are best performing (according to some threshold $\\epsilon$), and then compute the explanations of these models using whatever method you like, and then just give those models to the stakeholders with whom they have the highest agreement. What is the job of the models like DMAN, you are just using that to predict the model that will have the highest agreement (is that a correct understanding of the model's job?), but you don't need that you have the ground truth and you even say this in lines 268-269. \n\n8. [Major] The above were the problems with the problem setup, now coming to the experiment section. \n**a)** In table 1 you mention k= 0.25, what does k stand for? You have used k and l in Section 2.1 without ever defining them. For the same reason, I do not comprehend what does the k in caption of Figure 3 stand for. \n\n**b)** Why is SAEM technique bolded in Tables 1 and 2. The standard practice in ML papers to the bold the best performing method, but your technique, and this is misleading. \n\n**c)** If SAEM selects the models with the highest agreement with the stakeholder, why is it not the best for LR in Table 1 and 2? (especially in Table 2, it is pretty behind techniques that are not optimized for agreement, which alludes to the case that explanation disagreement might not even show up in actual experiments?)  \n\n**d**) how is FIS_LR computed? Is this not just the feature importance and since you are using ground-truth explanations from pre-trained LR, this should have 100% agreement and should be the best 8 number of times (currently in Table 1 it is best 0 times)?"
        ]
    },
    "v3XabZsB7j": {
        "venue": "ICLR 2025",
        "title": "CNN Variational autoencoders' reconstruction ability of long ECG signals",
        "link": "https://openreview.net/forum?id=v3XabZsB7j",
        "abstract": "Can variational auto-encoders (VAEs) generate flexible continuous latent space for long electrocardiogram (ECG) segments and reconstruct the input? A folded VAE architecture is introduced in this study which is able to encode long ECG segments by splitting an input segment into folds and process them in sequence using a narrow field-of-view in the encoder and concatenate them at the end, instead of processing the long segment at a time. The VAE decoder follows similar folding and concatenation strategy for reconstruction of the original ECG segments. The proposed folded VAE architecture is able to generate better reconstruction of long 30-second ECG segments compared to unfolded classical VAE approach which often produce trivial reconstruction of long ECG segments. Experimental results show that the latent representation generated by our folded VAE architecture not only retains rich compressed information but also aids designing interpretable models by providing decision-making insights.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- The paper's application area is an important problem of long-sequence reconstruction, which could enable the capture of essential clinical information in the latent space.",
            "- - Comprehensive Experimentation: The paper includes experiments with multiple datasets (MESA and MIT-BIH), demonstrating the generalizability of the proposed architecture across different signal sources and classification contexts.\n- Practical Applications: The application of this model to sleep stage classification is valuable and opens doors for future research on ECG-based sleep monitoring systems, which has relevance in healthcare.",
            "- 1. The problem of reconstructing long ECG sequence is interesting.\n\n2. The experiments were conducted on 2 datasets and consider not only reconstruction but also classification tasks for performance evaluation.",
            "- The question of whether variational auto-encoders can generate a flexible continuous latent space for long electrocardiogram (ECG) segments and reconstruct the input is interesting."
        ],
        "weaknesses": [
            "- 1. The paper's presentation is poor and does not meet the standards of ICLR or other relevant AI/ML venues. The authors are advised to proofread the paper carefully, as there are numerous grammatical errors, including missing commas, throughout the text, including in the abstract. Many of these errors could be easily fixed by using available grammar checkers, suggesting that the manuscript may not yet be ready for submission.\n2. The proposed method lacks novelty, as it primarily focuses on splitting signals in the input space.\n3. The results are not well-presented, making it unclear what the main contributions of this work are.",
            "- - Lack of Quantitative Metrics for Reconstruction Quality: The reconstruction results rely on visual analysis without quantitative metrics, which may lead to subjective conclusions.\n- Suboptimal Sleep Stage Classification Results: The model\u2019s classification accuracy (mean of 65% across subjects) is lower than other baseline methods.\n- Over-Reliance on Folding Technique Without Comparison to Alternative Approaches: While folding offers improved reconstruction, the authors do not compare this method against alternative architectures (e.g., hierarchical or multi-scale VAEs, or LSTM-based methods) that could also encode long signals effectively. \n- Clarity and Language: Some sections, particularly in the methodology, could benefit from improved clarity. For instance, equations used to describe the encoding and decoding of ECG segments could be elaborated on to better illustrate the folding approach.",
            "- 1. The premise of the study can be made more clear. Currently It is not clear what is the benefit of the presented method against simply using a sliding window on a long ECG sequence by reconstructing short sequences across the windows\n\n2. Some of the technical components can be better described. For instance, it is not clear what the summation means in equations (1-2) \u2014 does it really mean summation by averaging over the representation obtained from different folds? If yes, why? Similarly, it was not well justified why we can merge 30 8x4 features into 8x120 future map.\n\n3. The relation between the VAE described in 2.4 and the specific model described in 2.8 is not clear.\n\n4. There were baselines or comparative studies conducted evaluating the presented method with existing approaches to handle this, especially by simply learning to reconstruct for short sequences and apply to long sequences with a sliding window.",
            "- -\tRelated work section is missing completely. Can the authors comment on what work is out there that investigated similar problems as they do? How do for example the following works relate to the work proposed here (just to name a few):\n1. Comparison of Autoencoder Encodings for ECG Representation in Downstream Prediction Tasks. Christopher J. Harvey, Sumaiya Shomaji, Zijun Yao, Member, IEEE, Amit Noheria , 2024\n2. Multi-Domain Variational Autoencoders for Combined Modeling of MRI-Based Biventricular Anatomy and ECG-Based Cardiac Electrophysiology , Marcel Beetz, Abhirup Banerjee and Vicente Grau, Frontiers in Physiology, 2022\n3. Joint optimization of a \u03b2-VAE for ECG task-specific feature extraction. Viktor van der Valk, Douwe Atsma, Roderick Scherptong, and Marius Staring, arXiv 2023\n4. Feasibility of ECG Reconstruction From Minimal Lead Sets Using Convolutional Neural Networks, Maksymilian Matyschik; Henry Mauranen; Pietro Bonizzi; Jo\u00ebl Karel, IEEE 2020 Computing in Cardiology\n\n\nAs no related work is mentioned, also no baselines or comparisons are performed. I think the simplest comparison would be to take the vanilla VAE and take short sequences, and then concatenate. How does this compare to the proposed folded VAE?\n\nFurther the presentation of results is very poor. There are long vectors of numbers put into the text. Please create e.g. tables that show what method you use, what the result is for different architectures or modifications, and display this in a structured way."
        ]
    },
    "tKFZ53nerQ": {
        "venue": "ICLR 2025",
        "title": "Topic and Description Reasoning Generation based on User-Contributed Comments",
        "link": "https://openreview.net/forum?id=tKFZ53nerQ",
        "abstract": "We propose Topic and Description Reasoning Generation (TDRG), a text inference and generation method based on user-contributed comments with large language models (LLMs). Unlike summarization methods, TDRG can infer the topic according to comments contributed by different users, and generate a readable description that addresses the issue of the lack of interpretability in traditional topic modeling for text mining. In this paper, we adopted zero-shot and fine-tuning methods to generate topics and descriptions for comments. We use a human-annotated YouTube comment dataset to evaluate performance. Our results demonstrate that the potential of large language models of reasoning the topic and description. Generated topic titles and descriptions are similar to human references in textual semantics, but the words used are different from those of humans.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- * Contributes a prompt engineering approach to the task.\n* Contributes an automated evaluation using BLEU and ROUGE\n* Contributes additional experiments utilising video captions to see how performance varies.",
            "- Quality\nThe quality of the research is commendable, as it employs a rigorous methodology involving both zero-shot and fine-tuning techniques with large language models. The use of a human-annotated YouTube comment dataset for evaluation adds robustness to the study. The paper provides detailed experimental results, using metrics like BLEU and semantic textual similarity to substantiate its claims. However, the paper could benefit from a more extensive discussion on the limitations and potential biases in the dataset and model performance.\nClarity\nThe paper is generally clear in its presentation, with a well-structured format that guides the reader through the problem formulation, methodology, and results. The use of figures and tables to present experimental results enhances understanding. However, some sections, particularly those detailing the fine-tuning process and the impact of additional context, could be elaborated further to improve clarity. Additionally, a more explicit explanation of the principles guiding the TDRG method would be beneficial.",
            "- N/A",
            "- 1. The paper presents a novel framework that bridges the gap between topic modeling and text generation, addressing the topic interpretability through a structured approach that generates both concise topics and detailed descriptions.\n2. The research demonstrates the model effectiveness by comparing multiple LLM architectures (GPT-4o series and Llama) under different conditions (zero-shot, with captions, and fine-tuning)."
        ],
        "weaknesses": [
            "- * The task to be done is not clear.  The seperation and distinction between a topic and a description of a UGC video are not clearly defined nor examples shown.* I disagree that there is no suitable dataset for this task.  The dataset that is described by the authors are also hand annotated by the authors and preprocessed with settings that might be reasonable but seem quite arbitrary.  The reproducibility of the dataset by other researchers (in my opinion) would not be possible, given the level of description (for example, how is the cluster supposed to be done without any guidelines?).  Don't some form of user generated content (UGC) already come with descriptions?\n  * Related to this, what percentage of the dataset is excluded given such guidelines on preprocessing (I'm unsure a Gossiping forum necessarily would have most threads centred on named entities as the restriction criteria on L140 seems to support).\n* The findings of the work are unsurprising (larger models work better, closed models work better), but also fail to relate these findings to aspects of the task or the purported novelties of the paper.  Hence, there are little lessons/insights to be gleaned from reading the work as a reader.  \n* Space usage in the paper is not optimised for.  Figures 1 and 2 take up almost an entire page, yet yield very little accurate technical detail about the authors' methods.\n* There is a reliance on \"principles\" (magenta boxes in Fig. 1), but how these principles are applied and what form they take are unclear.\n* The result sections are mostly a hash on the result tables without much generalisation or discussion that can be linked back to the model or prompting strategy, so unfortunately (to this reader), are superficial and do not drive insight.",
            "- This paper primarily functions as a case study, focusing on the application of fine-tuned large language models (LLMs) to enhance the performance of the Topic and Description Reasoning Generation (TDRG) task. While the study provides valuable insights into the practical implementation and potential benefits of fine-tuning LLMs for this specific task, it does not introduce a significant degree of novelty in terms of theoretical advancements or groundbreaking methodologies.\nThe research builds upon existing techniques in natural language processing, particularly the use of LLMs, and applies them to a well-defined problem space. However, the paper does not present new algorithms or innovative approaches that substantially differentiate it from prior work in the field. Instead, it demonstrates how fine-tuning can be effectively utilized to improve task-specific outcomes, which, while useful, may not be considered a novel contribution in the broader context of NLP research.\nTo enhance the paper's impact and originality, the authors could explore integrating novel elements, such as developing new fine-tuning strategies, introducing innovative evaluation metrics, or applying the TDRG method to unexplored domains. Additionally, a deeper theoretical exploration of the underlying mechanisms that contribute to the observed improvements in TDRG performance could provide a more substantial contribution to the field. By addressing these aspects, the paper could transition from being a case study to offering more significant advancements in the understanding and application of LLMs in topic and description reasoning tasks.\nOther Weaknesses:\nLimited Dataset and Language Scope\nOne of the primary weaknesses of the paper is the limited scope of the dataset, which focuses solely on Chinese comments from a specific subset of YouTube channels. This narrow focus may limit the generalizability of the findings to other languages and cultural contexts. To improve, the authors could expand their dataset to include comments in multiple languages and from a more diverse range of YouTube channels. This would not only enhance the robustness of the results but also demonstrate the method's applicability across different linguistic and cultural settings.\nInsufficient Exploration of Model Limitations\nWhile the paper discusses the performance of different models, it lacks a thorough exploration of the limitations and potential biases inherent in these models. For instance, the paper notes that the Llama-3.1-8B model performs poorly compared to GPT-4o models, but it does not delve into the reasons behind this discrepancy. A more detailed analysis of the model's limitations, including potential biases in the training data or architectural constraints, would provide valuable insights. Additionally, discussing strategies to mitigate these limitations, such as incorporating more diverse training data or exploring alternative model architectures, would be beneficial.\nClarity in Methodological Details\nCertain methodological aspects, particularly the fine-tuning process and the role of additional context (e.g., video captions), are not sufficiently detailed. The paper could improve by providing a clearer explanation of the fine-tuning process, including the specific parameters and techniques used. Additionally, a more in-depth discussion on how video captions influence the model's output, supported by quantitative or qualitative analysis, would enhance the reader's understanding of the method's intricacies.\nEvaluation Metrics and Analysis\nThe paper primarily relies on BLEU scores and semantic textual similarity for evaluation, which may not fully capture the quality and interpretability of the generated topics and descriptions. Incorporating additional evaluation metrics, such as human judgment or user studies, could provide a more comprehensive assessment of the model's performance. Furthermore, a more detailed analysis of the results, including case studies or examples of generated topics and descriptions, would offer deeper insights into the model's strengths and weaknesses.",
            "- The paper lacks innovation and simply annotated a dataset, testing prompt based method and fine-tuning based method. The annotation details of the dataset were not provided, and there was no annotation consistency, so the quality of the dataset is questionable. The workload of the paper is also relatively small, more like a large course assignment.",
            "- 1. The study's focus on Chinese-language YouTube comments from only 25 YouTubers and 112 clusters raises concerns about the generalizability of the findings. A more diverse dataset incorporating multiple languages and platforms would strengthen the research's broader applicability.\n2. The paper lacks comparisons with traditional topic modeling approaches (e.g., LDA, BTM) and recent neural topic modeling methods, making it difficult to quantify the relative advancement of TDRG in the context of existing solutions.\n3. Only evaluating on BLEU scores and semantic similarity metrics may not fully capture the quality of generated topics and descriptions. The absence of human evaluation or task-specific metrics makes it difficult to evaluate the quality of the generated outputs.\n4. The fine-tuning process lacks crucial details about hyperparameters, training procedures, and computational requirements. This omission hampers reproducibility and makes it challenging for other researchers to build upon this work."
        ]
    },
    "sTQC4TeYo1": {
        "venue": "ICLR 2025",
        "title": "The GECo algorithm for Graph Neural Networks Explanation",
        "link": "https://openreview.net/forum?id=sTQC4TeYo1",
        "abstract": "Graph Neural Networks (GNNs) are powerful models that manage complex data sources and their interconnection links. One of GNNs' main drawbacks is their lack of interpretability, which limits their applicability in sensitive cases. In this paper, we introduce a new methodology involving graph communities to address the interpretability of graph classification problems. The proposal, called GECo (Graph Explanation by COmmunities), exploits the idea that a community, i.e., a subset of graph nodes densely connected, should play a crucial role in graph classification. This assumption is reasonable considering the message-passing mechanism, the core of GNNs. GECo analyzes the contribution to the classification result of the community graphs, building a mask that highlights graph-relevant structures. It first uses the trained GNN one wants to explain to classify the entire graph. Then, it detects the different communities; for each community, a smaller subgraph, including the community nodes\u2019 is created, and the trained GNN is run to see how likely the subgraph alone supports the predicted class. After evaluating all the subgraph communities, an average probability is calculated and set as a threshold. Finally, any subgraph community with a probability value higher than the threshold is assessed as necessary for the model's decision. The collection of these key communities is the basis for the final explanation since they allow the highlighting of the most relevant parts of the graph leading to the classification. GECo has been tested on GNN employing Graph Convolutional Networks layers, using six artificial and four real-world graph datasets. The six synthetic datasets were generated by adding some artificial motifs (e.g., house, cycle, etc.) to Erdos-Renyi and Barabasi-Albert graphs.  The real-world datasets contain molecule structures. Both categories of datasets are adopted in the experimental part of the state-of-the-art proposals for graph explainability. GECo has been compared with a random baseline explainer and four state-of-the-art approaches: PGExplainer, PGMExplainer, GNNExplainer, and SubgraphX. We chose these methods for their different strengths, specifically PGExplainer for its efficiency and generalization capability through a learned explanation model, PGMExplainer for its probabilistic approach based on causal graphs, GNNExplainer for its detailed subgraph and feature-level explanations, and SubgraphX for its theoretically grounded subgraph selection by Shapley values. These choices ensure a comprehensive evaluation of our approach against a wide range of robust techniques. We assessed GECo's performance using four evaluation criteria that leverage predicted and ground-truth explanations and use user-controlled parameters, such as the probability distribution obtained by the GNN. The results obtained by GECo consistently outperform state-of-the-art techniques across multiple metrics for synthetic and most real-world datasets. In addition, GECo is significantly faster than its competitors in terms of computational efficiency, making it an ideal solution for large-scale data analysis and practical applications. These strengths solidify GECo\u2019s role in generating accurate, efficient, and interpretable explanations in graph-based classification tasks.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- 1. GECo enhances the interpretability of GNNs by identifying the most influential subgraphs, providing a more intuitive and targeted explanation for classification results.\n\n2. GECo achieves significant explainability performance demonstrating effectiveness across both synthetic and real-world datasets.",
            "- 1. The paper is well-organized and clearly written.\n\n2. This paper introduces a novel community-based method for explaining GNNs, focusing on identifying key subgraphs rather than just individual nodes or edges.",
            "- 1.\tThe paper demonstrates that GECo outperforms existing methods in explainability across various datasets. It also highlights GECo's efficiency, offering faster computation times compared to other approaches.",
            "- - The problem of explainability in Machine Learning is important and highly relevant.\n- The proposed method is simple and straightforward.\n- The paper is  easy to read."
        ],
        "weaknesses": [
            "- 1. This paper does not adequately highlight the advantages it has over other explainability models. Specifically, it should analyze the limitations of existing explainability models mentioned in the Related Work section (such as GNNExplainer, PGExplainer, SubgraphX, and PGMExplainer) and convincingly argue the advantages and necessity of a community-based approach for explainability, based on these limitations.\n\n2. The approach in this study is straightforward and lacks novelty. In particular, using communities to generate explanations is an already known method [1]. Additionally, the overall methodology is very similar to studies that recognize motifs and predict their importance to produce explanations [2].\n\n- [1] Mart\u00ednez Mora, Andr\u00e9s, et al. \"Community-aware explanations in knowledge graphs with XP-GNN.\" bioRxiv (2024): 2024-01.\n- [2] Chen, Jialin, and Rex Ying. \"Tempme: Towards the explainability of temporal graph neural networks via motif discovery.\" Advances in Neural Information Processing Systems 36 (2023): 29005-29028.\n\n3. The proposed model explanation (Section 3.2) is less than one page in length, while the experimental section occupies most of the paper. However, the experimental settings largely replicate those of other studies, leaving little in terms of new insights.\n\n4. Experiments related to runtime performance are necessary.",
            "- 1. GECo uses Blondel et al.'s modularity optimization algorithm for community detection, which performs well on large sparse matrices. However, it does not discuss how different community detection algorithms might impact the explanation results, leading to a lack of robustness verification.\n\n2. GECo determines the threshold \ud835\udf0f by calculating the probability values of communities, using the mean or median as the threshold. However, this method may not be suitable for all cases, especially when the graph structure is uneven or community sizes vary. It is recommended to add experiments exploring adaptive adjustments of \ud835\udf0f in different situations.\n\n3. The baselines compared with GECo are not the latest methods. It would be useful to compare with some instance-level explanation models from the past two years.\n\n4. The paper uses fidelity-based metrics for evaluation. However, these metrics have limitations due to the OOD problem. Therefore, new metrics Fid_{\u03b11,+} and Fid_{\u03b12,-} [1] could be added to assess model fidelity.\n[1] Zheng, X., Shirani, F., Wang, T., Cheng, W., Chen, Z.,Chen, H., Wei, H., and Luo, D. Towards robust fidelity for evaluating explainability of graph neural networks. In ICLR, 2024.",
            "- 1. Lack of novelty compared to the conventional graph community method. The proposed explainability method is based on detecting contributory substructures, specifically graph communities. However, the authors do not clearly distinguish the novelty of this approach from conventional graph community-based methods. Since substructure discovery is a widely adopted strategy for explaining GNNs, a more detailed demonstration\u2014either theoretical or empirical\u2014of how the proposed method advances existing graph community approaches is necessary for this work. For example, the paper No.1, Aviyente, Selin, and Abdullah Karaaslanli. \"Explainability in Graph Data Science: Interpretability, replicability, and reproducibility of community detection.\" IEEE Signal Processing Magazine 39.4 (2022): 25-39., and paper No.2 Sangaiah, Arun Kumar, et al. \"Explainable AI in big data intelligence of community detection for digitalization e-healthcare services.\" Applied Soft Computing 136 (2023): 110119. present community-based methods to explain GNNs. A comparison and analysis between your proposed methods and previous ones can be included in this part.\n\n2. Lack of contribution. The abstract and introduction do not clearly articulate the motivation behind the proposed approach or highlight its novelty compared to existing methods. The contribution of using the graph community method for explainability should be well demonstrated in the paper, e.g., how is the proposed method different from the previous studies, and how does this study contribute to the graph community-based explainability methods?\n\n3. Weak and unclear presentation and writing through the paper: \n(1) The introduction contains loosely related details and lacks a clear explanation of the background, which may distract readers from the main focus of the work. This work presents a very brief summary of the study in the third paragraph, which doesn\u2019t give readers a clear sense of why using graph communities for explainability is effective. Therefore, a more detailed introduction of the proposed method is needed.\n(2) The related work section should be more systematically organized, ideally comparing the strengths and weaknesses of various GNN explainability methods. For example, an organized explainability method for graph explainability is preferred, e.g., following a widely-adopted classification of GNN explainability approaches such as perturbation-based methods, gradient-based, decomposition-based methods, etc., in the related work part will be preferred.\n(3) The tables and text on page 8 are not presented professionally or cohesively. More specifically, a more organized arrangement, including both text and table, is preferred.\n(4) Section \"3.2 The Proposed Methodology\" focuses solely on technical details without providing any analysis that would strengthen the theoretical foundation of the proposed approach. For example, providing more details about how the proposed approach detects optimal subgraphs can be helpful for readers to understand the principle behind this method, e.g., mathematical formulation to explain the subgraph detection process.\n\n4. Lack of clear visualization in Fig.1. The workflow in Figure 1 lacks clarity, particularly in how key components, such as the threshold value, are determined. The figure does not clearly illustrate the process, leaving important aspects of the methodology underexplained. A more detailed and explicit visual representation is needed to improve understanding. For example, how to perform graph community in step 2 and how to identify the most influential community can be visualized clearly\n\n5. Lack of sufficient reference. The references cited in this work are insufficient to comprehensively support the proposed methodology. Furthermore, a significant portion of the referenced works are not up-to-date, failing to incorporate the most recent advancements in the field. To strengthen the credibility and relevance of the study, the authors need to include more current and pertinent literature that reflects the latest developments in the area of research. For example, these papers are related to your study: the paper uses graph community for explainability, Aviyente, Selin, and Abdullah Karaaslanli. \"Explainability in Graph Data Science: Interpretability, replicability, and reproducibility of community detection.\" IEEE Signal Processing Magazine 39.4 (2022): 25-39, Sadler, Sophie, Derek Greene, and Daniel Archambault. \"Towards explainable community finding.\" Applied Network Science 7.1 (2022): 81., Mart\u00ednez Mora, Andr\u00e9s, et al. \"Community-aware explanations in knowledge graphs with XP-GNN.\" bioRxiv (2024): 2024-01.",
            "- - The authors did not mention or compare their work with the most recent studies in this area (e.g., [1], [2]).\n- A significant drawback of this method is its focus on communities. It is possible that only certain nodes within a community are important, but this approach may fail to identify those specific nodes.\n- The authors state, \"The algorithm is based on the hypothesis that a GNN learns to recognize specific structures in the input graph\" and claim that communities are these specific structures. However, they did not formally prove that this is indeed how GNNs learn, nor did they elaborate on or provide evidence for their claims.\n- An ablation study using different community detection algorithms is necessary to justify the choice of the one used in their method.\n- In Section 3.3, the explanation of datasets is too lengthy and could be partially moved to the appendix. Conversely, the method section is too brief and lacks formal mathematical details.\n- The font size in Figure 1 is too small, making it difficult to read.\n- The authors should consider incorporating sparsity as a metric and demonstrate how fidelity changes with different subgraph sizes, as the size of the subgraph plays an important role in the quality of the explanation.\n\n[1] Yaochen Xie, Sumeet Katariya, Xianfeng Tang, Edward Huang, Nikhil Rao, Karthik Subbian, and Shuiwang Ji. Task-agnostic graph explanations.\n[2] Jialin Chen, Rex Ying. TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery"
        ]
    },
    "p30YulvDbj": {
        "venue": "ICLR 2025",
        "title": "OPTIMIZED SINGLE EEG CHANNEL SELECTION FOR DETECTING MAJOR DEPRESSIVE DISORDER",
        "link": "https://openreview.net/forum?id=p30YulvDbj",
        "abstract": "Major depressive disorder (MDD) or depression is a chronic mental illness that significantly impacts individuals' well-being and is often diagnosed at advanced stages, increasing the risk of suicide. Current diagnostic practices, which rely heavily on subjective assessments and patient self-reports, are often hindered by challenges such as under-reporting and the failure to detect early, subtle symptoms. Early detection of MDD is crucial and requires monitoring vital signs in everyday living conditions. Electroencephalogram (EEG) is a valuable tool for monitoring brain activity, offering critical insights into MDD and its underlying neurological mechanisms. While traditional EEG systems typically involve multiple channels for recording, making them impractical for home-based monitoring, wearable sensors can effectively capture single-channel EEG data. However, generating meaningful features from this data poses challenges due to the need for specialized domain knowledge and significant computational power, which can hinder real-time processing. To address these issues, our study focuses on developing a deep learning model for the binary classification of MDD using single-channel EEG data. We focused on specific channels from various brain regions, including central (C3), frontal (Fp1), occipital (O1), temporal (T4), and parietal (P3). Our study found that the channels Fp1, C3, and O1 achieved an impressive accuracy of 88\\% when analyzed using a Convolutional Neural Network (CNN) with leave-one-subject-out cross-validation. Our study highlights the potential of utilizing single-channel EEG data for reliable MDD diagnosis, providing a less intrusive and more convenient wearable solution for mental health assessment.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- Simplicity: The authors put forward a simple model, whereby only one channel is used to perform the EEG recordings. This may decrease the complexity as well as the cost related to the acquisition of such data in clinical or practical settings.\n\nFocused Application: The concentrated nature of this work on the channels minimalism and application on MDD with mentioning of literature on how MA design becomes an issue while assessing patients makes sense.",
            "- 1) Applicability to Wearable Tech: Single-channel EEG setups have potential in practical applications, and the study aims to address this need without the need to go bald you can classify Major Depressive Disorder.\n2) Clear Writing: The methodology and CNN architecture design are explained concisely",
            "- Addressing the potentiality of using single channel EEG in real-time implementation.",
            "- None."
        ],
        "weaknesses": [
            "- Lack of novelty: There are no original components both in methodology and application. The employment of a single EEG channel does not come as exciting or new, and neither is there any evidence of improvements or breakthroughs in model structure or methodology that would enhance the discipline.\n\nNo comparative analysis: The study does not provide any analyses on other EEG-based MDD detection systems that are even more advanced in deep learning or more fundamental based on machine learning algorithms. Such a comparative analysis would enable positioning of the proposed MDD approach within the competitive scientific market and its merits or demerits would be illustrated.\n\nLimited dataset: The finding cannot be said to be fully conclusive since only one dataset was employed. Testing the model against several datasets would add weight to the evidence of the model application in different settings and sociocultural characteristics of the population.\n\nInsufficient performance gains: The paper does not exhibit any significant progresses relative to the three computed parameters such as the model accuracy, computation time, and clinical importance. In the absence of these parameters, the proposed approach does not provide credible case for progress in the field or major contributions in the area of MDD detection.",
            "- The study\u2019s impact is limited by its single dataset evaluation and absence of critical classification metrics :\n1) Omission of Key Metrics: Essential performance metrics (F1-score, precision, recall and AUC-ROC curves) are missing, reducing evaluation comprehensiveness.\n2) Underperformance: The model\u2019s 88% accuracy is lower than Bachmann et al. (2018), which achieved 92% with a classical machine learning approach on channel Pz. And what are further advamatges of \n3) Single Dataset Limitation: Evaluating on only one dataset limits generalizability, especially when other datasets like MODMA are available.\n4) Lack of Comparison: Similar single-channel or CNN-based approaches, such as DOI:10.1142/S0219622019500342 and https://doi.org/10.1177/1550059420916634, could serve as useful baselines but are not compared.",
            "- 1. Lack of novelty!! In specific authors doesn't propose application dedicated features or tailored network configuration. Simply utilised the deep learning models to classify. \n2. Recent publications proved that the understanding the underlying connectivity among the channels can give the gain in depression prediction efficacy. For instance (\"Delaunay Triangulated Simplicial Complex Generation for EEG Signal Classification\", \"GM-VRC: Semantic Topological Data Ensemble Approach for EEG Signal Classification\", \"Chromatic Alpha Complex Generation for EEG Signal Classification\"). \n3. Due to the high non-stationarity of the EEG signal behaviour, single channel utilization in neurological/ mental disorder identification is an uncertain task.\n4. Presented manuscript lacks the presentation skills of the results obtained.\n5. State of arts comparison with the irrelevant or out-dated articles, can be updated with the latest and advanced techniques of depression detection.\n6. Can improve the presentation quality of the manuscript.",
            "- 1. None of the techniques proposed for single-channel selection:\nThe paper emphasizes the importance of single-channel selection from the title itself, yet it does not propose any method for selecting the channels. Instead, the authors used predefined channels and measured performance based on those. It is surprising that the paper does not consider commonly used methods for channel selection such as Hyperparameter Optimization (HPO), Layer-wise Relevance Propagation (LRP), or attention maps.\n2. Lack of novelty of the model:\nThe paper uses a very generic CNN structure (3 layers of CNNs) without any optimization techniques specifically targeting single channels. \n3. Weak Results section:\nThere are no competing methods presented in the results. Where are the competing methods mentioned in the Introduction? \n4. Meaningless performance listing in the Introduction:\nIn the Introduction, the authors list various competing methods and their accuracies, but this serves no meaningful purpose. Simply listing accuracies from different papers applied to different datasets is not insightful. \n5. Limited usage of the dataset (58 samples)"
        ]
    },
    "koza5fePTs": {
        "venue": "ICLR 2025",
        "title": "Exploring and Benchmarking  Planning Capabilities of  Large Language Models",
        "link": "https://openreview.net/forum?id=koza5fePTs",
        "abstract": "Classical and natural language planning tasks remain a difficult domain for modern large language models (LLMs). In this work, we lay the foundations for improving planning capabilities of LLMs.\nFirst, we construct a comprehensive benchmark suite encompassing both classical planning benchmarks and natural language scenarios. \nThis suite includes algorithms to methodically generate instances of tasks with varying levels of difficulty, allowing for rigorous and systematic evaluation of LLM performance. \nNext, we investigate the use of many-shot in-context learning to enhance LLM planning, exploring the  relationship between increased context length and improved planning performance. In addition, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths. We also probe the efficacy of chain-of-thought reasoning methods to improve LLM planning performance.\nMoreover, we probe the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges. Finally, we investigate model's failure modes and reveal insights that hold true across different benchmarks.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- 1) The paper presents a consolidated set of results on two recently popular natural language planning benchmarks, along with additionally generated problems in PDDL planning domains.\n2) This is the first work that systematically analyzes LLM planning capabilities with fine-tuning on optimal solutions, and if fine-tuning is effective with respect to generalizing across different problem categories under a domain (for example, training on PDDL blocksworld problems with 3-7 blocks and testing with PDDL problems of 8-9 blocks, and so on).",
            "- 1. A new NL planning benchmark dataset.",
            "- In terms of originality, there are similar benchmarks such as PlanBench available in the literature.  The authors argue that the paper also considers many-shot in-context learning settings, as shown in Figure 2.  The number of few-shot exemplars ranges from 1 to 400.\nFigure 4 shows results from three inference-time ICL methods: ToT, MCTS, and Debate-as-reasoning in non-PDDL-based domains.\nLastly, this paper also shows the experiment results from the fine-tuning setting.",
            "- $Originality:$ This paper runs large-scale experiments on environments explored by the LLM planning community using new models.\n\n$Quality:$ This paper has plenty of results that substantiate past findings.\n\n$Clarity:$ The paper is written very clearly.\n\n$Significance:$ Some of the experiments may suggest that newer models deviate from past findings (see questions)."
        ],
        "weaknesses": [
            "- 1) Novelty - One of the biggest contributions that the work claims is of the planning dataset. However, the paper only combines existing PDDL planning dataset (PlanBench) and travel planning domains, with the addition of some newly generated test problems. This seems to be more like packaging the two domains together than presenting a new dataset in a paper. Can the authors provide more details to counter this argument - such as - what % of problems are new in this dataset that are not present in the two existing datasets?\n2) Unjustified evaluations for improved performance - It seems that the paper aims to establish that LLMs perform exceptionally well in planning domains, but at the cost of prompt engineering (line 73 - authors say that ICL methods show boost in performance with 'carefully instructing the model') and cherry-picking evaluation data (line 89 - authors say that 'data curation during training' can be helpful to avoid biases thereby helping LLMs). Why do we have to give all the benefit of the doubt to the LLMs at the expense of human effort during prompting and data processing? It is worthwhile noting the challenges that LLMs face (using failure mode analysis) without such human efforts, as they can be constructively useful for further improving their planning capabilities.\n3) Fine-tuning results - The authors show near-perfect accuracy for SFT results in Tables 3 and 4, but only in the cases where the train and the eval data consisted of the similar problem cases. This is a classic over-fitting example, and thus, does not lead to any conclusions regarding the so-called 'generalization' abilities of LLMs.",
            "- 1. Did not find the paper to have novel results (please refer to questions)\n2. No details on how the plans are being generated using FD (which heuristic is being used to generate optimal plans?)\n3. The NL translated prompt does not include PDDL domain, making me question how any LLM is supposed to know all actions, preconditions, and effects for a given domain?\n4. Not a complete coverage of literature on plan generation. Especially, authors in [1] introduce thought of search which steers LLMs to solve 100% planning problems. [2] already provided similar conclusions on SFT using even smaller parameter models and also talked about multiple plan generalization issues.\n\n[1] Katz, M., Kokel, H., Srinivas, K., & Sohrabi, S. (2024). Thought of Search: Planning with Language Models Through The Lens of Efficiency. In The First Workshop on System-2 Reasoning at Scale, NeurIPS'24.\n\n[2] Pallagani, V., Muppasani, B., Murugesan, K., Rossi, F., Srivastava, B., Horesh, L., ... & Loreggia, A. (2023). Understanding the capabilities of large language models for automated planning. arXiv preprint arXiv:2305.16151.",
            "- On the coverage of classical planning domains.\nThis paper argues that the proposed work lays the foundation for improving the planning capabilities of LLMs by presenting three classical planning domains. From a classical planning viewpoint, those domains were more than 20 years old and they don't have many features in PDDL and only have almost the minimal subset of the language features. Namely, it is almost STRIPS with a negative precondition. If we view the direction of translating PDDL to NL, the work should consider the coverage of the translation of the language.\n\nOn the quality of translations.\nThe paper mentioned that the translation is done by regular expressions. LLMs are supposed to be trained in human-written texts and natural languages. However, the translations are done by hand-crafted regular expressions that translate formal PDDL descriptions. It is doubtful how such a dataset would enhance the reasoning capability of LLMs.\nIf we consider the LLM as a symbolic reasoning engine like a planner such as fast-downward, the scale of problems is so tiny that it is doubtful how the proposed methods will be useful.",
            "- The main issue with this paper is that it has very little novelty to contribute to the LLM planning community since it reproduces many results and echoes insights that have been communicated in other papers. There is a contrast in narrative to existing work that claims LLMs cannot plan using base models with no inference time techniques [1, 2]; however, plenty of work exists that individually investigate and demonstrate that prompting with examples boosts performance [3, 4], inference-time prompting techniques boosts performance [5, 6, 7, 8], finetuning boosts performance [9, 10, 11], and do similar failure analyses [2, Figure 5].\n\nThe conclusion ends with a quick note on 3 key findings regarding plan generalization: (1) the superiority of SFT (2) curriculum learning effectiveness and (3) limitations of hard example training. These points have already been brought up in [12] that uses SFT to tackle plan generalization but uses a better approach of process supervision [13].\n\nIn addition, while there are 5 datasets total experimented on in this paper, the experiments focus mainly on Blocksworld and Logistics which has already been focused on in [2], with TravelPlan and CalendarPlan only being used for many-shot and inference-time experiments and MiniGrid only being used for the many-shot experiment. It is unclear why these 5 datasets are introduced as part of a benchmark suite but only subsets are experimented on.\n\nOverall, this paper reads primarily as a summary of well-investigated trends in the LLM planning community. To strengthen this paper, it would be useful to propose an approach atop the experiment findings to add novelty to the paper and further support the insights (for example, the balanced approach incorporating easy and hard examples in the conclusion). Since it would require non-trivial rewriting to improve this paper, I have suggested a rating of reject.\n\n[1] On the planning abilities of large language models-a critical investigation (Valmeekam et. al 2023)\n\n[2] Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change (Valmeekam et. al 2023)\n\n[3] Language Models are Few-Shot Learners (Brown et. al 2020)\n\n[4] Many-shot in-context learning (Agarwal et. al 2024)\n\n[5] ReAct: Synergizing Reasoning and Acting in Language Models (Yao et. al 2022)\n\n[6] Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Yao et. al 2023)\n\n[7] Graph of Thoughts: Solving Elaborate Problems with Large Language Models (Besta et. al 2023)\n\n[8] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models (Zhou et. al 2023)\n\n[9] FireAct: Toward Language Agent Fine-tuning (Chen et. al 2023)\n\n[10] Plansformer: Generating Symbolic Plans using Transformers (Pallagani et. al 2022)\n\n[11] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks (Kambhampati et. al 2024)\n\n[12] Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping (Lehnert et. al 2024)\n\n[13] Let's Verify Step by Step (Lightman et. al 2023)"
        ]
    },
    "hMjUnF3aQ8": {
        "venue": "ICLR 2025",
        "title": "SQT -- rough conservative actor critic",
        "link": "https://openreview.net/forum?id=hMjUnF3aQ8",
        "abstract": "Std $Q$-target is a conservative actor critic ensemble based $Q$-learning algorithm which based on a single key $Q$-formula--$Q$-networks standard deviation, an uncertainty penalty. A minimalistic solution to the problem of overestimation bias. We implement SQT on top of actor critic and test it against the SOTA actor critic algorithms on popular MuJoCo tasks. SQT shows a clear performance advantage over TD3, SAC and TD7 on the tested tasks majority.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- - The paper is relatively clear to understand.",
            "- **originality**\n- Although the proposed approach is very simple, the authors show it can lead to strong performance improvement when used properly \n\n**quality**\n- overall good quality\n- Simplicity: I appreciate the simplicity of the proposed approach. \n\n**clarity**\n- Overall paper is clear, easy to follow\n\n**significance**\n- strong empirical results \n- Table 2 shows for the three baseline algorithms, adding SQT will help performance consistently.",
            "- 1.\tThis paper improves the performance of the TD7/TD3/SAC algorithms through its SQT method.",
            "- 1. Simple nature of modification\n2. Some theoretical analysis\n3. Improved empirical performance"
        ],
        "weaknesses": [
            "- - This exact idea has already been published before: *\"Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters\"* (Ghasemipour et al, NeurIPS 2022). See its *Shared Targets (Method 2)*.\n- Experimental results are extremely weak.",
            "- - Most important: Lack of in-depth analysis of the effect of the proposed approach on bias reduction. The only thing shown in the paper is the performance, give the proposed change is quite simple, the paper can benefit from more related experiments to help understand the issue better. For example, one way to do this is to show how the amount of bias changes for different algorithms, you can get the Q predictions and estimate the true Q values with MC return (as done in the REDQ paper you cited), or you can look at how Double Q learning paper (\"Deep Reinforcement Learning with Double Q-learning\") did it. \n- Lack of technical details: there is no adequate discussion of hyperparameters and hyperparameter sensitivity, and other details such as how much computation the method requires compared to the baselines? \n- There are other algorithms that utilize the uncertainty of Q ensembles to help reduce bias, such as TQC (\"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics\") and REDQ (\"Randomized Ensembled Double Q-Learning: Learning Fast Without a Model\"), how does the proposed approach compare to these methods? \n- Since this paper focuses on empirical results, the paper can be made stronger by providing more extensive results, such as comparing to more baselines and test on more benchmarks (e.g. other MuJoCo environments, deep mind control, Atari). \n\nMinor issues: \n- Line 309 you may want to introduce what UTD stands for here.",
            "- 1. The contributions of this paper are insufficient. The primary contribution is simply the addition of the standard deviation of ensemble networks to the Q-target values. The authors should consider further how the experimental effect differs for different penalty terms, or give an explanation for why the standard deviation is used as a penalty term, and not some other difference, e.g. variance. In addition, the authors need to conduct ablation experiments on the hyperparameter in front of the penalty term to study the effects of different hyperparameters on the test results.\n2. The use of ensemble Q estimators is not particularly novel and the authors did not cite the relevant papers in Background section, such as some related ensemble learning methods.\n3. The citation format in the paper is inconsistent, and the use of abbreviations and table formatting are irregular, affecting the paper's readability and adherence to standards. Such as Eq. (10) first appears in the pseudocode without any explanation in the text. It is recommended to provide an explanation when the formula is introduced for the first time. Additionally, when abbreviations such as 'RL' and 'MDP' appear for the first time, the full terms also should be provided.\n4. The experiments in the paper only compare a few benchmark algorithms and do not include more recent reinforcement learning algorithms. It is recommended to include additional benchmark comparisons and statistical analyses, such as DreamerV2 and REM.",
            "- 1. I didn't like the way the paper is structured. The method is described in section 8 while most of the results are presented  before. I could not understand what is the proposed approach until the page 8. I strongly recommend authors to put the description of their approach (what is SQT operator) in section 4.\n2. Unclear motivation and description of the main approach. In equation 20 authors demonstrate the formula for their main approach, if I understand correctly. However what are s' and a' under the **mean** part? Those are supposed to be the next state and action in the other part of equation. Is it just a random set of states and actions from the replay buffer? If so, I do not see the motivation behind the schema. I fish authors clarify and motivate it better. I.e. why random set of state-actions is useful for learning Q(s, a) for a particular (s, a) pair? In case of SQT_{tuple} I have no questions as we use only the following pair.\n3. Questionable claim about the superiority of SQT when compared to SQT_{tuple}. It outperforms only on one task out of four. It looks very suspicious and the claim \" This highlights the advantage of SQT\u2019s global penalty mechanism, which leverages the wisdom of the crowd principle by applying a single penalty based on the batch standard deviation across all state-action pairs\" seems too strong to me. I consider it is as critical to show on more tasks, otherwise the efficiency is very questionable.  \n4. Limited evaluation. I'm not strongly aware about environments which are used for online RL approaches now but at least there should be a Hopper MuJoCo environment. Are there any more environments where authors can demonstrate that SQT is better than SQT_{tuple}?\n5. No statistical significance is reported. There are only mean scores in the tables and no learning curves.\n6. No offline RL results which could be very relevant here."
        ]
    },
    "gpKEDj9Dgg": {
        "venue": "ICLR 2025",
        "title": "Optimizing Large Language Models with Automatic Speech Recognition for Medication Corpus in Low-Resource Healthcare Settings.",
        "link": "https://openreview.net/forum?id=gpKEDj9Dgg",
        "abstract": "Automatic Speech Recognition (ASR) systems, while effective in general contexts, often face challenges in low-resource settings, especially in specialized domains such as healthcare. This study investigates the integration of Large Language Models (LLMs) with ASR systems to improve transcription accuracy in such environments. Focusing on medication-related conversations in healthcare, we fine-tuned the Whisper-Large ASR model on a custom dataset, Pharma-Speak, and applied the LLaMA 3 model for second-pass rescoring to correct ASR output errors. To achieve efficient fine-tuning without altering the full LLM parameters, we employed Low-Rank Adaptation (LoRA), which enables re-ranking of the ASR\u2019s N-best hypotheses while retaining the LLM's original knowledge. \nOur results demonstrate a significant reduction in Word Error Rate (WER) across multiple epochs, validating the effectiveness of the LLM-based rescoring method. The integration of LLMs in this framework shows potential for overcoming the limitations posed by conventional ASR models in low-resource settings. While computational constraints and the inherent strength of Whisper-Large presented some limitations, our approach lays the groundwork for further exploration of domain-specific ASR enhancements using LLMs, particularly in healthcare applications.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            5,
            1
        ],
        "strengths": [
            "- The paper presents a relevant use case and a potentially viable technical solution to the challenges that one may face in such use case. However, the arguments posed in the paper were not sufficiently validated through experimental results and/or analysis.",
            "- The authors described their experiments clearly.",
            "- The paper describes the mechanism of adapting Medical names in LLMs so errors from ASR can be corrected",
            "- The idea is an simple and efficient way to build a system for a specific use case and potentially accelerate and decrease the cost and availability of automatic speech recognition tools on a sector as important for society as helathcare."
        ],
        "weaknesses": [
            "- The paper is largely incomplete. Sections 4 and 5 only contain placeholder texts with no detailed information about the data used, the modeling investigation performed, the results obtained and the analysis of the findings from the experiments. The paper also contain several typographic errors and strongly urge the authors to proof read the paper.\n(1) Section 3, can benefit from having come detailed content on the dataset that was used. Given the low-resource nature of the target domain, it will be useful to specify how the data was collected, is the data publicly available? Providing some analysis of the data will also be useful, such as how much data was used (possibly in hours), some statistics on the number of speakers, gender, named-entities.\n(2) Some insight on how the modeling was performed is highly desired, for example what was the train-valid-test splits. What metrics were used to evaluate model performance. How were the model parameters tuned.\n(3) Section 4 does not specify any baseline system. Without a proper baseline system it is quite difficult to assess the merit of the proposed system. Comparing the proposed system against a baseline system usually helps to ascertain how useful the proposed system is, and whether the results presented significant.\n(4) Conclusion in section 5 should share the lessons learned from the investigation presented in the paper, providing some future directions, or room for improvement.\n(5) Typing errors: (a) section 1, page 1, line 044: \"medical settings1\" >> not clear what the 1 stands for, is it suppose to be a reference? (b) section 3, page 3, line 123, \"low-rank decomposition matrices 1\" >> not clear what the 1 stands for here as well.",
            "- The submission lacks originality and provides no basis for broader conclusions. The experiments conducted are simplistic and merely involve applying existing models to a customized dataset.",
            "- The paper itself has meagre details provided. Several limitations listed w.r.t data used by other papers needing high quality data, but the results presented in paper are also limited - dataset used in not holistic, as there are numerous other drugs that could not be captured, the chemical names are used instead of English words. There are no benchmarking evaluation and comparative study.\n\nThe paper doesn't talk about several other references which have used LLM for rescoring of ASR n-best hypothesis and how its approach is different\n\nThere is no visibly novel aspect in terms of approach except the domain in which this is applied seems under explored\n\n\nGiven the fact that there is no novel aspect reported in paper and insufficient benchmark, updated my review.",
            "- The document is brief and lack an exhaustive survey in the ASR space including other alternatives as finetuning LLMs for sumarization, rephrasing, plus alternative techniques to LoRA.\n\nThe domain and database is not described in detailed and the discussion and comparison with other techniques is not present. The statements are bold but they even don't demonstrate across multiple languages.\n\nI believe authors need to dedicate more effort in improving the document  and make it more reproducible."
        ]
    },
    "f7VXdQTbyW": {
        "venue": "ICLR 2025",
        "title": "ThreadsGAN: Enhancing Coherence and Diversity in Discussion Thread Generation",
        "link": "https://openreview.net/forum?id=f7VXdQTbyW",
        "abstract": "Current research on generating discussion threads faces challenges in coherence, interactivity, and multi-topic handling, which are crucial for meaningful responses. This paper introduces threadsGAN, a model that enhances thread generation by incorporating multi-topic and social response intention tags. By leveraging BERT and Transformer, threadsGAN ensures contextual coherence and manages topic consistency. Additionally, it employs conditional generation to align responses with specific discussion contexts, and its CNN-based discriminator assesses response quality by evaluating similarity between generated and real responses, improving overall performance in generating realistic and contextually appropriate discussion threads.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- * The work examines an interesting application of discussion thread generation.\n* The model uses relatively lightweight models for the characterisation (BERT) and generation, incorporating a CNN layer.\n* The work performs a human evaluation of the model and baselines.",
            "- N/A",
            "- The paper proposes a new customized method for a special task.",
            "- 1. ThreadsGAN integrates GAN with topic and response tags to enhance discussion thread coherence and diversity. It is novel to use GAN as a text generation model. \n2. The paper collected dataset from PTT website, which is across multiple subjects and has a large size of threads. This data is useful to train and evaluate models on thread generation task."
        ],
        "weaknesses": [
            "- * The related work section needs more contextualisation before getting into the specifics of individual works.  It is not clear of the organisational structure and overall relevances of which works were reviewed and why. How they are organised and presented is crucially missing -- e.g., the GAN (S2.2) section is a mismatch of single sentence summaries that do not present a coherent argument for understanding the prior work and how it motivates the current submission.  \n* The provenance of the data (source platform and actual chosen topics) was not clear from (L140-161).  The quality guidelines and how interannotator agreement on the application of the guidelines was not elaborated on, so the veracity and quality of the data are suspect.\n* There are design choices that are not well defended.  The authors decide to use BERT as part of their architecture, but choose to use a decoder-only transfomer architecture for generation, and the more natural choice of using a compatible embedding model that would be congruent with the decoder backend, was not defended.  I would need a more strong motivation and defense of the design choice. \n  * Similarly, the use of a CNN for thread generation is not defended.  The work is generating a sequence of tokens, so it's not clear why a patch oriented model (such as a CNN) is used.  \n* The dataset is somewhat limited and exactly why the authors applied the work only to this particular domain and discussion board was not clear.  Fig. 1 further shows 4 of the 10 boards as having fewer than 10 viable posts, so it's not clear that the study's output generalises well.\n* The descriptions of the baselines are insufficient to allow reproducibility.\n* The ThreadGAN model does not perform well against commercial methods (GPT-4) or its basic baseline (SeqGAN), which would need to be demonstrated for publication.\n* The human evaluation does not discuss inter-annotator agreement, which is a concern.  Without this aspect the quality of the evaluation is not assured, and the veracity and seriousness of the human evaluation is undermined.",
            "- The quality of the paper is very poor. The logical coherence between the paragraphs of the paper is poor and does not conform to the conventional writing style of a paper. The method proposed in the paper lacks any innovation. The experimental data in the paper also shows that the proposed method has poor performance.",
            "- - The proposed method is not sound, based on my understanding.\n\nAlthough the high-level idea of using GAN to develop a thread generation model is fine, the model details seem to have some issues. Specifically, the encoder takes in the next response as input (formula 3.2.3), but the decode generates the next response (formula 3.2.9), that is, the output is given as the input. Isn\u2019t this a problem? I might have misunderstood the model, but the description of the model is not clear enough. \n\n- The presentation needs improvement.\n\nAs mentioned above, the description of the method is not clear enough to resolve the main question I have about the validity of the generation process. Other than the method section, the other sections also lack important details and are sometimes not clear. For example, how are LLMs used to generate the threads? Do you use a prompt and if so, how does the prompt look like? \nMost importantly, while the introduction section discusses several challenges for discussion thread generation, such as ensuring authenticity and coherence of the generated posts and ability to adapt to diverse topics, it is not clearly explained why the proposed method addresses these challenges.\n\n- Lack of motivation.\n\nThe task studied is not well motivated. The proposed solution is not linked to the identified challenges and therefore is not well motivated, either.\n\n- The work has limited contribution. \n\nFirst, it is not clear to me why discussion thread generation is important and what its applications are. Although the authors try to motivate the work in the introduction section, there are very limited references supporting the claims that discussion thread generation is important and that much work has been done on this problem. The few cited papers are also old and do not represent the state-of-the-art solutions to text generation problems. The proposed method itself has some issues as pointed out above in the soundness section. In addition, the results do not show advantage of the proposed method over LLMs, which are now widely adopted for many tasks. Therefore, it is hard to imagine what other researchers can learn from this work.",
            "- Overall this paper needs a major revision. I list my comments below:\n1. Weak motivation. I am not convinced by the paper why thread generation is an important task to tackle with. The paper did not cite any related work in thread generation (Line 35), nor explained the usability or necessity for downstream application. How thread generation is different from conversation generation, or question answering? Why previous works cannot solve this task?\n2. Lacks clarity in methodology. The paper did not formally explain the task - what is expected input and output. The authors explained the model's input in Section 3.2.1, but without a formal definition of the task it was hard to understand. For example, the paper mentioned 'year' the first time in 3.2.1, which wasn't mentioned before, and the role of year is not explained. \n3. Missing explanation of model design. The proposed model has a lot of components, like BERT, CNN and GAN. There are a lot of alternatives that are generally better than BERT (RoBERTA) and CNN (LSTM), but why using these models as modules are not explained nor compared. \n4. Weak performance: Both automatic and human evaluation showed that the proposed model is worse than GPT4, and even SeqGAN in some criteria. It is not explained why the proposed method better (if not in performance, then in cost, explanability...), or takeaways of using GAN as a generation model. Furthermore, the baselines are too weak to compare. The paper used unprompted, off-the-shelf GPT-4 (since the paper did not mention any adjustment), which I think will perform stronger with a well-designed prompting or SFT."
        ]
    },
    "a8XwgTZzE0": {
        "venue": "ICLR 2025",
        "title": "Reconstruct the Understanding of Grokking through Dynamical Systems",
        "link": "https://openreview.net/forum?id=a8XwgTZzE0",
        "abstract": "\\textbf{Grokking}, or the \\textbf{delayed generalization phenomenon}, describes the abrupt and rapid improvement in test accuracy that occurs after a model has been overfitted for a prolonged period. This phenomenon was first identified by Power in the context of operations on a prime number field. Over the past two years, a range of mathematical analyses has been conducted to investigate grokking, typically involving the use of the hidden progress measure which mean a function that can anticipate the occurrence of grokking. We believe that a comprehensive and rigorous mathematical modeling approach can invigorate the research on this task and provide a unified perspective for understanding previous research. This paper introduces a novel approach by modeling the task as a unique dynamical system. Using mathematical derivation within this framework, we propose a robust hidden progress measure that effectively captures the grokking phenomenon across all operations on prime number fields. This approach not only provides a more complete understanding but also offers deeper insights into the underlying architecture of the model. Based on this understanding, we also proposed a method to accelerate grokking without involving regularization or altering the model architecture.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            5,
            1
        ],
        "strengths": [
            "- The topic of delayed generalization is interesting and suitable to the ICLR audience.",
            "- Viewing gradient descent updates to neural network parameters as the evolution of a dynamical system is an interesting (but not novel) perspective. Trying to relate generalisation phenomena to properties of this system is a cool approach. The paper seems to be comprised of two parts. The first is a 'theoretical' component, where the authors make some deductions regarding the property of the dynamical system, and the second is a discussion of their \"Main embedding diff\", a measure of to track grokking.",
            "- The proposed idea of studying grokking as emergent behavior from a dynamical system is in itself very interesting, and formal guarantees that could be drawn from such an analysis would greatly enhance our theoretical understanding of the phenomenon.",
            "- The study of grokking is very relevant and the idea of using dynamical systems theory to study it is interesting. The reduction of the grokking (Figure 7) is promising, and the new progress measure could be of interest to the field."
        ],
        "weaknesses": [
            "- I found this manuscript very difficult to follow and understand. There are many typos and the ideas are not clearly expressed. To give just a single example, Theorem 4 seems to contain no falsifiable statement. This is not to say there are no valuable ideas in the work, but as it stands I believe the work is well below the standards of publication for ICLR.",
            "- I found this paper challenging to read. The language is unclear and the figures are repetitive and not particularly informative. While I understand the concept of modeling neural network updates as an ODE, it is not clear what is the exact dynamical system described in this paper. The lack of clarity is endemic to this work, as in all main equations in this paper (1-3, 4-6, and 7-9), it is left unexplained what their different components are. I believe that W & U are neural network parameters, but what are x, e, y, gamma, s, c, u, and w? The authors should make abundantly clear what are the roles of the different parameters in the system they model.\nAll this makes the \"theoretical\" part of this paper extremely hard to follow. While I could vaguely figure out the authors are attempting to make a claim regarding the existence of stable points relating to the neural network parameters, it is not obvious what that claim is, how they arrive at it, and why it is in any way relevant to grokking. The authors need to do a better job at explaining these extremely critical details.\n\nIn the second part, the authors introduce \"Main embedding diff\" (MED), a way of tracking grokking by measuring discrepancy between embeddings within a model. There seems to be no relationship between the first section and this one, and the paper is unclear as to what MED is, as the components laid out in eq 12 are not explained to the reader. The results consist of repetitions of the same figure presenting the grokking phenomena along with MED values during training for different model and task settings. Aside from MED being vaguely correlated with the grokking phenomena, it is not explained how it precisely relates to it. Furthermore, the paper only presents results for a single task (modular addition), while the original grokking work discusses several types of problems. \n\nOverall, while the paper attempts to make theoretical contributions to understanding grokking, its unclear presentation, disconnected sections and weak results make it difficult to assess its actual scientific merit or practical value.",
            "- It is unclear how generalizable the analysis can be to arbitrary tasks, since it is applied to a very specific problem (even though the problem is, in some sense, canonical with respect to this phenomenon).\n\nThe manuscript offers a good formal description of the properties of the simple dynamical system. However, the connection between this projected system and its high-dimensional counterpart is not sufficiently motivated. This kind of discussion should be expanded.\n\nSome minor concerns: \nThere are several small typos throughout the text.\nDefinition 2 is not clear and should be expanded\nThe statement \u201cIf the model have learnt\u201d should be clarified. What does learning mean in a mathematical sense?",
            "- There are several major weaknesses of this paper: \n\n1. It was unclear to me what the embedding described in Section 3 is. Because this was unclear, much of the rest of the work was unclear. I would strongly recommend spending time making this section as clear as possible. In particular, \n     a. Figure 2 says that $p$ symbols are used to represent the range $0$ to $p - 1$. What does this mean? Are these the embeddings, $\\textbf{p}$. Is $p$ the same as the prime $p$ used in modular arthimetic?\n      b. What is $c$ and $\\gamma$? From context, I would assume that $c$ is the target output and $\\gamma$ is the error, but this was never defined. What size is $c$?\n      c. When is logit used? \n\n2. The establishment of the model was also confusing. In particular, explaining what the sums in Eqs. 4-5 are over (and why there are sums) would be helpful in understanding what the authors did. \n\n3. The authors make several assumptions that are not justified or motivated. For instance, Assumption 1 and 2 come out of nowhere. Similarly, saying $\\alphda_d$ is \"guessed to be approximately a linear function in specific situations\" is very vague. \n\n4. The main result (Theorem 4) is not stated mathematically and is vague. It was not clear what was really being gained from the theorem, nor was it clear how this was proven.\n\n5. The progress measure (Eq. 12) is defined to be a function of $n$ but $n$ is not present in any of the equations. This makes it unclear what Pro 1 is saying. \n\n6. \"According to our theory, another factor that influences the rate of grokking is the size of d_model\". It is not clear to me where the size of d_model comes in to the results presented in the main text. \n\n7. The authors claim that increasing the size of the dataset affects the rate of grokking. But then they choose to increase $p$ and not $\\beta$. The rationale for this was not clear. Additionally, given the bounds given in Sec, 5.2.2. $\\beta$ and $p$ are in the numerator. Therefore, increasing either leads to greater norm, which I thought meant longer time to achive high test accuracy. \n\n8. The mitigation of grokking (Figure 7) is interesting, but it is not clear how the authors use their previous results to choose a Toeplitz matrix. If anything, it feels like cheating slightly since the task is periodic and they choose a circulant matrix (or maybe not cheating, but rather they use features of the task instead of the dynamical systems picture they have developed). \n\n9. There are a number of major typos, including sentences that do not make grammatical sense, sentences that end abruptly, and mis-spelling that makes reading this manuscript challenging."
        ]
    },
    "XH3OiIhtvf": {
        "venue": "ICLR 2025",
        "title": "Unsupervised Federated Learning for Privacy Preserving in Face Recognition System",
        "link": "https://openreview.net/forum?id=XH3OiIhtvf",
        "abstract": "Recent advancements in face recognition involve training on a single computer, often containing sensitive personal information, raising privacy concerns. To address this, attention turns to federated learning for unsupervised face recognition, leveraging decentralized edge devices. Each device independently undergoes model training, transmitting results to a secure aggregator. We utilize GANs to diversify data without the need for transmission, thereby preserving privacy throughout the entire process. The aggregator integrates these diverse models into a single global model, which is then transmitted back to the edge devices for continued improvement. Experiments on CelebA datasets demonstrate that federated learning not only preserves privacy but also maintains high levels of performance.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            3,
            1
        ],
        "strengths": [
            "- The topic is important.",
            "- To improve model robustness, the paper proposes two methods for generating spoofed image data on edge devices: randomly selecting images of other individuals from the CelebA dataset and using GAN models to generate images.",
            "- + This work considers a practical scenario- a face verification system, which is meaningful for exploring privacy issues.",
            "- The paper addresses an important topic on face recognition in federated learning."
        ],
        "weaknesses": [
            "- The paper is overall poorly presented. \n1. The contributions are not clearly stated.\n2. The proposed method is just a very simple federated learning process. No innovations at all.\n3. Experimental results are very simple and do not provide any insights.",
            "- 1- Lack of innovation. The proposed method is a combination of several existing methods, and the innovative aspects are insufficient to support acceptance by the conference. \n2- Insufficient experiments. There are too few metrics for measuring facial recognition accuracy, and it is suggested to refer to the literature by Wang et al. (2022). The dataset used is singular, and it is hoped that additional training and testing datasets will be included.",
            "- However, this work has the following major concerns:\n\n**Lack of Novelty and Technical Contribution**: The idea of using GAN to generate impostor images is very straightforward. As explained on Page 4, \"In the system incorporating a secure aggregator, the process unfolds with distinct steps, including Local Training, Model Transmission to Secure Aggregator, Global Model Creation, Aggregated Model Transmission, and Distribution to Devices. Conversely, in the absence of a secure aggregator, the workflow proceeds similarly, albeit without involving the aggregator.\" The core idea is simply using secure aggregation, which is not novel. Recently, a lot of works improve the secure-aggregation-based federated learning for the goal of efficiency or scecurity.  For example,\n\n[Reference 1] FedCSCD-GAN: A secure and collaborative framework for clinical cancer diagnosis via optimized federated learning and GAN.\n\n[Reference 2] Ifl-gan: Improved federated learning generative adversarial network with maximum mean discrepancy model aggregation.\n\n[Reference 3] Scionfl: Efficient and robust secure quantized aggregation\n\n[Reference 4] Cryptography-inspired federated learning for generative adversarial networks and meta-learning.\n\n[Reference 5] Sear: Secure and efficient aggregation for byzantine-robust federated learning\n\n[Reference 6] Efficient Aggregation of Face Embeddings for Decentralized Face Recognition Deployments.\n\n[Reference 7] A comprehensive experimental comparison of the aggregation techniques for face recognition\n\nMoreover, secure aggregation with GAN has been proposed in reference 4, which supports stronger security than this work.\n\n **Insufficient Experimental Validation**: This work shows experiments conducted on the CelebA dataset. However, it does not provide a sufficient evaluation of other datasets (e.g.,  Wild (LFW) and YouTube Faces (YTF) datasets) or compare their work with other advanced works. The experimental results are very limited. This work lacks sufficient experimental results compared with related works (e.g., reference 3, reference 4, reference 6, and reference 7 as mentioned) that improve secure aggregation or federated GAN.\n\n**Lack of Security Analysis** The security model is not clear and not defined. Accordingly, the formal security analysis also remains lacking. There is no explanation or assumption of the attackers, e.g., the attacker's knowledge, malicious or honest. The explicit privacy guarantee is also not defined. Additionally, secure aggregation is not secure againset some attacks, such as,\n\n[reference 8] Secure Aggregation is Not Private Against Membership Inference Attacks.\n\n**Severe Wring issues.** The introduction does not give a clear summary of motivation and contribution to this work, such as improvement to prior works. For example, the work does not explain why not adopt the better secure aggregation (for example, reference 3) for improved efficiency. The related works of secure aggregation and federated GAN lacks. The explicit motivation to adopt secure aggregation and federated GAN in face recognization remains lacking. Besides, the figure size is weird, e.g, Figure 1 and Figure 3. The writing quality should be hugely improved to meet the paper's qualifications.",
            "- 1. The contribution in this paper is not clear. The paragraphs in the Introduction section lacks logic flow about the intuition, challenges and contribution of the work.\n2. In Section 3, the proposed method is not sufficiently explained. Figure 1 gives the whole framework, but it lacks comparison with prior research and why this framework is better than prior research. It seems it makes no difference from a traditional FL framework.\n3. The experiments do not support the effectiveness of the method. Only one dataset and one model is evaluated.\n4. Which GAN model is used in the work? It is not clearly mentioned in the paper.\n5. It seems the paper is not a finished work. The writing, presentation of the method, and experiments are way below the ICLR standard."
        ]
    },
    "UkGrcekmSZ": {
        "venue": "ICLR 2025",
        "title": "Leveraging deep learning for comprehensive classification of renal diseases: A transfer learning approach",
        "link": "https://openreview.net/forum?id=UkGrcekmSZ",
        "abstract": "The nightmare of cancer as a leading cause of premature deaths worldwide is becoming real and turns out to be one of the major problems of humanity nowadays. Cancer diagnostics at the early stage is Critical to cancer recovery and survival. In this context, renal diseases, including kidney cysts, stones, and tumors, pose significant global health challenges, affecting approximately 12\\% of the population and contributing to chronic kidney disease (CKD). Notably, renal cancer ranks as the tenth most prevalent cancer type, accounting for 2.7\\% of all cancer cases. This work presents a deep learning (DL) framework utilizing transfer learning (TL) for the early detection of renal diseases and categorizing the conditions into four binary classifications: Cyst\\_vs\\_Normal, Cyst\\_vs\\_Stone, Cyst\\_vs\\_Tumor, and Stone\\_vs\\_Tumor, allowing for a more specific understanding of each stage. By analyzing CT scans and microscopic histopathology images, the framework employs convolutional neural networks (CNNs) with pre-trained models to facilitate automatic and precise classification of renal conditions. Specifically, two CNN models ResNet-50 and EfficientNetV2 are implemented, providing a comprehensive analysis of each stage of the DL architecture. Comparative evaluations of training outcomes across various datasets revealed that EfficientNetV2 performed marginally better than ResNet-50, achieving an impressive testing accuracy of up to 100\\% for all cases. These results underscore the effectiveness of the DL-based system and highlight its potential for widespread clinical application in renal disease diagnosis.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- The paper is well-organized and clearly written.\n\nThe proposed method is technically sound.\n\nThe collected dataset for developing and evaluating the proposed method is relatively large-scale.",
            "- The study utilized multiple dataset.",
            "- I understand that making the following judgment on a paper requires caution, which I will clarify later: there is no strength in this paper",
            "- The paper presents the applied problem well."
        ],
        "weaknesses": [
            "- The novelty of this work is limited from a methodological perspective.\n\nThe interpretation of deep learning models is crucial in clinical practice; however, the authors have rarely presented such results.\n\nAdditionally, there is a lack of comprehensive comparisons with other studies.",
            "- The novelty of this work is limited. Transfer learning is already studied in medical image analysis for long years. There is no novel method or contribution in this work. The evaluation of this work is insufficient. Comprehensive evaluation is required. The figures quality needs improve.",
            "- I find that this paper seems to have been generated by a scientific LLM agent. There are many inconsistencies in citations and formatting that appear to be hallucinations.\n\n\nHere are just few examples that can be easily spot and verified:\n\nIntroduction section\n\n- (In paper) Kidney cancer is a growing public health concern, affecting over 10% of the global population, and\nprojected to rise from the tenth leading cause of death in 2024  (Can, accessed 20th Sep, 2023) \n- (My verification) There is no mention of a '10% global population' or a 'projection to rise from the tenth leading cause' in the provided link. Additionally, the citation format does not appear to be human-written.\n----\n\n-  (In paper) Kidney cysts are fluid-filled sacs, while nephrolithiasis affects approximately 17% of the global population (Alelign & Petros, 2018).\n- (My verification) Not 17%, it is 12% in the provided link\n----\n- (In paper) Computed tomography (CT) scans are particularly effective for kidney examinations, offering 3D (dimensional), cross-sectional images ideal for identifying abnormalities like cysts, stones, and tumors (Schmidt, 2012).\"\n- (My verification): The cited article does not provide this information or endorsement\n\n\nRelated work section\n\n- (In paper) \"CNN is a prominent DL algorithm for classifying grid-patterned datasets like images (Tan & Le,2020). \"\n- (My verification): Directly citing EfficientNet here feels out of place. In the community, it is more common to reference foundational papers like AlexNet, VGG, or those that pioneered CNN applications in the early stages of deep learning. Also, the phrase 'grid-patterned dataset' does not sound natural.\n-----\n- (In paper)Page 2 line 101-107: The statement \"I.....  I..... I.....\": \n- This resembles Claude Sonnet\u2019s writing style when it assists in designing code or formulating ideas.\n\n\nIn 3.2 INITIALIZATION OF WEIGHTS\n- (In paper) TL (Zhuang et al., 2020) is a powerful technique that significantly enhances training efficiency, especially when data is limited. It involves taking a pre-trained model, often trained on a large dataset like ImageNet, and fine-tuning it for a different but related task\n- It\u2019s quite odd to attribute transfer learning, a general machine learning technique, to a recent work from 2020. People in the community were familiar with this approach well before 2020, which makes it seem like an LLM hallucination.\n\nIn 3.3 TRAINING RESULT AND DL MODELS\n- (In paper)  Two modern CNN architectures ResNet-50 and EfficientNetV2 are used, and both models were trained and evaluated using TensorFlow (Tan & Le, 2021; He et al., 2016; Khan et al., 2020)\n- (My verification): This citation seems random; the correct citation for TensorFlow is: Abadi et al., 'TensorFlow: A System for Large-Scale Machine Learning.' Even if this citation were meant to credit EfficientNet or ResNet-50, the final reference to Khan et al., 2020 is irrelevant.\n-----\n- (In paper) Each of these blocks incorporates a skip connection, in addition to convolutional layers and batch normalization (Ioffe & Szegedy, 2015; Chen et al., 2020)\n- (My verification): Ioffe & Szegedy, 2015 for  batch normalization is correct, but the later citation is SimCLR which is completely irrelevant\n\nHallucination aside, let me summarize the weakness of experiment design\n- Reporting 100% accuracy without detailed analysis is suspicious, as this would imply a complete solution to the task\u2014something that is extremely rare in the machine learning community. Even the latest models struggle to achieve 100% accuracy on datasets like CIFAR-10\nhttps://paperswithcode.com/sota/image-classification-on-cifar-10\n- It is unclear why the authors transformed a multi-class classification dataset into binary classes (e.g., cyst vs. normal, tumor vs. normal). There appears to be no benefit in deploying multiple binary classification models over the original multi-class approach\n- Similar work was conducted in 2023, where ResNet50, EANet, and a proposed approach were compared on the same Kaggle dataset, rendering this paper's contributions negligible.\nhttps://ieeexplore.ieee.org/document/10074314",
            "- I am not sure what to think about this paper, because it has virtually no novelty and doesn't really seem like a research paper. There are no methodological contributions, and while the application may be interesting in its field, the experiment and the results are not extensive enough nor evaluated properly.\n\nMy major concerns with the paper have been stated already, but if a paper does not have a methodological contribution, I would expect it to have a scientific contribution somehow (in the application, for instance). Here, two existing neural networks are applied to an open dataset, and the highest mean performing model is dubbed the winner. All steps of the ML pipeline would have to be improved, basically. E.g., proper hyper-parameter search would be needed, more methods would have to be evaluated on more datasets, and confidence intervals, standard errors, or tests would have to be performed to say anything. But even with proper evaluation, this paper does not seem like a fit for ICLR, and should then rather be sent to a journal for renal disease studies."
        ]
    },
    "UYXq4q1GpW": {
        "venue": "ICLR 2025",
        "title": "A Healthy Food Recommender System Using Collaborative Filtering and Transformers",
        "link": "https://openreview.net/forum?id=UYXq4q1GpW",
        "abstract": "Unhealthy eating habits are a major contributing factor to public health problems such as the globally rising obesity rate. One way to help solve this problem is by creating systems that can suggest better food choices in order to improve the way people eat. A critical challenge with these systems is making sure they offer 1) suggestions that match what users like, while also 2) recommending healthy foods. In this paper, we introduce a novel food recommender system that provides healthy food recommendations similar to what the user has previously eaten. We used collaborative filtering to generate recommendations and re-ranked the recommendations using a novel health score and a BERT embedding similarity score. We evaluated our system on human subjects by conducting A/B testing on several methods deployed in a web application.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- S1: It is nice to consider the characteristics of food items, i.e., the food nutrition, when recommending the food items to users.",
            "- \\+ The paper tackles an interesting aspect of food recommendation, namely, the introduction of healthy suggestions alongside the usual accurate ones\n\n\\+ The authors conducted both an offline and an online evaluation of the proposed framework",
            "- - An impactful problem, they are working on healthy food recommendations \n- Straightforward solution, simple and well described. The paper is easy to follow",
            "- The paper attempts to integrate nutritional information with user preferences in food recommendations, which is a relevant and timely area of research. By using BERT for semantic embeddings, the authors leverage recent advancements in natural language processing to enhance the understanding of food relationships. This approach allows for a more nuanced recommendation that considers both taste and health aspects, potentially bridging a gap between traditional recommendation systems and health-conscious eating.\n\nThe methodology presented is well-structured, demonstrating a thoughtful design that combines established collaborative filtering techniques (EASE/SVD) with modern embedding approaches (BERT). The systematic evaluation of nutritional scores adds a robust layer to the recommendation process, ensuring that suggestions are not only appealing but also health-oriented. \n\nThe paper is clearly organized, with each section building logically on the previous one. The visual representation in Figure 1 provides a comprehensive overview of the system architecture, making it easier for readers to grasp the flow and interaction of components. The explanations of each step in the recommendation process are thorough, enhancing the reader's understanding of how user preferences and nutritional data are integrated.\n\nThe significance of this work lies in its potential impact on promoting healthier eating habits through personalized recommendations. By addressing the dual goals of satisfying user preferences and supporting health-related dietary choices, the system aligns with current trends in nutrition and wellness."
        ],
        "weaknesses": [
            "- W1: The core of healthy nutrients and unhealthy nutrients (Equations 7-8) is not well motivated. Why the four kinds of nutrition (Protein, Dietary Fibre, Vitamin, Potassium) are defined as healthy while the other three (Sugar, Sodium, Saturated Fats) are unhealthy? \n\nW2: The case of healthy and unhealthy is much varied with different genders and ages (old, young). The universe definition in Equations 7-8 might be inappropriate, especially the fixed weights.\n\nW3: The evaluation metrics on recommendation should include ranking metrics like AUC, NDCG, and Recall. The RMSE metric only is not enough in Section 4.1 AUTOMATIC EVALUATION.\n\nW4: Key details are missing in evaluation and implementation. What are the statistics of the datasets, how many users, how many interactions, how to split the training set and test set, how many words and tokens are in the food item descriptions, and how long of the average length of the food item descriptions, what is the K in K-means, any visualization e.g. t-SNE on the clustering foods?",
            "- \\- Despite the detailed introduction and presentation of the motivations behind the work, it gets very difficult to tell how the proposed approach is different from the related literature in terms of rationales, concepts, and technical aspects\n\n\\- Some important related work is missing from the paper, here to mention a few: [i, ii, iii, iv, v, vi, vii, viii]\n\n\\- Some technical details of the proposed framework are not adequately justified (see the questions section)\n\n\\- The experimental part of the paper seems quite limited and very few evaluation dimensions are taken into account; in the offline evaluation, I would have added comparisons against some of the missing citations mentioned in the second weaknesses point\n\n**References**\n\n[i] Zhenfeng Lei, Anwar Ul Haq, Adnan Zeb, Md Suzauddola, Defu Zhang: Is the suggested food your desired?: Multi-modal recipe recommendation with demand-based knowledge graph. Expert Syst. Appl. 186: 115708 (2021)\n\n[ii] Weiqing Min, Shuqiang Jiang, Ramesh C. Jain: Food Recommendation: Framework, Existing Solutions, and Challenges. IEEE Trans. Multim. 22(10): 2659-2671 (2020)\n\n[iii] Wenjie Wang, Ling-Yu Duan, Hao Jiang, Peiguang Jing, Xuemeng Song, Liqiang Nie: Market2Dish: Health-aware Food Recommendation. ACM Trans. Multim. Comput. Commun. Appl. 17(1): 33:1-33:19 (2021)\n\n[iv] Alain D. Starke, Cataldo Musto, Amon Rapp, Giovanni Semeraro, Christoph Trattner: \"Tell Me Why\": using natural language justifications in a recipe recommender system to support healthier food choices. User Model. User Adapt. Interact. 34(2): 407-440 (2024)\n\n[v] Alessandro Petruzzelli, Cataldo Musto, Michele Ciro Di Carlo, Giovanni Tempesta, Giovanni Semeraro: Recommending Healthy and Sustainable Meals exploiting Food Retrieval and Large Language Models. RecSys 2024: 1057-1061\n\n[vi] Mehrdad Rostami, Vahid Farrahi, Sajad Ahmadian, Seyed Mohammad Jafar Jalali, Mourad Oussalah: A novel healthy and time-aware food recommender system using attributed community detection. Expert Syst. Appl. 221: 119719 (2023)\n\n[vii] Alain Starke, Ayoub El Majjodi, Christoph Trattner: Boosting Health? Examining the Role of Nutrition Labels and Preference Elicitation Methods in Food Recommendation. IntRS@RecSys 2022: 67-84\n\n[viii] Alain D. Starke, Christoph Trattner: Promoting Healthy Food Choices Online: A Case for Multi-List Recommender Systems. IUI Workshops 2021",
            "- - No comparisons. They have to compare previous works w.r.t. proposed solutions, a quantitative comparisons would help to understand the real benefits of this solution \n- No baselines, really the weakness is around the fact that the authors should spend sometime trying to compare their solution",
            "- While the paper presents a food recommender system that integrates user preferences with nutritional data, it falls short in several key areas that limit its contribution to the field. The concepts largely rehash existing methods in food recommendation systems without introducing sufficiently novel insights. Although the authors attempt to integrate collaborative filtering and contextual embeddings, both EASE/SVD and BERT-based embeddings are well-established techniques. This combination, while functional, does not significantly advance the field or offer a new perspective on food recommendation challenges.\n\nTo make this work more impactful, I suggest the authors consider recent innovative approaches in food recommendation that extend beyond collaborative filtering and embeddings. For example, multi-objective optimization frameworks, such as those used by Gao et al. (2022), optimize for both health goals and user satisfaction simultaneously. Additionally, reinforcement learning-based recommender systems that adjust recommendations based on real-time user feedback could provide a more adaptive approach than static collaborative filtering models. Moreover, incorporating multi-view learning or multi-modal embeddings (e.g., combining user activity data, time of day, and food availability) could help capture a richer, context-aware recommendation space. These techniques, documented in recent studies like Zitouni et al. (2022), might allow the system to adapt recommendations dynamically, responding to factors that influence dietary choices.\n\nFurther innovation could be achieved through novel adaptations of existing methods. For example, graph-based models that map both food similarity and nutritional content in a unified framework could provide a complex, relational approach, setting this work apart from traditional collaborative filtering. These alternative formulations would help the authors demonstrate stronger novelty, particularly if they compared the proposed system against these advanced approaches. By exploring or incorporating some of these techniques, the paper could provide a more compelling case for its system\u2019s uniqueness and address the observed gaps in novelty. This would help strengthen its contribution to the development of food recommender systems that support healthier dietary choices.\n\nThe evaluation does not incorporate essential metrics such as Hit Rate, which is particularly important for food recommendation systems because it directly measures the system's success in presenting items the user is likely to engage with. For a system focused on promoting healthier food choices, accurately predicting relevant items is crucial to encouraging adoption and sustained user interest. Including Hit Rate as an evaluation metric would provide insight into the recommendation system's accuracy and appeal. Additionally, an ablation study is needed to assess the contributions of each component of the system, helping to clarify the impact of the collaborative filtering methods, BERT embeddings, and nutritional scoring on the overall performance. This would provide a clearer understanding of the strengths and weaknesses of the proposed approach. While the paper is structured and contains clear descriptions of the components involved, this clarity does not compensate for the lack of originality and rigorous evaluation. The overall presentation does not highlight any unique or insightful findings that would warrant further exploration or application.\n\nThe paper fails to address a substantial gap in the literature or offer a new solution that could influence the domain of food recommendations. Its contributions seem incremental rather than transformative, and the proposed system does not advance our understanding of dietary recommendation systems in a meaningful way. In conclusion, the paper does not sufficiently present novel contributions or robust evaluations that would warrant acceptance."
        ]
    },
    "RVSQpkfsLq": {
        "venue": "ICLR 2025",
        "title": "Evolving Virtual World with Delta-Engine",
        "link": "https://openreview.net/forum?id=RVSQpkfsLq",
        "abstract": "Game roles can be reflections of personas from a parallel world. In this paper, we propose a new style of game-play to bridge self-expression and role-playing: \\emph{open role-playing games (ORPGs)}, where players are given the autonomy to craft and embody their unique characters in the game world.\nOur vision is that, in the real world, we are individually similar when we are born, but we grow into unique ones as a result of the strongly different choices we make afterward. Therefore, in an ORPG, we empower players with freedom to decide their own growing curves through natural language inputs, ultimately becoming unique characters.\nTo technically do this, we propose a special engine called \\emph{Delta-Engine}. This engine is not a traditional game engine used for game development, but serves as an in-game module to provide new game-play experiences. A delta-engine consists of two components, a base engine and a neural proxy. The base engine programs the prototype of the character as well as the foundational settings of the game; the neural proxy is an LLM, which realizes the character growth by generating new code snippets on the base engine incrementally.\nIn this paper, we self-develop a specific ORPG based on delta-engines. It is adapted from the popular animated series ``Pok\u00e9mon''.\nWe present our efforts in generating out-of-domain and interesting role data in the development process as well as accessing the performance of a delta-engine. While the empirical results in this work are specific, we aim for them to provide general insights for future games.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- In this paper, the authors tried to make the game engine (a backbone of the virtual world) changing over time. It's interesting to update the backbone engine using Large Language Models.",
            "- -  Appendix provides a demo.",
            "- The core concept of the delta engine is an interesting one, though it's unclear whether this is substantially different than any game engine that includes generated content from a code generator LLM. Generating new Pokemon moves or base stats is technically novel, but it is not surprising that LLMs can generate code that works for these purposes.",
            "- See main review."
        ],
        "weaknesses": [
            "- This paper tries to tackle challenging problems to update game engines (usually static components of the virtual world) over time. However, the solution description on the problem is not clearly defined in the manuscripts. \n\nIn the Abstract, authors stated that \"existing virtual worlds are strictly defined by the back-end engine and cannot be changed by user's behavior.\" It's an interesting statement however, it's still questionable that the engine needs to be changed by user's behavior. It's a radical change to the world and there could be a solution to reflect the user's behavior's outcome to the world without changing the game engine itself. \n\nIn the Abstract, there are terms that make it difficult for readers to focus on the contribution. For example, they're \"scalability to user-generated content,\" \"dual aspects of algorithm and data,\" \"neural proxy,\" and \"novel and interesting data.\" It's relatively new to be difficult to grasp the concepts from the first reading. It's recommended to improve the summary to be readable for the audience. \n\nIn the Introduction, authors argue the necessity of \"evolving nature.\" What's the definition of the evolving in this paper to be used? I recommend authors to provide more explanation why back-bone engines needs to be \"evolving\" instead of other alternative solutions (evolving objects instead of changing the world itself) traditionally approached in many game-related articles. Also, it's good to add some evidence on \"Such dynamics is unpredictable and beyond the reach of existing systems.\" \n\nAuthors need to improve their manuscripts by avoiding unclear definitions of words or terminologies. For example, they're \"Its codebase will become more and more along with the world's evolution.\" \"God mode,\" \"Biodiversity,\" \"Imagination,\" and \"Tags of Interest\" so on. \n\nIn Chapter 3, the Delta-Engine description needs to be improved. For example, the Base engine part includes \"only walking ability\" \"learns to run and even fly.\" It seems that the engine is limited to the sample scenario. It's desirable to provide a general introduction of the methodology. In the incremental prediction part, please explain what is the input, and what is the value? In the retrieval, what is the sparse version? \n\nIn conclusion, this paper's weak point is unclear description of their ideas with ill-defined justification of research goals.",
            "- - The manuscript lacks clarity, making it difficult for readers to understand the significance and motivation behind the study. \n\n- The absence of a clear problem definition undermines the overall coherence of the paper. The concept of the Delta-engine is vaguely defined, and the paper does not provide definitions for fundamental terms, such as \u201cengine state,\u201d \u201cnew features,\u201d and their relationships with virtual environmental dynamics generated by the neural proxy. \n\n- Additionally, the description of the engine and evolving world lacks sufficient details, making it challenging to evaluate the feasibility and innovation of the proposed approach. The authors mention that the engine must address challenges in both algorithm design and data management. However, the experiments on AI co-design and the synthetic data generation process remain unclear in terms of how they work.",
            "- This paper has a very large number of issues in its current state. To organize my feedback I'll introduce these issues in each section. \n\n### Introduction\n\nThe introduction has the primary issue of not being particularly relevant to the actual research work that has been done. There is no clear pathway from the authors prompt-based Pokemon generation to the virtual worlds they describe. It would be better to focus the paper on the specific research project being presented. I would further suggest that the authors remove all unsupported claims like \"Its codebase will become more and more along with the world\u2019s evolution\" or \"Delta-engines can serve as the basic components of the world to simulate their evolving processes, encompassing roles, surroundings, props, and other integral components\", given there's no proof of either in this paper or in prior work. Similarly, I would suggest the authors remove tangential sentences like \"The evolution is triggered by specific signals within the world, e.g. observations, behavior, and events.\". The introduction also introduces two other recurring issues in the paper. The first is that many of the figures are not legible, like the choice and size of font in figure 1. The second is that the language is very poor with many grammar or inappropriate wording issues, such as \"Such dynamics is unpredictable\"->\"Such dynamics are unpredictable\" or the use of \"Pokemon role\" when I believe the authors may have meant \"Pokemon character\". \n\nThe authors notably claim that code, data, and a demonstration are available in the supplementary materials, but this is untrue. There is a small Pokemon battling clone with some pre-generated elements.\n\n### Related Work\n\nThe authors cite a great deal of prior work, which is great. However, many of the citations are not relevant to this work. For example, the authors do not need to list all the AI work across various games in the final paragraph. Instead, I would have recommended that the authors discuss prior work in generating code for game characters [1,2]. In addition, the authors likely should have touched on prior work on generating Pokemon, though the majority of this prior work is focused on generating Pokemon-like visuals [3,4,5], some of the prior work does touch on descriptions [3] or type information [5]. \n\nIt is also notable that GameNGen is not based on prompts.\n\n### Delta Engine and Playground: Free Pokemon and Training Data Generation\n\nI am grouping all three of these as they make up the system overview equivalent sections of the paper. In general, the authors would ideally have included all technical aspects of the work in sufficient detail that they could be replicated. But this is not the case, the authors do not include their representation of Pokemon or rules (except via examples), they do not give their prompts or prompt structure, and most importantly they do not give how their co-creative setup works. Part of the problem here may be the lack of clarity (writing and language issues) in the paper.\n\n### Experiments \n\nThe authors present an experiment to show that their approach can recreate existing Pokemon and that it can recreate Pokemon created by human \"volunteers\". The issues here are primarily with the volunteers with the experiments otherwise being very reasonable for evaluating Pokemon generation. The authors do not specify if they had ethics approval for this human subject work or if the participants were compensated. It's also unclear what prior knowledge they had or what their relationship is to the authors. Without full methodological information it is impossible as a reader to judge the validity of their date, making the results related to it similarly difficult to trust. Similarly, it's unclear what the co-creative experience was or who the humans were who took part in it, making it very unclear how to interpret the \"& CO.\" results. These issues remove any generalizable knowledge other researchers may have been able to take from this experiment. \n\nFigures 5 and 6 similarly have issues in terms of how they were made and what they mean. For Figure 5, it's unclear if this experiment was run a single time or what all the colours and lines indicate. For Figure 6, it's unclear how the authors created their semantics and interestingness spaces or how they are projected into two dimensions.  \n\n\n1. Butler, Eric, Kristin Siu, and Alexander Zook. \"Program synthesis as a generative method.\" Proceedings of the 12th International Conference on the Foundations of Digital Games. 2017.\n2. Sorochan, Kynan, and Matthew Guzdial. \"Generating real-time strategy game units using search-based procedural content generation and monte carlo tree search.\" arXiv preprint arXiv:2212.03387 (2022).\n3. Geissler, Dominique, et al. \"Pok\u00e9rator-unveil your inner Pok\u00e9mon.\" 11th International Conference on Computational Creativity, ICCC 2020. 2020.\n4. Liapis, Antonios. \"Recomposing the pok\u00e9mon color palette.\" Applications of Evolutionary Computation: 21st International Conference, EvoApplications 2018, Parma, Italy, April 4-6, 2018, Proceedings 21. Springer International Publishing, 2018.\n5. Gonzalez, Adrian, Matthew Guzdial, and Felix Ramos. \"Generating gameplay-relevant art assets with transfer learning.\" arXiv preprint arXiv:2010.01681 (2020).",
            "- See main review."
        ]
    },
    "LqB8cRuBua": {
        "venue": "ICLR 2025",
        "title": "Diffusion SigFormer for Interference Time-series Signal Recognition",
        "link": "https://openreview.net/forum?id=LqB8cRuBua",
        "abstract": "The various interferences in the actual environment make electromagnetic signal recognition challenging, and this topic has extremely important application value.\nIn this paper, a novel interference signal recognition transformer is proposed, named Diffusion SigFormer.\nFirstly, we explored the interference law of electromagnetic signals and designed a signal interference mechanism. \nSecondly, diffusion signal denoising modulewas proposed to denoise the input interference signal. We also use various types of noise to improve its denoising effect on electromagnetic signals.\nThirdly, SigFormer is designed to extract and classify the denoised signal.\nFor the characteristics of electromagnetic signals, SigFormer leverages 1-D Patch Embedding and combines transformer with convolution. \nFinally, we conducted experimental verification on datasets RML2016.10a, RML2016.10b and BT dataset. \nThe experimental results show that the proposed method has excellent anti-interference ability.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- 1. The application of diffusion models with a Transformer in the context of interference signal recognition has not been fully explored. The proposed model addresses a gap in current methodologies.\u00a0\n2. A mechanism for electromagnetic signal interference is designed to add an appropriate amount of interference to clean signals. SigFormer combines Transformer with convolution, enabling the model to have excellent local feature extraction capabilities and perform well in global context modeling.\u00a0 \u00a0\n3. The results are well-presented, with thorough comparisons against existing models. The use of relevant benchmarks is appropriate for evaluating performance. The experimental results show the anti-interference ability of the findings.",
            "- 1. This paper proposes using a diffusion-denoising model to resolve the time-series data recognition problem. Generally, using a diffusion model to separate noise makes sense.\n2. The visualization shown in Figure 4 figure 5 and Table 1 shows the feasibility of the proposed method.",
            "- The topic is interesting.",
            "- The authors compared the proposed method with several basic architectures, such as LSTM, ViT, and Mamba."
        ],
        "weaknesses": [
            "- 1. There are several grammatical errors throughout the manuscript.   \n\nFor example, in the Abstract section, the sentence \"Secondly, diffusion signal denoising module was proposed to denoise the input interference signal.\" should be revised to \"Secondly, a diffusion signal denoising module was proposed to denoise the input interference signal.\"   \n\nIn line 45 of the introduction section, the sentence \"Compared with LB methods, FB methods have stronger robustness and effectiveness, but its recognition performance depends on manually designed features and classifiers.\" should be revised to \"Compared with LB methods, FB methods have stronger robustness and effectiveness, but their recognition performance depends on manually designed features and classifiers.\"  \n\nThere are many similar grammatical errors in the article. The author should check them carefully.  \n\n2. Some sentences in the paper are difficult to read and understand. The author should improve his English writing skills.  \n\nFor example, in the Abstract section, the sentence \"We also use various types of noise to improve its denoising effect on electromagnetic signals.\" This sentence is not very clear. How can noise improve the denoising effect of the signal?  \n\nIn line 81 of the introduction section, the sentence \"In this paper, inspired by the powerful denoising ability of diffusion models and considering the interference of a large number of signals in reality, combined with the powerful temporal processing capability of transformers, we propose a novel signal recognition method based on diffusion model and transformer.\" The sentence is quite long and can be made clearer by breaking it up and improving some phrasing.  \n\n3. Each picture has no caption, which makes it difficult for readers to understand the picture. The method does not present enough details.   \n\nFor example, in Figure 1, it is not clear how different types of noise are added to the original signals. In addition, in the Diffusion signal denoising module, does the diffusion process only go through four steps? Is the ellipsis missing?  \nIn Figure 2, the meaning of some blocks should be explained.   \n   \n4. There are some problems in the picture. In Figure 1, the image and the legend overlap. And the borderlines are very rough. In the Diffusion Process, Some arrows are missing, and the directions are not clear.  \n\n5. Some mathematical symbols are not explained. For example, \"\u03c3tz\" is not explained. There seems to be something wrong with the derivation from Formula 3 to Formula 4. There should be a more detailed derivation process.  \n\n6. The contributions of this paper are limited. Diffusion and transformers have been proposed for a long time. For Sigformer, transformer and convolution are just simple combinations. The method proposed in this paper seems to be a patchwork of existing techniques.  \n\n7. This paper lacks ablation experiments. Additional experiments are needed to verify the role of each component of Sigformer.",
            "- 1. The Phase Problem is not clearly explained. When applying DDPMs to time series data, one of the biggest challenges is the phase of the generated signals. However, according to Figures 4 and 5, the phase miss-match problem between the real part and the imaginary part still remains severe. However, it seems in Table 1, that this problem are not affecting a lot. How the model overcomes the phase shift and the potential impact is not clear. \n\n2. For the experiment part Table 3, the authors compare SigFormer with Mamba and ViT.  Are the CNN-based model,  SigFormer and ViT all going with the DSDM process? Or does only the SigFormer go with the DSDM process, while others just go through with their original process for precision?",
            "- I strongly recommend that the authors thoroughly revise Section 2.3, beginning with clear definitions for each mathematical notation. Given that denoising diffusion models are well-studied, it is essential to differentiate prior work from the novel contributions of this paper.\n\nI also recommend expanding Section 2.4 with a comprehensive review of existing approaches. This would help position the proposed methodology more effectively within the domain. For instance, one technique discussed involves patching a 1D signal with positional encoding. The authors are encouraged to reference existing applications of this technique\u2014not necessarily limited to electromagnetic signal recognition\u2014and to clarify the motivation for its use in this context.\n\nA general comment: prior work should not be confined to the introduction or related work sections. I encourage the authors to cite relevant studies for each scientific claim and technique throughout the paper. Clearly distinguishing which methods contribute to the paper\u2019s original work will aid readers in understanding its position within the field and will highlight its unique contributions.\n\nFinally, a specific note: as both denoising diffusion models and transformers are well-researched topics, a straightforward application of these techniques to a new domain may not sufficiently establish originality. Greater emphasis on how these techniques are adapted or innovatively applied would strengthen the paper's contribution.",
            "- The logic of the writing itself needs to be strengthened. In the Introduction, the author describes the related research on electromagnetic signal recognition using AI and deep learning in recent years (l48-58). However, the problems mentioned in the next paragraph are not problems encountered by existing methods (l59-65). It only describes the problems encountered by traditional signal recognition. Although more recent related research is introduced in A.1 and A.2, these methods have also not been analyzed and compared with the proposed method.\n\nThe DSDM part is very confusing. In particular, the description and formula in Section 2.3 do not match the algorithm in Section A.3 at all. According to the context, algorithm in Section A.3 combines the SIR which is described in Section 2.2, and I guess Section A.3 would be a more correct description of DSDM. In addition, placing the DSDM algorithm in the related work section will make readers confused whether DSDM is the method proposed by the author or an existing method.\n\nAdding noise to the training data is a long-established method to enhance the performance of time-series signal recognition. The experiments cannot explain whether the improvement in effect comes from the denoising process of DSDM or the noise adding for training.\n\nSince the DSDM proposed by the author constrain the step number of diffusion processes t to 1, the author should show the relevant ablation study. Including the constrain of t, not using t as input, and comparison with the original diffusion, etc. The proposed SigFormer also needs to be verified. The author mentioned that the original transformer will cause training instability, but there is no relevant experimental comparison and analysis. Papers that use 1x1 convolution to enhance the performance of the transformer on time-series recognition have not been well cited and compared, such as [a]. And paper that add condition on signal side [b] should also be included in the comparison.\n\nOverall, the correctness and logic of the writing of this paper need to be strengthened. There is also a lack of analytical and experimental results sufficient to verify the effectiveness of the proposed method.\n\n[a] Eldele, Emadeldeen, et al. \"Tslanet: Rethinking transformers for time series representation learning.\" ICML (2024).\n[b] Li, Yuxin, et al. \"Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting.\" ICLR (2024)."
        ]
    },
    "JXvEzl8YkS": {
        "venue": "ICLR 2025",
        "title": "Regularised Jump Models for Regime Identification and Feature Selection",
        "link": "https://openreview.net/forum?id=JXvEzl8YkS",
        "abstract": "A regime modelling framework can be employed to address the complexities of financial markets. Under the framework, market periods are grouped into distinct regimes, each distinguished by similar statistical characteristics. Regimes in financial markets are not directly observable but are often manifested in market and macroeconomic variables. The objective of regime modelling is to accurately identify the active regime from these variables at a point in time, a process known as regime identification. One way to enhance the accuracy of regime identification is to select features that are most responsible for statistical differences between regimes, a process known as feature selection. Models based on the Jump Model framework have recently been developed to address the joint problem of regime identification and feature selection. In the following work, we propose a new set of models called Regularised Jump Models that are founded upon the Jump Model framework. These models perform feature selection that is more interpretable than that from the Sparse Jump Model, a model proposed in the literature pertaining to the Jump Model framework. Through a simulation experiment, we find evidence that these new models outperform the Standard and Sparse Jump Models, both in terms of regime identification and feature selection.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- - Inclusion of multiple penalty terms in the proposed method.\n\n-",
            "- The authors identify a relevant problem in Jump Models for financial time series. With more development, this work could offer useful insights into this type of models and new ways to improve model regularization.",
            "- The contribution of the paper is clearly presented.",
            "- The Regularised Jump Models adapt the Regularised K-means approach to the Jump Model framework and are designed to improve interpretability and accuracy over the Standard and Sparse Jump Models."
        ],
        "weaknesses": [
            "- - Many typos throughout the manuscript.\n\n- Math notations not defined, such as $\\bar{\\mu}$ in (1.2).\n\n- Many details of the algorithms are missing.\n\n- Real data application is missing with lowers the practical importance of the proposed methodologies.\n\n- It seems that the total number of states K is assumed to be known. This is a restrictive assumption, especially in real data applications.",
            "- - There is very limited information on related work; \n- The structure of the paper is not always clear; \n- The adaptations to the existing model are somewhat minor, and intuition isn't always provided on how the proposed methodology;\n- There is no real-world data application/comparison; \n- The proposed model does not always outperform existing models (standard jump model and sparse jump model) and no recommendation is given on which one should be chosen in which situations (same holds for the different penalties in the proposed approach);",
            "- Presentational Clarity:\nThere is a general lack of presentational clarity throughout. The reader has to guess what is intended much of the time (and if you're relying on this, then don't be unhappy if they can't follow):\n* p.1. how does \\bold{Y} relate to \\bold{y}_t?\n* p.2.\\bar{\\bold{\\mu}} seems to be undefined.\n* p.2. what mathematical object a cluster C_k entails is not clear.\n* p.3 is \\underline{\\bold{\\mu}} related to the \\bold{\\mu}_k? (c.f. line 2).\n* p.3. Is \\bold{\\mu}_{\\cdot,j} the same as or different to \\bold{\\mu}_j? It looks like there is different notation for the same object. (Same comment for \\bold{y}_t and \\bold{y}_{\\cdot,j} in Appendix B.)\n* p.6: It's not clear from what is written exactly what Step 3 in Algorithm actually means. Maybe it means (just guessing...) that one locates the largest (by some norm?) cluster centre magnitudes, take those observations allocated to those clusters, and then perform K-means clustering on these observations only? This detail (or whatever the detail actually is) really needs to be clearer.\n* p.7 para 3: The  meaning of this notation is not clear. \\psi takes a single p-dimensional vector X and returns a cluster ID integer. What is \\Psi in relation to this? And why does \\Psi have some parameter \\boldysymbol{\\theta} as a condition and \\psi does not?\n* Following from the previous point: this becomes more confusing when the clustering distance function d can seemingly take both \\psi and \\Psi as inputs (definitions 3.1 and 3.2), when these are clearly different objects.\n* d in Definition 3.1 seems to be dependent on X and Y, which are random variables. This makes d a random variable. Is this correct and the paper's intention? Distances are typically deterministic. \n* This is related to the expectation in (3.1). What is this expectation with respect to? Z_1 and Z_2 are draws from F. Once drawn and put into (3.1) they are fixed. So what is the expectation with respect to?\n* Proposition B1: is Y here related to \\bold{Y} in the main text? Is M related to \\Psi in the main text?\n\n\nContribution:\na) The overall contribution seems fairly incremental. The paper adds a standard regularisation penalisation term to the jump model objective function in exactly the same way that a standard regularisation penalisation term is added to k-means. To be clear: there's nothing wrong with this, but it's a fairly low contribution (and not ICLR standard), and the simulations presented were not particularly convincing in support of this contribution (see more below).\nb) There is no real motivation presented at the start of the paper for why the proposed work is needed or has value. Typically one would expect to see a strong justification to solve the problem, and a literature review discussing what has currently been done, and their strengths and weaknessess. This then allows the paper to present the justification for the work it contains in a clear and as strong as possible light, and against the current state of the art. This is not present in the current paper, which just lists things that were done.\nc) Conclusion paragraph 2: Given the paper's incremental contribution, it would have been helpful to include the mentioned theoretical results \"future work\" in the current paper, though these results would likely have been standard extensions of existing results.\n\n\nGeneral Comments:\na) Section 2.2: S is introduced as a soft thresholding operator. S is also used as the vector of states (notation clash).\nb) Section 2.2: Presumably Algorithm 2 should be referenced in here somewhere? \nc) Regardless, algorithm 2 (sparse jump model calibration) is not really used in the development of the proposed regularised jump model (nor its implementation), so its not clear why it should be included on p.6 taking up lots of space that could be used  for improving the current paper. Suggest it is moved to supporting information at best.\nd) Section 2.3: The strategy in publication [11] of repeating the optimisation with 7 initial state sequences seems alarmingly ad hoc and situation specific/dependent, and one might be wary of following such a strategy in general, as well as recommending it as a basis for new work. It should also be made clearer how this initialisation strategy \"incorporates potential sparsity in the initial cluster centres\", and indeed why there is any hope that this is likely to get the user anywhere close to the global optimum of the target function. (Note that \"most optimal\" in this paragraph should just be \"optimal\".)\ne) P.6. Algorithm 3: does Step 3 include (with the \"100% largest d_j\") an exact repeat of Step 1?\nf) Clustering stability (definition 3.2) seems to prioritise the tendency of producing similar clusters over (say) parsimony. Is it clear that this is the best approach? (E.g. an algorithm that gives clusters of (1,1,1, ..., 1) deterministically each time without variation will be preferred over any other.) It would be good to understand a little more (via a discussion) why this prioritisation is aligned with the goals of the analysis.\n\n\nSimulation Study:\nIt was difficult to take much from this section, as written. It would benefit from a complete re-write (and expansion).\na) The simulation study details are all in another paper [9]. The reader is not going to go and read that paper so that they can understand what has been done in this paper. All relevant detail should be contained in the paper it is used in. As such, it was difficult for this reader to take much from this simulation study. \nb) This includes algorithm/simulation settings etc. Was K fixed at the true value of K each time? How would it perform if it was not? etc. We have no idea.\nc) Section 4.1 para 1: BAC is not defined (or at least, this reader couldn't find it), and so can't be understood. \\mu here is not bold face. Previously \\mu was \\underline{\\bold{\\mu}} was the matrix of the cluster centres \\bold{\\mu}_k. Are these related? \\mu seems to be a single integer in Tables 2 and 3. This really all makes no sense.\nd) Section 4.1 para 4: \\mu seems to impact algorithm performance, which seems incredible if it is a cluster centre.\ne) The captions in the tables are not sufficient for the reader to know what the content of the tables is.\nf) p.9 para 2: Its not clear what calculation was performed here.\ng) Much of the discussion in Section 4.1 is of the form of empirical observations on tables of numbers, without attempting real understanding of what this means as general principles of algorithm performance. That is, we only have very specific, empirical observations.",
            "- 1. Although I am quite familiar with regime-switching, identification, and financial markets, the logical structure of this paper is challenging to follow. The introduction directly presents model definitions and the authors\u2019 proposed improvements without first providing an overview of related work, the motivation, or the current state of research. This structure makes it difficult for readers to understand the unique contributions of this work and distinguish them from existing models. A clearer, more organized flow would greatly enhance readability and clarify the authors\u2019 contributions.\n\n2. The paper lacks a detailed motivation explaining why Regularised K-means, specifically, is optimal for adapting into the Jump Model framework. A comparison of Regularised K-means with alternative approaches could clarify why it is most suitable in this context.\n\n3.  The paper briefly mentions Hidden Markov Models (HMM) and other regime-switching models but does not provide a thorough comparison, such as the latest model - RHINE: A Regime-Switching Model with Nonlinear Representation for Discovering and Forecasting Regimes in Financial Markets (SIAM SDM2024). Besides, although feature selection is central to the model, there is limited discussion on alternative feature selection methods in time series analysis.\n\n4. The formulas (such as those in Eq. 1.6) lack detailed explanations for how each component, including the penalty terms $ P(\\mu) $, specifically enhances feature selection and regime accuracy. \n\n5. The experimental part does not demonstrate how regime identification or interpretability is achieved. Additionally, there are no actual experimental results presented in the main text, yet the pseudo-code of the algorithms takes up several pages.\n\n6. Appendix A is left blank, and the purpose of Proposition B.1 in Appendix B is unclear\u2014is it merely meant to illustrate the classic partitioning principle of K-means? This is a well-known concept in machine learning, and furthermore, the authors\u2019 so-called \u201cproof\u201d is missing."
        ]
    },
    "JNZ3Om6NPS": {
        "venue": "ICLR 2025",
        "title": "On inherent limitations of GPT/LLM \\\\ Architecture",
        "link": "https://openreview.net/forum?id=JNZ3Om6NPS",
        "abstract": "This paper shows that reasoning/proving issues of $GPT/LLM$ are an inherent logical consequence of the architecture. Namely, they are due to a schema of its prediction mechanism of the next token in a sequence, and randomization involved in the process.\n \nAfter the natural formalization of the problem into a domain of finite graphs,  $G({\\omega})$, we prove the following general theorem:\n\nFor almost all proofs, any learning algorithm of inference, that uses randomization in $G({\\omega})$, and necessitates veracity of inference, is almost surely a literal learning.\n\nIn the context, \"literal learning\" stands for one which is either vacuous, i.e. $\\forall x~[P(x) \\implies Q(x)]$ where $P(x)$ is false for every $x$, or create a random inference from a false assumption (hallucination), or it essentially memorizes the inferences from training/synthetic data.\n\nA few corollaries follow. For instance, if its formulation is somewhat original, it is easy to notice the issue of solving mathematical problems with $LLMs$ in the case of even low-complexity tasks.  Since its solution is unlikely to be found in a holistic form in a training dataset, a correct proof is not to be expected. \n\nIt is because, in a rigorous context, $GPT$ has exponentially decreasing odds of finding a valid proof of the result unless it simply \u201crepeats\u201d a known proof, perhaps with trivial modifications. Another observation is that the degradation has an exponential rate by the length of a proof. In other words, an attempt to prove a complex enough statement virtually has no chance to be \nsuccessful.\n\nIn a novel rigorous context (i.e., when $GPT$-based architecture is looking to prove a new result, for instance, a hypothesis), that is virtually impossible even for a long enough fragment. The probability of success becomes infinitesimal quickly for either a fragment of possible proof or a weaker non-trivial statement. That also was empirically shown for data mixtures and confirmed experimentally.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- - The paper aims to understand fundamental limitations of autoregressive next-token predictors which is an important problem today for ML.",
            "- The problem of assessing reasoning capabilities of LLMs versus memorization is very relevant to the current machine learning community.",
            "- 1) The question whether LLMs can produce rigorous proofs or not is fascinating, and at the center of scientific interest. \n2) The logic-based model is potentially an interesting model to address such questions.",
            "- It is a nice topic."
        ],
        "weaknesses": [
            "- -\tThe presentation in the paper severely lacks a level of formalism necessary a technical conference. The writing is at times vague and this makes it hard to verify or critique the main Theorems and Corollaries presented. Some of the issues with the formalism are listed in the questions section below.\n-\tTheorem 1 seems to be wrong (?) It is well-known that GPT/LLM models can generalize and produce novel proofs so the statement that for almost all proofs, the model is performing literal learning appears to be incorrect?\n-\tThe paper relies heavily on \u201crandomization in inference\u201d of GPT/LLM models. This randomization is not clearly quantified or defined anywhere which makes it hard to follow the argument.\n-\tMoreover, there are many post-training techniques one could employ to increase the chances of producing correct output. The paper\u2019s claim seems to potentially hold only in the limit where the size of reasoning chains tend to infinity and we wish to be correct over all possible novel statements on the graph.",
            "- - The paper seems to be not polished: in the introduction there is a repeated paragraph (lines 69-73), several concepts (e.g. literal learning, first order model graph, generic 0-1 law, hallucination) should be defined formally or explained better, there is no literature review paragraph, the flow is hard to follow. Section 2.1 cannot be followed without reading the appendix, a re-organization of the flow would be helpful.\n- The formatting in Section 2.1 is not clear: Q.E.D in line 171 does not seem to be the end of the proof of Theorem 1, and it is not mentioned where the proof of Theorem 1 can be found. Same for lines 176 and 191. In Corollary 3 it is not clear where the statement ends. \n- Because the lack of formalism in the presentation, the results seem unconvincing in this version of the paper.",
            "- 1) There is an empirical evidence that LLMs have some capability in providing rigorous proofs, even if currently a weak one. The paper does not seem to address this evidence too much (except the Trinh et al 2024 paper on line 229), and explain why its reasoning is not applicable in those cases. Much more detailed comparison is required. \n\n2) Section 2.1 presents the first order-logic model, and the main theorems. I am not familiar with first-order logic (as said), but this model appear to be general, and not tightly connected to neither any specific machine learning algorithm, nor LLMs or GPT. Theorem 1 appears colloquial \u2013 what does it rigorously mean \u201cnecessitates veracity of inference\u201d? How is \u201cliteral learning\u201d is defined? To establish the statement targeted by the authors, I think that a much more detailed model is required. Afterwards, there are examples of inference problems the are not solvable by LLMs. These are just mentioned without any justification. \n\n2) Experiments are lacking: Section 2.2 pretenses a couple of \u201cquestion and answer\u201d examples, with a few models, and it is claim that \u201cCalude was the most pleasant and sensible\u201d. This part is not scientific. These are just simple experimentation withe models and general impression. \n\n3) Section 2.3: It is impossible for me to understand this example from the displayed code.\n\n4) The concluding paragraph following \u2013 \u201cThere is a widespread belief that because the training set\u201d \u2013 reads more like a personal opinion rather than a solid conclusion from the paper. \n\n5) Proofs: These are difficult to follow because there is no methodological structure of accurate definitions of terms, follows by a clear definition of the probabilistic model, before the proofs. For a quick read, there is no technical challenge in the proofs, and these are just a description in mathematical terms of the ideas that explained in the paper in words (except for the use of Borell-Cantelli lemmas to prove 0-1 laws)\n\n6) While it is occasionally mentioned that the attention mechanism of GPT is an inherent cause for its impossibility to provide novel rigorous proofs, this is not actually discussed in the paper.",
            "- The presentation e.g. the theorem and proof on lines 167-171 is unconvincing."
        ]
    },
    "JEmNgjuQHU": {
        "venue": "ICLR 2025",
        "title": "KidSat: satellite imagery to map childhood poverty",
        "link": "https://openreview.net/forum?id=JEmNgjuQHU",
        "abstract": "Satellite imagery has emerged as an important tool to analyze demographic, health, and development indicators. While various deep learning models have been built for these tasks, each is specific to a particular problem, with few standard benchmarks available. We propose a new dataset pairing satellite imagery and high-quality survey data on child poverty to benchmark satellite feature representations. Our dataset consists of 33,608 images, each 10 km \u00d7 10 km, from 16 countries in Eastern and Southern Africa in the time period 1997-2022. As defined by UNICEF, multidimensional child poverty comprises six fundamental factors\u2014housing, sanitation, water, nutrition, education, and health (UNICEF, 2021)\u2014which can be calculated from geocoded, face-to-face Demographic and Health Surveys (DHS) Program data. Using our dataset we benchmark multiple feature representations for encoding satellite imagery, from low-level satellite imagery models such as MOSAIKS (Rolf et al., 2021), to deep learning foundation models, which include both generic vision models such as DINOv2 (Oquab et al., 2023) and specific satellite imagery models such as SatMAE (Cong et al., 2022). As part of the benchmark, we test spatial as well as temporal generalization, by testing on unseen locations, and on data beyond the training years. We provide open source code to reproduce and extend our entire pipeline: building the satellite imagery dataset, obtaining ground truth data from DHS, and comparing the various models considered in our work.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3,
            1
        ],
        "strengths": [
            "- Originality\nThe \"KidSat\" paper presents a dataset that combines high-resolution satellite imagery with multidimensional child poverty data from Demographic and Health Surveys (DHS) across 16 countries in Eastern and Southern Africa. This dataset applies satellite imagery to estimate socioeconomic indicators related to child poverty, focusing on UNICEF\u2019s six poverty dimensions\u2014housing, water, sanitation, nutrition, health, and education. The dataset introduces a method for analyzing child-specific poverty indicators rather than general economic metrics.\n\nQuality\nThe paper follows a methodical approach, providing reproducible steps for dataset construction and benchmark testing. The authors compare a range of models, including Gaussian Process Regression, MOSAIKS, DINOv2, and SatMAE, to assess performance on spatial and temporal generalization tasks. Fine-tuning, spatial and temporal benchmarks, and cross-validation are used to evaluate each model\u2019s generalization capability.\n\nClarity\nThe paper is structured to clearly present the models, dataset, and evaluation metrics. The benchmark results, including error metrics for each model type, allow for straightforward comparisons of model performance in child poverty prediction. Visual aids, such as the heatmap of survey cluster locations and summary tables, provide concise representations of key findings.\n\nSignificance\nThis research provides a benchmark for child poverty estimation using satellite imagery, offering potential applications for data-informed policy and social science research in regions lacking detailed survey data. The dataset and benchmark may support further studies in poverty estimation using satellite data. Given the global availability of satellite imagery, this framework could be adapted to analyze other social or environmental indicators.",
            "- * Impacting problem and extensive dataset.",
            "- - The dataset focuses on an important task in the Global South, while most existing remote sensing datasets were developed for the Global North.\n\n- The paper includes two simple baselines that help to understand the dataset.",
            "- - The paper gives sufficient background about the definition of childhood poverty and how it is measured\n- The paper contributes a benchmark dataset that covers an application area of health/development that is not well covered by existing benchmarks\n- Two simple baselines that use no satellite imagery are reported to ground the lower limit of expected performance"
        ],
        "weaknesses": [
            "- 1. Limited Exploitation of Multispectral Data\nWhile the \"KidSat\" dataset includes multispectral satellite imagery from sources like Sentinel-2, the study does not fully utilize the additional spectral bands beyond RGB. The paper mentions that DINOv2 relies solely on RGB bands, potentially limiting the model\u2019s ability to capture information relevant to socioeconomic indicators, such as vegetation health or water bodies. Future work could include experiments with multispectral configurations or foundation models designed to leverage the entire spectral range available in Sentinel and Landsat imagery.\n\n2. Constraints on Temporal Prediction Capability\nThe temporal benchmark reveals challenges in predicting poverty indicators in years beyond the training period (2020\u20132022). This suggests that the current models may be overfitting to historical data patterns and struggle to generalize to newer datasets. Incorporating temporal encoding or exploring time-series models may improve the framework\u2019s capability to handle time-sensitive socioeconomic changes. Furthermore, training models with time-stamped data could aid in detecting trends, which would be valuable for forecasting.\n\n3. Fixed Input Size and Resizing Effects on Model Performance\nThe study limits DINOv2 and SatMAE\u2019s performance by requiring the imagery to be resized to a specific input size (224\u00d7224 for certain configurations). Resizing can lead to information loss, particularly when working with high-resolution satellite images. Evaluating models with variable or adaptive input sizes, or employing patch-based processing for high-resolution images, might preserve more spatial detail and enhance prediction accuracy.\n\n4. Absence of Uncertainty Estimation\nThe current approach is deterministic and does not quantify uncertainty in model predictions, which is crucial for policymakers who may rely on this data to inform decisions. Including uncertainty metrics, such as confidence intervals or probabilistic models, would provide a more comprehensive assessment of model reliability, particularly in areas with sparse data coverage.\n\n5. Limited Comparison with Non-ViT Architectures\nAlthough the study highlights the use of Vision Transformers (ViTs), the experiments with alternative architectures, like CNN-based models, are limited and do not include any recent transformer alternatives. Comparing ViTs with other state-of-the-art architectures, particularly those optimized for spatial analysis, would help clarify the advantages and limitations of ViTs in the context of satellite imagery-based poverty estimation.\n\n6. Dataset Coverage and Geographic Bias\nThe dataset, while extensive, is geographically focused on Eastern and Southern Africa. This focus may limit the applicability of the benchmark models to other regions with different socioeconomic and environmental conditions. An expanded dataset that includes diverse regions globally could help generalize the findings and provide a more comprehensive tool for poverty estimation.\n\n7. Lack of Direct Validation with Ground Truth\nThe study\u2019s reliance on DHS survey data as ground truth is practical, but there is limited discussion on validating model predictions with additional sources or cross-referencing with other poverty metrics. Incorporating data from other surveys, censuses, or economic indicators could enhance model robustness and provide a broader validation framework.",
            "- * methodological questions are not investigated in-depth. E.g., what impact does the pre-training algorithm have on downstream performance in general. Are (reconstruction-based) MAE features better than Dino (joint-embedding based) features (see for instance, Shekhar et al., 2023 for a comparison here) in general for poverty mapping? Do takeaways in that direction align with existing papers? \n\nShekhar, S., Bordes, F., Vincent, P., & Morcos, A. (2023). Objectives matter: Understanding the impact of self-supervised objectives on vision transformer representations. arXiv preprint arXiv:2304.13089.",
            "- - The description of the satellite data is incomplete (I am aware that the satellite specifications are listed in the supplementary material but I am referring to the preprocessed satellite imagery used as model input). For example, key information such as the processing level, the selected spectral bands, and the spatial resolution of the satellite imagery are missing. This also applies to the abstract which does not provide any information on the satellite imagery.\n\n- A literature review on relevant remote sensing studies mapping poverty is missing. Instead, the related work section cites several less relevant datasets such as xView2 or SpaceNet 7.\n\n- Since nighttime light data is a key modality for poverty mapping, it is surprising that the authors did not include it. Therefore, I strongly suggest enriching the dataset with nighttime light data from, for example, SDGSAT-1 (ideal due to its high spatial resolution) or VIIRS.  \n\n- The visualizations are not useful for understanding the spatial patterns the models produce since they cover extensive areas. Therefore, I request some examples of model outputs over metropolitan areas of cities known to have deprived neighborhoods  (e.g., Maputo, Mozambique). This will lead to great insight into the capability of models trained on KidSat. \n \n- For the spatial benchmark, I assume the clusters were randomly split into five folds. However, this split largely overlooks spatial autocorrelations and does not sufficiently test generalization ability across space (see Rolf et al., 2024). This limitation is also indicated by the performance of the Gaussian process regression baseline. Therefore, I recommend adding experiments for a region-based split.",
            "- - The biggest limitation of this paper in my opinion is that it is unclear if this problem should be formulated as a satellite ML benchmark at all. Since the measures in the child poverty index are in most cases not directly observable from satellite data (especially at the resolution of Landsat and S2), it's unclear what features the authors expect to be learned. They even say on L385-386: \"Satellite imagery is at best a proxy for multidimensional child poverty.\" Should we be prioritizing the use of ML to predict variables based on satellite data when that data is *at best* a proxy (at worst, not informative at all?)? What are the risks of the conclusions that can be drawn from such analyses?\n \n- The results in Table 2 show that *some* useful information might be present in the satellite data because all of the imagery-based models do at least marginally better than the no-imagery baselines. However, it's not clear from the results if more sophisticated methods are needed to improve results, or if more sophisticated data are needed, since all of the models perform in a similar range that is not dramatically better than a no-imagery baseline.\n\n- There is a lot of space in the text spent on redundant descriptions of the analysis, data, and discussion. Much of this space would have been better spent on additional experiments that would help better characterize the results and their utility, and evaluation with more modeling scenarios. For example: \n  - The authors only tested self-supervised foundation models that ingest images. I think simpler baselines that use the satellite data but do not learn spatial correlations, like a random forest or other pixel-based methods, would be useful baselines to include to test the hypothesis that spatial structure is actually being learned. \n  - The authors say that they only used RGB imagery to \"directly compare\" DINO with SatMAE. However, these methods are not directly comparable anyway - for example, different setups for the temporal benchmark. This does not seem like a sufficient reason not to test the multispectral versions of the data with SatMAE.\n  - Results are reported as an average across the entire test dataset. Disaggregating these results by subgroup (e.g., country or the bins in Figure A.4) would give a more nuanced understanding of model performance and differences in performance. This would also be more useful for stakeholders to understand the utility (or lack thereof) of using the methods measured on this benchmark.  \n\n- Why was the SatMAE temporal model used if you did not include temporal information (replicated the same single-timestep input)? There is no temporal information being given to the model (except maybe the year of the data?).\n\n- Questions about the dataset\n  - The Sentinel-2 archive is only available since 2015/2016 but the survey data covers 1997-2022. Does this mean less data is used in the Sentinel-2 experiments?\n  - For Landsat, data was drawn from Landsats 5, 7, and 8 to cover the survey period. There are some differences between these three datasets that could increase the heterogeneity of observations in the survey period, potentially a source of error that was not discussed in the paper. Did the authors used a harmonized version of these products?"
        ]
    },
    "C9BA0T3xhq": {
        "venue": "ICLR 2025",
        "title": "Optimizing Q-Learning Using Expectile Regression: A Dual Approach to Handle In-Sample and Out-of-Sample Data",
        "link": "https://openreview.net/forum?id=C9BA0T3xhq",
        "abstract": "Offline Reinforcement Learning (RL) presents unique challenges, primarily due to the constraint of learning from a static dataset without additional environmental interaction. Traditional methods often face limitations in effectively exploiting the available data, particularly when navigating the exploration-exploitation trade-off inherent in RL. This paper introduces a novel algorithm inspired by Implicit Q-Learning, designed to extend the utility of the Bellman update to actions not explicitly present in the dataset. Our approach, termed Extended Implicit Q-Learning (EIQL), strategically incorporates actions beyond the dataset constraints by allowing selection actions with maximum Q. By doing so, it leverages the maximization capability of the Bellman update, while simultaneously mitigating error extrapolation risks. We demonstrate the efficacy of EIQL through a series of experiments that show its improved performance over traditional offline RL algorithms, particularly in environments characterized by sparse rewards or those containing suboptimal and incomplete trajectories. Our results suggest that EIQL enhances the potential of offline RL by utilizing a broader action spectrum.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3
        ],
        "strengths": [
            "- N/A",
            "- The problem being worked on is well-motivated and important. There are a lot of experiments with potentially significant results.",
            "- No",
            "- 1. The paper is well structured. The proposed idea is simple and easy to implement.\n2. Empirical results are provided on various robotics benchmarks, showcasing the effectiveness of the approach."
        ],
        "weaknesses": [
            "- This paper exhibits a lack of rigor and completeness, with several critical issues that impact both readability and credibility. The related work section is notably underdeveloped, which severely hampers an understanding of how this work fits within the existing body of research and fails to establish a clear contribution. Additionally, the paper's notation is inconsistent and ambiguous, with numerous undefined terms and unclear derivations, leading to confusion in understanding the methodology. Key elements such as Theorem 4.1 are poorly presented, lacking in both formal results and coherence. Section 4.3 appears disconnected from the rest of the paper, as there is minimal context or explanation for its inclusion. Moreover, the paper is marred by formatting errors, including unformatted algorithm environments and incorrect citation formats, which contribute to an unprofessional presentation. In its current form, the paper does not meet the standards expected for an ICLR submission and would benefit greatly from a substantial revision before being reconsidered for publication.",
            "- I had a hard time understanding this paper. The writing is unclear in many parts and there are inconsistencies in the notation. I detail some of them below, but I think the amount of rewriting required to get this paper to an acceptable level is beyond what can be done in the rebuttal period.\n\n- I understand that this work builds on prior works, however, every paper should stand on its own in some way. In this work, the authors build on Implicit Q-Learning, but there have been no introductions of IQL even just to set up the notation, the background and give some context. It is not necessary to give an in-depth discussion but if it is a central work that is being built upon, a short summary or setup would clarify what is being added. \n  - For example, $L_2^\\tau$ is not defined anywhere in the current work. Similarly, L164 mentions that the value function in (Kostrikov et al., 2021) is about to be defined but there is no context as to why, what it means, or how it relates to the current work. The few words in L171-173 only add to the confusion as it seems to assume that the reader is coming in with the discussion of (Kostrikov et al, 2021) fresh in their minds. \n- The notation changes from L163 to L167-170 for the $\\tau$-th expectile of a random variable $X$.\n\n- Some examples should be cited in L097 for the offline RL algorithms that are alluded to. \n- It\u2019s unclear what Section 4.3 is trying to show until the end of the section, this should be stated clearly at the beginning.\n\n- 3 seeds for the experimental results is too small. Even just comparing to IQL, there should be at least 10 per experiment.\n- Figures 1 and 2 looks like each experiment was only done with one seed, which makes it hard to draw conclusions with confidence. I also don\u2019t see how the conclusion in L358-359 was derived from Figure 2. All the methods look pretty much the same.\n\nMinor:\n- The citations should be in parenthesis when they are not used as the subject of the sentence (e.g. in 042, it should be (Levin et al, 2020), similarly in L045, L048, etc.\n- Eq (2), $V_\\psi$ has not rendered correctly\n- The fonts and rendering of Figures 1 and 2 are too small to be readable on paper and become blurry when zoomed in\n- These figures are labelled as \u201canalysis\u201d but I would call them experimental results.",
            "- 1. The theoretical analysis is misleading as a loss function cannot be regarded as a random variable. \n2. The experiments exclude the SOTA offline RL algorithms",
            "- 1. The paper could benefit from a small discussion on model-based approaches for offline RL, such as MOReL[1] and MOPO[2]. \nPenalizing OOD (Out-of-Distribution) actions is not the only way to handle action extrapolation in offline RL.\n\n2. The related work section is missing key references, particularly the omission of Sparse Q-learning (SQL) and Exponential Q-learning (EQL) [3], which is critical. These methods have outperformed IQL on suboptimal trajectories.\n\n3. The paper does not explain Implicit Q-Learning (IQL) in the preliminaries, making it difficult to follow Equation 1. It is important to describe the notations used with the equation to improve readability.\n\nMinor:\n\nTypo in quotes line 321\n\nNo hyperparameters have been provided in appendix making it challenging to reproduce the work\n\n[1] Kidambi, Rahul, et al. \"Morel: Model-based offline reinforcement learning.\" Advances in neural information processing systems 33 (2020): 21810-21823.\n\n[2] Yu, Tianhe, et al. \"Mopo: Model-based offline policy optimization.\" Advances in Neural Information Processing Systems 33 (2020): 14129-14142.\n\n[3] Xu, Haoran, et al. \"Offline rl with no ood actions: In-sample learning via implicit value regularization.\" arXiv preprint arXiv:2303.15810 (2023)."
        ]
    },
    "B6xUlbgP7j": {
        "venue": "ICLR 2025",
        "title": "BRAIN: Behavioral Responses and Artificial Intelligence Neural-Modeling for Consumer Decision-Making",
        "link": "https://openreview.net/forum?id=B6xUlbgP7j",
        "abstract": "This research investigates consumer neuroscience and neuromarketing through a multivariate methodology, employing Principal Component Analysis (PCA) and deep learning neural networks to interpret consumer responses to functional products. EEG signals were collected, recorded, and analyzed from 16 individuals aged 20 to 29 to identify significant neuronal markers related to consumer choices. The pivotal factors influencing decision-making were identified as the low beta and low gamma frequency bands, as well as participants' attention and meditation levels. The findings validate the effectiveness of our approach, demonstrating its applicability across various fields requiring accurate and reliable classification. Additionally, it is recommended to explore the potential applications of this study in the food industry by creating personalized nutrition strategies based on individuals' brain activity patterns.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- Regrettably, the submission does not meet the rigorous standards expected of an academic publication.",
            "- The paper presents a new and promising idea. However, the chosen approach is quite simple and does not introduce new methods or models that advance existing research. The study would benefit from a more innovative methodological contribution to set it apart from previous work in the field.",
            "- Interesting Application: Using EEG data in this research for consumer preference assessment of functional foods falls within the currently developing interests in personalized nutrition and neuromarketing.\n\nCombining PCA and a DCNN in EEG data management is a good choice because this study focused on the decision-making analysis on the beta and gamma bands.\n\nPractical implications: These findings are valuable pieces of information that could be very useful for direct marketing and product development directed toward consumers in the food industry, especially regarding healthier products.",
            "- The logic of the paper is clear, with a well-structured presentation of the model framework that effectively integrates PCA with DCNN framework. The strong performance metrics in application underscore the model\u2019s capability to predict consumer preferences."
        ],
        "weaknesses": [
            "- The writing style of the manuscript presents significant challenges to comprehension. Specifically, it lacks adequate detail regarding the reproducibility of the research, particularly in terms of the specifications for the machine learning models employed, including differentiating between image data and time-series data, among other factors.",
            "- The introduction lacks a comprehensive summary and does not adequately convey the motivation for the study. Instead, it reads more like a general document on the use of EEG in consumer choice analysis without clarifying the specific approach or objectives of the present work. Furthermore, the introduction consists of a single paragraph with little conceptual linkage between the topics covered. A clearer structure and a more coherent explanation of the purpose and methodology of the research are needed to explain in the introduction.\n\nThe study only considers the sensory or taste aspects of the product as the primary factor influencing consumer preferences, which is insufficient to provide valuable insights for the food industry in the context of product development. A more comprehensive assessment that includes additional factors such as texture, aroma, visual appeal and emotional response would provide a more complete understanding of consumer preferences and increase the relevance of the study to the industry\n\nSections 2.2 to 2.3.4 of the paper primarily resemble tutorials on EEG signals and their acquisition processes rather than focused discussions relevant to the study's research questions.\n\nThere is no related work provided on the existing studies on the given topic.\n\nOverall, this manuscript lacks the scholarly depth and clarity expected of a research paper. The presentation of ideas is often unclear and insufficient attention is paid to structure, coherence and technical detail necessary to effectively communicate the research objectives, methodology and results.",
            "- Small Sample Size and Generalizability: The small sample size of 16 limits the generalizability of the findings. Testing a larger and more diverse population will provide a more robust base for the findings.\nI'd say this study lacks comparative analysis with previous models or even traditional machine learning techniques since the outperformance of this proposed approach over simpler or alternative models is not clear.\n\nLack of Reproduction Instructions: Important parameters like PCA as well as the DCNN architecture used have not been described. An entire hyperparameter table along with data augmentation strategies would be useful in further increasing reproduction and clarity.\n\nOveremphasis on Beta and Gamma Bands: Though beta and gamma rhythms are relevant to decision-making, excessive concentration may neglect other EEG components that could be significant for consumer preferences.",
            "- - The paper lacks a comparative analysis, making it difficult to assess how the proposed method measures up against existing approaches. \n\n- It is challenging to determine the solidity of the contribution, as the overall framework\u2014comprising EEG acquisition followed by a deep convolutional neural network (DCNN)\u2014appears relatively simple, and the innovative aspects of the research are not clearly articulated.\n\n- While the application results are promising, the paper does not provide sufficient validation to validate the results of these findings. \n\n- Additionally, the tables included in the paper appear to be screenshots, resulting in distortion that affects their readability and clarity.\n\nIn the captions of Figures 7, 8, and 9, the authors refer to the \u201cEfficiency of BRAIN Architecture including $\\bar{\\beta}$ and $\\bar{\\gamma}$ brain rhythms in training, validation, and test phases.\u201d However, they provide no context or explanation on how the data was split into training, validation, and testing. Additionally, the figures themselves only present confusion matrices and a single ROC curve, with no clear indication of how validation and testing were performed or represented."
        ]
    },
    "5XL8c0Vg9k": {
        "venue": "ICLR 2025",
        "title": "Infinite-parameter Large Language Model",
        "link": "https://openreview.net/forum?id=5XL8c0Vg9k",
        "abstract": "In the standard transformer architecture, increasing model parameters leads to linear growth in computational cost and activation memory. To address this issue, we propose a novel Infinite Parameter Large Language Model (IP-LLM) architecture that decouples model size from computational cost and device memory. Existing large language models are all fixed-parameter models, while human knowledge is infinite and expands daily. Finite parameters are inherently limited in their capacity to accommodate this boundless knowledge. Our IP-LLM architecture can potentially accommodate infinite knowledge, resolving this issue and laying the foundation for realizing a truly omniscient and omnipotent artificial general intelligence in the future.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- \u2022 An interesting attempt to look for continuous scalable architecture for LLMs.",
            "- The idea of routing to domain-specific parameters is interesting (though more discussion of related work is needed, e.g. [1][2]). \n\n\n[1] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models, Li et al 2022\n\n[2] Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM, Sukhbaatar et al COLM 2024",
            "- The paper tackles an important question.",
            "- The authors propose a novel pre-training strategy to parse the general linguistic comprehension skills in the base model and later train individual experts on top on different domains."
        ],
        "weaknesses": [
            "- \u2022 It is not clearly to me how the proposal can achieve infinite-parameter models just by classifying the input to into different classes and training/using different networks for different classes.",
            "- The paper appears to be in an early stage. For example, key details of the method and experiment are not described or justified, and only 1 experiment (not fully described) has been performed. There is also missing discussion of key related work (e.g. [1], [2]). Here are some specific examples:\n\n- The datasets have not been described. The data can substantially impact the downstream tasks that are evaluated in the experiment. Similarly, the number of tokens trained on is important and has not been reported.\n\n- The evaluation is done on a proprietary evaluation pipeline, making reproducibility difficult.\n\n- The experiments need a controlled comparison of the method against alternatives. Currently it is difficult to draw conclusions from the experiment provided. For example, IPLLM-24B and Qwen1.5-32B are not comparable since IPLLM has been trained on additional domain-specific data (which has not been specified, and may be relevant for the experimental comparison). One example comparison could be finetuning Qwen1.5-32B on the union of corpora that IPLLM finetunes on.\n\n- Several claims made in the introduction and conclusion have not been justified. For example:\n    - \"Significant advantages in terms of reduced device memory requirements for both training and inference\": this has not been justified. For example, the proposed method requires two forward passes at inference time, and an additional 4 x (number-of-domains + 1) layers to train.\n    - \"Enabling the model to learn new knowledge without catastrophic forgetting\". This has not been justified experimentally.\n\n- Ablations on key design decisions have not been done. For example, the routing strategy, number of layers, and the base model.\n\n- Regarding novelty, Branch-Train-Merge [1] proposed to train different parts of the model independently on different subsets of the data (each subset corresponding to a domain, such as scientific or legal text). They also have a domain posterior that models the probability of a sequence belong to each domain (akin to the functionality of the proposed router). BTM and related follow-up work such as Branch-Train-MiX [2] should be discussed and compared with.\n\nI would encourage the authors to continue improving the work since their idea has potential, but I believe the current manuscript is not yet ready for ICLR. \n\n[1] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models, Li et al 2022\n[2] Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM, Sukhbaatar et al COLM 2024",
            "- - The paper does not make a distinction between the proposed approach from MoE training. And if there is indeed a difference, please include the MoE baseline. \n- **Major issue**: The paper compares their model trained on downstream tasks with other pre-trained models, zero-shot on the downstream tasks. Therefore, it is not making an apples-to-apples comparison\n- It would be important to add in a couple of baselines to showcase the benefit of the proposed method over others\n  - A single model trained on all the data that the base, router, and individual experts are trained on\n  - MoE baseline trained on the data used to train the IP-LM. \n- Why are there no entries in the table for some models on C-Eval?\n- There are not many details provided on training, model architecture, and dataset. Unclear what data is additionally used to train the base model. What is the architecture of the model? \n- The writing in the paper is clear but not precise. For example, the abstract or intro does not tell you anything concrete about what the paper builds, it only goes so far as to specify the problem and motivate it.",
            "- 1. Evaluation\n\n- I think the biggest issue in current manuscript is evaluation of the model. While the current evaluation only includes monolithic architectures without MoE strategy, wouldn't it be fairer to include the models using MoE's in terms of both performance and memory/compute efficiency? \n\n- The authors claim in the list of contributions that the new approach allows higher routing accuracy but I cannot find the explicit result for this.\n\n- Also the authors claim the memory and training efficiency but having a explicit numerical comparison and what exact 'training cost' is meant here. \n\n2. Related works on lifelong learning / continual learning using MOE\n\n- I believe there are already few literatures on lifelong learning of LLM using MoE e.g Chen et al (2023) https://arxiv.org/pdf/2305.12281. I suggest authors to incorporate more relevant literatures and what is the novelty of their method. \n\n3. Paper presentation should be improved\n\n- There are many typos and inconsistent citing notations which makes readability very low. For example, I found Section 2 related work very hard to parse the included citations (spacing, parentheses etc). I highly recommend authors to do careful proofreading of the entire manuscript. \n\n- Section 4 training strategy can be improved with adding a schema for better delivery."
        ]
    },
    "473sH8qki8": {
        "venue": "ICLR 2025",
        "title": "Reward as Observation: Learning Reward-based Policies for Rapid Adaptation",
        "link": "https://openreview.net/forum?id=473sH8qki8",
        "abstract": "This paper explores a reward-based policy to achieve zero-shot transfer between source and target environments with completely different observation spaces. While humans can demonstrate impressive adaptation capabilities, deep neural network policies often struggle to adapt to a new environment and require a considerable amount of samples for successful transfer. Instead, we propose a novel reward-based policy only conditioned on rewards and actions, enabling zero-shot adaptation to new environments with completely different observations. We discuss the challenges and feasibility of a reward-based policy and then propose a practical algorithm for training. We demonstrate that a reward policy can be trained within three different environments, Pointmass, Cartpole, and 2D Car Racing, and transferred to completely different observations, such as different color palettes or 3D rendering, in a zero-shot manner. We also demonstrate that a reward-based policy can further guide the training of an observation-based policy in the target environment.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3
        ],
        "strengths": [
            "- 1. The paper is well written and easy to understand\n2. The method is principled and intuitive. \n3. The authors perform ablations and provide statistically significant results.",
            "- The paper is very well-written and very easy to follow and understand.",
            "- **S1. Thorough Challenge Analysis**: The authors provide a useful in-depth examination of the difficulties inherent to reward-based policies, such as poor observability, difficulty in estimating value functions, and limited exploration. This highlights the limitations of using a scalar reward signal rather than high-dimensional state information. They emphasize the necessity of dense rewards, showing how the method struggles as dimensionality increases.\n\n**S2. Expert Guidance Ablation.** An ablation study on the behavior cloning loss component shows how important it is, especially in the more complex task like Car Racing.\n\n**S3. Simplicity and Comprehensibility.** The method is straightforward, with minimal components and no additional tunable hyperparameters, making it easy to implement and understand.\n\n**S4. Robustness to Observation Quality**: Once trained, the model is independent of observation quality, allowing it to perform well even with noisy or degraded visual observations.\n\n**S5. Effective Reward Estimation for Transfer.** In Section 4.4, the authors demonstrate the transfer capabilities by estimating rewards directly from images, allowing the reward-based policy to perform well even without access to the true reward at inference. They further show that transferring from a 1D reward signal is easier and more reliable than transferring from high-dimensional observations, requiring minimal data and proving more resilient than state estimation.",
            "- - **Novel Approach:** The paper presents an innovative method for training policies using only reward and action histories, offering a fresh perspective in reinforcement learning.\n\n- **Intuitive Explanation:** The authors clearly and convincingly explain why reward-based policy learning can be effective, especially in navigation tasks. For example, learning a goal-conditioned policy remains feasible by only incorporating the reward signal."
        ],
        "weaknesses": [
            "- 1. **Lack of a justifiable motivation:** It's unclear why one would use only rewards as the observation space in practice. For any complex dynamical system, having a policy dependent on an observation is required. Theoretically, this would only work in stationary bandit-like settings, where a single action optimal action exists from the ones available and a history of past actions and rewards would suffice to take the right action. However, in all other \u201cRL\u201d settings where an action taken changes the state of the world, i.e., environments with a transition function, this method would break. Also, it\u2019s hard to motivate this method from a practical perspective - in most real applications, one would utilize all possible information available to learn a behavior. In short, the authors claim they study a much more complex problem - that of reinforcement learning, while applying a primitive set of assumptions - those of a multiarm bandit. I don\u2019t believe this method would work under the advertised conditions. Additionally, I question the claims of generalization in the paper - but of course, the method would generalize across visual observation perturbations. It is not conditioned on these perturbed inputs.  \n2. **Unclear why this works:** I suspect the method is able to solve tasks due to the demonstrations available to it - not due to the online PPO. This is clear in the ablations also - for tasks that actually have a transition function like Car Racing, the demonstrations yield appreciable improvements over not using them. \n3. **Lack of Novelty:** It\u2019s unclear to me whether this paper adds to existing knowledge in any way. Multiarmed Bandits are well studied and memory based agents are well studied also. The paper currently does not present any new theoretical insights, nor does it show massively scaled experimentation.",
            "- I think the major problem with the paper is that the studied setting is very unrealistic. The paper provides some discussions, which also acknowledge how difficult it is for their assumptions to be satisfied. At the end, it is still difficult to think of any application where the proposed approach is going to be useful.\n\nSpecifically, I am not worried about the assumption that the reward is observable. But what are some real-world problems / environments where an RL agent can perform reasonably well just by seeing the previous actions and rewards? As the paper acknowledges, this is exactly like trying to play a game by looking only at the score and no other parts of the screen. Other than environments where it is possible to memorize the solution, it seems to me that these conditions are extremely hard to satisfy. And this is not even the only assumption. In addition to these, there should also be some transfer concern where the robot's transition and reward functions stay the same but the observations change. I don't see any useful application.\n\nA somewhat promising part of the paper is the section where it tries to estimate rewards from observations. However, that section requires access to a state-based policy that solves the task. Again, this is very unrealistic. If the problem is a POMDP, how would one have access to a state-based policy? The states are not even known to the agent.\n\nFinally, the main promise of the paper is not very interesting. When the learned policy does not depend on the observations, of course the observation function can be changed in any arbitrary way. Under that setting, that function is completely irrelevant. So the experiments are completely unsurprising and obvious.\n\nBelow are my other comments that are more minor:\n- et al. is plural, so those citations should be thought as \"they\" instead of \"it\". There are some grammatical errors about this.\n- The POMDP definition is missing the function that maps the states to observations.\n- The so called reward-based policy is defined as $R\\times A \\to A$ but my understanding is that a full history of rewards and actions are inputted, not just the most recent ones. In that case, this definition of the policy function is incorrect.\n- Incorrect capitalization in line 248.\n- Again, the second hypothesis/question (line 253) is validated by the definition of reward-based policy. Why is it even a hypothesis?",
            "- **W1. Numerous Assumptions.** The method makes many assumptions for successful implementation, limiting its adaptability and real-world applicability.\n\n1. **Dense and Accurate Rewards**. The authors state that rewards must be *sufficiently dense* and *give a good value throughout the state space*, yet there is no analysis of how sparse they need to be. This lack of clarity makes it uncertain how sparse or dense rewards can be before training fails. In practice, it likely means that transitions without an informative reward signal contribute minimally to training, making the approach highly sample-inefficient. In realistic settings, feedback is often imperfect or delayed. For example, in robotics, sensors might provide inaccurate readings due to interference or hardware limitations, leading to noisy or inconsistent rewards. Similarly, in environments where rewards are human-generated (like feedback in recommendation systems), subjective or inconsistent responses can introduce noise.\n2. **Domain Compatibility**. The source and target environments should share the same transition dynamics, action and observation spaces, and reward structure. This overlooks many practical cases, where slight discrepancies in dynamics or observation structures are the norm. The authors don\u2019t discuss where these conditions might realistically apply, leaving practical feasibility unexplored.\n3. **Expert Guidance**. The ablation study shows that expert guidance significantly boosts performance, particularly on complex tasks. However, this dependence undermines the claimed benefits of a \u201creward-only\u201d approach, as it reintroduces standard RL processes the authors aim to bypass. Moreover, training an observation-based expert policy adds a computational overhead, and since training is done online, inference also needs to be run on the expert model. \n\n**W2. Loss of Spatial Context.** By relying solely on rewards and actions, the method lacks spatial awareness, which is critical in tasks requiring an understanding of position or orientation. For example, in navigation tasks like maze-solving, an agent without spatial context will struggle to differentiate between distinct but similarly rewarding areas, such as identical-looking corridors or dead ends. In manipulation tasks, the lack of positional feedback leads to incorrect actions, like reaching for objects without adjusting for their relative location. Without such spatial cues, the method is limited to simple tasks where positioning doesn\u2019t play a role. The agent\u2019s ability to keep the car centered on the track in the Car Racing environment hinges on the overly-engineered reward function that promotes this behavior. Relying on such finely-tuned rewards is impractical in real-world applications, where crafting reward functions to this level of precision is rarely feasible.\n\n**W3. Lack of methodological novelty**. The approach lacks innovation, relying on a simple combination of an LSTM and behavior cloning to train PPO. This simplicity offers little advancement over existing techniques and contributes minimally to the field, as both components are well-established in RL. Moreover, the explanation of the method is scarce. Neither the text nor the caption of Figure 3 adequately explains the diagram, making it difficult to interpret. For example, it\u2019s not immediately evident that the \u2018options\u2019 in the testing phase of Figure 3 depict the modes where the environment provides the reward and where the reward is learned.\n\n**W4. Weak Experimental Evaluation.**\n1. The experiments are limited to simple environments (Pointmass, Cartpole, and Car Racing), with the authors suggesting that more complex environments with higher state dimensions or sparse rewards are \"impossible\" for this method. This raises questions about practical utility\u2014if the method can't handle harder, more realistic tasks, its applicability remains unclear.\n2. The authors only compare their reward-based policy to a regular observation-based policy, which is supposed to serve as an upper bound. However, they include no comparisons with other established techniques whatsoever, offering no context on how their approach measures up to alternative methods in this setting.\n3. While evaluation is done over 50 trials, the number of seeds used for training isn\u2019t specified. Without this, it\u2019s hard to assess the robustness of the method.\n4. The authors state that *it is essential that some recurrent network is used*, and that that *a single reward/action pair is not sufficient*. However, they provide no analysis or ablation study to clarify the LSTM\u2019s specific contribution. Alternative approaches for maintaining temporal history, like stacking previous rewards and actions, are not explored, leaving it unclear whether the LSTM is genuinely necessary or if simpler methods could achieve comparable results.\n\n**W5. Transfer Limitations**. The authors attempt 2D-to-3D transfer by using the Car Racing and AirSim environments, as shown in Figure 1, considering a scenario where changes only occur in pixel-based observations. This leads them to circumvent the realistic challenges of transfer by manually reimplementing the kinematics to match across environments. This undermines the notion of true zero-shot transfer, as manual alignment is rarely viable in real-world applications and reveals the method\u2019s limited applicability. Furthermore, no quantitative results are provided for the 3D transfer; it\u2019s merely claimed that the policy shows \u201creasonable driving performance,\u201d leaving the success of this transfer largely unsubstantiated. The authors\u2019 statement that \u201cour focus is not on dealing with the dynamics shift\u201d dismisses the real complexities of 2D-to-3D transfer, where adapting to dynamic differences is unavoidable. This approach fails to demonstrate genuine transfer.",
            "- - **Limited Real-World Applicability:** Training without direct observations restricts the method\u2019s applicability, as real-world scenarios often involve more complex changes beyond observation shifts. This limitation raises concerns about the generalizability of reward-conditioned policies in diverse settings. What are the possible scenarios in the real world where only observation changes within the MDP? \n- **Scalability Issues:** While the reward-based learning approach shows promise in simpler environments like Cartpole, its effectiveness in more complex tasks such as locomotion, manipulation, or humanoid control remains uncertain. Can reward-based policy handle complex locomotion or manipulation tasks? Since this work focuses on understanding transfer benefits, there is concern that it might struggle to learn necessary behaviors before transfer."
        ]
    },
    "2DD4AXOAZ8": {
        "venue": "ICLR 2025",
        "title": "Inference-Friendly Models With MixAttention",
        "link": "https://openreview.net/forum?id=2DD4AXOAZ8",
        "abstract": "The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by Character.AI. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            3,
            1
        ],
        "strengths": [
            "- The idea is simple and clear, the experimental setup is also quite clear.",
            "- 1. The combination of sparsifying the token of sequence and sharing the KV cache across layers seems to be a promising method to reduce the inference cost. This paper conducts some interesting experiments, from pre-training to evaluation, to give us some insights regarding the impact of different choices of the setups of such combination.\n2. The experiment setup is reasonably designed.",
            "- - Cache sharing across layers has not been extensively studied and ablated over, and so this paper provides additional sample points that show the relationship between cache sharing approach and performance. \n- The authors tested their results on RULER which is a long-context benchmark and more conventional evals such as MMLU and HellaSwag through the Gauntlet evals framework which unveils differences in performance between different KV-cache sharing approaches.\n- Some of these KV-cache sharing variants perform as well as standard attention while being significantly cheaper in compute and memory.",
            "- The paper is easy to follow and unlike most approaches that use custom device-level code to make inference efficient, the approach doesn't require any custom kernels. This makes the approach easier to adapt to slight changes in the model architecture or running inference on hardware from other vendors."
        ],
        "weaknesses": [
            "- 1. This paper lacks innovation; both the recent window and multi-layer attention are established techniques. The paper simply combines these two methods without any improvements.\n\n2. The experimental results are presented solely as bar charts. I believe it would be beneficial to include a table with some precise values.\n\n3. This paper resembles more of a technical report rather than an innovative and well-developed research paper,  which does not meet the high standards of ICLR.",
            "- 1. The novelty is limited in two ways. Firstly, it is a straightforward combination of two existing techniques without many adjustments. Secondly, this combination has already been explicitly described in the blog of character.ai, as cited by the authors.\n2. I can get that the value of this paper is to provide some empirical guidelines of this combination method, but still, the new information brought by this paper is also limited. For example, \u201c\u2026having the standard KV cache computed in the deeper layers is more important for long context abilities than the standard KV cache of the first few layers.\u201d has been declared by some existing studies. In general, the experiment conclusions of this paper are some high-level phenomenons, instead of some practical methodology.\n3. The experiments are all based on a 5B MoE model, which makes the generalisability of the conclusions less convincing. \n4. There are quite a few new hyper-parameters getting involved, e.g., for a N-layer model, how to decide which layers are standard attention, which layers are sliding window? how many layers for a KV-sharing group? These decisions are pre-defined in this paper, but what\u2019s really interesting is how to make these decisions wisely given a new model.",
            "- - Lack of insight or discussion as to why certain cache-sharing approaches perform better or worse.\n- The paper lacks novelty, as it mostly relies on architectural configurations proposed by a blog by CharacterAI [1], and as a consequence, it lacks explanation as to why these configurations were selected in the first place.\n- In general, the main critique is that the paper presents only surface level analysis of the observations and does not contribute much to a deeper understanding of why certain cache-sharing approaches perform better than others.\n\n[1] Character.AI. Optimizing AI Inference at Character.AI \u2014 research.character.ai. https://research.character.ai/optimizing-inference/, 2024.",
            "- 1. There is no novelty in the approach. The paper just evaluates the approach proposed in the [blog](https://research.character.ai/optimizing-inference/) by character.AI with slight modifications. Also, there is nothing new written in the paper different from the blog.\n2. The authors have not put in enough effort for the paper. There is no optimization done in SGLang to optimize the inference for sliding window attention baseline.\n3. The paper is poorly written and there are some typos in the paper. For instance, line 199 uses the word 'sequence' twice in succession.\n4. The paper also says to refer to the appendix for a few experiments, however, there is no appendix in the paper.\n5. I don't believe that any amount of experiments can make the paper in an acceptable format since there is no novelty."
        ]
    },
    "f6GMwpxXHG": {
        "venue": "ICLR 2025",
        "title": "ZEPHYR GAN: REDEFINING GAN WITH FLEXIBLE GRADIENT CONTROL",
        "link": "https://openreview.net/forum?id=f6GMwpxXHG",
        "abstract": "Generative adversarial networks (GANs) are renowned for their ability to generate highly realistic and diverse data samples. However, the performance of GANs is heavily dependent on the choice of loss functions, and commonly used losses such as cross-entropy and least squares are often susceptible to outliers, vanishing gradients, and training instability. To overcome these limitations, we introduce zephyr loss\u2014a novel, convex, smooth, and Lipschitz continuous loss function designed to enhance robustness and provide flexible gradient control. Leveraging this new loss function, we propose ZGAN, a refined GAN model that guarantees a unique optimal discriminator and stabilizes the overall training dynamics. Furthermore, we demonstrate that optimizing ZGAN's generator objective minimizes a weighted total variation between the real and generated data distributions. Through rigorous theoretical analysis, including convergence proofs, we substantiate the robustness and effectiveness of ZGAN, positioning it as a compelling and reliable alternative for stable GAN training. Extensive experiments further demonstrate that ZGAN surpasses leading methods in generative modeling.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            3,
            3,
            1
        ],
        "strengths": [
            "- Originality and significance: To the best of my knowledge this is the first work to propose this type of GAN training objective. Improving stability of GAN training by increasing robustness properties via losses more robust to outliers and with bounded gradients is an interesting idea worth investigating. \n\nClarity and Quality: The paper is well written, and the approach is properly related and contrasted with the related GAN literature. Motivations are clearly stated. The algorithm is well described and easy to understand. The paper provides both a theoretical analysis of the algorithm\u2019s property and quantitative and qualitative empirical evaluation.",
            "- * Presentation of the loss function is clear, and paper is well organized",
            "- The article introduces a novel loss function, called zephyr loss, for GANs, which is referred to as ZGAN. This loss function is convex, smooth, and Lipschitz continuous, designed to enhance robustness and provide flexible gradient control.",
            "- 1. The zephyr loss function is proposed to address common GAN issues (e.g., vanishing gradients, training instability). This is especially relevant given that the loss is smooth around zero, theoretically improving gradient stability and mitigating exploding/vanishing gradients.\n2. The authors provide detailed proofs for the zephyr loss properties, including convexity, smoothness, and Lipschitz continuity, which contribute to ZGAN's stability.\n3. The experiments on datasets (CIFAR-10, CIFAR-100, STL-10, SVHN) provide evidence that ZGAN outperforms traditional GANs (e.g., LSGAN, WGAN) in terms of inception scores, indicating higher quality and diversity in generated images.",
            "- Novel loss function, could potential help stability (not enough proof for that claim yet). Needs a lot more work to validate it."
        ],
        "weaknesses": [
            "- The main weakness of the paper is what I believe may be an unsound equation derivation (in Eq 2 and 4) which invalidates the proof that the stated generator\u2019s optimization objective approximately minimizes total loss variation, and thus invalidates the larger part of the theoretical analysis.\nMoreover I think that how the generator\u2019s objective is implemented in the code does not even correspond to what is stated in the paper (see question section for details).\n\nThe Zephyr-loss looks very close to a Huber loss, this connection is neither referenced nor discussed.\n\nAnother weakness is that the paper claims benefits of the approach \u2013 compared to its closest parent LSGAN \u2013 but without providing evidence that LSGAN training actually suffers from the ills that ZephyrGAN purports to cure, s.a. Gradient (in)stability and (insufficient) robustness to outliers. It would have been more convincing to measure and highlight these failures/difficulties, to show that they are indeed represent in LSGAN and then cured by ZephyrGAN.\nOne claimed advantage, \u201cFlexibility in tuning with alpha\u201d, clearly isn\u2019t one, since alpha plays the same role as the learning rate (which is present in all other GAN variants).\n\nLastly, the difference in generation quality cannot really be assessed qualitatively on low resolution images (CIFAR-10, STL-10, SVHN). These experiments are much too low scale and quality for claiming the superiority of a generative modeling approach by nowadays\u2019 standards.",
            "- * Much of the theory presented in this work seems incremental (especially Theorems 1-3), essentially reproducing the original convergence results of Goodfellow et al 2016 and Nowozin et al 2016 for a specific loss function. Accordingly, there is the same limitation that convergence results assume the discriminator is fully trained at each step, which is not realistic in practice.\n\n* Concerningly, I do not see any references to the Huber loss or f-gan related papers [1,2]. It seems to me that this method is most likely a specific instance of an f-gan, and would benefit from analysis under this framework.\n\n* While the experimental results seem promising, these datasets are fairly simple and I am not convinced that hyperparameter tuning will not be an issue for more complex datasets.\n\n\n[1] Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-gan: Training generative neural samplers using variational divergence minimization.\" Advances in neural information processing systems 29 (2016).\n\n[2] Kurri, Gowtham R., et al. \"\u03b1-GAN: Convergence and estimation guarantees.\" 2022 IEEE International Symposium on Information Theory (ISIT). IEEE, 2022.",
            "- 1. The empirical results do not strongly support the claims made in the article. For instance, in Figure 4, the visual results of your ZGAN are unsatisfactory and significantly worse than those of state-of-the-art GAN models.\n\n2. The article makes a mistake: even if the loss function is convex, smooth, and Lipschitz continuous, this does not ensure that the training process of GAN is Lipschitz continuous and dynamically stable. I recommend that the author include more analysis on how zephyr loss can stabilize GAN training and ensure that ZGAN meets Lipschitz continuity.\n\n3. The article does not provide proof of dynamic stability near Nash equilibrium points. I advise the author to add this proof about local stability analysis around equilibrium points.",
            "- 1. My primary concern is the Inadequate experimental evaluation conducted in the paper.\n  \u2022 The authors rely solely on the Inception Score (IS) to assess the performance of different GANs, which is insufficient for a comprehensive evaluation. Additional metrics, such as FID and KID, should be incorporated to provide a more reasonable comparison. The FID compares the distribution of generated images to that of real images in the feature space of the Inception model. It captures both the quality and diversity by measuring the distance between the two distributions. FID is particularly useful because it correlates well with human judgment of image quality, making it a more reliable indicator of GAN performance. KID is similar to FID but is based on a different statistical approach. It calculates the squared maximum mean discrepancy between the feature distributions of generated and real images. One of the advantages of KID is that it is unbiased and can be applied to small sample sizes, which makes it useful in scenarios where the number of generated images is limited. incorporating FID and KID alongside Inception Score enables a more nuanced evaluation of GAN performance, addressing the shortcomings of relying on a single metric and providing insights into both the realism and diversity of the generated outputs.\n  \u2022 All experiments are conducted using relatively simple networks and datasets. To strengthen the validation of the proposed loss function, the authors are suggested to employ the proposed loss function to some more advanced architectures like StyleGAN2 and StyleGAN-XL, and test them on more challenging datasets such as FFHQ and AFHQ. StyleGAN2 and StyleGAN-XL are the most popular and advanced GANs. It is necessary to verify the proposed loss function based on these advanced models and challenging datasets.\n  \u2022 Since the paper focuses on the stable training of GANs, it is well known that GANs are even more unstable when trained on a small amount of data. Authors are suggested to conduct experiments with a small dataset to verify whether the proposed loss function can also stabilize GAN training on small amount of training data. Obama-100, Sketch-100, and FFHQ-Sunglasses are three common few-shot image generation datasets. By focusing on these aspects, you can get a clearer picture of the model's stability and robustness.\n  \u2022 The authors only compare the proposed method with WGAN and LSGAN. More comparison with WGAN-GP, WGAN-LP are expected.\n  \u2022 Note that the proposed loss function contains two variables, the authors should conduct ablation experiments on these two variables and list the experimental results in the form of figures or lists, rather than just briefly mentioning them in the main text. Given the current experiment setup, it is difficult for me to be fully convinced of the performance advantages claimed for the proposed method.\n\n2. This paper provides a proof of the Lipschitz continuity for the proposed loss function. However, as far as I am aware, other loss functions commonly used in GANs also share this property. Ensuring the continuity of the discriminator is fundamental to achieving stable GAN training.\n\n3. The 5th section of the paper seems to be redundant, and is not supported by any experimental results. It might be better to place it in the analysis of experimental results in 6th section.",
            "- Theorem 2 is framed incorrectly, its approximately similar, not equal. It would be great to instead give lower and upper bounds.\n\nAs mentioned in WGAN, https://arxiv.org/abs/1701.07875, the Total Variation is not continuously differentiable with respect to the parameters. Since Zephyr loss is equivalent to estimating the Total Variation distance, this is not ideal.\n\nThe experiments alone prevent giving this paper a good score. The generated images are very poor quality, often completely broken and the inception score is extremely low (For CIFAR-10, the authors get an FID of 2.9 when the smallest FID of the WGAN-GP 7 years old paper is 5.34 and they can reach up to 8.59). We are not in 2016, currently they were done with the original setup from DCGAN. The experiments need to be done with modern implementations using Adamw(beta1=0), EMA, and modern methods. The baselines should be taken from their respective papers or re-implemented with modern methods with similar or better FID/IS than original results. \n\nAt this point in time the big problem with GANs is stability when scaling. Making a new loss function is unlikely to solve this problem. The authors needs better experiments and it would be useful to also have DiracGAN (https://github.com/ChristophReich1996/Dirac-GAN) experiments and convergence proof.\n\nIS is not a good metric, this has been mentioned many times in the literature. FID is better, but not perfect. Still, its a better choice."
        ]
    },
    "PcE0yAGAGW": {
        "venue": "ICLR 2025",
        "title": "FSL-MIC: An Attentional Few-Shot Learning Framework for EEG Motor Imagery Classification",
        "link": "https://openreview.net/forum?id=PcE0yAGAGW",
        "abstract": "Electroencephalography (EEG) is a key non-invasive technique used to investigate brain activity, particularly in motor imagery (MI) research. Traditional methods for classifying EEG signals often rely on handcrafted features and heuristic parameters, which can limit generalization across tasks and subjects. Recent advances in deep learning, particularly few-shot learning (FSL), offer promising alternatives to improve classification accuracy in scenarios with limited training data. This study explores the effectiveness of FSL algorithms, including Relation Networks, to enhance MI classification. It also examines how transfer learning and data augmentation techniques contribute to improving classification performance.\n\nWe propose a novel framework with three core modules\u2014feature embedding, attention, and relation\u2014that facilitates the classification of unseen subject categories using only a few labeled samples. The attention mechanism identifies key features related to the query data, while the relation module predicts query labels by modeling relationships between support and query data across subjects. Our experimental results demonstrate the effectiveness of our approach on two benchmark datasets, BCI 2a and BCI 2b, as well as our experimental dataset. The proposed FSL framework significantly outperforms traditional methods, offering promising applications in real-time Brain-Computer Interface (BCI) systems across various EEG setups. This research advances the understanding of machine learning in EEG applications and highlights the potential of FSL techniques in overcoming the challenges of limited training data in MI classification.",
        "decision": "Reject",
        "review scores": [
            1,
            3,
            1,
            3,
            3
        ],
        "strengths": [
            "- This research enhances the understanding of machine learning applications in EEG and emphasizes the potential of FSL techniques to address the challenges posed by limited training data in Motor Imagery (MI) classification.",
            "- The authors have collected a novel MI dataset and have indicated that they will make it publicly available. This contribution is a valuable new resource for the community.\nIn addition, data scarcity is a common issue in BCI, and the authors address this challenge by introducing their few-shot learning framework.",
            "- The authors introduce a relation network-based meta-learning framework for EEG-based motor imagery classification.",
            "- Work addresses the appropriate need for user-specific variability in EEG data and chooses to experiment with comparatively less explored approach of the few-shot learning.\n\nThe paper is decently written and easy to read and interpret.",
            "- **Key Strengths**\n\n* Problem Significance: EEG-based MI classification for BCI applications is a highly relevant research area, especially given the challenges of data scarcity and cross-subject variability.\n* Dataset Contribution: The authors introduce a new dataset specific to their experimental needs, potentially serving as a resource for further MI EEG research.\n* Attention Mechanism and Data Augmentation: The use of an attention mechanism to enhance feature extraction and a data augmentation strategy to improve classification accuracy aligns well with current trends in EEG and time-series signal analysis."
        ],
        "weaknesses": [
            "- There is no substantial innovation in proposed method combining the existing approaches without any significant modifications.\n\nNo comparisons were conducted with existing state-of-the-art methods that have addressed the same issue by leveraging meta-learning, domain adaptation/generalization, etc.",
            "- Major\n- The performance of the few-shot learning framework \u201cRelationNet-attention\u201d does not seem competitive because the baseline \u201cCNN-attention-All\u201d, which is trained on the same data as RelationNet-attention but without using examples from the test subject, systematically performs better. The difference in accuracy seems significant as it is systematically greater than 10%.\n\nMinor\n- Figure 3 misses its x-axis.\n- The method is evaluated on only two benchmark datasets. The claims could be strengthened by conducting experiments on additional datasets. A large collection of MI datasets can be found in the MOABB library (http://moabb.neurotechx.com/docs/dataset_summary.html).\n- The acronym DA is used both for \u201cdata augmentation\u201d and \u201cdomain adaptation\u201d. \n- The \u201cdomain adaptation accuracy\u201d is not defined.\n- As I understand, \u201cCNN-attention-relation\u201d and \u201cRelationNet-attention\u201d refer to the same model. To improve readability, I would recommend using a single name throughout the paper.\n- The quality of the figures and diagrams can be improved.\n- In my opinion, the writing could be improved to better guide the reader through the method.",
            "- (1)\tMotivation \u2013 The meta-learning computational framework focuses on \u201clearning to learn\u201d various tasks (meta training) that contribute to downstream tasks (meta testing). During the meta training, the model\u2019s goal is to uncover common patterns among these tasks and acquire broad knowledge that can be applied in solving new tasks. However, the authors apply very few tasks, specifically left and right hand classification, and these tasks are directly related to downstream applications. The stated motivation for meta-learning is somewhat limited. The authors should clearly indicate how they train their framework in a meta-learning fashion. \n\n(2)\tMotivation \u2013 The authors propose to use few-shot learning, where the model should be trained on very limited data. However, the authors only use the few-shot examples as the \u201csupport set\u201d during the testing phase, which could leak classification information.\n\n(3)\tRelated Works \u2013 Many related works are indeed missing in the field, such as Hou et al., GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-Resolved EEG Motor Imagery Signals, In IEEE TNNLS.\n\n(4)\tExperiments \u2013 The authors are encouraged to conduct experiments on larger benchmarks, such as the PhysioNet dataset and the High Gamma dataset.\n\n(5)    The authors should provide high-quality figures.",
            "- The work doesn't cite a very similar approach by An et al. (2023). However, the authors cite work from An et al. from 2020. Authors must elaborate on their novelty and benchmark performance against similar approaches to claim state-of-the-art performance on few-shot learning. \nLink to the work by An et al. (2023)\nhttps://ieeexplore.ieee.org/abstract/document/10167679/?casa_token=ffiyMyxrlIYAAAAA:XHnQorLPEOuFdPLMhuSnkOj18y4baOutFkRqO4Zu6J1N2pKEBdsQ0cN0PvtXe3_M9R3VZvL1deH3\n\nEEG tends to have high noise and authors though cite this concern and also claim interpretability mentioning: \"A key advantage of this model is its interpretability\", do not share any results, comment or compare the neurophysiological basis of the model predictions. \n\n\nEthical guidelines while collecting personal data need to be clarified. Details on the code of ethics before releasing the data are necessary but missing.",
            "- **Major Concerns and Areas for Improvement:**\n\n1. Limited Novelty in Approach and Research Question\n    * While the problem is essential, the paper does not introduce significant advancements in methodology or approach, primarily adapting existing frameworks for few-shot learning.\n    * The techniques, including data augmentation and attention mechanisms, are well-known and lack customization to the problem at hand.\n2. Insufficient Literature Review\n    * The manuscript leans heavily on a limited set of cited works, neglecting a broader body of relevant and foundational literature. This oversight is evident, for example, in the omission of a citation for the seminal paper on the attention mechanism (Vaswani, A. \"Attention is all you need.\" (2017)), which is essential for context or the omission of the Grad-CAM paper (Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\"\u00a0(2017)), or proper citation to the utilized baseline methods, etc.\n    * The limited literature results in redundancy, where the few sources cited appear multiple times, reducing the depth of the discussion.\n3. Redundant Content and Limited Focus on Methodology\n    * A large portion of the paper is dedicated to reintroducing prior works and discussing the dataset, with limited space allocated to details of the proposed method.\n    * The method's description lacks sufficient depth to fully understand its contribution beyond existing frameworks, making it challenging to assess its true impact.\n4. Results and Experimental Design\n    * The reported results do not demonstrate outperformance over baseline models (CNN-attention-All and CNN-attention-Few). This lack of improvement questions the validity of the proposed framework as a state-of-the-art advancement in EEG classification.\n    * It is unclear why the authors have not tested their own method using a 40-sample case, as they did with CNN-attention baselines. Including this setup would provide a more equitable basis for comparison, potentially even enhancing the own results. The authors do not address any limitations that might prevent this configuration, leaving the rationale for this decision unclear.\n    * The experimental design could be expanded to assess the model\u2019s performance to a broader range of baseline methods.\n  5. Interpretability of Attention Mechanism\nOne of the noted strengths of the proposed framework is its attention mechanism. However, while the authors suggest interpretability as a benefit, no specific analysis or visualization is provided to demonstrate how the attention scores contribute to understanding EEG signal dynamics. Adding such interpretability analysis would clarify the attention module\u2019s effectiveness in isolating relevant features in EEG data.\n\n\n**Minor Concern:**\n\nPresentation and Figure Quality: The quality of figures is low, which detracts from the visual clarity and effectiveness of the results. Enhancing figure resolution would improve the readability and professional presentation of the study.\n\n**Recommendation for Extended Testing on Diverse Tasks:**\nGiven the framework's potential for adaptation beyond EEG data, the authors could include further testing on additional EEG classification tasks or even generalize their method to other time-series datasets. This would reinforce the flexibility and generalizability of the FSL-MIC model and provide a more robust foundation for the claimed broader applicability.\n\n\nThe paper requires major revisions, including a more comprehensive literature review, expanded experiments, and detailed methodology. Enhancing the experimental setup and introducing a wider array of baselines could make this work more impactful."
        ]
    },
    "EQAHilKZ8D": {
        "venue": "ICLR 2025",
        "title": "Utilizing Visual Properties to Achieve Better Representations of Objects",
        "link": "https://openreview.net/forum?id=EQAHilKZ8D",
        "abstract": "In recent years, large vision models have made significant advancements and excelled in tasks such as detection, segmentation, and tracking. This is partly due to vision models\u2018 good representation of visual objects. Although the recently proposed SAM (the Segment Anything Model ) or the one/few-shot models based on SAM have wide applicability across many tasks, some researchers have found that they do not perform well on certain downstream tasks . In this paper, we focused on a specific group of these objects, which can be summarized as glass-like objects, and quantitatively studied the inadequacies related to the vision models\u2019 feature representation of glass-like objects using the representation accuracy(RA) metric we proposed. Then, we proposed a novel, extremely simple method that introduces almost no additional computations to address these inadequacies. The main idea is utilizing the visual properties of target objects to find representation dimensions which dominate in recognizing them and leveraging these information accordingly to achieve better representations of target objects. Using representation accuracy  and setting these representations as reference in  one-shot segmentation tasks, our experiments demonstrated the substantial effectiveness of our method.",
        "decision": "Reject",
        "review scores": [
            3,
            3,
            3,
            1,
            1
        ],
        "strengths": [
            "- - The paper tackles a challenging open problem in visual understanding. \n- A new metric is introduced.\n- The core idea of selecting feature dimensions related to the effect of glass or mirrors has some potential, but should be worked out in a more clever way.",
            "- 1) A new metric called representation accuracy is defined to compute the representation accuracy of a specific vision model. This metric is used to test with DINOv2 on glass-like datasets, showing that current VLMs are less effective in segmenting these glass-like objects.\n\n2) A new method is proposed to utilize the visual properties of objects to extract the most important feature dimensions to achieve better representations. It takes no extra computation or any other training. \n\n3) The experiments are conducted on three datasets showing the efficacy of the proposed method on glass-like segmentation task.",
            "- -The paper sheds light on the not so common problem of segmenting reflective objects.",
            "- * The high-level motivation of modifying foundation model features for specific object types is sound.",
            "- - The proposed method is computationally efficient (at \"training\" time) and requires minimal learnable parameters or hyperparameter tuning.\n- The comparative study using real-world pairs of images with and without the glass-like objects is original."
        ],
        "weaknesses": [
            "- - The paper is badly structured and, as a consequence, hard to follow. Figures are not referred to in the text. Text has wrong references to tables (e.g. l. 199 refers to Table 2, that has mIOU results while text discusses RA results). There are a lot of forward references, e.g. section3 on problem analysis discusses results of tables from section 5 on experimental results, without telling the reader what data is used or what the exact setup is. \n\n- Overall, the above point makes it hard to know precisely what the authors did exactly. I had to make some guesses at several points. Most importantly, I'm still not sure about the actual task they are performing / evaluating. In some parts it's suggested this is about  segmentation of glass/mirror objects. But in other places it's about matching between a reference image and a target image.  I assume what the authors did in the end is close to the one-shot segmentation of Matcher. \n\n- The method is not described rigorously, making it impossible to reproduce the results. In particular, the 'most' function in eq. 4 is only vaguely described. The text refers to the appendix for more details, but their only numerical values are given, still not explaining the precise algorithm. \n\n- The reported results are anecdotal. Results are reported only for 3-4 images per dataset. It's unclear which images these are and how they were selected.  At the very least, averaged results over the entire dataset should be reported. \n\n- Results show only a minor improvement (in the range of 1 or 2 %) over the very poor results of the baseline. Results are reported for one set of (manually selected?) training images. No details on how these images were selected are given. At the very least, the results should have been repeated for different sets of training images, so the standard deviation on the results could be added to the tables and the reader could get an idea on whether these results are significant or not.\n\n- Given that a new metric is used, more naive baselines should be added: what RA values would one get with a random representation ? Are the numbers reported significantly better ? \n\n- The whole paper builds on one baseline work, Matcher. The proposed method is applied only on top of that method and the results are only compared against that method.  Other state-of-the-art methods, or extra baselines, should be added to the comparison. There is no further analysis, such as a sensitivity analysis of the hyperparameters used, an ablation study or comparison of different variants of the method (e.g. determining the lambda parameter for each of the selected dimensions separately, based on the observed differences in the training data). There are no qualitative results included neither.",
            "- 1) Regarding the definition of representation accuracy in Equation 2, what is the definition mask of the target (M_t) and reference image (M_r)? The paper lacks of detailed explanation of defining the masks and the way to compute them.\n\n2) In methodology, how to find the image pairs (comparative images) that show semantic similarity? Do you define the comparative images as semantically similar with slight visual differences?\n\n3) What is the definition of subtractive comparison among the features? Any math equation referring to it?\n\n4) In the captions of Figure 3, is it possible to provide further explanation on the \"interior and exterior aspects of the mirrored scenes\"?",
            "- -The manuscript is contains many spelling/grammar mistakes e.g. L081.\n\n-The citations are not properly handled (citep vs citet) e.g. L097.\n\n-Some claims are too bold and not justified e.g. \u201cMatcher uses DINOv2 with a ViT-L/14 as the default image encoder and also in this paper authors found that DINOv2 has better patch-level representation ability than SAM, which promotes exact patch matching between different images so it can be considered that DINOv2 is the best VFM for representing similarities at the patch level.\u201d\n\n-The RA metric is not novel, it is the accuracy of a NN retrieval classifier.\n\n-The paper hard to read. The introduced notation does not dislose scalar/matrix/tensor dimensions which makes it confusing.\n\n-The addition and removal of glass barriers is not clearly defined. The definition of a \u201cglass barrier\u201d is also unclear to me at this point. Figure 2 supposedly explains this but there is not pointer to that figure in the text if I am not mistaken.\n\n-From a high-level point of view, what the authors are doing is labeling additional data. I don't think their method is superior to training/finetuning the feature extractors with the additional labeled data.",
            "- The paper is difficult to follow, lacks a logical flow in its presentation, and is missing a convincing motivation and generalizable results. The notation is improperly used and inconsistent throughout. Additionally, the paper does not adhere to general writing guidelines, and figures are not referenced in the text.\n\n**Structural Problems**\n* The concept of Representation Accuracy (RA) is discussed at length, yet it lacks a formal definition or a clear explanation in plain English. RA is neither well-motivated nor supported by any explanation or experiment validating its usefulness as a metric. Furthermore, Equation 2 is not adequately explained as presented.\n* All experiments are conducted on randomly sampled image sets of only 3 or 4 images, which is insufficient to support claims of generalization. As a result, the claims in the paper are limited to being a proof of concept for manually editing features for a small number of images, rendering all algorithmic claims unsupported.\n* In Table 1, due to the lack of motivation and explanation for RA, the experimental results and conclusions appear disconnected. Since the experiments are based on just 3 or 4 selected images, the results presented in Table 2 are also not valid.\n\n\n**Some Writing Issues**\n* Section 3 contains several notation issues. For instance, in Equation 1, the term $S$, which is not mentioned in the text, should have a subscript, $S_{rt}$, as it is defined over patches $r$ and $t$. Line 198 refers to Table 2 for RA comparison, but Table 2 presents an mIoU comparison. In line 212, there are misused variables, with \u201ctarget image of $p^i_r$\u201d actually referring to \u201ctarget image of $M_t$\u201d and \u201c$M_r$\u201d should be \u201c$M_t$\u201d in the sentence \u201cRepresents the mask of the target image,\u201d based on context.\n* The usage of variables in Equation 3 and the corresponding text is inconsistent.\n* In line 302, it says \"$F_i$  and  $F_i$\" , but these should be two different terms.",
            "- - The overall clarity of the paper needs improvement. I recommend starting with a clear introduction to the problem and a stronger motivation for why it is important to address.\n- The writing throughout the paper is often difficult to follow, affecting readability.\n- The proposed method is based on a comparative study using a very limited number of image pairs. It is unclear how the conclusions drawn from such a small sample size can be generalized. This issue is evident, for example, in the dataset dependence of the parameter $\\lambda$. The method also seems to overlap with what could be achieved by training an adapter on top of DINOv2 [2] features using the reference images.\n- The method relies on large Vision Foundation Models (VFMs) such as DINOv2 [2] and SAM [3]. Comparing its performance to standard segmentation approaches, such as a linear segmentation head on top of frozen features, would be useful to justify the high computational cost of the proposed method at inference and in general to put things into perspective.\n- The overall performance improvements are modest, raising questions about the method\u2019s practical impact."
        ]
    },
    "AfZH9EEuRR": {
        "venue": "ICLR 2025",
        "title": "EgoQR: Efficient QR Code Reading in Egocentric Settings",
        "link": "https://openreview.net/forum?id=AfZH9EEuRR",
        "abstract": "QR codes have become ubiquitous in daily life, enabling rapid information exchange. With the increasing adoption of smart wearable devices, there is a need for efficient, and friction-less QR code reading capabilities from Egocentric point-of-views. However, adapting existing phone-based QR code readers to egocentric images poses significant challenges.\nCode reading from egocentric images bring unique challenges such as wide field-of-view, code distortion and lack of visual feedback as compared to phones where users can adjust the position and framing. Furthermore, wearable devices impose constraints on resources like compute, power and memory.\nTo address these challenges, we present EgoQR, a novel system for reading QR codes from egocentric images, and is well suited for deployment on wearable devices. Our approach consists of two primary components: detection and decoding, designed to operate on high-resolution images on the device with minimal power consumption and added latency. The detection component efficiently locates potential QR codes within the image, while our enhanced decoding component extracts and interprets the encoded information. We incorporate innovative techniques to handle the specific challenges of egocentric imagery, such as varying perspectives, wider field of view, and motion blur.\nWe evaluate our approach on a dataset of egocentric images, demonstrating 34% improvement in reading the code compared to an existing state of the art QR code readers.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            3,
            3,
            3
        ],
        "strengths": [
            "- 1.complex system implementation\n2.great practical significance\n3.experimental results in real-world environments",
            "- S1. The proposed method resolves stylistic QR code and small QR code problems by utilizing image enhancement modules, such as color inversion, multi-scale processing, contrast enhancement morphological operation, and super-resolution.",
            "- -  The proposed EgoQR system demonstrated a significant 34% improvement in QR code reading success over existing state-of-the-art readers. \n- By incorporating image enhancement techniques like super-resolution and adaptive histogram equalization, the system improves decoding success rates even in challenging conditions such as motion blur and varying lighting.",
            "- 1. The paper identifies a practical problem: the growing need for efficient and frictionless QR code reading capabilities from egocentric viewpoints, particularly with the rising adoption of smart wearable devices with limited computational resources.\n\n2. The paper introduces a solution to tackle issues like code distortion and resource constraints. Experimental results on a self-collected dataset demonstrate its superior performance compared to other commercial applications.",
            "- The paper addresses a practical and relevant problem, as QR code reading in egocentric/wearable settings is becoming increasingly important with the rise of AR/VR devices."
        ],
        "weaknesses": [
            "- 1.The method lacks novelty\n2.The method is a simple combination of existing modules",
            "- W1. Novelty. \n* The paper proposes a simple pipeline that combines a QR code detection module, an image enhancement module, and a QR code decoding module. It is difficult to find any new idea, novelty, or new perspective for each component. ICLR requires new and brilliant ideas, perspectives, and contributions to various research fields. However, the paper's contribution is not enough to meet ICLR standards. \n\nW2. Presentation\n* The paper presentation needs to be clarified. For instance, Table 2 doesn't properly reference each method.\nAlso, Fig 8 is mentioned earlier than Fig 1. The figure and table should be mentioned in their order.\n\nW3. Experiments\n* The effect of each image enhancement method is unclear. For instance, the stylistic QR code problem is resolved by introducing a number of enhancement tools, such as color inversion, multi-scale processing, contrast enhancement, and morphological operation. Are they all necessary? How do they affect the performance of stylistic QR code reading?",
            "- - Lack of novelty: The paper builds upon well-established techniques such as Faster R-CNN for detection and common image enhancement methods for decoding.\n- Concern about efficiency: The proposed system employs high-resolution image processing and machine learning models which could be computationally intensive. It would be beneficial to see a more detailed analysis of the trade-offs between performance and resource consumption.",
            "- 1. The novelty of the paper is limited, resembling more of a technical report addressing an engineering problem than a formal academic paper. The core modules of the proposed pipeline are nearly all previous methods.\n\n2. The experimental section is insufficient, as it only reports the accuracy of different methods without providing in-depth analysis or conducting an ablation study on various components.\n \n3. The paper lacks a comparison of the public dataset.",
            "- **Note that this paper is over-paged (It should be desk rejected?)**\n\n1. Limited Technical Novelty:\n- The paper primarily integrates existing methods (Faster R-CNN, traditional image processing techniques, LRSRN for super-resolution) without introducing significant algorithmic innovations\n- The image enhancement pipeline is a straightforward combination of standard techniques (color inversion, multi-scale processing, CLAHE, morphological operations)\n- The disambiguation approach relies on existing ROI detection from another system (Lumos)\n\n2. Questionable Experimental Setup:\n- The dataset collection methodology lacks rigor and proper controls\n- No clear description of what constitutes an \"egocentric\" image in their data collection\n- No comparison with actual egocentric devices or properly collected egocentric datasets\n- The baseline comparisons (zxing, pyzbar, etc.) are with general-purpose QR code readers rather than systems specifically designed for egocentric settings\n\n3. Insufficient Validation:\n- The paper claims to address egocentric-specific challenges but doesn't provide quantitative analysis of how their system handles these specific issues\n- No ablation studies to justify the choice and order of enhancement techniques\n- Limited analysis of computational efficiency and power consumption, despite claiming suitability for wearable devices\n- The claimed 34% improvement lacks context as it's compared against general-purpose QR readers rather than egocentric-specific solutions\n\n4. Methodological Issues:\n- The dataset size (528 images) seems inadequate for comprehensive evaluation\n- No clear distinction between training and test sets\n- Lack of detailed analysis of failure cases and their relationship to egocentric challenges\n- No discussion of statistical significance in the reported improvements\n\n5. Incomplete Resource Analysis:\n- Despite targeting wearable devices, there's limited discussion of memory usage and power consumption\n- No concrete benchmarks on actual wearable hardware\n\nWhile the problem is interesting and practically relevant, the paper's limitations in technical novelty and experimental validation make it unsuitable for ICLR. The work would be more appropriate for an applied computer vision or system-focused venue after addressing the identified issues."
        ]
    },
    "w5h443GIGo": {
        "venue": "ICLR 2025",
        "title": "On the Convergence of Symbolic Pattern Forests and Silhouette Coefficients for Robust Time Series Clustering",
        "link": "https://openreview.net/forum?id=w5h443GIGo",
        "abstract": "Clustering algorithms are fundamental to data mining, serving dual roles as exploratory tools and preprocessing steps for advanced analytics. A persistent challenge in this domain is determining the optimal number of clusters, particularly for time series data where prevalent algorithms like k-means and k-shape require a priori knowledge of cluster quantity. This paper presents the first approach to time series clustering that does not require prior specification of cluster numbers. We introduce a novel extension of the Symbolic Pattern Forest (SPF) algorithm that automatically optimizes the number of clusters for time series datasets. Our method integrates SPF for cluster generation with the Silhouette Coefficient, computed on a two-stage vector representation: first transforming time series into Symbolic Aggregate approXimation (SAX) representations, then deriving both bag-of-words and TF-IDF vectors. Rigorous evaluation on diverse datasets from the UCR archive demonstrates that our approach significantly outperforms traditional baseline methods. This work contributes to the field of time series analysis by providing a truly unsupervised, data-driven approach to clustering, with potential impacts across various temporal data mining applications where the underlying number of clusters is unknown or variable.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- S1. Timely and important problem especially due to the rise of IoT applications and the need for unsupervised data exploration\nS2. Simply and intuitive ideas\nS3. Results support the overall claims in the paper",
            "- S1. The paper addresses the relevant problem of automatically determining the number of clusters.\nS2. The empirical evaluation makes use of a large number of benchmarking datasets.",
            "- (S1) Incorporating BoW and TF-IDF with the concepts of the SPF algorithm sounds like a very sensible approach. Both are a good choice for term-based similarity evaluation and are still commonly used in other settings.\n\n(S2) Aside from minor issues, the submission is well-written and easily understandable while providing an extensive overview of the formulas related to the problem.\n\n(S3) The problem setting is significant as k-estimation is a significant part of clustering in general, which also applies to the setting of time series clustering. The usage of SPF is well-founded due to its low complexity. Introducing k-estimation to the approach helps mitigate one of its weaknesses."
        ],
        "weaknesses": [
            "- W1. Lack of technical depth\nW2. Unclear how different methods/distances can be compared\nW3. Missing potential baselines\nW4. Duplicate references or wrong references",
            "- W1. The method assumes that silhouette coefficient is a suitable metric for finding the best number of clusters, without justifying this choice. This is a major concern as the silhouette coefficient considers (Euclidean) distance to cluster centres, which is not aligned with the clustering objective of the SPF method. The paper should provide justification for using the silhouette coefficient, or discuss potential limitations of this choice given the SPF method's clustering approach. Moreover, the silhouette coefficient is a well-known metric, so it is unclear what the novelty should be.\nW2. The empirical evaluation does not consider the SPF method, but only weak baselines constructed from the proposed method, meaning that the empirical evaluation does not allow assessment of the performance of the proposed method with respect to state of the art. It is important to compare directly to SPF in the experiments, in order to demonstrate improvement over state of the art.\nW3. The empirical evaluation only considers performance metrics accuracy and near-miss-rate, different from other work in the field, and in the SPF paper (e.g. NMI), making it impossible to compare with those works directly.\nW4. The discussion of related work is overly brief, and fails to present clear assessment of the suitability of existing methods and metrics. E.g. Davies-Bouldin Index and its perceived suitability for the task. Also, there is a large body of work on similarity assessment of time series or clustering of time series, e.g. Keogh et al 2005, Rakthanmanon  et al 2012, Paparrizos et al 2015. The paper should discuss these, and explain differences and similarities with the proposed method.\nW5. On the other hand, references UTSAD and STGAT seem out of context, as they do not address clustering of time series. The paper should clarify the relevance of UTSAD and STGAT to the proposed work, or remove these references if they are indeed not directly related.\nW6. The paper contains several redundant sections, such as the description of SAX.\nW7. There are some minor issues, such that Li et al 2019 appears twice in the references, there is a typesetting error in the definition of pi_i(T_i).",
            "- (W1) Novelty: The abstract of the submission makes the claim that there are no time series clustering methods capable of working without the specification of cluster number k. However, such methods exist already:\n\na) \u201cSpectral Clustering for Time Series\u201d by Fei Wang and Changshui Zhang (2005) is able to discover the optimal number of clusters based on the eigenstructure using a threshold on the value of the eigenvalues.\nb) \u201cClustering Time Series with Hidden Markov Models and Dynamic Time Warping\u201d by Tim Oates et. al. (1999) also provides a way to estimate the number of clusters based on Dynamic Time Warping. However, even if the submission is not the only method that does k-estimation on time series, it is still a valid and useful direction. It also appears to be the only method that does so for the Symbolic Pattern Forest algorithm.\nc) The paper \u201cTrendlets: A novel probabilistic representational structures for clustering the time series data\u201d by Johnpaul C I et al. (2020) uses the Silhouette Score for cluster number analysis for time series as well, though it does so based on hierarchical clustering methods. This paper should be explicitly covered in related work or even a competitor.\n\n(W2) Despite TF-IDF being considered the better of the two proposed strategies, there is no actual description of the performance metrics outside of the graph and the overall relative performance value. Similarly, near misses should be added to the text for BoW. The results of both BoW and TF-IDF are the same in the Tables in the supplementary files, though Figure 1 claims that TF-IDF performed slightly better.\n\n(W3) As the method works by optimizing the silhouette score, both the values for the score and the actual clustering performance with the given parameters should be indicated. While the cluster numbers match, the detected clusters may not necessarily correspond to the actual ground truth clusters, which could further mean that different cluster numbers may lead to a better performance. Furthermore, an analysis of the stability of the parameters should have been performed, especially as the method has multiple parameters, which themselves include an upper and lower bound. Additionally, an intuition behind choosing the parameters should be given if they strongly affect the performance.\n\n(W4) Regarding the actual experiment, a better analysis of the behavior should be done, considering under what conditions the k-estimation of each of the three approaches failed and whether or not a reason behind it could be established. The section on Relative Improvement is redundant as it only recontextualizes prior results, and the space could be used to do a more in-depth result analysis instead. Similarly, the remaining 2 pages could have been used for this.\n\n(W6) Neither the parameter w nor the alpha ranges seem to be specified anywhere. The code is unavailable, though it should be possible to reimplement given the information provided. Still, this hampers the reproducibility of the results.\n\n(W7) There should be citations for TF-IDF and BoW. Other papers also do not consistently do it, so it is not a major issue. Nonetheless, it would have been better if it had been done. Furthermore, UCI should be cited upon first mention outside of the abstract, not just at a later point.\n\nMinor Issues:\n* Linear time complexity time series clustering with symbolic pattern forest by Li et. al., is cited twice as 2019a and 2019b despite referring to the same paper \n* The formatting appears to be broken for lists, as they are just written in a line without comma separation (see line 291 and lines 314-315) \n* A similar issue happened with the variables for the optimization problem, as they are also not properly separated in line 305\n* The subscript on several equations appears to be broken (see (22)/319 and (23)/321)\n* The near miss metric should probably be more dynamic based on the ground truth cluster number, as claiming 2 clusters for a 3-cluster setting seems more problematic than claiming 70 for 71 true clusters. The chosen datasets generally only have a few clusters, so the current definition isn\u2019t problematic for the submission. It may be relevant for the extension to the full UCI database, however. \n* The formulation for Near Misses, as currently given, would also include all correctly determined cluster counts but does not do so in the evaluation."
        ]
    },
    "w2C7gJqaai": {
        "venue": "ICLR 2025",
        "title": "Integrated Multi-system Prediction via Equilibrium State Evaluation",
        "link": "https://openreview.net/forum?id=w2C7gJqaai",
        "abstract": "This study presents a new paradigm of prediction, Equilibrium State Evaluation (ESE), which excels in multi-system prediction where systems interact with each other and every system needs its own prediction. Unlike mainstream prediction approaches, ESE views each system as an integral part under one structure and predicts all systems simultaneously in one go.  It evaluates these systems' equilibrium state by analyzing the dynamics of their attributes in a holistic manner, instead of treating each system as an individual time series. The effectiveness of ESE is verified in synthetic and real world scenarios, in particular COVID-19 transmission, where each geographic region can be viewed as a system.  So cases spreading across regions against the medical competency and demographic traits of these regions can be considered as an equilibrium problem rather than a time series problem.  Extensive analysis and experiments show that ESE is linear in complexity and can be 10+ times faster than SOTA methods, yet achieving comparable or better prediction accuracy.  More importantly, ESE can be integrated with these prediction methods to achieve both high accuracy and high speed, making it a powerful prediction mechanism, especially for scenarios that involve multiple systems. When the dimensionality of the multi-system increases, e.g. more systems joining, the advantages of ESE would become even more apparent.",
        "decision": "Reject",
        "review scores": [
            1,
            5,
            1
        ],
        "strengths": [
            "- -",
            "- The primary strength of this paper lies in its effort to apply a novel conceptual framework for modeling interconnected systems. It draws on ideas from Nash equilibrium, zero-sum constraints, and cointegration. This approach offers an interesting perspective by reinterpreting multi-system predictions through game-theoretic concepts, particularly utilizing Nash equilibrium principles and payoff functions to illustrate the mutual influences among subsystems. While these concepts are mainly interpretative, they provide a fresh way to think about multi-system interactions, which could have significant implications for interdisciplinary applications across fields such as economics, epidemiology, and regional forecasting. The paper demonstrates the practical feasibility of the proposed model, ESE, by applying it to a real-world COVID-19 dataset. This highlights the model's potential to capture complex interdependencies and offers insights into multi-region transmission dynamics. The presentation is generally clear, with structured explanations of equilibrium concepts and the role of each component, such as the equilibrium index. Furthermore, if the model\u2019s empirical performance in terms of accuracy and computational efficiency is further substantiated, it suggests that ESE could serve as a competitive alternative for high-dimensional applications where subsystems are highly interconnected.",
            "- There aren't any significant strengths in the paper"
        ],
        "weaknesses": [
            "- **I stopped reviewing this paper, as there were multiple issues. The objective and the problem setting are not clear, assumptions are not clearly stated, related work are missing, and writing is very poor.**\n\nThe definition of a \"system\" in the first place is not clear. The authors casually drop \"properties\" of this said system/multi-system, which vaguely correspond to some concepts, such as Nash equilibria, but not clear at all if those are assumptions/setting the authors consider. \n\nThe paper is almost impossible to read, with abuse of notation used absolutely for no reason. It is not really clear what problem the paper is dealing with and it is very poorly formulated throughout.\n\nIt is not clear how the COVID example fits into the Constraints 1 & 2. Why Eq. (4) needs to hold for the COVID case for instance?\n\nRelated work section mentions mostly datasets and equilibrium examples from different domains, but I could not spot any ML related work that consider a similar problem. \n\nWhy do you define ${\\cal M} {\\cal S}$ to be the sum of the target variables in systems, whereas it was first defined as the set of systems? The notation should be improved across the board, but this is just one example.\n\nWhy does Eq. (1) hold *in general*? Is that a setting you consider?\n\nYou cannot refer to Constraint 1 when defining Constraint 1 itself?\n\nLine 158 - Please refer to where did you describe your ESE method as claimed.\n\nWhat is the takeaway from Figure 1 exactly?\n\nThe connection to the Nash equilibrium is never formally introduced or motivated. It is not clear where  Lemma 1 comes from. There are no proper citations as well.",
            "- A significant weakness of the paper lies in its insufficient support for the claim that the system will reach a true equilibrium state. While the model employs Nash equilibrium and zero-sum constraints to define static equilibrium conditions, these concepts alone do not provide a mechanism to ensure that the system will naturally progress toward equilibrium over time. Without a dynamic framework or time-dependent interactions\u2014such as differential equations or explicit stability conditions\u2014there is no mathematical basis for assuming that the system will move from an arbitrary initial state toward equilibrium. Furthermore, although the paper incorporates cointegration in its training process, this is a statistical technique that assumes the existence of a long-term equilibrium relationship rather than actively guiding the system toward it. This distinction weakens the model\u2019s theoretical foundation, as it does not establish how equilibrium is achieved dynamically. Another critical limitation is that the equilibrium definition based on Nash equilibrium and payoff functions does not exclude the possibility of stable oscillatory behavior. This means the system could theoretically settle into a stable oscillation rather than converging to a true steady state. Additionally, the ESE model shows strong mathematical similarities to traditional multi-compartment models, which use conservation principles to maintain balance across subsystems. While the introduction of game-theoretic concepts like Nash equilibrium and payoff functions offers a fresh interpretive layer, it does not constitute a substantial mathematical advancement over established multi-compartment models. To improve the model\u2019s robustness, the authors would need to provide a formal analysis of convergence conditions, explicitly address the exclusion of oscillatory states, and better differentiate their approach from conventional multi-compartment modeling frameworks.",
            "- - The paper is confusing to follow. The introduction is poorly written. The mathematical notation is very dense without sufficient explanation. \n- Complex method workflow is not fully explained\n- It is not apparent to me why proportions must follow zero-sum rule. Doesn't explain how to handles growing-shrinking total system values.\n- It is unclear why is specific normalization is chosen \u03bb\u1d62,\u2c7c = (\u03b1\u1d62,\u2c7c - mean(at\u2c7c)) / (max(at\u2c7c) - min(at\u2c7c)), why is this normalization beneficial as compared to other ways to normalize? \n- Algorithm 1 is vaguely described, and testing and training process aren't well defined. It doesn't map succinctly with the figure 2 of the paper."
        ]
    },
    "v3DwQlyGbv": {
        "venue": "ICLR 2025",
        "title": "Paramanu-Ganita: An Efficient Pre-trained Generative Mathematics Language Model with Chain-of-Thought Instruction Fine-Tuning",
        "link": "https://openreview.net/forum?id=v3DwQlyGbv",
        "abstract": "In this paper, we pose the following question: whether domain specific pretraining of tiny generative language models from scratch with domain specialized tokenizer and Chain-of-Thought (CoT) instruction fine-tuning results in very competitive performance on mathematical reasoning than LLMs which are trained on trillion of tokens and humongous parameters? Secondly, we pose our second RQ: whether domain specific pretraining from scratch is environmentally sustainable, highly cost efficient? To address these research questions, we present Paramanu-Ganita, a 208 million-parameter novel Auto Regressive (AR) decoder based language model on mathematics. We performed pretraining from scratch on 31.5 billion tokens using a context size of 4096 on a mixed mathematical corpus consisting of mathematical web pages, mathematics related source code such as AlgebraStack, mathematical textbooks, Chain-of-Thought (CoT) templatised mathematical StackOverflow question answers pairs, and mathematical lecture notes in LaTeX curated by us. We also trained a math and code specialised BPE tokenizer. We proposed and performed Chain-of-Thought instruction fine-tuning of Paramanu-Ganita on the MetaMathQA dataset. We evaluate our model on GSM8K and MATH mathematical benchmarks, and on logical deductive reasoning (LogiQA) and multiple choice high school and college level math questions from SAT (AGIEVAL-SAT-Math), GRE/GMAT questions (AGIEVAL-AQuA-RAT), college and high school level math questions from MMLU.\nOur model Paramanu-Ganita, despite being 34 times smaller than the 7B LLMs, outperforms general LLMs by approximately 30% points, and even math-specialised LLMs by 3-23% points in GSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the various models by 6-8% points. On other benchmarks such as LogiQA logical deductive reasoning benchmark, mathematical high school level multi-choice questions (MMLU-math-high-school), GRE-GMAT level quantitative questions (AGIEVAL-AQuA-RAT), SAT level math questions, Paramanu-Ganita was better than the others by about 1-4% points. The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language models are just not restricted to those with humongous number of parameters. Paramanu-Ganita took only 170 hours of A100 training whereas large LLMs such as the math-specialised LLM, LLEMMA 7B, was trained for 23,000 A100 equivalent hours. Thus, our approach of pretraining powerful domain-specialised language models from scratch for domain adaptation is much more cost-effective and environmental friendly than performing continual training of LLMs.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- The paper is well-written. I appreciate the background and the description of how the model is trained. The idea of targeting mathematics is important and building LLMs specializing in math (at least some part of it) is important. \n\nThe dataset is an important contribution, however I am not sure whether the authors plan to make it public.",
            "- - Good empirical results despite smaller model size, demonstrating the effectiveness of their approach\n- Demonstrates that smaller, more efficient models can achieve good mathematical reasoning performance",
            "- 1. A novel decoder model, that is 34 times smaller than existing LLMs and can outperform them by a huge margin\n2. A detailed explanation of the training process required\n3. Detailed benchmarking on GSM8K, MATH and other datasets.\n4. Emphasis on the training time required and compared it to other existing LLMs, showing computation and environmental prowess in training an exclusive tiny model from scratch."
        ],
        "weaknesses": [
            "- I feel the paper explores an interesting direction, but there are some concerns:\n\n1. Firstly, GSM8k tests basic math word problem skills and given the model's GSM8k performance is pretty poor, I do not feel the model is ready yet. I think more experimentation is required. Also, how are Table 2 values computed? It seems the MetaMath paper reports GSM8K performance to be 82.3. Why is it 66.5 here? [1]\n\n2. What is mostly missing from the paper are proper motivations and justification as to what \"contributes\" or what is expected to contribute to the \"improved\" performance? \n\n - Looking at this from a different point of view, why did the authors not start with MetaMath, then say change the tokenizers or change the dataset? Then, slowly demonstrate how all the innovations are truly necessary. At the least such ablations would have showed the necessity of new models. \n\n - Secondly, given the model's performance is not so great, what are we gaining by spending so much training time and cost?\n\n3. One more important aspect is, what are the domains that the model targets? What are the grade levels? Is it the expectation that we will also do IMO problems starting from GSM8k? Or, are we targeting sub-disciplines algebra, pre-algebra, calculus etc.? I think this depth is also missing, so is related papers that investigate the need of such models [2].\n\n[1] https://openreview.net/forum?id=N8N0hgNDRt\n[2] MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning",
            "- - The overall presentation of the paper still needs much improvement. The paper is not in ready-to-review or ready-to-submit status. The figures are pretty rough and unclear for what the authors want to express. For example, Figure 2 shows GPU Power Usage during pretraining of Paramanu-Ganita. But what conclusions do the authors want to make here? How does it illustrate the environment friendly nature of the model? For the figure 1, what does the blue line mean here?\n- Limited Ablation Studies. The paper doesn't analyze the relative importance of different components of their training data (web text vs. code vs. lecture notes). It is unclear why the authors want to utilize these data sources and why the data mixture should be adopted as it is in the paper.\n- Contamination issues. The model achieves good performance on GSM8K and MATH with 200M parameters. It is unclear whether there is data contamination issue.",
            "- 1. The paper uses Qwen-72B to label the corpus and use a score >= 0.6 for training the model, ensuring only a high-quality dataset is used. However, apart from this, the training process used is not novel. Specifically, there is no novelty in the model architecture or training paradigm used that can justify the complete novelty of the paper and also puts into question the improved performance of a 208 million parameter model over LLMs\n2. The paper does not touch upon, newer and difficult mathematical datasets such as MATHBENCH or JEEBENCH. These are some datasets that were released after training cutoff time for some models, ensuring they are not part of their training data. These datasets are also much more difficult as compared to gms8k. This will ensure that the proposed model is robust in solving difficult problems that it hasn't seen before.\n3. Will the checkpoint-filtered corpus used for training be publicly available?\n4. How does the model perform on out-of-distribution data points, this can be checked by first doing a sanity check of data memorization/contamination [1]. Performing simple algorithms 1 and 2 from the paper will ensure that the model has not seen the evaluation dataset, making the results more robust.\n5. The empirical analysis is missing from the paper. A thorough qualitative comparison of reasoning chains produced by Paramanu-Ganita versus other models on a few representative problems from the benchmark datasets. For example, what errors are made by existing LLMs vs. Paramanu-ganita and in which area does it improve?\n\nReference\n[1] Golchin, Shahriar, and Mihai Surdeanu. \"Time travel in llms: Tracing data contamination in large language models.\" arXiv preprint arXiv:2308.08493 (2023)."
        ]
    },
    "tccML2tDd4": {
        "venue": "ICLR 2025",
        "title": "Perceptual Piercing: Human Visual Cue-Based Object Detection in Low Visibility Conditions",
        "link": "https://openreview.net/forum?id=tccML2tDd4",
        "abstract": "This study proposes a novel deep learning framework inspired by atmospheric scattering and human visual cortex mechanisms to enhance object detection under poor visibility scenarios such as fog, smoke, and haze. These conditions pose significant challenges for object recognition, impacting various sectors, including autonomous driving, aviation management, and security systems. The objective is to enhance the precision and reliability of detection systems under adverse environmental conditions. The research investigates the integration of human-like visual cues, particularly focusing on selective attention and environmental adaptability, to ascertain their impact on object detection's computational efficiency and accuracy. This paper proposes a multi-tiered strategy that integrates an initial quick detection process, followed by targeted region-specific dehazing, and concludes with an in-depth detection phase. The approach is validated using the Foggy Cityscapes, RESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance standards in detection accuracy while significantly optimizing computational efficiency. The findings offer a viable solution for enhancing object detection in poor visibility and contribute to the broader understanding of integrating human visual principles into deep learning algorithms for intricate visual recognition challenges.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- Good idea to apply human visual cues to object detection in foggy scenes.\n\nClear figures to show the proposed overall architecture of Perceptual Piercing and the AOD-NetX modules, making it easier for readers to understand, which improves the quality of the presentation.",
            "- No strengths.",
            "- In general this paper is very well written. The writing style is very clear, with few grammatical mistakes. It was easy to understand the points being made, and the methodology was reasonably well presented.\n\nThe motivations of the paper were well expressed, highlighting a clear need for improved object detection capability within low-visibility scenarios, while minimizing end-to-end computation costs.\n\nThe literature review was reasonably comprehensive, considered multiple different approaches and challenges within low-visibility environments, and sufficiently identified gaps within the existing literature."
        ],
        "weaknesses": [
            "- This paper appears more like a course project rather than a conference paper. The presentation of experimental results looks like ablation studies, lacking extensive comparison with other state-of-the-art methods.\n\nIn addition, it lacks comprehensive ablation studies on pre-trained models, module design details, or other hyperparameters.\n\nShifting from a one-stage to a multi-stage approach, along with heavy reliance on YOLO and AOD-Net, limits the originality of this paper.",
            "- Is it a homework?\n- This paper feels more like an assignment than a research contribution, as much of the content consists of summarizing existing works like Foggy-Cityscapes, AOD-Net, and YOLO.\n- Despite the title's claim to address vision tasks in low visibility conditions, the paper only focuses on fog, neglecting other relevant scenarios.\n- The approach merely combines a dehazing technique with an object detection method, offering little innovation.\n- The paper lacks clear motivation and meaningful contributions, and the overall writing quality is very poor.",
            "- ## Main Concerns\n---\nMy main concern is the interpretation of the results. From the provided quantitative results, it is not clear to me that the proposed AOD-NetX architecture provides a sufficient improvement in terms of either accuracy or computation resource reduction for this paper to be accepted. The introduction, literature review, and discussion sections highlight the need for computational efficiency in applications that might use de-hazing. One of the strengths of the paper is the focus towards end-to-end inference time, given the selection of the nano yolo models, and AOD-Net which is one order of magnitude quicker than its predecessors for de-hazed image generation. It is strange to me then that there is no quantitative analysis of inference time, at least in table 1-3, to show that the end-to-end speed is hardly slower than the baseline nano model. \n\nI disagree with the interpretation of the results of Table 2. On line 351 the authors write: 'such as AOD-Net and AOD-NetX, consistently improves object detection performance in both clear and foggy conditions.' \nWhile, YOLOv5s+AOD-NetX+YOLOv5x/YOLOv8n+AOD-NetX+YOLOv8x yield the best performance for foggy conditions, they actually have the worst performance in clear conditions (0.4896 and 0.5150 mAP respectively). \n\nI disagree with the interpretation of results of Table 3. On line 402 the authors write: 'The addition of AOD-Net generally improved performance for YOLOv8 but had a diminishing effect on YOLOv5.' Inclusion of AOD-Net in fact leads to a reduction of performance from 0.7125 to 0.6458 on OTS and 0.6978 to 0.6125 on RTTS. This statement further contradicts with what the authors said previously on line 376 'that the YOLOv8x architecture achieved the highest mAP scores under foggy conditions, with 0.7125 on OTS and 0.6978 on RTTS' (i.e. that actually the baseline YOLOv8x architecture performed the best, outperforming the addition of AOD-Net).\n\nGiven these results, I do not find the conclusion to hold. On line 480 the authors write: 'Our proposed AODNetX architecture outperforms state-of-the-art models, excelling in both standard and out-of-distribution datasets.' We need more comparisons with state of the art models, such as those suggested in the literature review; Gao et al. (2023), Yang et al. (2023b), Zheng et al. (2023). Additionally, authors should consider evaluating existing SOTA methods for fog analysis for comparison with respect to both accuracy and computation cost, such as YOLOv5-s-Fog [1] which achieves better performance than AOD-NetX on RTTS dataset and claims real-time performance. Currently only AOD-Net is compared to, and it is not obvious from the results that AOD-NetX out-performs this approach. The results in table 3 suggest AOD-Net and the baseline model have higher mAP on out of distribution datasets compared to AOD-NetX.\n\nMy second concern is that the methodology of this paper is not sufficiently different from existing solutions. The provided solution is to combine the outputs of an existing AOD-Net model with an object detector model to yield an image with selectively de-hazed image regions. Inference is then run on the same object detector with the new selectively de-hazed image as input. Neither the object detection architecture nor the AOD-Net architecture are new.\n\n---\n## Other concerns:\n---\nIt is odd that AOD-Net is not present in the literature review; given it is highly relevant to the task. Furthermore, Fig. 2 assumes a knowledge of transmission map K that is assumed from the AOD-Net paper. Given the critical role that the AOD-Net architecture plays in the construction of AOD-NetX, a more thorough explanation of its components is required. At the very least, K should be explicitly defined in the body of the paper (for example modify line 258 to '... utilizes the transmission map K, created...').\n\nDomain adaptation is a large field of study that is often used within foggy scenes, but is entirely ignored in the literature review. A brief description of de-hazing abilities of domain adaptation (e.g. [2-4]) would help contextualise the choice of AOD-Net better.\n\n---\n## Minor details I picked up that did not influence my decision:\n---\n- line 027, abstract: 'The code for perceptual piercing is available here.' - where? Or has this been removed for purposes of double blind review?\n- Section 3.2 gives descriptions of biological motivations. Given the specific claims of this section, references are required here. The authors might consider feature integration theory [5] or other works within cognitive psychology [6], and early neural networks that use and reference biological inspiration [7]. Also Stone (2018) [8] investigates biological systems with respect to information theory. This book may contain references that are useful.\n- Fig 1: Its caption separates the proposed solution into three components, a, b, and c. The figure should represent these divisions somehow i.e. there should be an explicit reference to parts a, b, and c in the figure itself.\n- line 229 (fig 1 caption): Missing punctuation: '(a)Preliminary detection.'\n- line 350: Missing a space: 'Table 2trained and tested'\n- line 455: 'To address the issue of generalizability in single-dataset training, two potential approaches are proposed' - this is unclear to me. It suggests that the paper has already proposed two approaches to address the issue of generalizability. I would re-word this to something like 'we propose two potential directions for future work.'\n- line 480: 'AODNetX'... Everywhere else it is referred to as AOD-NetX\n\n---\n## References\n---\n- [1] Xianglin Meng, Yi Liu, Lili Fan, Jingjing Fan.YOLOv5s-Fog: An Improved Model Based on YOLOv5s for Object Detection in Foggy Weather Scenarios. Sensors 23(11): 5321 (2023)\n- [2] Naif Alshammari, Samet Akcay, Toby P. Breckon. Multi-Modal Learning for Real-Time Automotive Semantic Foggy Scene Understanding via Domain Adaptation. IV 2021: 1428-1435\n- [3] Hanyu Zhou, Yi Chang, Wending Yan, Luxin Yan. Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow. CVPR 2023: 9569-9578\n- [4] Xianzheng Ma, Zhixiang Wang, Yacheng Zhan, Yinqiang Zheng, Zheng Wang, Dengxin Dai, Chia-Wen Lin. Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding. CVPR 2022: 18900-18909\n- [5] Treisman, A. M., & Gelade, G. (1980). A feature-integration theory of attention. Cognitive Psychology, 12(1), 97\u2013136. https://doi.org/10.1016/0010-0285(80)90005-5\n- [6] C. Koch and S. Ullman, \u201cShifts in Selective Visual Attention: Towards the Underlying Neural Circuitry,\u201d Human Neurobiology,\nvol. 4, pp. 219\u2013227, 1985\n- [7] Laurent Itti, Christof Koch, Ernst Niebur. A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20(11): 1254-1259 (1998)\n- [8] James V. Stone. 2018. Principles of Neural Information Theory: Computational Neuroscience and Metabolic Efficiency (1st. ed.). Sebtel Press."
        ]
    },
    "rKMz6cDE7W": {
        "venue": "ICLR 2025",
        "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
        "link": "https://openreview.net/forum?id=rKMz6cDE7W",
        "abstract": "Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$  computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still necessitates $O(n^2)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in a streaming fashion. This method employs sublinear space  $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- The authors address a fundamental problem that is important. They produce an algorithm with clear guarantees. Furthermore most of the assumptions are clearly stated, the proofs easy to follow and the pseudocode is clean and easy to understand.",
            "- --",
            "- Addressing LLMs with very large context length appears important and timely."
        ],
        "weaknesses": [
            "- The author doesn't introduce any new techniques but this seems to be the application of the sketching machinery on top of the algorithm of Alman and Song.  \n\nThe resulting algorithm has small asymptotic complexity but seems to be in extremely impractical in several ways. First it relies on $n$ being large $n \\geq 2^d $ where as in practice $d \\sim 10^4$ for even small transformers. As a result this assumption is quite strong. Secondly , the type of operations necessary are not easy to implement fast in modern hardware.",
            "- 1. Attention computation can be done without using $O(n^2)$ space. See Rabe and Staats \"Self-Attention does not need $O(n^2)$ memory\".\n2. The version of the problem being studied i.e., without using causal masking is not super relevant to the current LLMs which all use causal masking.\n3. As stated, the results only output at most $k \\cdot d$ nonzero entries for the entire matrix $V$ which has $n$ rows. Since we expect each of the rows in the full attention output to have similar norms, many useful rows are completely marked to zero by this algorithm unless $k \\ge n/d$ at which point the sketches stored by the algorithm are linear in size, at which point the claim of streaming algorithm does not make sense and the algorithm is essentially no different from Alman and Song.\n4. No experimental verification of the ideas. At least showing how the algorithm works on small instances, even without implementing the additional sparse recovery steps would have been useful.",
            "- - The writing is poor,\u00a0the paper is littered with typos and the general structure is very hard to follow. I found the paper very hard to read. Other comments aside, it cannot be published in its current state.\n- I find it hard to evaluate the novelty of this\u00a0paper. The related work section seems to be just a lump of citations, and it is not clear what is the actual contribution. It looks like the authors simply joined existing techniques to get the results.\n- I am not sure about the motivation for a streaming computation. In what scenario will the computation be executed in a streaming fashion (i.e., the matrices need to be stored somewhere, right?)\n- There is no experimental evaluation."
        ]
    },
    "o1efpbvR6v": {
        "venue": "ICLR 2025",
        "title": "Application of Metric Transformation in One-Step Retrosynthesis",
        "link": "https://openreview.net/forum?id=o1efpbvR6v",
        "abstract": "In this article, we investigate the impact of Deep Metric Learning and Transformer architecture on predicting the retrosynthesis of Simplified Molecular Input Line Entry System (SMILES) chemical compounds.\n\nWe demonstrate that combining the Attention mechanism with Proxy Anchor Loss is effective for classification tasks due to its strengths in capturing both local and global contexts and differentiating between various classes. \n\nOur approach, which requires no prior chemical knowledge, achieves promising results on the USPTO-FULL dataset, with accuracies of 53.4\\%, 83.8\\%, 90.6\\%, and 97.5\\% for top-1, top-5, top-10, and top-50 predictions, respectively.\n\nWe further validate the practical application of our approach by correctly predicting the retrosynthesis pathways for 63 out of 100 randomly selected compounds from the ChEMBL database and for 39 out of 60 compounds selected by Bayer's chemists and from PubChem.",
        "decision": "Reject",
        "review scores": [
            1,
            1,
            5
        ],
        "strengths": [
            "- The use of Attention mechanisms and Proxy Anchor Loss enhances the performance of classification tasks by capturing both local and global contexts.",
            "- None",
            "- The research effectively addresses a practical challenge in the field - the imbalance of template distributions in datasets. The approach demonstrates measurable improvements on the USPTO full dataset, offering a practical solution to reduce intra-class variance. The methodology shows progression in handling imbalanced data, with a clear focus on improving classification accuracy through hierarchical structuring."
        ],
        "weaknesses": [
            "- The retrosynthesis accuracy is relatively low compared to state-of-the-art models.\nThe model is restricted to template-based approaches, which hinders its applicability and scalability across a broader spectrum of chemical environments.",
            "- 1. Retrosynthesis accuracy is low: Compared with state-of-the-art models such as LocalRetro, the retrosynthesis accuracy is low, performing poorly across multiple datasets.\n2. Lacks comparative experiments: There is a lack of comparative experiments to demonstrate the improvement achieved by the applied method, making it difficult to see the advantages of this approach.\n3. Limited diversity and practicality: The model is restricted to template-based methods and cannot be extended to a broader chemical space.\n4. Inconsistent writing format: The manuscript's format presents issues,  such as the abstract, which lacks clear segmentation into distinct sections. This impacts the clarity and structured presentation of key information, making it challenging for readers to discern the main contributions and findings.",
            "- Despite its contributions, the paper suffers from several limitations. The methodology presentation lacks clarity and precision, requiring interpretation to understand the core concepts. While improvements are shown, the achieved 50% accuracy falls behind the current state-of-the-art performance. The paper also omits comparisons with recent superior methods, raising questions about its relative contribution to the field. Additionally, the overall writing style lacks precision and clarity, making it challenging to fully grasp the technical details without considerable effort."
        ]
    },
    "nTZOIlf8YH": {
        "venue": "ICLR 2025",
        "title": "Differentiation of Multi-objective Data-driven Decision Pipeline",
        "link": "https://openreview.net/forum?id=nTZOIlf8YH",
        "abstract": "Real-world scenarios frequently involve multi-objective data-driven optimization problems, characterized by unknown problem coefficients and multiple conflicting objectives. Traditional two-stage methods independently apply a machine learning model to estimate problem coefficients, followed by invoking a solver to tackle the predicted optimization problem. The independent use of optimization solvers and prediction models may lead to suboptimal performance due to mismatches between their objectives. Recent efforts have focused on end-to-end training of predictive models that use decision loss derived from the downstream optimization problem. However, these methods have primarily focused on single-objective optimization problems, thus limiting their applicability. We aim to propose a multiobjective decision-focused approach to address this gap. In order to better align with the inherent properties of multi-objective optimization problems, we propose a set of novel loss functions. These loss functions are designed to capture the discrepancies between predicted and true decision problems, considering solution space, objective space, and decision quality, named landscape loss, Pareto set loss, and decision loss, respectively. Our experimental results demonstrate that our proposed method significantly outperforms traditional two-stage methods and most current decision-focused methods.",
        "decision": "Reject",
        "review scores": [
            3,
            1,
            3
        ],
        "strengths": [
            "- They consider some loss functions to capture the discrepancies between predicted and true decision problems.\nThey explore a particular case of web advertisement allocation within the Anonymous App, aiming to optimize overall click metrics and enhance user visitation on the following day.",
            "- --",
            "- This paper investigates DFL within MOP, presenting a novel problem to solve. The proposed method achieves state-of-the-art performance across multiple benchmark datasets and metrics."
        ],
        "weaknesses": [
            "- 1.This work proposes empirical loss functions for multi-objective decision problems without providing theoretical guarantees, and its novelty is questionable. \n2.The comparison methods used in this study are outdated and do not represent the current state-of-the-art solutions for this problem. \n3.The experimental validation is limited, with few datasets and relatively basic experiments, making it difficult to substantiate the method's effectiveness. \n4.The related work section only covers literature prior to 2022, lacking analysis and comparison with current research developments.",
            "- Your paper's main text exceeds the conference requirements, extending to page 11, which is one page over the maximum limit of 10 pages. According to the link https://iclr.cc/Conferences/2025/CallForPapers, it states \"New this year, the main text must be between 6 and 10 pages (inclusive). This limit will be strictly enforced. Papers with main text on the 11th page will be desk rejected. The page limit applies to both the initial and final camera ready version.\" By reading your paper, we can get a general idea of \u200b\u200bits structure. So, maybe consider improving lines 76 to 82 to make them more concise.",
            "- 1. The last paragraph of the paper exceeds the page limit. According to the policy, it should be desk rejected.\n2. The paper is difficult to follow. The problem description lacks clarity, and the technical challenges in MOP are not well articulated, making the motivation for the proposed methods unclear. This issue is evident in all three loss components.\n3. The technical contribution of the paper is not sufficiently defined. While a full page is dedicated to describing sRMMD, the derivations are largely taken from the source. The second and third loss functions are relatively straightforward, making their novelty hard to assess. This concern is underscored by ablation results indicating that the majority of performance improvement derives from the sRMMD loss."
        ]
    }
}